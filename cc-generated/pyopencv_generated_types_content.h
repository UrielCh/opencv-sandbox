//================================================================================
// AKAZE (Generic)
//================================================================================

// GetSet (AKAZE)



// Methods (AKAZE)

static Napi::Value pyopencv_cv_AKAZE_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_descriptor_type = NULL;
    AKAZE_DescriptorType descriptor_type=AKAZE::DESCRIPTOR_MLDB;
    Napi::Value* pyobj_descriptor_size = NULL;
    int descriptor_size=0;
    Napi::Value* pyobj_descriptor_channels = NULL;
    int descriptor_channels=3;
    Napi::Value* pyobj_threshold = NULL;
    float threshold=0.001f;
    Napi::Value* pyobj_nOctaves = NULL;
    int nOctaves=4;
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=4;
    Napi::Value* pyobj_diffusivity = NULL;
    KAZE_DiffusivityType diffusivity=KAZE::DIFF_PM_G2;
    Ptr<AKAZE> retval;

    const char* keywords[] = { "descriptor_type", "descriptor_size", "descriptor_channels", "threshold", "nOctaves", "nOctaveLayers", "diffusivity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOO:AKAZE.create", (char**)keywords, &pyobj_descriptor_type, &pyobj_descriptor_size, &pyobj_descriptor_channels, &pyobj_threshold, &pyobj_nOctaves, &pyobj_nOctaveLayers, &pyobj_diffusivity) &&
        jsopencv_to_safe(info, pyobj_descriptor_type, descriptor_type, ArgInfo("descriptor_type", 0)) &&
        jsopencv_to_safe(info, pyobj_descriptor_size, descriptor_size, ArgInfo("descriptor_size", 0)) &&
        jsopencv_to_safe(info, pyobj_descriptor_channels, descriptor_channels, ArgInfo("descriptor_channels", 0)) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaves, nOctaves, ArgInfo("nOctaves", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)) &&
        jsopencv_to_safe(info, pyobj_diffusivity, diffusivity, ArgInfo("diffusivity", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::AKAZE::create(descriptor_type, descriptor_size, descriptor_channels, threshold, nOctaves, nOctaveLayers, diffusivity));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getDescriptorChannels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDescriptorChannels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getDescriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDescriptorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getDescriptorType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    AKAZE::DescriptorType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDescriptorType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getDiffusivity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    KAZE::DiffusivityType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDiffusivity());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaveLayers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaves());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setDescriptorChannels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_dch = NULL;
    int dch=0;

    const char* keywords[] = { "dch", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setDescriptorChannels", (char**)keywords, &pyobj_dch) &&
        jsopencv_to_safe(info, pyobj_dch, dch, ArgInfo("dch", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDescriptorChannels(dch));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setDescriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_dsize = NULL;
    int dsize=0;

    const char* keywords[] = { "dsize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setDescriptorSize", (char**)keywords, &pyobj_dsize) &&
        jsopencv_to_safe(info, pyobj_dsize, dsize, ArgInfo("dsize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDescriptorSize(dsize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setDescriptorType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_dtype = NULL;
    AKAZE_DescriptorType dtype=static_cast<AKAZE_DescriptorType>(0);

    const char* keywords[] = { "dtype", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setDescriptorType", (char**)keywords, &pyobj_dtype) &&
        jsopencv_to_safe(info, pyobj_dtype, dtype, ArgInfo("dtype", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDescriptorType(dtype));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setDiffusivity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_diff = NULL;
    KAZE_DiffusivityType diff=static_cast<KAZE_DiffusivityType>(0);

    const char* keywords[] = { "diff", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setDiffusivity", (char**)keywords, &pyobj_diff) &&
        jsopencv_to_safe(info, pyobj_diff, diff, ArgInfo("diff", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDiffusivity(diff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_octaveLayers = NULL;
    int octaveLayers=0;

    const char* keywords[] = { "octaveLayers", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setNOctaveLayers", (char**)keywords, &pyobj_octaveLayers) &&
        jsopencv_to_safe(info, pyobj_octaveLayers, octaveLayers, ArgInfo("octaveLayers", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaveLayers(octaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_octaves = NULL;
    int octaves=0;

    const char* keywords[] = { "octaves", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setNOctaves", (char**)keywords, &pyobj_octaves) &&
        jsopencv_to_safe(info, pyobj_octaves, octaves, ArgInfo("octaves", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaves(octaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AKAZE_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AKAZE> * self1 = 0;
    if (!pyopencv_AKAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    Ptr<cv::AKAZE> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AKAZE.setThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (AKAZE)

static PyGetSetDef pyopencv_AKAZE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_AKAZE_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_create_static, METH_STATIC), "create([, descriptor_type[, descriptor_size[, descriptor_channels[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]]) -> retval\n.   @brief The AKAZE constructor\n.   \n.       @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,\n.       DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.\n.       @param descriptor_size Size of the descriptor in bits. 0 -\\> Full size\n.       @param descriptor_channels Number of channels in the descriptor (1, 2, 3)\n.       @param threshold Detector response threshold to accept point\n.       @param nOctaves Maximum octave evolution of the image\n.       @param nOctaveLayers Default number of sublevels per scale level\n.       @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n.       DIFF_CHARBONNIER"},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDescriptorChannels", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getDescriptorChannels, 0), "getDescriptorChannels() -> retval\n."},
    {"getDescriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getDescriptorSize, 0), "getDescriptorSize() -> retval\n."},
    {"getDescriptorType", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getDescriptorType, 0), "getDescriptorType() -> retval\n."},
    {"getDiffusivity", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getDiffusivity, 0), "getDiffusivity() -> retval\n."},
    {"getNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_getThreshold, 0), "getThreshold() -> retval\n."},
    {"setDescriptorChannels", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setDescriptorChannels, 0), "setDescriptorChannels(dch) -> None\n."},
    {"setDescriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setDescriptorSize, 0), "setDescriptorSize(dsize) -> None\n."},
    {"setDescriptorType", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setDescriptorType, 0), "setDescriptorType(dtype) -> None\n."},
    {"setDiffusivity", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setDiffusivity, 0), "setDiffusivity(diff) -> None\n."},
    {"setNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setNOctaveLayers, 0), "setNOctaveLayers(octaveLayers) -> None\n."},
    {"setNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setNOctaves, 0), "setNOctaves(octaves) -> None\n."},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_AKAZE_setThreshold, 0), "setThreshold(threshold) -> None\n."},

    {NULL,          NULL}
};

// Converter (AKAZE)

template<>
struct PyOpenCV_Converter< Ptr<cv::AKAZE> >
{
    static PyObject* from(const Ptr<cv::AKAZE>& r)
    {
        return pyopencv_AKAZE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::AKAZE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::AKAZE> * dst_;
        if (pyopencv_AKAZE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::AKAZE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// AffineFeature (Generic)
//================================================================================

// GetSet (AffineFeature)



// Methods (AffineFeature)

static Napi::Value pyopencv_cv_AffineFeature_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_backend = NULL;
    Ptr<Feature2D> backend;
    Napi::Value* pyobj_maxTilt = NULL;
    int maxTilt=5;
    Napi::Value* pyobj_minTilt = NULL;
    int minTilt=0;
    Napi::Value* pyobj_tiltStep = NULL;
    float tiltStep=1.4142135623730951f;
    Napi::Value* pyobj_rotateStepBase = NULL;
    float rotateStepBase=72;
    Ptr<AffineFeature> retval;

    const char* keywords[] = { "backend", "maxTilt", "minTilt", "tiltStep", "rotateStepBase", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:AffineFeature.create", (char**)keywords, &pyobj_backend, &pyobj_maxTilt, &pyobj_minTilt, &pyobj_tiltStep, &pyobj_rotateStepBase) &&
        jsopencv_to_safe(info, pyobj_backend, backend, ArgInfo("backend", 0)) &&
        jsopencv_to_safe(info, pyobj_maxTilt, maxTilt, ArgInfo("maxTilt", 0)) &&
        jsopencv_to_safe(info, pyobj_minTilt, minTilt, ArgInfo("minTilt", 0)) &&
        jsopencv_to_safe(info, pyobj_tiltStep, tiltStep, ArgInfo("tiltStep", 0)) &&
        jsopencv_to_safe(info, pyobj_rotateStepBase, rotateStepBase, ArgInfo("rotateStepBase", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::AffineFeature::create(backend, maxTilt, minTilt, tiltStep, rotateStepBase));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AffineFeature_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AffineFeature> * self1 = 0;
    if (!pyopencv_AffineFeature_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AffineFeature' or its derivative)");
    Ptr<cv::AffineFeature> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AffineFeature_getViewParams(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AffineFeature> * self1 = 0;
    if (!pyopencv_AffineFeature_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AffineFeature' or its derivative)");
    Ptr<cv::AffineFeature> _self_ = *(self1);
    Napi::Value* pyobj_tilts = NULL;
    vector_float tilts;
    Napi::Value* pyobj_rolls = NULL;
    vector_float rolls;

    const char* keywords[] = { "tilts", "rolls", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:AffineFeature.getViewParams", (char**)keywords, &pyobj_tilts, &pyobj_rolls) &&
        jsopencv_to_safe(info, pyobj_tilts, tilts, ArgInfo("tilts", 0)) &&
        jsopencv_to_safe(info, pyobj_rolls, rolls, ArgInfo("rolls", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getViewParams(tilts, rolls));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AffineFeature_setViewParams(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AffineFeature> * self1 = 0;
    if (!pyopencv_AffineFeature_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AffineFeature' or its derivative)");
    Ptr<cv::AffineFeature> _self_ = *(self1);
    Napi::Value* pyobj_tilts = NULL;
    vector_float tilts;
    Napi::Value* pyobj_rolls = NULL;
    vector_float rolls;

    const char* keywords[] = { "tilts", "rolls", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:AffineFeature.setViewParams", (char**)keywords, &pyobj_tilts, &pyobj_rolls) &&
        jsopencv_to_safe(info, pyobj_tilts, tilts, ArgInfo("tilts", 0)) &&
        jsopencv_to_safe(info, pyobj_rolls, rolls, ArgInfo("rolls", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setViewParams(tilts, rolls));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (AffineFeature)

static PyGetSetDef pyopencv_AffineFeature_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_AffineFeature_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_AffineFeature_create_static, METH_STATIC), "create(backend[, maxTilt[, minTilt[, tiltStep[, rotateStepBase]]]]) -> retval\n.   @param backend The detector/extractor you want to use as backend.\n.       @param maxTilt The highest power index of tilt factor. 5 is used in the paper as tilt sampling range n.\n.       @param minTilt The lowest power index of tilt factor. 0 is used in the paper.\n.       @param tiltStep Tilt sampling step \\f$\\delta_t\\f$ in Algorithm 1 in the paper.\n.       @param rotateStepBase Rotation sampling step factor b in Algorithm 1 in the paper."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_AffineFeature_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getViewParams", CV_JS_FN_WITH_KW_(pyopencv_cv_AffineFeature_getViewParams, 0), "getViewParams(tilts, rolls) -> None\n."},
    {"setViewParams", CV_JS_FN_WITH_KW_(pyopencv_cv_AffineFeature_setViewParams, 0), "setViewParams(tilts, rolls) -> None\n."},

    {NULL,          NULL}
};

// Converter (AffineFeature)

template<>
struct PyOpenCV_Converter< Ptr<cv::AffineFeature> >
{
    static PyObject* from(const Ptr<cv::AffineFeature>& r)
    {
        return pyopencv_AffineFeature_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::AffineFeature>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::AffineFeature> * dst_;
        if (pyopencv_AffineFeature_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::AffineFeature> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// AgastFeatureDetector (Generic)
//================================================================================

// GetSet (AgastFeatureDetector)



// Methods (AgastFeatureDetector)

static Napi::Value pyopencv_cv_AgastFeatureDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_threshold = NULL;
    int threshold=10;
    Napi::Value* pyobj_nonmaxSuppression = NULL;
    bool nonmaxSuppression=true;
    Napi::Value* pyobj_type = NULL;
    AgastFeatureDetector_DetectorType type=AgastFeatureDetector::OAST_9_16;
    Ptr<AgastFeatureDetector> retval;

    const char* keywords[] = { "threshold", "nonmaxSuppression", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:AgastFeatureDetector.create", (char**)keywords, &pyobj_threshold, &pyobj_nonmaxSuppression, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nonmaxSuppression, nonmaxSuppression, ArgInfo("nonmaxSuppression", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::AgastFeatureDetector::create(threshold, nonmaxSuppression, type));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_getNonmaxSuppression(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNonmaxSuppression());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_getType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    AgastFeatureDetector::DetectorType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_setNonmaxSuppression(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_f = NULL;
    bool f=0;

    const char* keywords[] = { "f", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AgastFeatureDetector.setNonmaxSuppression", (char**)keywords, &pyobj_f) &&
        jsopencv_to_safe(info, pyobj_f, f, ArgInfo("f", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNonmaxSuppression(f));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    int threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AgastFeatureDetector.setThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AgastFeatureDetector_setType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AgastFeatureDetector> * self1 = 0;
    if (!pyopencv_AgastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    Ptr<cv::AgastFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_type = NULL;
    AgastFeatureDetector_DetectorType type=static_cast<AgastFeatureDetector_DetectorType>(0);

    const char* keywords[] = { "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AgastFeatureDetector.setType", (char**)keywords, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setType(type));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (AgastFeatureDetector)

static PyGetSetDef pyopencv_AgastFeatureDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_AgastFeatureDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_create_static, METH_STATIC), "create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getNonmaxSuppression", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getNonmaxSuppression, 0), "getNonmaxSuppression() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getThreshold, 0), "getThreshold() -> retval\n."},
    {"getType", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getType, 0), "getType() -> retval\n."},
    {"setNonmaxSuppression", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_setNonmaxSuppression, 0), "setNonmaxSuppression(f) -> None\n."},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_setThreshold, 0), "setThreshold(threshold) -> None\n."},
    {"setType", CV_JS_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_setType, 0), "setType(type) -> None\n."},

    {NULL,          NULL}
};

// Converter (AgastFeatureDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::AgastFeatureDetector> >
{
    static PyObject* from(const Ptr<cv::AgastFeatureDetector>& r)
    {
        return pyopencv_AgastFeatureDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::AgastFeatureDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::AgastFeatureDetector> * dst_;
        if (pyopencv_AgastFeatureDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::AgastFeatureDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// Algorithm (Generic)
//================================================================================

// GetSet (Algorithm)



// Methods (Algorithm)

static Napi::Value pyopencv_cv_Algorithm_clear(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Algorithm> * self1 = 0;
    if (!pyopencv_Algorithm_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    Ptr<cv::Algorithm> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Algorithm_empty(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Algorithm> * self1 = 0;
    if (!pyopencv_Algorithm_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    Ptr<cv::Algorithm> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Algorithm_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Algorithm> * self1 = 0;
    if (!pyopencv_Algorithm_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    Ptr<cv::Algorithm> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Algorithm_read(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Algorithm> * self1 = 0;
    if (!pyopencv_Algorithm_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    Ptr<cv::Algorithm> _self_ = *(self1);
    Napi::Value* pyobj_fn = NULL;
    cv::FileNode fn;

    const char* keywords[] = { "fn", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Algorithm.read", (char**)keywords, &pyobj_fn) &&
        jsopencv_to_safe(info, pyobj_fn, fn, ArgInfo("fn", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(fn));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Algorithm_save(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Algorithm> * self1 = 0;
    if (!pyopencv_Algorithm_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    Ptr<cv::Algorithm> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Algorithm.save", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->save(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Algorithm_write(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Algorithm> * self1 = 0;
    if (!pyopencv_Algorithm_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    Ptr<cv::Algorithm> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;

    const char* keywords[] = { "fs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Algorithm.write", (char**)keywords, &pyobj_fs) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(*fs));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:Algorithm.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(*fs, name));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("write");

    return NULL;
}



// Tables (Algorithm)

static PyGetSetDef pyopencv_Algorithm_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_Algorithm_methods[] =
{
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_Algorithm_clear, 0), "clear() -> None\n.   @brief Clears the algorithm state"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_Algorithm_empty, 0), "empty() -> retval\n.   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read"},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_Algorithm_getDefaultName, 0), "getDefaultName() -> retval\n.   Returns the algorithm string identifier.\n.   This string is used as top level xml/yml node tag when the object is saved to a file or string."},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_Algorithm_read, 0), "read(fn) -> None\n.   @brief Reads algorithm parameters from a file storage"},
    {"save", CV_JS_FN_WITH_KW_(pyopencv_cv_Algorithm_save, 0), "save(filename) -> None\n.   Saves the algorithm to a file.\n.   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs)."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_Algorithm_write, 0), "write(fs) -> None\n.   @brief Stores algorithm parameters in a file storage\n\n\n\nwrite(fs, name) -> None\n.   * @overload"},

    {NULL,          NULL}
};

// Converter (Algorithm)

template<>
struct PyOpenCV_Converter< Ptr<cv::Algorithm> >
{
    static PyObject* from(const Ptr<cv::Algorithm>& r)
    {
        return pyopencv_Algorithm_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::Algorithm>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::Algorithm> * dst_;
        if (pyopencv_Algorithm_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::Algorithm> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// AlignExposures (Generic)
//================================================================================

// GetSet (AlignExposures)



// Methods (AlignExposures)

static Napi::Value pyopencv_cv_AlignExposures_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignExposures> * self1 = 0;
    if (!pyopencv_AlignExposures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignExposures' or its derivative)");
    Ptr<cv::AlignExposures> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    vector_Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;
    Napi::Value* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:AlignExposures.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    vector_Mat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;
    Napi::Value* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:AlignExposures.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}



// Tables (AlignExposures)

static PyGetSetDef pyopencv_AlignExposures_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_AlignExposures_methods[] =
{
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignExposures_process, 0), "process(src, dst, times, response) -> None\n.   @brief Aligns images\n.   \n.       @param src vector of input images\n.       @param dst vector of aligned images\n.       @param times vector of exposure time values for each image\n.       @param response 256x1 matrix with inverse camera response function for each pixel value, it should\n.       have the same number of channels as images."},

    {NULL,          NULL}
};

// Converter (AlignExposures)

template<>
struct PyOpenCV_Converter< Ptr<cv::AlignExposures> >
{
    static PyObject* from(const Ptr<cv::AlignExposures>& r)
    {
        return pyopencv_AlignExposures_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::AlignExposures>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::AlignExposures> * dst_;
        if (pyopencv_AlignExposures_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::AlignExposures> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// AlignMTB (Generic)
//================================================================================

// GetSet (AlignMTB)



// Methods (AlignMTB)

static Napi::Value pyopencv_cv_AlignMTB_calculateShift(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img0 = NULL;
    Mat img0;
    Napi::Value* pyobj_img1 = NULL;
    Mat img1;
    Point retval;

    const char* keywords[] = { "img0", "img1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:AlignMTB.calculateShift", (char**)keywords, &pyobj_img0, &pyobj_img1) &&
        jsopencv_to_safe(info, pyobj_img0, img0, ArgInfo("img0", 0)) &&
        jsopencv_to_safe(info, pyobj_img1, img1, ArgInfo("img1", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->calculateShift(img0, img1));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img0 = NULL;
    UMat img0;
    Napi::Value* pyobj_img1 = NULL;
    UMat img1;
    Point retval;

    const char* keywords[] = { "img0", "img1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:AlignMTB.calculateShift", (char**)keywords, &pyobj_img0, &pyobj_img1) &&
        jsopencv_to_safe(info, pyobj_img0, img0, ArgInfo("img0", 0)) &&
        jsopencv_to_safe(info, pyobj_img1, img1, ArgInfo("img1", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->calculateShift(img0, img1));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("calculateShift");

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_computeBitmaps(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_tb = NULL;
    Mat tb;
    Napi::Value* pyobj_eb = NULL;
    Mat eb;

    const char* keywords[] = { "img", "tb", "eb", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:AlignMTB.computeBitmaps", (char**)keywords, &pyobj_img, &pyobj_tb, &pyobj_eb) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_tb, tb, ArgInfo("tb", 1)) &&
        jsopencv_to_safe(info, pyobj_eb, eb, ArgInfo("eb", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->computeBitmaps(img, tb, eb));
        return Py_BuildValue("(NN)", jsopencv_from(tb), jsopencv_from(eb));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_tb = NULL;
    UMat tb;
    Napi::Value* pyobj_eb = NULL;
    UMat eb;

    const char* keywords[] = { "img", "tb", "eb", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:AlignMTB.computeBitmaps", (char**)keywords, &pyobj_img, &pyobj_tb, &pyobj_eb) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_tb, tb, ArgInfo("tb", 1)) &&
        jsopencv_to_safe(info, pyobj_eb, eb, ArgInfo("eb", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->computeBitmaps(img, tb, eb));
        return Py_BuildValue("(NN)", jsopencv_from(tb), jsopencv_from(eb));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("computeBitmaps");

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_getCut(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCut());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_getExcludeRange(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getExcludeRange());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_getMaxBits(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxBits());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    vector_Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;
    Napi::Value* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    vector_Mat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;
    Napi::Value* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    vector_Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    vector_Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_setCut(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    bool value=0;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AlignMTB.setCut", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCut(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_setExcludeRange(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    Napi::Value* pyobj_exclude_range = NULL;
    int exclude_range=0;

    const char* keywords[] = { "exclude_range", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AlignMTB.setExcludeRange", (char**)keywords, &pyobj_exclude_range) &&
        jsopencv_to_safe(info, pyobj_exclude_range, exclude_range, ArgInfo("exclude_range", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setExcludeRange(exclude_range));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_setMaxBits(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    Napi::Value* pyobj_max_bits = NULL;
    int max_bits=0;

    const char* keywords[] = { "max_bits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AlignMTB.setMaxBits", (char**)keywords, &pyobj_max_bits) &&
        jsopencv_to_safe(info, pyobj_max_bits, max_bits, ArgInfo("max_bits", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxBits(max_bits));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AlignMTB_shiftMat(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AlignMTB> * self1 = 0;
    if (!pyopencv_AlignMTB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    Ptr<cv::AlignMTB> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_shift = NULL;
    Point shift;

    const char* keywords[] = { "src", "shift", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:AlignMTB.shiftMat", (char**)keywords, &pyobj_src, &pyobj_shift, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_shift, shift, ArgInfo("shift", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->shiftMat(src, dst, shift));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_shift = NULL;
    Point shift;

    const char* keywords[] = { "src", "shift", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:AlignMTB.shiftMat", (char**)keywords, &pyobj_src, &pyobj_shift, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_shift, shift, ArgInfo("shift", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->shiftMat(src, dst, shift));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("shiftMat");

    return NULL;
}



// Tables (AlignMTB)

static PyGetSetDef pyopencv_AlignMTB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_AlignMTB_methods[] =
{
    {"calculateShift", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_calculateShift, 0), "calculateShift(img0, img1) -> retval\n.   @brief Calculates shift between two images, i. e. how to shift the second image to correspond it with the\n.       first.\n.   \n.       @param img0 first image\n.       @param img1 second image"},
    {"computeBitmaps", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_computeBitmaps, 0), "computeBitmaps(img[, tb[, eb]]) -> tb, eb\n.   @brief Computes median threshold and exclude bitmaps of given image.\n.   \n.       @param img input image\n.       @param tb median threshold bitmap\n.       @param eb exclude bitmap"},
    {"getCut", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_getCut, 0), "getCut() -> retval\n."},
    {"getExcludeRange", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_getExcludeRange, 0), "getExcludeRange() -> retval\n."},
    {"getMaxBits", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_getMaxBits, 0), "getMaxBits() -> retval\n."},
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_process, 0), "process(src, dst, times, response) -> None\n.   \n\n\n\nprocess(src, dst) -> None\n.   @brief Short version of process, that doesn't take extra arguments.\n.   \n.       @param src vector of input images\n.       @param dst vector of aligned images"},
    {"setCut", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_setCut, 0), "setCut(value) -> None\n."},
    {"setExcludeRange", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_setExcludeRange, 0), "setExcludeRange(exclude_range) -> None\n."},
    {"setMaxBits", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_setMaxBits, 0), "setMaxBits(max_bits) -> None\n."},
    {"shiftMat", CV_JS_FN_WITH_KW_(pyopencv_cv_AlignMTB_shiftMat, 0), "shiftMat(src, shift[, dst]) -> dst\n.   @brief Helper function, that shift Mat filling new regions with zeros.\n.   \n.       @param src input image\n.       @param dst result image\n.       @param shift shift value"},

    {NULL,          NULL}
};

// Converter (AlignMTB)

template<>
struct PyOpenCV_Converter< Ptr<cv::AlignMTB> >
{
    static PyObject* from(const Ptr<cv::AlignMTB>& r)
    {
        return pyopencv_AlignMTB_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::AlignMTB>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::AlignMTB> * dst_;
        if (pyopencv_AlignMTB_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::AlignMTB> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// AsyncArray (Generic)
//================================================================================

// GetSet (AsyncArray)



// Methods (AsyncArray)

static int pyopencv_cv_AsyncArray_AsyncArray(pyopencv_AsyncArray_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::AsyncArray>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::AsyncArray()));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_AsyncArray_get(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AsyncArray> * self1 = 0;
    if (!pyopencv_AsyncArray_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AsyncArray' or its derivative)");
    Ptr<cv::AsyncArray> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:AsyncArray.get", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->get(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:AsyncArray.get", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->get(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_timeoutNs = NULL;
    double timeoutNs=0;
    bool retval;

    const char* keywords[] = { "timeoutNs", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:AsyncArray.get", (char**)keywords, &pyobj_timeoutNs, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_timeoutNs, timeoutNs, ArgInfo("timeoutNs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get(dst, timeoutNs));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(dst));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_timeoutNs = NULL;
    double timeoutNs=0;
    bool retval;

    const char* keywords[] = { "timeoutNs", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:AsyncArray.get", (char**)keywords, &pyobj_timeoutNs, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_timeoutNs, timeoutNs, ArgInfo("timeoutNs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get(dst, timeoutNs));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(dst));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("get");

    return NULL;
}

static Napi::Value pyopencv_cv_AsyncArray_release(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AsyncArray> * self1 = 0;
    if (!pyopencv_AsyncArray_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AsyncArray' or its derivative)");
    Ptr<cv::AsyncArray> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AsyncArray_valid(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AsyncArray> * self1 = 0;
    if (!pyopencv_AsyncArray_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AsyncArray' or its derivative)");
    Ptr<cv::AsyncArray> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->valid());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_AsyncArray_wait_for(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::AsyncArray> * self1 = 0;
    if (!pyopencv_AsyncArray_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'AsyncArray' or its derivative)");
    Ptr<cv::AsyncArray> _self_ = *(self1);
    Napi::Value* pyobj_timeoutNs = NULL;
    double timeoutNs=0;
    bool retval;

    const char* keywords[] = { "timeoutNs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:AsyncArray.wait_for", (char**)keywords, &pyobj_timeoutNs) &&
        jsopencv_to_safe(info, pyobj_timeoutNs, timeoutNs, ArgInfo("timeoutNs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->wait_for(timeoutNs));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (AsyncArray)

static PyGetSetDef pyopencv_AsyncArray_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_AsyncArray_methods[] =
{
    {"get", CV_JS_FN_WITH_KW_(pyopencv_cv_AsyncArray_get, 0), "get([, dst]) -> dst\n.   Fetch the result.\n.       @param[out] dst destination array\n.   \n.       Waits for result until container has valid result.\n.       Throws exception if exception was stored as a result.\n.   \n.       Throws exception on invalid container state.\n.   \n.       @note Result or stored exception can be fetched only once.\n\n\n\nget(timeoutNs[, dst]) -> retval, dst\n.   Retrieving the result with timeout\n.       @param[out] dst destination array\n.       @param[in] timeoutNs timeout in nanoseconds, -1 for infinite wait\n.   \n.       @returns true if result is ready, false if the timeout has expired\n.   \n.       @note Result or stored exception can be fetched only once."},
    {"release", CV_JS_FN_WITH_KW_(pyopencv_cv_AsyncArray_release, 0), "release() -> None\n."},
    {"valid", CV_JS_FN_WITH_KW_(pyopencv_cv_AsyncArray_valid, 0), "valid() -> retval\n."},
    {"wait_for", CV_JS_FN_WITH_KW_(pyopencv_cv_AsyncArray_wait_for, 0), "wait_for(timeoutNs) -> retval\n."},

    {NULL,          NULL}
};

// Converter (AsyncArray)

template<>
struct PyOpenCV_Converter< Ptr<cv::AsyncArray> >
{
    static PyObject* from(const Ptr<cv::AsyncArray>& r)
    {
        return pyopencv_AsyncArray_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::AsyncArray>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::AsyncArray> * dst_;
        if (pyopencv_AsyncArray_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::AsyncArray> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BFMatcher (Generic)
//================================================================================

// GetSet (BFMatcher)



// Methods (BFMatcher)

static int pyopencv_cv_BFMatcher_BFMatcher(pyopencv_BFMatcher_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    Napi::Value* pyobj_normType = NULL;
    int normType=NORM_L2;
    Napi::Value* pyobj_crossCheck = NULL;
    bool crossCheck=false;

    const char* keywords[] = { "normType", "crossCheck", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:BFMatcher", (char**)keywords, &pyobj_normType, &pyobj_crossCheck) &&
        jsopencv_to_safe(info, pyobj_normType, normType, ArgInfo("normType", 0)) &&
        jsopencv_to_safe(info, pyobj_crossCheck, crossCheck, ArgInfo("crossCheck", 0)))
    {
        new (&(self->v)) Ptr<cv::BFMatcher>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::BFMatcher(normType, crossCheck)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_BFMatcher_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_normType = NULL;
    int normType=NORM_L2;
    Napi::Value* pyobj_crossCheck = NULL;
    bool crossCheck=false;
    Ptr<BFMatcher> retval;

    const char* keywords[] = { "normType", "crossCheck", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:BFMatcher.create", (char**)keywords, &pyobj_normType, &pyobj_crossCheck) &&
        jsopencv_to_safe(info, pyobj_normType, normType, ArgInfo("normType", 0)) &&
        jsopencv_to_safe(info, pyobj_crossCheck, crossCheck, ArgInfo("crossCheck", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::BFMatcher::create(normType, crossCheck));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (BFMatcher)

static PyGetSetDef pyopencv_BFMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BFMatcher_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_BFMatcher_create_static, METH_STATIC), "create([, normType[, crossCheck]]) -> retval\n.   @brief Brute-force matcher create method.\n.       @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are\n.       preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and\n.       BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor\n.       description).\n.       @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k\n.       nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with\n.       k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the\n.       matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent\n.       pairs. Such technique usually produces best results with minimal number of outliers when there are\n.       enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper."},

    {NULL,          NULL}
};

// Converter (BFMatcher)

template<>
struct PyOpenCV_Converter< Ptr<cv::BFMatcher> >
{
    static PyObject* from(const Ptr<cv::BFMatcher>& r)
    {
        return pyopencv_BFMatcher_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BFMatcher>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BFMatcher> * dst_;
        if (pyopencv_BFMatcher_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BFMatcher> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BOWImgDescriptorExtractor (Generic)
//================================================================================

// GetSet (BOWImgDescriptorExtractor)



// Methods (BOWImgDescriptorExtractor)

static int pyopencv_cv_BOWImgDescriptorExtractor_BOWImgDescriptorExtractor(pyopencv_BOWImgDescriptorExtractor_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    Napi::Value* pyobj_dextractor = NULL;
    Ptr<DescriptorExtractor> dextractor;
    Napi::Value* pyobj_dmatcher = NULL;
    Ptr<DescriptorMatcher> dmatcher;

    const char* keywords[] = { "dextractor", "dmatcher", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:BOWImgDescriptorExtractor", (char**)keywords, &pyobj_dextractor, &pyobj_dmatcher) &&
        jsopencv_to_safe(info, pyobj_dextractor, dextractor, ArgInfo("dextractor", 0)) &&
        jsopencv_to_safe(info, pyobj_dmatcher, dmatcher, ArgInfo("dmatcher", 0)))
    {
        new (&(self->v)) Ptr<cv::BOWImgDescriptorExtractor>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::BOWImgDescriptorExtractor(dextractor, dmatcher)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_BOWImgDescriptorExtractor_compute(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWImgDescriptorExtractor> * self1 = 0;
    if (!pyopencv_BOWImgDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    Ptr<cv::BOWImgDescriptorExtractor> _self_ = *(self1);
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_imgDescriptor = NULL;
    Mat imgDescriptor;

    const char* keywords[] = { "image", "keypoints", "imgDescriptor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:BOWImgDescriptorExtractor.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_imgDescriptor) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 0)) &&
        jsopencv_to_safe(info, pyobj_imgDescriptor, imgDescriptor, ArgInfo("imgDescriptor", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute2(image, keypoints, imgDescriptor));
        return jsopencv_from(imgDescriptor);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWImgDescriptorExtractor_descriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWImgDescriptorExtractor> * self1 = 0;
    if (!pyopencv_BOWImgDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    Ptr<cv::BOWImgDescriptorExtractor> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->descriptorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWImgDescriptorExtractor_descriptorType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWImgDescriptorExtractor> * self1 = 0;
    if (!pyopencv_BOWImgDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    Ptr<cv::BOWImgDescriptorExtractor> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->descriptorType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWImgDescriptorExtractor_getVocabulary(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWImgDescriptorExtractor> * self1 = 0;
    if (!pyopencv_BOWImgDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    Ptr<cv::BOWImgDescriptorExtractor> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVocabulary());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWImgDescriptorExtractor_setVocabulary(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWImgDescriptorExtractor> * self1 = 0;
    if (!pyopencv_BOWImgDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    Ptr<cv::BOWImgDescriptorExtractor> _self_ = *(self1);
    Napi::Value* pyobj_vocabulary = NULL;
    Mat vocabulary;

    const char* keywords[] = { "vocabulary", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BOWImgDescriptorExtractor.setVocabulary", (char**)keywords, &pyobj_vocabulary) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVocabulary(vocabulary));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (BOWImgDescriptorExtractor)

static PyGetSetDef pyopencv_BOWImgDescriptorExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BOWImgDescriptorExtractor_methods[] =
{
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_compute, 0), "compute(image, keypoints[, imgDescriptor]) -> imgDescriptor\n.   @overload\n.       @param keypointDescriptors Computed descriptors to match with vocabulary.\n.       @param imgDescriptor Computed output image descriptor.\n.       @param pointIdxsOfClusters Indices of keypoints that belong to the cluster. This means that\n.       pointIdxsOfClusters[i] are keypoint indices that belong to the i -th cluster (word of vocabulary)\n.       returned if it is non-zero."},
    {"descriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_descriptorSize, 0), "descriptorSize() -> retval\n.   @brief Returns an image descriptor size if the vocabulary is set. Otherwise, it returns 0."},
    {"descriptorType", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_descriptorType, 0), "descriptorType() -> retval\n.   @brief Returns an image descriptor type."},
    {"getVocabulary", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_getVocabulary, 0), "getVocabulary() -> retval\n.   @brief Returns the set vocabulary."},
    {"setVocabulary", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_setVocabulary, 0), "setVocabulary(vocabulary) -> None\n.   @brief Sets a visual vocabulary.\n.   \n.       @param vocabulary Vocabulary (can be trained using the inheritor of BOWTrainer ). Each row of the\n.       vocabulary is a visual word (cluster center)."},

    {NULL,          NULL}
};

// Converter (BOWImgDescriptorExtractor)

template<>
struct PyOpenCV_Converter< Ptr<cv::BOWImgDescriptorExtractor> >
{
    static PyObject* from(const Ptr<cv::BOWImgDescriptorExtractor>& r)
    {
        return pyopencv_BOWImgDescriptorExtractor_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BOWImgDescriptorExtractor>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BOWImgDescriptorExtractor> * dst_;
        if (pyopencv_BOWImgDescriptorExtractor_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BOWImgDescriptorExtractor> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BOWKMeansTrainer (Generic)
//================================================================================

// GetSet (BOWKMeansTrainer)



// Methods (BOWKMeansTrainer)

static int pyopencv_cv_BOWKMeansTrainer_BOWKMeansTrainer(pyopencv_BOWKMeansTrainer_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    Napi::Value* pyobj_clusterCount = NULL;
    int clusterCount=0;
    Napi::Value* pyobj_termcrit = NULL;
    TermCriteria termcrit;
    Napi::Value* pyobj_attempts = NULL;
    int attempts=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=KMEANS_PP_CENTERS;

    const char* keywords[] = { "clusterCount", "termcrit", "attempts", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:BOWKMeansTrainer", (char**)keywords, &pyobj_clusterCount, &pyobj_termcrit, &pyobj_attempts, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_clusterCount, clusterCount, ArgInfo("clusterCount", 0)) &&
        jsopencv_to_safe(info, pyobj_termcrit, termcrit, ArgInfo("termcrit", 0)) &&
        jsopencv_to_safe(info, pyobj_attempts, attempts, ArgInfo("attempts", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        new (&(self->v)) Ptr<cv::BOWKMeansTrainer>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::BOWKMeansTrainer(clusterCount, termcrit, attempts, flags)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_BOWKMeansTrainer_cluster(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWKMeansTrainer> * self1 = 0;
    if (!pyopencv_BOWKMeansTrainer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWKMeansTrainer' or its derivative)");
    Ptr<cv::BOWKMeansTrainer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cluster());
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_descriptors = NULL;
    Mat descriptors;
    Mat retval;

    const char* keywords[] = { "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BOWKMeansTrainer.cluster", (char**)keywords, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cluster(descriptors));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("cluster");

    return NULL;
}



// Tables (BOWKMeansTrainer)

static PyGetSetDef pyopencv_BOWKMeansTrainer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BOWKMeansTrainer_methods[] =
{
    {"cluster", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWKMeansTrainer_cluster, 0), "cluster() -> retval\n.   \n\n\n\ncluster(descriptors) -> retval\n."},

    {NULL,          NULL}
};

// Converter (BOWKMeansTrainer)

template<>
struct PyOpenCV_Converter< Ptr<cv::BOWKMeansTrainer> >
{
    static PyObject* from(const Ptr<cv::BOWKMeansTrainer>& r)
    {
        return pyopencv_BOWKMeansTrainer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BOWKMeansTrainer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BOWKMeansTrainer> * dst_;
        if (pyopencv_BOWKMeansTrainer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BOWKMeansTrainer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BOWTrainer (Generic)
//================================================================================

// GetSet (BOWTrainer)



// Methods (BOWTrainer)

static Napi::Value pyopencv_cv_BOWTrainer_add(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWTrainer> * self1 = 0;
    if (!pyopencv_BOWTrainer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    Ptr<cv::BOWTrainer> _self_ = *(self1);
    Napi::Value* pyobj_descriptors = NULL;
    Mat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BOWTrainer.add", (char**)keywords, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->add(descriptors));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWTrainer_clear(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWTrainer> * self1 = 0;
    if (!pyopencv_BOWTrainer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    Ptr<cv::BOWTrainer> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWTrainer_cluster(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWTrainer> * self1 = 0;
    if (!pyopencv_BOWTrainer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    Ptr<cv::BOWTrainer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cluster());
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_descriptors = NULL;
    Mat descriptors;
    Mat retval;

    const char* keywords[] = { "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BOWTrainer.cluster", (char**)keywords, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cluster(descriptors));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("cluster");

    return NULL;
}

static Napi::Value pyopencv_cv_BOWTrainer_descriptorsCount(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWTrainer> * self1 = 0;
    if (!pyopencv_BOWTrainer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    Ptr<cv::BOWTrainer> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->descriptorsCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BOWTrainer_getDescriptors(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BOWTrainer> * self1 = 0;
    if (!pyopencv_BOWTrainer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    Ptr<cv::BOWTrainer> _self_ = *(self1);
    std::vector<Mat> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDescriptors());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (BOWTrainer)

static PyGetSetDef pyopencv_BOWTrainer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BOWTrainer_methods[] =
{
    {"add", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWTrainer_add, 0), "add(descriptors) -> None\n.   @brief Adds descriptors to a training set.\n.   \n.       @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a\n.       descriptor.\n.   \n.       The training set is clustered using clustermethod to construct the vocabulary."},
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWTrainer_clear, 0), "clear() -> None\n."},
    {"cluster", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWTrainer_cluster, 0), "cluster() -> retval\n.   @overload\n\n\n\ncluster(descriptors) -> retval\n.   @brief Clusters train descriptors.\n.   \n.       @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.\n.       Descriptors are not added to the inner train descriptor set.\n.   \n.       The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first\n.       variant of the method, train descriptors stored in the object are clustered. In the second variant,\n.       input descriptors are clustered."},
    {"descriptorsCount", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWTrainer_descriptorsCount, 0), "descriptorsCount() -> retval\n.   @brief Returns the count of all descriptors stored in the training set."},
    {"getDescriptors", CV_JS_FN_WITH_KW_(pyopencv_cv_BOWTrainer_getDescriptors, 0), "getDescriptors() -> retval\n.   @brief Returns a training set of descriptors."},

    {NULL,          NULL}
};

// Converter (BOWTrainer)

template<>
struct PyOpenCV_Converter< Ptr<cv::BOWTrainer> >
{
    static PyObject* from(const Ptr<cv::BOWTrainer>& r)
    {
        return pyopencv_BOWTrainer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BOWTrainer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BOWTrainer> * dst_;
        if (pyopencv_BOWTrainer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BOWTrainer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BRISK (Generic)
//================================================================================

// GetSet (BRISK)



// Methods (BRISK)

static Napi::Value pyopencv_cv_BRISK_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_thresh = NULL;
    int thresh=30;
    Napi::Value* pyobj_octaves = NULL;
    int octaves=3;
    Napi::Value* pyobj_patternScale = NULL;
    float patternScale=1.0f;
    Ptr<BRISK> retval;

    const char* keywords[] = { "thresh", "octaves", "patternScale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:BRISK.create", (char**)keywords, &pyobj_thresh, &pyobj_octaves, &pyobj_patternScale) &&
        jsopencv_to_safe(info, pyobj_thresh, thresh, ArgInfo("thresh", 0)) &&
        jsopencv_to_safe(info, pyobj_octaves, octaves, ArgInfo("octaves", 0)) &&
        jsopencv_to_safe(info, pyobj_patternScale, patternScale, ArgInfo("patternScale", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::BRISK::create(thresh, octaves, patternScale));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_radiusList = NULL;
    vector_float radiusList;
    Napi::Value* pyobj_numberList = NULL;
    vector_int numberList;
    Napi::Value* pyobj_dMax = NULL;
    float dMax=5.85f;
    Napi::Value* pyobj_dMin = NULL;
    float dMin=8.2f;
    Napi::Value* pyobj_indexChange = NULL;
    vector_int indexChange=std::vector<int>();
    Ptr<BRISK> retval;

    const char* keywords[] = { "radiusList", "numberList", "dMax", "dMin", "indexChange", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:BRISK.create", (char**)keywords, &pyobj_radiusList, &pyobj_numberList, &pyobj_dMax, &pyobj_dMin, &pyobj_indexChange) &&
        jsopencv_to_safe(info, pyobj_radiusList, radiusList, ArgInfo("radiusList", 0)) &&
        jsopencv_to_safe(info, pyobj_numberList, numberList, ArgInfo("numberList", 0)) &&
        jsopencv_to_safe(info, pyobj_dMax, dMax, ArgInfo("dMax", 0)) &&
        jsopencv_to_safe(info, pyobj_dMin, dMin, ArgInfo("dMin", 0)) &&
        jsopencv_to_safe(info, pyobj_indexChange, indexChange, ArgInfo("indexChange", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::BRISK::create(radiusList, numberList, dMax, dMin, indexChange));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_thresh = NULL;
    int thresh=0;
    Napi::Value* pyobj_octaves = NULL;
    int octaves=0;
    Napi::Value* pyobj_radiusList = NULL;
    vector_float radiusList;
    Napi::Value* pyobj_numberList = NULL;
    vector_int numberList;
    Napi::Value* pyobj_dMax = NULL;
    float dMax=5.85f;
    Napi::Value* pyobj_dMin = NULL;
    float dMin=8.2f;
    Napi::Value* pyobj_indexChange = NULL;
    vector_int indexChange=std::vector<int>();
    Ptr<BRISK> retval;

    const char* keywords[] = { "thresh", "octaves", "radiusList", "numberList", "dMax", "dMin", "indexChange", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OOO:BRISK.create", (char**)keywords, &pyobj_thresh, &pyobj_octaves, &pyobj_radiusList, &pyobj_numberList, &pyobj_dMax, &pyobj_dMin, &pyobj_indexChange) &&
        jsopencv_to_safe(info, pyobj_thresh, thresh, ArgInfo("thresh", 0)) &&
        jsopencv_to_safe(info, pyobj_octaves, octaves, ArgInfo("octaves", 0)) &&
        jsopencv_to_safe(info, pyobj_radiusList, radiusList, ArgInfo("radiusList", 0)) &&
        jsopencv_to_safe(info, pyobj_numberList, numberList, ArgInfo("numberList", 0)) &&
        jsopencv_to_safe(info, pyobj_dMax, dMax, ArgInfo("dMax", 0)) &&
        jsopencv_to_safe(info, pyobj_dMin, dMin, ArgInfo("dMin", 0)) &&
        jsopencv_to_safe(info, pyobj_indexChange, indexChange, ArgInfo("indexChange", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::BRISK::create(thresh, octaves, radiusList, numberList, dMax, dMin, indexChange));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_getOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getOctaves());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_getPatternScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPatternScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_setOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    Napi::Value* pyobj_octaves = NULL;
    int octaves=0;

    const char* keywords[] = { "octaves", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BRISK.setOctaves", (char**)keywords, &pyobj_octaves) &&
        jsopencv_to_safe(info, pyobj_octaves, octaves, ArgInfo("octaves", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setOctaves(octaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_setPatternScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    Napi::Value* pyobj_patternScale = NULL;
    float patternScale=0.f;

    const char* keywords[] = { "patternScale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BRISK.setPatternScale", (char**)keywords, &pyobj_patternScale) &&
        jsopencv_to_safe(info, pyobj_patternScale, patternScale, ArgInfo("patternScale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPatternScale(patternScale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BRISK_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BRISK> * self1 = 0;
    if (!pyopencv_BRISK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    Ptr<cv::BRISK> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    int threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BRISK.setThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (BRISK)

static PyGetSetDef pyopencv_BRISK_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BRISK_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_create_static, METH_STATIC), "create([, thresh[, octaves[, patternScale]]]) -> retval\n.   @brief The BRISK constructor\n.   \n.       @param thresh AGAST detection threshold score.\n.       @param octaves detection octaves. Use 0 to do single scale.\n.       @param patternScale apply this scale to the pattern used for sampling the neighbourhood of a\n.       keypoint.\n\n\n\ncreate(radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n.   @brief The BRISK constructor for a custom pattern\n.   \n.       @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n.       keypoint scale 1).\n.       @param numberList defines the number of sampling points on the sampling circle. Must be the same\n.       size as radiusList..\n.       @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n.       scale 1).\n.       @param dMin threshold for the long pairings used for orientation determination (in pixels for\n.       keypoint scale 1).\n.   @param indexChange index remapping of the bits.\n\n\n\ncreate(thresh, octaves, radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n.   @brief The BRISK constructor for a custom pattern, detection threshold and octaves\n.   \n.       @param thresh AGAST detection threshold score.\n.       @param octaves detection octaves. Use 0 to do single scale.\n.       @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n.       keypoint scale 1).\n.       @param numberList defines the number of sampling points on the sampling circle. Must be the same\n.       size as radiusList..\n.       @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n.       scale 1).\n.       @param dMin threshold for the long pairings used for orientation determination (in pixels for\n.       keypoint scale 1).\n.   @param indexChange index remapping of the bits."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_getOctaves, 0), "getOctaves() -> retval\n."},
    {"getPatternScale", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_getPatternScale, 0), "getPatternScale() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_getThreshold, 0), "getThreshold() -> retval\n."},
    {"setOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_setOctaves, 0), "setOctaves(octaves) -> None\n.   @brief Set detection octaves.\n.       @param octaves detection octaves. Use 0 to do single scale."},
    {"setPatternScale", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_setPatternScale, 0), "setPatternScale(patternScale) -> None\n.   @brief Set detection patternScale.\n.       @param patternScale apply this scale to the pattern used for sampling the neighbourhood of a\n.       keypoint."},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BRISK_setThreshold, 0), "setThreshold(threshold) -> None\n.   @brief Set detection threshold.\n.       @param threshold AGAST detection threshold score."},

    {NULL,          NULL}
};

// Converter (BRISK)

template<>
struct PyOpenCV_Converter< Ptr<cv::BRISK> >
{
    static PyObject* from(const Ptr<cv::BRISK>& r)
    {
        return pyopencv_BRISK_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BRISK>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BRISK> * dst_;
        if (pyopencv_BRISK_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BRISK> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BackgroundSubtractor (Generic)
//================================================================================

// GetSet (BackgroundSubtractor)



// Methods (BackgroundSubtractor)

static Napi::Value pyopencv_cv_BackgroundSubtractor_apply(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractor> * self1 = 0;
    if (!pyopencv_BackgroundSubtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractor' or its derivative)");
    Ptr<cv::BackgroundSubtractor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_fgmask = NULL;
    Mat fgmask;
    Napi::Value* pyobj_learningRate = NULL;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:BackgroundSubtractor.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &pyobj_learningRate) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) &&
        jsopencv_to_safe(info, pyobj_learningRate, learningRate, ArgInfo("learningRate", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->apply(image, fgmask, learningRate));
        return jsopencv_from(fgmask);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_fgmask = NULL;
    UMat fgmask;
    Napi::Value* pyobj_learningRate = NULL;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:BackgroundSubtractor.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &pyobj_learningRate) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) &&
        jsopencv_to_safe(info, pyobj_learningRate, learningRate, ArgInfo("learningRate", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->apply(image, fgmask, learningRate));
        return jsopencv_from(fgmask);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("apply");

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractor_getBackgroundImage(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractor> * self1 = 0;
    if (!pyopencv_BackgroundSubtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractor' or its derivative)");
    Ptr<cv::BackgroundSubtractor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_backgroundImage = NULL;
    Mat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:BackgroundSubtractor.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        jsopencv_to_safe(info, pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getBackgroundImage(backgroundImage));
        return jsopencv_from(backgroundImage);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_backgroundImage = NULL;
    UMat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:BackgroundSubtractor.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        jsopencv_to_safe(info, pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getBackgroundImage(backgroundImage));
        return jsopencv_from(backgroundImage);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getBackgroundImage");

    return NULL;
}



// Tables (BackgroundSubtractor)

static PyGetSetDef pyopencv_BackgroundSubtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BackgroundSubtractor_methods[] =
{
    {"apply", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractor_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n.   @brief Computes a foreground mask.\n.   \n.       @param image Next video frame.\n.       @param fgmask The output foreground mask as an 8-bit binary image.\n.       @param learningRate The value between 0 and 1 that indicates how fast the background model is\n.       learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n.       rate. 0 means that the background model is not updated at all, 1 means that the background model\n.       is completely reinitialized from the last frame."},
    {"getBackgroundImage", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractor_getBackgroundImage, 0), "getBackgroundImage([, backgroundImage]) -> backgroundImage\n.   @brief Computes a background image.\n.   \n.       @param backgroundImage The output background image.\n.   \n.       @note Sometimes the background image can be very blurry, as it contain the average background\n.       statistics."},

    {NULL,          NULL}
};

// Converter (BackgroundSubtractor)

template<>
struct PyOpenCV_Converter< Ptr<cv::BackgroundSubtractor> >
{
    static PyObject* from(const Ptr<cv::BackgroundSubtractor>& r)
    {
        return pyopencv_BackgroundSubtractor_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BackgroundSubtractor>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BackgroundSubtractor> * dst_;
        if (pyopencv_BackgroundSubtractor_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BackgroundSubtractor> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BackgroundSubtractorKNN (Generic)
//================================================================================

// GetSet (BackgroundSubtractorKNN)



// Methods (BackgroundSubtractorKNN)

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getDetectShadows(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDetectShadows());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getDist2Threshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDist2Threshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getHistory(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getHistory());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getNSamples(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getShadowThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getShadowThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getShadowValue(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getShadowValue());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_getkNNSamples(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getkNNSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setDetectShadows(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj_detectShadows = NULL;
    bool detectShadows=0;

    const char* keywords[] = { "detectShadows", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setDetectShadows", (char**)keywords, &pyobj_detectShadows) &&
        jsopencv_to_safe(info, pyobj_detectShadows, detectShadows, ArgInfo("detectShadows", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDetectShadows(detectShadows));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setDist2Threshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj__dist2Threshold = NULL;
    double _dist2Threshold=0;

    const char* keywords[] = { "_dist2Threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setDist2Threshold", (char**)keywords, &pyobj__dist2Threshold) &&
        jsopencv_to_safe(info, pyobj__dist2Threshold, _dist2Threshold, ArgInfo("_dist2Threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDist2Threshold(_dist2Threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setHistory(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj_history = NULL;
    int history=0;

    const char* keywords[] = { "history", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setHistory", (char**)keywords, &pyobj_history) &&
        jsopencv_to_safe(info, pyobj_history, history, ArgInfo("history", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setHistory(history));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setNSamples(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj__nN = NULL;
    int _nN=0;

    const char* keywords[] = { "_nN", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setNSamples", (char**)keywords, &pyobj__nN) &&
        jsopencv_to_safe(info, pyobj__nN, _nN, ArgInfo("_nN", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNSamples(_nN));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setShadowThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setShadowThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setShadowThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setShadowValue(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    int value=0;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setShadowValue", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setShadowValue(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorKNN_setkNNSamples(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorKNN> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorKNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    Ptr<cv::BackgroundSubtractorKNN> _self_ = *(self1);
    Napi::Value* pyobj__nkNN = NULL;
    int _nkNN=0;

    const char* keywords[] = { "_nkNN", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorKNN.setkNNSamples", (char**)keywords, &pyobj__nkNN) &&
        jsopencv_to_safe(info, pyobj__nkNN, _nkNN, ArgInfo("_nkNN", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setkNNSamples(_nkNN));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (BackgroundSubtractorKNN)

static PyGetSetDef pyopencv_BackgroundSubtractorKNN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BackgroundSubtractorKNN_methods[] =
{
    {"getDetectShadows", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getDetectShadows, 0), "getDetectShadows() -> retval\n.   @brief Returns the shadow detection flag\n.   \n.       If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorKNN for\n.       details."},
    {"getDist2Threshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getDist2Threshold, 0), "getDist2Threshold() -> retval\n.   @brief Returns the threshold on the squared distance between the pixel and the sample\n.   \n.       The threshold on the squared distance between the pixel and the sample to decide whether a pixel is\n.       close to a data sample."},
    {"getHistory", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getHistory, 0), "getHistory() -> retval\n.   @brief Returns the number of last frames that affect the background model"},
    {"getNSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getNSamples, 0), "getNSamples() -> retval\n.   @brief Returns the number of data samples in the background model"},
    {"getShadowThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getShadowThreshold, 0), "getShadowThreshold() -> retval\n.   @brief Returns the shadow threshold\n.   \n.       A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in\n.       the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel\n.       is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,\n.       *Detecting Moving Shadows...*, IEEE PAMI,2003."},
    {"getShadowValue", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getShadowValue, 0), "getShadowValue() -> retval\n.   @brief Returns the shadow value\n.   \n.       Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0\n.       in the mask always means background, 255 means foreground."},
    {"getkNNSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getkNNSamples, 0), "getkNNSamples() -> retval\n.   @brief Returns the number of neighbours, the k in the kNN.\n.   \n.       K is the number of samples that need to be within dist2Threshold in order to decide that that\n.       pixel is matching the kNN background model."},
    {"setDetectShadows", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setDetectShadows, 0), "setDetectShadows(detectShadows) -> None\n.   @brief Enables or disables shadow detection"},
    {"setDist2Threshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setDist2Threshold, 0), "setDist2Threshold(_dist2Threshold) -> None\n.   @brief Sets the threshold on the squared distance"},
    {"setHistory", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setHistory, 0), "setHistory(history) -> None\n.   @brief Sets the number of last frames that affect the background model"},
    {"setNSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setNSamples, 0), "setNSamples(_nN) -> None\n.   @brief Sets the number of data samples in the background model.\n.   \n.       The model needs to be reinitalized to reserve memory."},
    {"setShadowThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setShadowThreshold, 0), "setShadowThreshold(threshold) -> None\n.   @brief Sets the shadow threshold"},
    {"setShadowValue", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setShadowValue, 0), "setShadowValue(value) -> None\n.   @brief Sets the shadow value"},
    {"setkNNSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setkNNSamples, 0), "setkNNSamples(_nkNN) -> None\n.   @brief Sets the k in the kNN. How many nearest neighbours need to match."},

    {NULL,          NULL}
};

// Converter (BackgroundSubtractorKNN)

template<>
struct PyOpenCV_Converter< Ptr<cv::BackgroundSubtractorKNN> >
{
    static PyObject* from(const Ptr<cv::BackgroundSubtractorKNN>& r)
    {
        return pyopencv_BackgroundSubtractorKNN_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BackgroundSubtractorKNN>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BackgroundSubtractorKNN> * dst_;
        if (pyopencv_BackgroundSubtractorKNN_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BackgroundSubtractorKNN> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BackgroundSubtractorMOG2 (Generic)
//================================================================================

// GetSet (BackgroundSubtractorMOG2)



// Methods (BackgroundSubtractorMOG2)

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_apply(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_fgmask = NULL;
    Mat fgmask;
    Napi::Value* pyobj_learningRate = NULL;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:BackgroundSubtractorMOG2.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &pyobj_learningRate) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) &&
        jsopencv_to_safe(info, pyobj_learningRate, learningRate, ArgInfo("learningRate", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->apply(image, fgmask, learningRate));
        return jsopencv_from(fgmask);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_fgmask = NULL;
    UMat fgmask;
    Napi::Value* pyobj_learningRate = NULL;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:BackgroundSubtractorMOG2.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &pyobj_learningRate) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) &&
        jsopencv_to_safe(info, pyobj_learningRate, learningRate, ArgInfo("learningRate", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->apply(image, fgmask, learningRate));
        return jsopencv_from(fgmask);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("apply");

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getBackgroundRatio(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBackgroundRatio());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getComplexityReductionThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getComplexityReductionThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getDetectShadows(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDetectShadows());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getHistory(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getHistory());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getNMixtures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNMixtures());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getShadowThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getShadowThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getShadowValue(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getShadowValue());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getVarInit(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarInit());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getVarMax(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarMax());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getVarMin(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarMin());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getVarThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_getVarThresholdGen(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarThresholdGen());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setBackgroundRatio(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_ratio = NULL;
    double ratio=0;

    const char* keywords[] = { "ratio", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setBackgroundRatio", (char**)keywords, &pyobj_ratio) &&
        jsopencv_to_safe(info, pyobj_ratio, ratio, ArgInfo("ratio", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBackgroundRatio(ratio));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setComplexityReductionThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_ct = NULL;
    double ct=0;

    const char* keywords[] = { "ct", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setComplexityReductionThreshold", (char**)keywords, &pyobj_ct) &&
        jsopencv_to_safe(info, pyobj_ct, ct, ArgInfo("ct", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setComplexityReductionThreshold(ct));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setDetectShadows(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_detectShadows = NULL;
    bool detectShadows=0;

    const char* keywords[] = { "detectShadows", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setDetectShadows", (char**)keywords, &pyobj_detectShadows) &&
        jsopencv_to_safe(info, pyobj_detectShadows, detectShadows, ArgInfo("detectShadows", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDetectShadows(detectShadows));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setHistory(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_history = NULL;
    int history=0;

    const char* keywords[] = { "history", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setHistory", (char**)keywords, &pyobj_history) &&
        jsopencv_to_safe(info, pyobj_history, history, ArgInfo("history", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setHistory(history));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setNMixtures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_nmixtures = NULL;
    int nmixtures=0;

    const char* keywords[] = { "nmixtures", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setNMixtures", (char**)keywords, &pyobj_nmixtures) &&
        jsopencv_to_safe(info, pyobj_nmixtures, nmixtures, ArgInfo("nmixtures", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNMixtures(nmixtures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setShadowThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setShadowThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setShadowThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setShadowValue(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    int value=0;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setShadowValue", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setShadowValue(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setVarInit(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_varInit = NULL;
    double varInit=0;

    const char* keywords[] = { "varInit", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setVarInit", (char**)keywords, &pyobj_varInit) &&
        jsopencv_to_safe(info, pyobj_varInit, varInit, ArgInfo("varInit", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVarInit(varInit));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setVarMax(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_varMax = NULL;
    double varMax=0;

    const char* keywords[] = { "varMax", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setVarMax", (char**)keywords, &pyobj_varMax) &&
        jsopencv_to_safe(info, pyobj_varMax, varMax, ArgInfo("varMax", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVarMax(varMax));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setVarMin(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_varMin = NULL;
    double varMin=0;

    const char* keywords[] = { "varMin", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setVarMin", (char**)keywords, &pyobj_varMin) &&
        jsopencv_to_safe(info, pyobj_varMin, varMin, ArgInfo("varMin", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVarMin(varMin));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setVarThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_varThreshold = NULL;
    double varThreshold=0;

    const char* keywords[] = { "varThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setVarThreshold", (char**)keywords, &pyobj_varThreshold) &&
        jsopencv_to_safe(info, pyobj_varThreshold, varThreshold, ArgInfo("varThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVarThreshold(varThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_BackgroundSubtractorMOG2_setVarThresholdGen(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::BackgroundSubtractorMOG2> * self1 = 0;
    if (!pyopencv_BackgroundSubtractorMOG2_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    Ptr<cv::BackgroundSubtractorMOG2> _self_ = *(self1);
    Napi::Value* pyobj_varThresholdGen = NULL;
    double varThresholdGen=0;

    const char* keywords[] = { "varThresholdGen", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BackgroundSubtractorMOG2.setVarThresholdGen", (char**)keywords, &pyobj_varThresholdGen) &&
        jsopencv_to_safe(info, pyobj_varThresholdGen, varThresholdGen, ArgInfo("varThresholdGen", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVarThresholdGen(varThresholdGen));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (BackgroundSubtractorMOG2)

static PyGetSetDef pyopencv_BackgroundSubtractorMOG2_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BackgroundSubtractorMOG2_methods[] =
{
    {"apply", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n.   @brief Computes a foreground mask.\n.   \n.       @param image Next video frame. Floating point frame will be used without scaling and should be in range \\f$[0,255]\\f$.\n.       @param fgmask The output foreground mask as an 8-bit binary image.\n.       @param learningRate The value between 0 and 1 that indicates how fast the background model is\n.       learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n.       rate. 0 means that the background model is not updated at all, 1 means that the background model\n.       is completely reinitialized from the last frame."},
    {"getBackgroundRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getBackgroundRatio, 0), "getBackgroundRatio() -> retval\n.   @brief Returns the \"background ratio\" parameter of the algorithm\n.   \n.       If a foreground pixel keeps semi-constant value for about backgroundRatio\\*history frames, it's\n.       considered background and added to the model as a center of a new component. It corresponds to TB\n.       parameter in the paper."},
    {"getComplexityReductionThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getComplexityReductionThreshold, 0), "getComplexityReductionThreshold() -> retval\n.   @brief Returns the complexity reduction threshold\n.   \n.       This parameter defines the number of samples needed to accept to prove the component exists. CT=0.05\n.       is a default value for all the samples. By setting CT=0 you get an algorithm very similar to the\n.       standard Stauffer&Grimson algorithm."},
    {"getDetectShadows", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getDetectShadows, 0), "getDetectShadows() -> retval\n.   @brief Returns the shadow detection flag\n.   \n.       If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorMOG2 for\n.       details."},
    {"getHistory", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getHistory, 0), "getHistory() -> retval\n.   @brief Returns the number of last frames that affect the background model"},
    {"getNMixtures", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getNMixtures, 0), "getNMixtures() -> retval\n.   @brief Returns the number of gaussian components in the background model"},
    {"getShadowThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getShadowThreshold, 0), "getShadowThreshold() -> retval\n.   @brief Returns the shadow threshold\n.   \n.       A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in\n.       the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel\n.       is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,\n.       *Detecting Moving Shadows...*, IEEE PAMI,2003."},
    {"getShadowValue", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getShadowValue, 0), "getShadowValue() -> retval\n.   @brief Returns the shadow value\n.   \n.       Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0\n.       in the mask always means background, 255 means foreground."},
    {"getVarInit", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarInit, 0), "getVarInit() -> retval\n.   @brief Returns the initial variance of each gaussian component"},
    {"getVarMax", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarMax, 0), "getVarMax() -> retval\n."},
    {"getVarMin", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarMin, 0), "getVarMin() -> retval\n."},
    {"getVarThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarThreshold, 0), "getVarThreshold() -> retval\n.   @brief Returns the variance threshold for the pixel-model match\n.   \n.       The main threshold on the squared Mahalanobis distance to decide if the sample is well described by\n.       the background model or not. Related to Cthr from the paper."},
    {"getVarThresholdGen", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarThresholdGen, 0), "getVarThresholdGen() -> retval\n.   @brief Returns the variance threshold for the pixel-model match used for new mixture component generation\n.   \n.       Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the\n.       existing components (corresponds to Tg in the paper). If a pixel is not close to any component, it\n.       is considered foreground or added as a new component. 3 sigma =\\> Tg=3\\*3=9 is default. A smaller Tg\n.       value generates more components. A higher Tg value may result in a small number of components but\n.       they can grow too large."},
    {"setBackgroundRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setBackgroundRatio, 0), "setBackgroundRatio(ratio) -> None\n.   @brief Sets the \"background ratio\" parameter of the algorithm"},
    {"setComplexityReductionThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setComplexityReductionThreshold, 0), "setComplexityReductionThreshold(ct) -> None\n.   @brief Sets the complexity reduction threshold"},
    {"setDetectShadows", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setDetectShadows, 0), "setDetectShadows(detectShadows) -> None\n.   @brief Enables or disables shadow detection"},
    {"setHistory", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setHistory, 0), "setHistory(history) -> None\n.   @brief Sets the number of last frames that affect the background model"},
    {"setNMixtures", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setNMixtures, 0), "setNMixtures(nmixtures) -> None\n.   @brief Sets the number of gaussian components in the background model.\n.   \n.       The model needs to be reinitalized to reserve memory."},
    {"setShadowThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setShadowThreshold, 0), "setShadowThreshold(threshold) -> None\n.   @brief Sets the shadow threshold"},
    {"setShadowValue", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setShadowValue, 0), "setShadowValue(value) -> None\n.   @brief Sets the shadow value"},
    {"setVarInit", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarInit, 0), "setVarInit(varInit) -> None\n.   @brief Sets the initial variance of each gaussian component"},
    {"setVarMax", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarMax, 0), "setVarMax(varMax) -> None\n."},
    {"setVarMin", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarMin, 0), "setVarMin(varMin) -> None\n."},
    {"setVarThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarThreshold, 0), "setVarThreshold(varThreshold) -> None\n.   @brief Sets the variance threshold for the pixel-model match"},
    {"setVarThresholdGen", CV_JS_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarThresholdGen, 0), "setVarThresholdGen(varThresholdGen) -> None\n.   @brief Sets the variance threshold for the pixel-model match used for new mixture component generation"},

    {NULL,          NULL}
};

// Converter (BackgroundSubtractorMOG2)

template<>
struct PyOpenCV_Converter< Ptr<cv::BackgroundSubtractorMOG2> >
{
    static PyObject* from(const Ptr<cv::BackgroundSubtractorMOG2>& r)
    {
        return pyopencv_BackgroundSubtractorMOG2_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BackgroundSubtractorMOG2>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BackgroundSubtractorMOG2> * dst_;
        if (pyopencv_BackgroundSubtractorMOG2_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BackgroundSubtractorMOG2> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// BaseCascadeClassifier (Generic)
//================================================================================

// GetSet (BaseCascadeClassifier)



// Methods (BaseCascadeClassifier)



// Tables (BaseCascadeClassifier)

static PyGetSetDef pyopencv_BaseCascadeClassifier_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_BaseCascadeClassifier_methods[] =
{

    {NULL,          NULL}
};

// Converter (BaseCascadeClassifier)

template<>
struct PyOpenCV_Converter< Ptr<cv::BaseCascadeClassifier> >
{
    static PyObject* from(const Ptr<cv::BaseCascadeClassifier>& r)
    {
        return pyopencv_BaseCascadeClassifier_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::BaseCascadeClassifier>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::BaseCascadeClassifier> * dst_;
        if (pyopencv_BaseCascadeClassifier_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::BaseCascadeClassifier> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// CLAHE (Generic)
//================================================================================

// GetSet (CLAHE)



// Methods (CLAHE)

static Napi::Value pyopencv_cv_CLAHE_apply(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CLAHE> * self1 = 0;
    if (!pyopencv_CLAHE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Ptr<cv::CLAHE> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:CLAHE.apply", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->apply(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:CLAHE.apply", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->apply(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("apply");

    return NULL;
}

static Napi::Value pyopencv_cv_CLAHE_collectGarbage(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CLAHE> * self1 = 0;
    if (!pyopencv_CLAHE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Ptr<cv::CLAHE> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->collectGarbage());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CLAHE_getClipLimit(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CLAHE> * self1 = 0;
    if (!pyopencv_CLAHE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Ptr<cv::CLAHE> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getClipLimit());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CLAHE_getTilesGridSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CLAHE> * self1 = 0;
    if (!pyopencv_CLAHE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Ptr<cv::CLAHE> _self_ = *(self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTilesGridSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CLAHE_setClipLimit(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CLAHE> * self1 = 0;
    if (!pyopencv_CLAHE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Ptr<cv::CLAHE> _self_ = *(self1);
    Napi::Value* pyobj_clipLimit = NULL;
    double clipLimit=0;

    const char* keywords[] = { "clipLimit", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CLAHE.setClipLimit", (char**)keywords, &pyobj_clipLimit) &&
        jsopencv_to_safe(info, pyobj_clipLimit, clipLimit, ArgInfo("clipLimit", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setClipLimit(clipLimit));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CLAHE_setTilesGridSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CLAHE> * self1 = 0;
    if (!pyopencv_CLAHE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Ptr<cv::CLAHE> _self_ = *(self1);
    Napi::Value* pyobj_tileGridSize = NULL;
    Size tileGridSize;

    const char* keywords[] = { "tileGridSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CLAHE.setTilesGridSize", (char**)keywords, &pyobj_tileGridSize) &&
        jsopencv_to_safe(info, pyobj_tileGridSize, tileGridSize, ArgInfo("tileGridSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTilesGridSize(tileGridSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (CLAHE)

static PyGetSetDef pyopencv_CLAHE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_CLAHE_methods[] =
{
    {"apply", CV_JS_FN_WITH_KW_(pyopencv_cv_CLAHE_apply, 0), "apply(src[, dst]) -> dst\n.   @brief Equalizes the histogram of a grayscale image using Contrast Limited Adaptive Histogram Equalization.\n.   \n.       @param src Source image of type CV_8UC1 or CV_16UC1.\n.       @param dst Destination image."},
    {"collectGarbage", CV_JS_FN_WITH_KW_(pyopencv_cv_CLAHE_collectGarbage, 0), "collectGarbage() -> None\n."},
    {"getClipLimit", CV_JS_FN_WITH_KW_(pyopencv_cv_CLAHE_getClipLimit, 0), "getClipLimit() -> retval\n."},
    {"getTilesGridSize", CV_JS_FN_WITH_KW_(pyopencv_cv_CLAHE_getTilesGridSize, 0), "getTilesGridSize() -> retval\n."},
    {"setClipLimit", CV_JS_FN_WITH_KW_(pyopencv_cv_CLAHE_setClipLimit, 0), "setClipLimit(clipLimit) -> None\n.   @brief Sets threshold for contrast limiting.\n.   \n.       @param clipLimit threshold value."},
    {"setTilesGridSize", CV_JS_FN_WITH_KW_(pyopencv_cv_CLAHE_setTilesGridSize, 0), "setTilesGridSize(tileGridSize) -> None\n.   @brief Sets size of grid for histogram equalization. Input image will be divided into\n.       equally sized rectangular tiles.\n.   \n.       @param tileGridSize defines the number of tiles in row and column."},

    {NULL,          NULL}
};

// Converter (CLAHE)

template<>
struct PyOpenCV_Converter< Ptr<cv::CLAHE> >
{
    static PyObject* from(const Ptr<cv::CLAHE>& r)
    {
        return pyopencv_CLAHE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::CLAHE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::CLAHE> * dst_;
        if (pyopencv_CLAHE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::CLAHE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// CalibrateCRF (Generic)
//================================================================================

// GetSet (CalibrateCRF)



// Methods (CalibrateCRF)

static Napi::Value pyopencv_cv_CalibrateCRF_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateCRF> * self1 = 0;
    if (!pyopencv_CalibrateCRF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateCRF' or its derivative)");
    Ptr<cv::CalibrateCRF> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:CalibrateCRF.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:CalibrateCRF.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}



// Tables (CalibrateCRF)

static PyGetSetDef pyopencv_CalibrateCRF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_CalibrateCRF_methods[] =
{
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateCRF_process, 0), "process(src, times[, dst]) -> dst\n.   @brief Recovers inverse camera response.\n.   \n.       @param src vector of input images\n.       @param dst 256x1 matrix with inverse camera response function\n.       @param times vector of exposure time values for each image"},

    {NULL,          NULL}
};

// Converter (CalibrateCRF)

template<>
struct PyOpenCV_Converter< Ptr<cv::CalibrateCRF> >
{
    static PyObject* from(const Ptr<cv::CalibrateCRF>& r)
    {
        return pyopencv_CalibrateCRF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::CalibrateCRF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::CalibrateCRF> * dst_;
        if (pyopencv_CalibrateCRF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::CalibrateCRF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// CalibrateDebevec (Generic)
//================================================================================

// GetSet (CalibrateDebevec)



// Methods (CalibrateDebevec)

static Napi::Value pyopencv_cv_CalibrateDebevec_getLambda(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateDebevec> * self1 = 0;
    if (!pyopencv_CalibrateDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    Ptr<cv::CalibrateDebevec> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLambda());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateDebevec_getRandom(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateDebevec> * self1 = 0;
    if (!pyopencv_CalibrateDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    Ptr<cv::CalibrateDebevec> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRandom());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateDebevec_getSamples(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateDebevec> * self1 = 0;
    if (!pyopencv_CalibrateDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    Ptr<cv::CalibrateDebevec> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateDebevec_setLambda(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateDebevec> * self1 = 0;
    if (!pyopencv_CalibrateDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    Ptr<cv::CalibrateDebevec> _self_ = *(self1);
    Napi::Value* pyobj_lambda = NULL;
    float lambda=0.f;

    const char* keywords[] = { "lambda_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CalibrateDebevec.setLambda", (char**)keywords, &pyobj_lambda) &&
        jsopencv_to_safe(info, pyobj_lambda, lambda, ArgInfo("lambda", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLambda(lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateDebevec_setRandom(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateDebevec> * self1 = 0;
    if (!pyopencv_CalibrateDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    Ptr<cv::CalibrateDebevec> _self_ = *(self1);
    Napi::Value* pyobj_random = NULL;
    bool random=0;

    const char* keywords[] = { "random", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CalibrateDebevec.setRandom", (char**)keywords, &pyobj_random) &&
        jsopencv_to_safe(info, pyobj_random, random, ArgInfo("random", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRandom(random));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateDebevec_setSamples(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateDebevec> * self1 = 0;
    if (!pyopencv_CalibrateDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    Ptr<cv::CalibrateDebevec> _self_ = *(self1);
    Napi::Value* pyobj_samples = NULL;
    int samples=0;

    const char* keywords[] = { "samples", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CalibrateDebevec.setSamples", (char**)keywords, &pyobj_samples) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSamples(samples));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (CalibrateDebevec)

static PyGetSetDef pyopencv_CalibrateDebevec_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_CalibrateDebevec_methods[] =
{
    {"getLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_getLambda, 0), "getLambda() -> retval\n."},
    {"getRandom", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_getRandom, 0), "getRandom() -> retval\n."},
    {"getSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_getSamples, 0), "getSamples() -> retval\n."},
    {"setLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_setLambda, 0), "setLambda(lambda_) -> None\n."},
    {"setRandom", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_setRandom, 0), "setRandom(random) -> None\n."},
    {"setSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_setSamples, 0), "setSamples(samples) -> None\n."},

    {NULL,          NULL}
};

// Converter (CalibrateDebevec)

template<>
struct PyOpenCV_Converter< Ptr<cv::CalibrateDebevec> >
{
    static PyObject* from(const Ptr<cv::CalibrateDebevec>& r)
    {
        return pyopencv_CalibrateDebevec_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::CalibrateDebevec>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::CalibrateDebevec> * dst_;
        if (pyopencv_CalibrateDebevec_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::CalibrateDebevec> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// CalibrateRobertson (Generic)
//================================================================================

// GetSet (CalibrateRobertson)



// Methods (CalibrateRobertson)

static Napi::Value pyopencv_cv_CalibrateRobertson_getMaxIter(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateRobertson> * self1 = 0;
    if (!pyopencv_CalibrateRobertson_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    Ptr<cv::CalibrateRobertson> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxIter());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateRobertson_getRadiance(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateRobertson> * self1 = 0;
    if (!pyopencv_CalibrateRobertson_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    Ptr<cv::CalibrateRobertson> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRadiance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateRobertson_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateRobertson> * self1 = 0;
    if (!pyopencv_CalibrateRobertson_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    Ptr<cv::CalibrateRobertson> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateRobertson_setMaxIter(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateRobertson> * self1 = 0;
    if (!pyopencv_CalibrateRobertson_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    Ptr<cv::CalibrateRobertson> _self_ = *(self1);
    Napi::Value* pyobj_max_iter = NULL;
    int max_iter=0;

    const char* keywords[] = { "max_iter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CalibrateRobertson.setMaxIter", (char**)keywords, &pyobj_max_iter) &&
        jsopencv_to_safe(info, pyobj_max_iter, max_iter, ArgInfo("max_iter", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxIter(max_iter));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CalibrateRobertson_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CalibrateRobertson> * self1 = 0;
    if (!pyopencv_CalibrateRobertson_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    Ptr<cv::CalibrateRobertson> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    float threshold=0.f;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CalibrateRobertson.setThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (CalibrateRobertson)

static PyGetSetDef pyopencv_CalibrateRobertson_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_CalibrateRobertson_methods[] =
{
    {"getMaxIter", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_getMaxIter, 0), "getMaxIter() -> retval\n."},
    {"getRadiance", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_getRadiance, 0), "getRadiance() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_getThreshold, 0), "getThreshold() -> retval\n."},
    {"setMaxIter", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_setMaxIter, 0), "setMaxIter(max_iter) -> None\n."},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_setThreshold, 0), "setThreshold(threshold) -> None\n."},

    {NULL,          NULL}
};

// Converter (CalibrateRobertson)

template<>
struct PyOpenCV_Converter< Ptr<cv::CalibrateRobertson> >
{
    static PyObject* from(const Ptr<cv::CalibrateRobertson>& r)
    {
        return pyopencv_CalibrateRobertson_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::CalibrateRobertson>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::CalibrateRobertson> * dst_;
        if (pyopencv_CalibrateRobertson_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::CalibrateRobertson> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// CascadeClassifier (Generic)
//================================================================================

// GetSet (CascadeClassifier)



// Methods (CascadeClassifier)

static int pyopencv_cv_CascadeClassifier_CascadeClassifier(pyopencv_CascadeClassifier_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::CascadeClassifier>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::CascadeClassifier()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CascadeClassifier", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        new (&(self->v)) Ptr<cv::CascadeClassifier>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::CascadeClassifier(filename)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("CascadeClassifier");

    return -1;
}

static Napi::Value pyopencv_cv_CascadeClassifier_convert_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_oldcascade = NULL;
    String oldcascade;
    Napi::Value* pyobj_newcascade = NULL;
    String newcascade;
    bool retval;

    const char* keywords[] = { "oldcascade", "newcascade", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:CascadeClassifier.convert", (char**)keywords, &pyobj_oldcascade, &pyobj_newcascade) &&
        jsopencv_to_safe(info, pyobj_oldcascade, oldcascade, ArgInfo("oldcascade", 0)) &&
        jsopencv_to_safe(info, pyobj_newcascade, newcascade, ArgInfo("newcascade", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::CascadeClassifier::convert(oldcascade, newcascade));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_detectMultiScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    vector_Rect objects;
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=1.1;
    Napi::Value* pyobj_minNeighbors = NULL;
    int minNeighbors=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minSize = NULL;
    Size minSize;
    Napi::Value* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOO:CascadeClassifier.detectMultiScale", (char**)keywords, &pyobj_image, &pyobj_scaleFactor, &pyobj_minNeighbors, &pyobj_flags, &pyobj_minSize, &pyobj_maxSize) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_minNeighbors, minNeighbors, ArgInfo("minNeighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(image, objects, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return jsopencv_from(objects);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    vector_Rect objects;
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=1.1;
    Napi::Value* pyobj_minNeighbors = NULL;
    int minNeighbors=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minSize = NULL;
    Size minSize;
    Napi::Value* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOO:CascadeClassifier.detectMultiScale", (char**)keywords, &pyobj_image, &pyobj_scaleFactor, &pyobj_minNeighbors, &pyobj_flags, &pyobj_minSize, &pyobj_maxSize) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_minNeighbors, minNeighbors, ArgInfo("minNeighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(image, objects, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return jsopencv_from(objects);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectMultiScale");

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_detectMultiScale2(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    vector_Rect objects;
    vector_int numDetections;
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=1.1;
    Napi::Value* pyobj_minNeighbors = NULL;
    int minNeighbors=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minSize = NULL;
    Size minSize;
    Napi::Value* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOO:CascadeClassifier.detectMultiScale2", (char**)keywords, &pyobj_image, &pyobj_scaleFactor, &pyobj_minNeighbors, &pyobj_flags, &pyobj_minSize, &pyobj_maxSize) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_minNeighbors, minNeighbors, ArgInfo("minNeighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(image, objects, numDetections, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return Py_BuildValue("(NN)", jsopencv_from(objects), jsopencv_from(numDetections));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    vector_Rect objects;
    vector_int numDetections;
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=1.1;
    Napi::Value* pyobj_minNeighbors = NULL;
    int minNeighbors=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minSize = NULL;
    Size minSize;
    Napi::Value* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOO:CascadeClassifier.detectMultiScale2", (char**)keywords, &pyobj_image, &pyobj_scaleFactor, &pyobj_minNeighbors, &pyobj_flags, &pyobj_minSize, &pyobj_maxSize) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_minNeighbors, minNeighbors, ArgInfo("minNeighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(image, objects, numDetections, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return Py_BuildValue("(NN)", jsopencv_from(objects), jsopencv_from(numDetections));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectMultiScale2");

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_detectMultiScale3(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    vector_Rect objects;
    vector_int rejectLevels;
    vector_double levelWeights;
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=1.1;
    Napi::Value* pyobj_minNeighbors = NULL;
    int minNeighbors=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minSize = NULL;
    Size minSize;
    Napi::Value* pyobj_maxSize = NULL;
    Size maxSize;
    Napi::Value* pyobj_outputRejectLevels = NULL;
    bool outputRejectLevels=false;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", "outputRejectLevels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOOO:CascadeClassifier.detectMultiScale3", (char**)keywords, &pyobj_image, &pyobj_scaleFactor, &pyobj_minNeighbors, &pyobj_flags, &pyobj_minSize, &pyobj_maxSize, &pyobj_outputRejectLevels) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_minNeighbors, minNeighbors, ArgInfo("minNeighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) &&
        jsopencv_to_safe(info, pyobj_outputRejectLevels, outputRejectLevels, ArgInfo("outputRejectLevels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(image, objects, rejectLevels, levelWeights, scaleFactor, minNeighbors, flags, minSize, maxSize, outputRejectLevels));
        return Py_BuildValue("(NNN)", jsopencv_from(objects), jsopencv_from(rejectLevels), jsopencv_from(levelWeights));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    vector_Rect objects;
    vector_int rejectLevels;
    vector_double levelWeights;
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=1.1;
    Napi::Value* pyobj_minNeighbors = NULL;
    int minNeighbors=3;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minSize = NULL;
    Size minSize;
    Napi::Value* pyobj_maxSize = NULL;
    Size maxSize;
    Napi::Value* pyobj_outputRejectLevels = NULL;
    bool outputRejectLevels=false;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", "outputRejectLevels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOOO:CascadeClassifier.detectMultiScale3", (char**)keywords, &pyobj_image, &pyobj_scaleFactor, &pyobj_minNeighbors, &pyobj_flags, &pyobj_minSize, &pyobj_maxSize, &pyobj_outputRejectLevels) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_minNeighbors, minNeighbors, ArgInfo("minNeighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) &&
        jsopencv_to_safe(info, pyobj_outputRejectLevels, outputRejectLevels, ArgInfo("outputRejectLevels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(image, objects, rejectLevels, levelWeights, scaleFactor, minNeighbors, flags, minSize, maxSize, outputRejectLevels));
        return Py_BuildValue("(NNN)", jsopencv_from(objects), jsopencv_from(rejectLevels), jsopencv_from(levelWeights));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectMultiScale3");

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_empty(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_getFeatureType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFeatureType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_getOriginalWindowSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getOriginalWindowSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_isOldFormatCascade(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isOldFormatCascade());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_load(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CascadeClassifier.load", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->load(filename));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_CascadeClassifier_read(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::CascadeClassifier> * self1 = 0;
    if (!pyopencv_CascadeClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Ptr<cv::CascadeClassifier> _self_ = *(self1);
    Napi::Value* pyobj_node = NULL;
    cv::FileNode node;
    bool retval;

    const char* keywords[] = { "node", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:CascadeClassifier.read", (char**)keywords, &pyobj_node) &&
        jsopencv_to_safe(info, pyobj_node, node, ArgInfo("node", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->read(node));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (CascadeClassifier)

static PyGetSetDef pyopencv_CascadeClassifier_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_CascadeClassifier_methods[] =
{
    {"convert", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_convert_static, METH_STATIC), "convert(oldcascade, newcascade) -> retval\n."},
    {"detectMultiScale", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_detectMultiScale, 0), "detectMultiScale(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) -> objects\n.   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n.       of rectangles.\n.   \n.       @param image Matrix of the type CV_8U containing an image where objects are detected.\n.       @param objects Vector of rectangles where each rectangle contains the detected object, the\n.       rectangles may be partially outside the original image.\n.       @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n.       @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n.       to retain it.\n.       @param flags Parameter with the same meaning for an old cascade as in the function\n.       cvHaarDetectObjects. It is not used for a new cascade.\n.       @param minSize Minimum possible object size. Objects smaller than that are ignored.\n.       @param maxSize Maximum possible object size. Objects larger than that are ignored. If `maxSize == minSize` model is evaluated on single scale."},
    {"detectMultiScale2", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_detectMultiScale2, 0), "detectMultiScale2(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) -> objects, numDetections\n.   @overload\n.       @param image Matrix of the type CV_8U containing an image where objects are detected.\n.       @param objects Vector of rectangles where each rectangle contains the detected object, the\n.       rectangles may be partially outside the original image.\n.       @param numDetections Vector of detection numbers for the corresponding objects. An object's number\n.       of detections is the number of neighboring positively classified rectangles that were joined\n.       together to form the object.\n.       @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n.       @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n.       to retain it.\n.       @param flags Parameter with the same meaning for an old cascade as in the function\n.       cvHaarDetectObjects. It is not used for a new cascade.\n.       @param minSize Minimum possible object size. Objects smaller than that are ignored.\n.       @param maxSize Maximum possible object size. Objects larger than that are ignored. If `maxSize == minSize` model is evaluated on single scale."},
    {"detectMultiScale3", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_detectMultiScale3, 0), "detectMultiScale3(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize[, outputRejectLevels]]]]]]) -> objects, rejectLevels, levelWeights\n.   @overload\n.       This function allows you to retrieve the final stage decision certainty of classification.\n.       For this, one needs to set `outputRejectLevels` on true and provide the `rejectLevels` and `levelWeights` parameter.\n.       For each resulting detection, `levelWeights` will then contain the certainty of classification at the final stage.\n.       This value can then be used to separate strong from weaker classifications.\n.   \n.       A code sample on how to use it efficiently can be found below:\n.       @code\n.       Mat img;\n.       vector<double> weights;\n.       vector<int> levels;\n.       vector<Rect> detections;\n.       CascadeClassifier model(\"/path/to/your/model.xml\");\n.       model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);\n.       cerr << \"Detection \" << detections[0] << \" with weight \" << weights[0] << endl;\n.       @endcode"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_empty, 0), "empty() -> retval\n.   @brief Checks whether the classifier has been loaded."},
    {"getFeatureType", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_getFeatureType, 0), "getFeatureType() -> retval\n."},
    {"getOriginalWindowSize", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_getOriginalWindowSize, 0), "getOriginalWindowSize() -> retval\n."},
    {"isOldFormatCascade", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_isOldFormatCascade, 0), "isOldFormatCascade() -> retval\n."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_load, 0), "load(filename) -> retval\n.   @brief Loads a classifier from a file.\n.   \n.       @param filename Name of the file from which the classifier is loaded. The file may contain an old\n.       HAAR classifier trained by the haartraining application or a new cascade classifier trained by the\n.       traincascade application."},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_read, 0), "read(node) -> retval\n.   @brief Reads a classifier from a FileStorage node.\n.   \n.       @note The file may contain a new cascade classifier (trained by the traincascade application) only."},

    {NULL,          NULL}
};

// Converter (CascadeClassifier)

template<>
struct PyOpenCV_Converter< Ptr<cv::CascadeClassifier> >
{
    static PyObject* from(const Ptr<cv::CascadeClassifier>& r)
    {
        return pyopencv_CascadeClassifier_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::CascadeClassifier>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::CascadeClassifier> * dst_;
        if (pyopencv_CascadeClassifier_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::CascadeClassifier> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// CirclesGridFinderParameters (Generic)
//================================================================================

// GetSet (CirclesGridFinderParameters)


static PyObject* pyopencv_CirclesGridFinderParameters_get_convexHullFactor(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.convexHullFactor);
}

static int pyopencv_CirclesGridFinderParameters_set_convexHullFactor(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the convexHullFactor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.convexHullFactor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_densityNeighborhoodSize(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.densityNeighborhoodSize);
}

static int pyopencv_CirclesGridFinderParameters_set_densityNeighborhoodSize(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the densityNeighborhoodSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.densityNeighborhoodSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_edgeGain(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.edgeGain);
}

static int pyopencv_CirclesGridFinderParameters_set_edgeGain(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the edgeGain attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.edgeGain, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_edgePenalty(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.edgePenalty);
}

static int pyopencv_CirclesGridFinderParameters_set_edgePenalty(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the edgePenalty attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.edgePenalty, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_existingVertexGain(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.existingVertexGain);
}

static int pyopencv_CirclesGridFinderParameters_set_existingVertexGain(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the existingVertexGain attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.existingVertexGain, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_keypointScale(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.keypointScale);
}

static int pyopencv_CirclesGridFinderParameters_set_keypointScale(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the keypointScale attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.keypointScale, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_kmeansAttempts(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.kmeansAttempts);
}

static int pyopencv_CirclesGridFinderParameters_set_kmeansAttempts(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the kmeansAttempts attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.kmeansAttempts, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_maxRectifiedDistance(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.maxRectifiedDistance);
}

static int pyopencv_CirclesGridFinderParameters_set_maxRectifiedDistance(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxRectifiedDistance attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxRectifiedDistance, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minDensity(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minDensity);
}

static int pyopencv_CirclesGridFinderParameters_set_minDensity(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDensity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minDensity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minDistanceToAddKeypoint(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minDistanceToAddKeypoint);
}

static int pyopencv_CirclesGridFinderParameters_set_minDistanceToAddKeypoint(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDistanceToAddKeypoint attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minDistanceToAddKeypoint, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minGraphConfidence(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minGraphConfidence);
}

static int pyopencv_CirclesGridFinderParameters_set_minGraphConfidence(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minGraphConfidence attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minGraphConfidence, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minRNGEdgeSwitchDist(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minRNGEdgeSwitchDist);
}

static int pyopencv_CirclesGridFinderParameters_set_minRNGEdgeSwitchDist(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minRNGEdgeSwitchDist attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minRNGEdgeSwitchDist, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_squareSize(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.squareSize);
}

static int pyopencv_CirclesGridFinderParameters_set_squareSize(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the squareSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.squareSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_vertexGain(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.vertexGain);
}

static int pyopencv_CirclesGridFinderParameters_set_vertexGain(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the vertexGain attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.vertexGain, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_vertexPenalty(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.vertexPenalty);
}

static int pyopencv_CirclesGridFinderParameters_set_vertexPenalty(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the vertexPenalty attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.vertexPenalty, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (CirclesGridFinderParameters)

static int pyopencv_cv_CirclesGridFinderParameters_CirclesGridFinderParameters(pyopencv_CirclesGridFinderParameters_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::CirclesGridFinderParameters());
        return 0;
    }

    return -1;
}



// Tables (CirclesGridFinderParameters)

static PyGetSetDef pyopencv_CirclesGridFinderParameters_getseters[] =
{
    {(char*)"convexHullFactor", (getter)pyopencv_CirclesGridFinderParameters_get_convexHullFactor, (setter)pyopencv_CirclesGridFinderParameters_set_convexHullFactor, (char*)"convexHullFactor", NULL},
    {(char*)"densityNeighborhoodSize", (getter)pyopencv_CirclesGridFinderParameters_get_densityNeighborhoodSize, (setter)pyopencv_CirclesGridFinderParameters_set_densityNeighborhoodSize, (char*)"densityNeighborhoodSize", NULL},
    {(char*)"edgeGain", (getter)pyopencv_CirclesGridFinderParameters_get_edgeGain, (setter)pyopencv_CirclesGridFinderParameters_set_edgeGain, (char*)"edgeGain", NULL},
    {(char*)"edgePenalty", (getter)pyopencv_CirclesGridFinderParameters_get_edgePenalty, (setter)pyopencv_CirclesGridFinderParameters_set_edgePenalty, (char*)"edgePenalty", NULL},
    {(char*)"existingVertexGain", (getter)pyopencv_CirclesGridFinderParameters_get_existingVertexGain, (setter)pyopencv_CirclesGridFinderParameters_set_existingVertexGain, (char*)"existingVertexGain", NULL},
    {(char*)"keypointScale", (getter)pyopencv_CirclesGridFinderParameters_get_keypointScale, (setter)pyopencv_CirclesGridFinderParameters_set_keypointScale, (char*)"keypointScale", NULL},
    {(char*)"kmeansAttempts", (getter)pyopencv_CirclesGridFinderParameters_get_kmeansAttempts, (setter)pyopencv_CirclesGridFinderParameters_set_kmeansAttempts, (char*)"kmeansAttempts", NULL},
    {(char*)"maxRectifiedDistance", (getter)pyopencv_CirclesGridFinderParameters_get_maxRectifiedDistance, (setter)pyopencv_CirclesGridFinderParameters_set_maxRectifiedDistance, (char*)"maxRectifiedDistance", NULL},
    {(char*)"minDensity", (getter)pyopencv_CirclesGridFinderParameters_get_minDensity, (setter)pyopencv_CirclesGridFinderParameters_set_minDensity, (char*)"minDensity", NULL},
    {(char*)"minDistanceToAddKeypoint", (getter)pyopencv_CirclesGridFinderParameters_get_minDistanceToAddKeypoint, (setter)pyopencv_CirclesGridFinderParameters_set_minDistanceToAddKeypoint, (char*)"minDistanceToAddKeypoint", NULL},
    {(char*)"minGraphConfidence", (getter)pyopencv_CirclesGridFinderParameters_get_minGraphConfidence, (setter)pyopencv_CirclesGridFinderParameters_set_minGraphConfidence, (char*)"minGraphConfidence", NULL},
    {(char*)"minRNGEdgeSwitchDist", (getter)pyopencv_CirclesGridFinderParameters_get_minRNGEdgeSwitchDist, (setter)pyopencv_CirclesGridFinderParameters_set_minRNGEdgeSwitchDist, (char*)"minRNGEdgeSwitchDist", NULL},
    {(char*)"squareSize", (getter)pyopencv_CirclesGridFinderParameters_get_squareSize, (setter)pyopencv_CirclesGridFinderParameters_set_squareSize, (char*)"squareSize", NULL},
    {(char*)"vertexGain", (getter)pyopencv_CirclesGridFinderParameters_get_vertexGain, (setter)pyopencv_CirclesGridFinderParameters_set_vertexGain, (char*)"vertexGain", NULL},
    {(char*)"vertexPenalty", (getter)pyopencv_CirclesGridFinderParameters_get_vertexPenalty, (setter)pyopencv_CirclesGridFinderParameters_set_vertexPenalty, (char*)"vertexPenalty", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_CirclesGridFinderParameters_methods[] =
{

    {NULL,          NULL}
};

// Converter (CirclesGridFinderParameters)

template<>
struct PyOpenCV_Converter< cv::CirclesGridFinderParameters >
{
    static PyObject* from(const cv::CirclesGridFinderParameters& r)
    {
        return pyopencv_CirclesGridFinderParameters_Instance(r);
    }
    static bool to(PyObject* src, cv::CirclesGridFinderParameters& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::CirclesGridFinderParameters * dst_;
        if (pyopencv_CirclesGridFinderParameters_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::CirclesGridFinderParameters for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// DISOpticalFlow (Generic)
//================================================================================

// GetSet (DISOpticalFlow)



// Methods (DISOpticalFlow)

static Napi::Value pyopencv_cv_DISOpticalFlow_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_preset = NULL;
    int preset=DISOpticalFlow::PRESET_FAST;
    Ptr<DISOpticalFlow> retval;

    const char* keywords[] = { "preset", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:DISOpticalFlow.create", (char**)keywords, &pyobj_preset) &&
        jsopencv_to_safe(info, pyobj_preset, preset, ArgInfo("preset", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::DISOpticalFlow::create(preset));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getFinestScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFinestScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getGradientDescentIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGradientDescentIterations());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getPatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPatchSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getPatchStride(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPatchStride());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getUseMeanNormalization(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseMeanNormalization());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getUseSpatialPropagation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseSpatialPropagation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getVariationalRefinementAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVariationalRefinementAlpha());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getVariationalRefinementDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVariationalRefinementDelta());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getVariationalRefinementGamma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVariationalRefinementGamma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_getVariationalRefinementIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVariationalRefinementIterations());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setFinestScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setFinestScale", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFinestScale(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setGradientDescentIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setGradientDescentIterations", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGradientDescentIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setPatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setPatchSize", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPatchSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setPatchStride(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setPatchStride", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPatchStride(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setUseMeanNormalization(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setUseMeanNormalization", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseMeanNormalization(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setUseSpatialPropagation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setUseSpatialPropagation", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseSpatialPropagation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setVariationalRefinementAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setVariationalRefinementAlpha", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVariationalRefinementAlpha(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setVariationalRefinementDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setVariationalRefinementDelta", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVariationalRefinementDelta(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setVariationalRefinementGamma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setVariationalRefinementGamma", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVariationalRefinementGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DISOpticalFlow_setVariationalRefinementIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DISOpticalFlow> * self1 = 0;
    if (!pyopencv_DISOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DISOpticalFlow' or its derivative)");
    Ptr<cv::DISOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DISOpticalFlow.setVariationalRefinementIterations", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVariationalRefinementIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (DISOpticalFlow)

static PyGetSetDef pyopencv_DISOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_DISOpticalFlow_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_create_static, METH_STATIC), "create([, preset]) -> retval\n.   @brief Creates an instance of DISOpticalFlow\n.   \n.       @param preset one of PRESET_ULTRAFAST, PRESET_FAST and PRESET_MEDIUM"},
    {"getFinestScale", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getFinestScale, 0), "getFinestScale() -> retval\n.   @brief Finest level of the Gaussian pyramid on which the flow is computed (zero level\n.           corresponds to the original image resolution). The final flow is obtained by bilinear upscaling.\n.   @see setFinestScale"},
    {"getGradientDescentIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getGradientDescentIterations, 0), "getGradientDescentIterations() -> retval\n.   @brief Maximum number of gradient descent iterations in the patch inverse search stage. Higher values\n.           may improve quality in some cases.\n.   @see setGradientDescentIterations"},
    {"getPatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getPatchSize, 0), "getPatchSize() -> retval\n.   @brief Size of an image patch for matching (in pixels). Normally, default 8x8 patches work well\n.           enough in most cases.\n.   @see setPatchSize"},
    {"getPatchStride", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getPatchStride, 0), "getPatchStride() -> retval\n.   @brief Stride between neighbor patches. Must be less than patch size. Lower values correspond\n.           to higher flow quality.\n.   @see setPatchStride"},
    {"getUseMeanNormalization", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getUseMeanNormalization, 0), "getUseMeanNormalization() -> retval\n.   @brief Whether to use mean-normalization of patches when computing patch distance. It is turned on\n.           by default as it typically provides a noticeable quality boost because of increased robustness to\n.           illumination variations. Turn it off if you are certain that your sequence doesn't contain any changes\n.           in illumination.\n.   @see setUseMeanNormalization"},
    {"getUseSpatialPropagation", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getUseSpatialPropagation, 0), "getUseSpatialPropagation() -> retval\n.   @brief Whether to use spatial propagation of good optical flow vectors. This option is turned on by\n.           default, as it tends to work better on average and can sometimes help recover from major errors\n.           introduced by the coarse-to-fine scheme employed by the DIS optical flow algorithm. Turning this\n.           option off can make the output flow field a bit smoother, however.\n.   @see setUseSpatialPropagation"},
    {"getVariationalRefinementAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getVariationalRefinementAlpha, 0), "getVariationalRefinementAlpha() -> retval\n.   @brief Weight of the smoothness term\n.   @see setVariationalRefinementAlpha"},
    {"getVariationalRefinementDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getVariationalRefinementDelta, 0), "getVariationalRefinementDelta() -> retval\n.   @brief Weight of the color constancy term\n.   @see setVariationalRefinementDelta"},
    {"getVariationalRefinementGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getVariationalRefinementGamma, 0), "getVariationalRefinementGamma() -> retval\n.   @brief Weight of the gradient constancy term\n.   @see setVariationalRefinementGamma"},
    {"getVariationalRefinementIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_getVariationalRefinementIterations, 0), "getVariationalRefinementIterations() -> retval\n.   @brief Number of fixed point iterations of variational refinement per scale. Set to zero to\n.           disable variational refinement completely. Higher values will typically result in more smooth and\n.           high-quality flow.\n.   @see setGradientDescentIterations"},
    {"setFinestScale", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setFinestScale, 0), "setFinestScale(val) -> None\n.   @copybrief getFinestScale @see getFinestScale"},
    {"setGradientDescentIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setGradientDescentIterations, 0), "setGradientDescentIterations(val) -> None\n.   @copybrief getGradientDescentIterations @see getGradientDescentIterations"},
    {"setPatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setPatchSize, 0), "setPatchSize(val) -> None\n.   @copybrief getPatchSize @see getPatchSize"},
    {"setPatchStride", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setPatchStride, 0), "setPatchStride(val) -> None\n.   @copybrief getPatchStride @see getPatchStride"},
    {"setUseMeanNormalization", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setUseMeanNormalization, 0), "setUseMeanNormalization(val) -> None\n.   @copybrief getUseMeanNormalization @see getUseMeanNormalization"},
    {"setUseSpatialPropagation", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setUseSpatialPropagation, 0), "setUseSpatialPropagation(val) -> None\n.   @copybrief getUseSpatialPropagation @see getUseSpatialPropagation"},
    {"setVariationalRefinementAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setVariationalRefinementAlpha, 0), "setVariationalRefinementAlpha(val) -> None\n.   @copybrief getVariationalRefinementAlpha @see getVariationalRefinementAlpha"},
    {"setVariationalRefinementDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setVariationalRefinementDelta, 0), "setVariationalRefinementDelta(val) -> None\n.   @copybrief getVariationalRefinementDelta @see getVariationalRefinementDelta"},
    {"setVariationalRefinementGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setVariationalRefinementGamma, 0), "setVariationalRefinementGamma(val) -> None\n.   @copybrief getVariationalRefinementGamma @see getVariationalRefinementGamma"},
    {"setVariationalRefinementIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_DISOpticalFlow_setVariationalRefinementIterations, 0), "setVariationalRefinementIterations(val) -> None\n.   @copybrief getGradientDescentIterations @see getGradientDescentIterations"},

    {NULL,          NULL}
};

// Converter (DISOpticalFlow)

template<>
struct PyOpenCV_Converter< Ptr<cv::DISOpticalFlow> >
{
    static PyObject* from(const Ptr<cv::DISOpticalFlow>& r)
    {
        return pyopencv_DISOpticalFlow_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::DISOpticalFlow>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::DISOpticalFlow> * dst_;
        if (pyopencv_DISOpticalFlow_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::DISOpticalFlow> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// DMatch (Generic)
//================================================================================

// GetSet (DMatch)


static PyObject* pyopencv_DMatch_get_distance(pyopencv_DMatch_t* p, void *closure)
{
    return jsopencv_from(p->v.distance);
}

static int pyopencv_DMatch_set_distance(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the distance attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.distance, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_DMatch_get_imgIdx(pyopencv_DMatch_t* p, void *closure)
{
    return jsopencv_from(p->v.imgIdx);
}

static int pyopencv_DMatch_set_imgIdx(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the imgIdx attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.imgIdx, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_DMatch_get_queryIdx(pyopencv_DMatch_t* p, void *closure)
{
    return jsopencv_from(p->v.queryIdx);
}

static int pyopencv_DMatch_set_queryIdx(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the queryIdx attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.queryIdx, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_DMatch_get_trainIdx(pyopencv_DMatch_t* p, void *closure)
{
    return jsopencv_from(p->v.trainIdx);
}

static int pyopencv_DMatch_set_trainIdx(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the trainIdx attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.trainIdx, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (DMatch)

static int pyopencv_cv_DMatch_DMatch(pyopencv_DMatch_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(3);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::DMatch());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__queryIdx = NULL;
    int _queryIdx=0;
    Napi::Value* pyobj__trainIdx = NULL;
    int _trainIdx=0;
    Napi::Value* pyobj__distance = NULL;
    float _distance=0.f;

    const char* keywords[] = { "_queryIdx", "_trainIdx", "_distance", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:DMatch", (char**)keywords, &pyobj__queryIdx, &pyobj__trainIdx, &pyobj__distance) &&
        jsopencv_to_safe(info, pyobj__queryIdx, _queryIdx, ArgInfo("_queryIdx", 0)) &&
        jsopencv_to_safe(info, pyobj__trainIdx, _trainIdx, ArgInfo("_trainIdx", 0)) &&
        jsopencv_to_safe(info, pyobj__distance, _distance, ArgInfo("_distance", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::DMatch(_queryIdx, _trainIdx, _distance));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__queryIdx = NULL;
    int _queryIdx=0;
    Napi::Value* pyobj__trainIdx = NULL;
    int _trainIdx=0;
    Napi::Value* pyobj__imgIdx = NULL;
    int _imgIdx=0;
    Napi::Value* pyobj__distance = NULL;
    float _distance=0.f;

    const char* keywords[] = { "_queryIdx", "_trainIdx", "_imgIdx", "_distance", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:DMatch", (char**)keywords, &pyobj__queryIdx, &pyobj__trainIdx, &pyobj__imgIdx, &pyobj__distance) &&
        jsopencv_to_safe(info, pyobj__queryIdx, _queryIdx, ArgInfo("_queryIdx", 0)) &&
        jsopencv_to_safe(info, pyobj__trainIdx, _trainIdx, ArgInfo("_trainIdx", 0)) &&
        jsopencv_to_safe(info, pyobj__imgIdx, _imgIdx, ArgInfo("_imgIdx", 0)) &&
        jsopencv_to_safe(info, pyobj__distance, _distance, ArgInfo("_distance", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::DMatch(_queryIdx, _trainIdx, _imgIdx, _distance));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("DMatch");

    return -1;
}



// Tables (DMatch)

static PyGetSetDef pyopencv_DMatch_getseters[] =
{
    {(char*)"distance", (getter)pyopencv_DMatch_get_distance, (setter)pyopencv_DMatch_set_distance, (char*)"distance", NULL},
    {(char*)"imgIdx", (getter)pyopencv_DMatch_get_imgIdx, (setter)pyopencv_DMatch_set_imgIdx, (char*)"imgIdx", NULL},
    {(char*)"queryIdx", (getter)pyopencv_DMatch_get_queryIdx, (setter)pyopencv_DMatch_set_queryIdx, (char*)"queryIdx", NULL},
    {(char*)"trainIdx", (getter)pyopencv_DMatch_get_trainIdx, (setter)pyopencv_DMatch_set_trainIdx, (char*)"trainIdx", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_DMatch_methods[] =
{

    {NULL,          NULL}
};

// Converter (DMatch)

template<>
struct PyOpenCV_Converter< cv::DMatch >
{
    static PyObject* from(const cv::DMatch& r)
    {
        return pyopencv_DMatch_Instance(r);
    }
    static bool to(PyObject* src, cv::DMatch& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::DMatch * dst_;
        if (pyopencv_DMatch_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::DMatch for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// DenseOpticalFlow (Generic)
//================================================================================

// GetSet (DenseOpticalFlow)



// Methods (DenseOpticalFlow)

static Napi::Value pyopencv_cv_DenseOpticalFlow_calc(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DenseOpticalFlow> * self1 = 0;
    if (!pyopencv_DenseOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DenseOpticalFlow' or its derivative)");
    Ptr<cv::DenseOpticalFlow> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_I0 = NULL;
    Mat I0;
    Napi::Value* pyobj_I1 = NULL;
    Mat I1;
    Napi::Value* pyobj_flow = NULL;
    Mat flow;

    const char* keywords[] = { "I0", "I1", "flow", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:DenseOpticalFlow.calc", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow) &&
        jsopencv_to_safe(info, pyobj_I0, I0, ArgInfo("I0", 0)) &&
        jsopencv_to_safe(info, pyobj_I1, I1, ArgInfo("I1", 0)) &&
        jsopencv_to_safe(info, pyobj_flow, flow, ArgInfo("flow", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->calc(I0, I1, flow));
        return jsopencv_from(flow);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_I0 = NULL;
    UMat I0;
    Napi::Value* pyobj_I1 = NULL;
    UMat I1;
    Napi::Value* pyobj_flow = NULL;
    UMat flow;

    const char* keywords[] = { "I0", "I1", "flow", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:DenseOpticalFlow.calc", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow) &&
        jsopencv_to_safe(info, pyobj_I0, I0, ArgInfo("I0", 0)) &&
        jsopencv_to_safe(info, pyobj_I1, I1, ArgInfo("I1", 0)) &&
        jsopencv_to_safe(info, pyobj_flow, flow, ArgInfo("flow", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->calc(I0, I1, flow));
        return jsopencv_from(flow);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("calc");

    return NULL;
}

static Napi::Value pyopencv_cv_DenseOpticalFlow_collectGarbage(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DenseOpticalFlow> * self1 = 0;
    if (!pyopencv_DenseOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DenseOpticalFlow' or its derivative)");
    Ptr<cv::DenseOpticalFlow> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->collectGarbage());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (DenseOpticalFlow)

static PyGetSetDef pyopencv_DenseOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_DenseOpticalFlow_methods[] =
{
    {"calc", CV_JS_FN_WITH_KW_(pyopencv_cv_DenseOpticalFlow_calc, 0), "calc(I0, I1, flow) -> flow\n.   @brief Calculates an optical flow.\n.   \n.       @param I0 first 8-bit single-channel input image.\n.       @param I1 second input image of the same size and the same type as prev.\n.       @param flow computed flow image that has the same size as prev and type CV_32FC2."},
    {"collectGarbage", CV_JS_FN_WITH_KW_(pyopencv_cv_DenseOpticalFlow_collectGarbage, 0), "collectGarbage() -> None\n.   @brief Releases all inner buffers."},

    {NULL,          NULL}
};

// Converter (DenseOpticalFlow)

template<>
struct PyOpenCV_Converter< Ptr<cv::DenseOpticalFlow> >
{
    static PyObject* from(const Ptr<cv::DenseOpticalFlow>& r)
    {
        return pyopencv_DenseOpticalFlow_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::DenseOpticalFlow>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::DenseOpticalFlow> * dst_;
        if (pyopencv_DenseOpticalFlow_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::DenseOpticalFlow> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// DescriptorMatcher (Generic)
//================================================================================

// GetSet (DescriptorMatcher)



// Methods (DescriptorMatcher)

static Napi::Value pyopencv_cv_DescriptorMatcher_add(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_descriptors = NULL;
    vector_Mat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.add", (char**)keywords, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->add(descriptors));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_descriptors = NULL;
    vector_UMat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.add", (char**)keywords, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->add(descriptors));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("add");

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_clear(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_clone(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    Napi::Value* pyobj_emptyTrainData = NULL;
    bool emptyTrainData=false;
    Ptr<DescriptorMatcher> retval;

    const char* keywords[] = { "emptyTrainData", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:DescriptorMatcher.clone", (char**)keywords, &pyobj_emptyTrainData) &&
        jsopencv_to_safe(info, pyobj_emptyTrainData, emptyTrainData, ArgInfo("emptyTrainData", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->clone(emptyTrainData));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_descriptorMatcherType = NULL;
    String descriptorMatcherType;
    Ptr<DescriptorMatcher> retval;

    const char* keywords[] = { "descriptorMatcherType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.create", (char**)keywords, &pyobj_descriptorMatcherType) &&
        jsopencv_to_safe(info, pyobj_descriptorMatcherType, descriptorMatcherType, ArgInfo("descriptorMatcherType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::DescriptorMatcher::create(descriptorMatcherType));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_matcherType = NULL;
    DescriptorMatcher_MatcherType matcherType=static_cast<DescriptorMatcher_MatcherType>(0);
    Ptr<DescriptorMatcher> retval;

    const char* keywords[] = { "matcherType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.create", (char**)keywords, &pyobj_matcherType) &&
        jsopencv_to_safe(info, pyobj_matcherType, matcherType, ArgInfo("matcherType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::DescriptorMatcher::create(matcherType));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_empty(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_getTrainDescriptors(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    std::vector<Mat> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainDescriptors());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_isMaskSupported(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isMaskSupported());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_knnMatch(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    Napi::Value* pyobj_trainDescriptors = NULL;
    Mat trainDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_k = NULL;
    int k=0;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "k", "mask", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_k, &pyobj_mask, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->knnMatch(queryDescriptors, trainDescriptors, matches, k, mask, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    Napi::Value* pyobj_trainDescriptors = NULL;
    UMat trainDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_k = NULL;
    int k=0;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "k", "mask", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_k, &pyobj_mask, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->knnMatch(queryDescriptors, trainDescriptors, matches, k, mask, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_k = NULL;
    int k=0;
    Napi::Value* pyobj_masks = NULL;
    vector_Mat masks;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "k", "masks", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_k, &pyobj_masks, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->knnMatch(queryDescriptors, matches, k, masks, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_k = NULL;
    int k=0;
    Napi::Value* pyobj_masks = NULL;
    vector_UMat masks;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "k", "masks", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_k, &pyobj_masks, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->knnMatch(queryDescriptors, matches, k, masks, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("knnMatch");

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_match(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    Napi::Value* pyobj_trainDescriptors = NULL;
    Mat trainDescriptors;
    vector_DMatch matches;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->match(queryDescriptors, trainDescriptors, matches, mask));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    Napi::Value* pyobj_trainDescriptors = NULL;
    UMat trainDescriptors;
    vector_DMatch matches;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->match(queryDescriptors, trainDescriptors, matches, mask));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    vector_DMatch matches;
    Napi::Value* pyobj_masks = NULL;
    vector_Mat masks;

    const char* keywords[] = { "queryDescriptors", "masks", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_masks) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->match(queryDescriptors, matches, masks));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    vector_DMatch matches;
    Napi::Value* pyobj_masks = NULL;
    vector_UMat masks;

    const char* keywords[] = { "queryDescriptors", "masks", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_masks) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->match(queryDescriptors, matches, masks));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("match");

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_radiusMatch(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    Napi::Value* pyobj_trainDescriptors = NULL;
    Mat trainDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_maxDistance = NULL;
    float maxDistance=0.f;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "maxDistance", "mask", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_maxDistance, &pyobj_mask, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_maxDistance, maxDistance, ArgInfo("maxDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->radiusMatch(queryDescriptors, trainDescriptors, matches, maxDistance, mask, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    Napi::Value* pyobj_trainDescriptors = NULL;
    UMat trainDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_maxDistance = NULL;
    float maxDistance=0.f;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "maxDistance", "mask", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_maxDistance, &pyobj_mask, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_maxDistance, maxDistance, ArgInfo("maxDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->radiusMatch(queryDescriptors, trainDescriptors, matches, maxDistance, mask, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_maxDistance = NULL;
    float maxDistance=0.f;
    Napi::Value* pyobj_masks = NULL;
    vector_Mat masks;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "maxDistance", "masks", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_maxDistance, &pyobj_masks, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_maxDistance, maxDistance, ArgInfo("maxDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->radiusMatch(queryDescriptors, matches, maxDistance, masks, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    vector_vector_DMatch matches;
    Napi::Value* pyobj_maxDistance = NULL;
    float maxDistance=0.f;
    Napi::Value* pyobj_masks = NULL;
    vector_UMat masks;
    Napi::Value* pyobj_compactResult = NULL;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "maxDistance", "masks", "compactResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_maxDistance, &pyobj_masks, &pyobj_compactResult) &&
        jsopencv_to_safe(info, pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        jsopencv_to_safe(info, pyobj_maxDistance, maxDistance, ArgInfo("maxDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)) &&
        jsopencv_to_safe(info, pyobj_compactResult, compactResult, ArgInfo("compactResult", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->radiusMatch(queryDescriptors, matches, maxDistance, masks, compactResult));
        return jsopencv_from(matches);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("radiusMatch");

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_read(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.read", (char**)keywords, &pyobj_fileName) &&
        jsopencv_to_safe(info, pyobj_fileName, fileName, ArgInfo("fileName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(fileName));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arg1 = NULL;
    cv::FileNode arg1;

    const char* keywords[] = { "arg1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.read", (char**)keywords, &pyobj_arg1) &&
        jsopencv_to_safe(info, pyobj_arg1, arg1, ArgInfo("arg1", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(arg1));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("read");

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_train(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->train());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_DescriptorMatcher_write(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::DescriptorMatcher> * self1 = 0;
    if (!pyopencv_DescriptorMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    Ptr<cv::DescriptorMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DescriptorMatcher.write", (char**)keywords, &pyobj_fileName) &&
        jsopencv_to_safe(info, pyobj_fileName, fileName, ArgInfo("fileName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(fileName));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:DescriptorMatcher.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(*fs, name));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("write");

    return NULL;
}



// Tables (DescriptorMatcher)

static PyGetSetDef pyopencv_DescriptorMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_DescriptorMatcher_methods[] =
{
    {"add", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_add, 0), "add(descriptors) -> None\n.   @brief Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor\n.       collection.\n.   \n.       If the collection is not empty, the new descriptors are added to existing train descriptors.\n.   \n.       @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n.       train image."},
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_clear, 0), "clear() -> None\n.   @brief Clears the train descriptor collections."},
    {"clone", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_clone, 0), "clone([, emptyTrainData]) -> retval\n.   @brief Clones the matcher.\n.   \n.       @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n.       that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n.       object copy with the current parameters but with empty train data."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_create_static, METH_STATIC), "create(descriptorMatcherType) -> retval\n.   @brief Creates a descriptor matcher of a given type with the default parameters (using default\n.       constructor).\n.   \n.       @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are\n.       supported:\n.       -   `BruteForce` (it uses L2 )\n.       -   `BruteForce-L1`\n.       -   `BruteForce-Hamming`\n.       -   `BruteForce-Hamming(2)`\n.       -   `FlannBased`\n\n\n\ncreate(matcherType) -> retval\n."},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_empty, 0), "empty() -> retval\n.   @brief Returns true if there are no train descriptors in the both collections."},
    {"getTrainDescriptors", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_getTrainDescriptors, 0), "getTrainDescriptors() -> retval\n.   @brief Returns a constant link to the train descriptor collection trainDescCollection ."},
    {"isMaskSupported", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_isMaskSupported, 0), "isMaskSupported() -> retval\n.   @brief Returns true if the descriptor matcher supports masking permissible matches."},
    {"knnMatch", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_knnMatch, 0), "knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) -> matches\n.   @brief Finds the k best matches for each descriptor from a query set.\n.   \n.       @param queryDescriptors Query set of descriptors.\n.       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n.       collection stored in the class object.\n.       @param mask Mask specifying permissible matches between an input query and train matrices of\n.       descriptors.\n.       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n.       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n.       less than k possible matches in total.\n.       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.       the matches vector does not contain matches for fully masked-out query descriptors.\n.   \n.       These extended variants of DescriptorMatcher::match methods find several best matches for each query\n.       descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match\n.       for the details about query and train descriptors.\n\n\n\nknnMatch(queryDescriptors, k[, masks[, compactResult]]) -> matches\n.   @overload\n.       @param queryDescriptors Query set of descriptors.\n.       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n.       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n.       less than k possible matches in total.\n.       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n.       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n.       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.       the matches vector does not contain matches for fully masked-out query descriptors."},
    {"match", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_match, 0), "match(queryDescriptors, trainDescriptors[, mask]) -> matches\n.   @brief Finds the best match for each descriptor from a query set.\n.   \n.       @param queryDescriptors Query set of descriptors.\n.       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n.       collection stored in the class object.\n.       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n.       descriptor. So, matches size may be smaller than the query descriptors count.\n.       @param mask Mask specifying permissible matches between an input query and train matrices of\n.       descriptors.\n.   \n.       In the first variant of this method, the train descriptors are passed as an input argument. In the\n.       second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is\n.       used. Optional mask (or masks) can be passed to specify which query and training descriptors can be\n.       matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n.       mask.at\\<uchar\\>(i,j) is non-zero.\n\n\n\nmatch(queryDescriptors[, masks]) -> matches\n.   @overload\n.       @param queryDescriptors Query set of descriptors.\n.       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n.       descriptor. So, matches size may be smaller than the query descriptors count.\n.       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n.       descriptors and stored train descriptors from the i-th image trainDescCollection[i]."},
    {"radiusMatch", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_radiusMatch, 0), "radiusMatch(queryDescriptors, trainDescriptors, maxDistance[, mask[, compactResult]]) -> matches\n.   @brief For each query descriptor, finds the training descriptors not farther than the specified distance.\n.   \n.       @param queryDescriptors Query set of descriptors.\n.       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n.       collection stored in the class object.\n.       @param matches Found matches.\n.       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.       the matches vector does not contain matches for fully masked-out query descriptors.\n.       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n.       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n.       in Pixels)!\n.       @param mask Mask specifying permissible matches between an input query and train matrices of\n.       descriptors.\n.   \n.       For each query descriptor, the methods find such training descriptors that the distance between the\n.       query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are\n.       returned in the distance increasing order.\n\n\n\nradiusMatch(queryDescriptors, maxDistance[, masks[, compactResult]]) -> matches\n.   @overload\n.       @param queryDescriptors Query set of descriptors.\n.       @param matches Found matches.\n.       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n.       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n.       in Pixels)!\n.       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n.       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n.       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.       the matches vector does not contain matches for fully masked-out query descriptors."},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_read, 0), "read(fileName) -> None\n.   \n\n\n\nread(arg1) -> None\n."},
    {"train", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_train, 0), "train() -> None\n.   @brief Trains a descriptor matcher\n.   \n.       Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n.       train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher)\n.       have an empty implementation of this method. Other matchers really train their inner structures (for\n.       example, FlannBasedMatcher trains flann::Index )."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_write, 0), "write(fileName) -> None\n.   \n\n\n\nwrite(fs, name) -> None\n."},

    {NULL,          NULL}
};

// Converter (DescriptorMatcher)

template<>
struct PyOpenCV_Converter< Ptr<cv::DescriptorMatcher> >
{
    static PyObject* from(const Ptr<cv::DescriptorMatcher>& r)
    {
        return pyopencv_DescriptorMatcher_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::DescriptorMatcher>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::DescriptorMatcher> * dst_;
        if (pyopencv_DescriptorMatcher_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::DescriptorMatcher> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FaceDetectorYN (Generic)
//================================================================================

// GetSet (FaceDetectorYN)



// Methods (FaceDetectorYN)

static Napi::Value pyopencv_cv_FaceDetectorYN_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config;
    Napi::Value* pyobj_input_size = NULL;
    Size input_size;
    Napi::Value* pyobj_score_threshold = NULL;
    float score_threshold=0.9f;
    Napi::Value* pyobj_nms_threshold = NULL;
    float nms_threshold=0.3f;
    Napi::Value* pyobj_top_k = NULL;
    int top_k=5000;
    Napi::Value* pyobj_backend_id = NULL;
    int backend_id=0;
    Napi::Value* pyobj_target_id = NULL;
    int target_id=0;
    Ptr<FaceDetectorYN> retval;

    const char* keywords[] = { "model", "config", "input_size", "score_threshold", "nms_threshold", "top_k", "backend_id", "target_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOOOO:FaceDetectorYN.create", (char**)keywords, &pyobj_model, &pyobj_config, &pyobj_input_size, &pyobj_score_threshold, &pyobj_nms_threshold, &pyobj_top_k, &pyobj_backend_id, &pyobj_target_id) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)) &&
        jsopencv_to_safe(info, pyobj_input_size, input_size, ArgInfo("input_size", 0)) &&
        jsopencv_to_safe(info, pyobj_score_threshold, score_threshold, ArgInfo("score_threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nms_threshold, nms_threshold, ArgInfo("nms_threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_top_k, top_k, ArgInfo("top_k", 0)) &&
        jsopencv_to_safe(info, pyobj_backend_id, backend_id, ArgInfo("backend_id", 0)) &&
        jsopencv_to_safe(info, pyobj_target_id, target_id, ArgInfo("target_id", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::FaceDetectorYN::create(model, config, input_size, score_threshold, nms_threshold, top_k, backend_id, target_id));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_detect(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_faces = NULL;
    Mat faces;
    int retval;

    const char* keywords[] = { "image", "faces", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:FaceDetectorYN.detect", (char**)keywords, &pyobj_image, &pyobj_faces) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_faces, faces, ArgInfo("faces", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detect(image, faces));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(faces));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_faces = NULL;
    UMat faces;
    int retval;

    const char* keywords[] = { "image", "faces", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:FaceDetectorYN.detect", (char**)keywords, &pyobj_image, &pyobj_faces) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_faces, faces, ArgInfo("faces", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detect(image, faces));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(faces));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_getInputSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getInputSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_getNMSThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNMSThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_getScoreThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScoreThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_getTopK(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTopK());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_setInputSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    Napi::Value* pyobj_input_size = NULL;
    Size input_size;

    const char* keywords[] = { "input_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FaceDetectorYN.setInputSize", (char**)keywords, &pyobj_input_size) &&
        jsopencv_to_safe(info, pyobj_input_size, input_size, ArgInfo("input_size", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInputSize(input_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_setNMSThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    Napi::Value* pyobj_nms_threshold = NULL;
    float nms_threshold=0.f;

    const char* keywords[] = { "nms_threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FaceDetectorYN.setNMSThreshold", (char**)keywords, &pyobj_nms_threshold) &&
        jsopencv_to_safe(info, pyobj_nms_threshold, nms_threshold, ArgInfo("nms_threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNMSThreshold(nms_threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_setScoreThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    Napi::Value* pyobj_score_threshold = NULL;
    float score_threshold=0.f;

    const char* keywords[] = { "score_threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FaceDetectorYN.setScoreThreshold", (char**)keywords, &pyobj_score_threshold) &&
        jsopencv_to_safe(info, pyobj_score_threshold, score_threshold, ArgInfo("score_threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScoreThreshold(score_threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceDetectorYN_setTopK(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceDetectorYN> * self1 = 0;
    if (!pyopencv_FaceDetectorYN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceDetectorYN' or its derivative)");
    Ptr<cv::FaceDetectorYN> _self_ = *(self1);
    Napi::Value* pyobj_top_k = NULL;
    int top_k=0;

    const char* keywords[] = { "top_k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FaceDetectorYN.setTopK", (char**)keywords, &pyobj_top_k) &&
        jsopencv_to_safe(info, pyobj_top_k, top_k, ArgInfo("top_k", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTopK(top_k));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (FaceDetectorYN)

static PyGetSetDef pyopencv_FaceDetectorYN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FaceDetectorYN_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_create_static, METH_STATIC), "create(model, config, input_size[, score_threshold[, nms_threshold[, top_k[, backend_id[, target_id]]]]]) -> retval\n.   @brief Creates an instance of this class with given parameters\n.        *\n.        *  @param model the path to the requested model\n.        *  @param config the path to the config file for compability, which is not requested for ONNX models\n.        *  @param input_size the size of the input image\n.        *  @param score_threshold the threshold to filter out bounding boxes of score smaller than the given value\n.        *  @param nms_threshold the threshold to suppress bounding boxes of IoU bigger than the given value\n.        *  @param top_k keep top K bboxes before NMS\n.        *  @param backend_id the id of backend\n.        *  @param target_id the id of target device"},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_detect, 0), "detect(image[, faces]) -> retval, faces\n.   @brief A simple interface to detect face from given image\n.        *\n.        *  @param image an image to detect\n.        *  @param faces detection results stored in a cv::Mat"},
    {"getInputSize", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_getInputSize, 0), "getInputSize() -> retval\n."},
    {"getNMSThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_getNMSThreshold, 0), "getNMSThreshold() -> retval\n."},
    {"getScoreThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_getScoreThreshold, 0), "getScoreThreshold() -> retval\n."},
    {"getTopK", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_getTopK, 0), "getTopK() -> retval\n."},
    {"setInputSize", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_setInputSize, 0), "setInputSize(input_size) -> None\n.   @brief Set the size for the network input, which overwrites the input size of creating model. Call this method when the size of input image does not match the input size when creating model\n.        *\n.        * @param input_size the size of the input image"},
    {"setNMSThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_setNMSThreshold, 0), "setNMSThreshold(nms_threshold) -> None\n.   @brief Set the Non-maximum-suppression threshold to suppress bounding boxes that have IoU greater than the given value\n.        *\n.        * @param nms_threshold threshold for NMS operation"},
    {"setScoreThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_setScoreThreshold, 0), "setScoreThreshold(score_threshold) -> None\n.   @brief Set the score threshold to filter out bounding boxes of score less than the given value\n.        *\n.        * @param score_threshold threshold for filtering out bounding boxes"},
    {"setTopK", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceDetectorYN_setTopK, 0), "setTopK(top_k) -> None\n.   @brief Set the number of bounding boxes preserved before NMS\n.        *\n.        * @param top_k the number of bounding boxes to preserve from top rank based on score"},

    {NULL,          NULL}
};

// Converter (FaceDetectorYN)

template<>
struct PyOpenCV_Converter< Ptr<cv::FaceDetectorYN> >
{
    static PyObject* from(const Ptr<cv::FaceDetectorYN>& r)
    {
        return pyopencv_FaceDetectorYN_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::FaceDetectorYN>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::FaceDetectorYN> * dst_;
        if (pyopencv_FaceDetectorYN_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::FaceDetectorYN> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FaceRecognizerSF (Generic)
//================================================================================

// GetSet (FaceRecognizerSF)



// Methods (FaceRecognizerSF)

static Napi::Value pyopencv_cv_FaceRecognizerSF_alignCrop(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceRecognizerSF> * self1 = 0;
    if (!pyopencv_FaceRecognizerSF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceRecognizerSF' or its derivative)");
    Ptr<cv::FaceRecognizerSF> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src_img = NULL;
    Mat src_img;
    Napi::Value* pyobj_face_box = NULL;
    Mat face_box;
    Napi::Value* pyobj_aligned_img = NULL;
    Mat aligned_img;

    const char* keywords[] = { "src_img", "face_box", "aligned_img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FaceRecognizerSF.alignCrop", (char**)keywords, &pyobj_src_img, &pyobj_face_box, &pyobj_aligned_img) &&
        jsopencv_to_safe(info, pyobj_src_img, src_img, ArgInfo("src_img", 0)) &&
        jsopencv_to_safe(info, pyobj_face_box, face_box, ArgInfo("face_box", 0)) &&
        jsopencv_to_safe(info, pyobj_aligned_img, aligned_img, ArgInfo("aligned_img", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->alignCrop(src_img, face_box, aligned_img));
        return jsopencv_from(aligned_img);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src_img = NULL;
    UMat src_img;
    Napi::Value* pyobj_face_box = NULL;
    UMat face_box;
    Napi::Value* pyobj_aligned_img = NULL;
    UMat aligned_img;

    const char* keywords[] = { "src_img", "face_box", "aligned_img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FaceRecognizerSF.alignCrop", (char**)keywords, &pyobj_src_img, &pyobj_face_box, &pyobj_aligned_img) &&
        jsopencv_to_safe(info, pyobj_src_img, src_img, ArgInfo("src_img", 0)) &&
        jsopencv_to_safe(info, pyobj_face_box, face_box, ArgInfo("face_box", 0)) &&
        jsopencv_to_safe(info, pyobj_aligned_img, aligned_img, ArgInfo("aligned_img", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->alignCrop(src_img, face_box, aligned_img));
        return jsopencv_from(aligned_img);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("alignCrop");

    return NULL;
}

static Napi::Value pyopencv_cv_FaceRecognizerSF_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config;
    Napi::Value* pyobj_backend_id = NULL;
    int backend_id=0;
    Napi::Value* pyobj_target_id = NULL;
    int target_id=0;
    Ptr<FaceRecognizerSF> retval;

    const char* keywords[] = { "model", "config", "backend_id", "target_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:FaceRecognizerSF.create", (char**)keywords, &pyobj_model, &pyobj_config, &pyobj_backend_id, &pyobj_target_id) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)) &&
        jsopencv_to_safe(info, pyobj_backend_id, backend_id, ArgInfo("backend_id", 0)) &&
        jsopencv_to_safe(info, pyobj_target_id, target_id, ArgInfo("target_id", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::FaceRecognizerSF::create(model, config, backend_id, target_id));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FaceRecognizerSF_feature(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceRecognizerSF> * self1 = 0;
    if (!pyopencv_FaceRecognizerSF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceRecognizerSF' or its derivative)");
    Ptr<cv::FaceRecognizerSF> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_aligned_img = NULL;
    Mat aligned_img;
    Napi::Value* pyobj_face_feature = NULL;
    Mat face_feature;

    const char* keywords[] = { "aligned_img", "face_feature", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:FaceRecognizerSF.feature", (char**)keywords, &pyobj_aligned_img, &pyobj_face_feature) &&
        jsopencv_to_safe(info, pyobj_aligned_img, aligned_img, ArgInfo("aligned_img", 0)) &&
        jsopencv_to_safe(info, pyobj_face_feature, face_feature, ArgInfo("face_feature", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->feature(aligned_img, face_feature));
        return jsopencv_from(face_feature);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_aligned_img = NULL;
    UMat aligned_img;
    Napi::Value* pyobj_face_feature = NULL;
    UMat face_feature;

    const char* keywords[] = { "aligned_img", "face_feature", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:FaceRecognizerSF.feature", (char**)keywords, &pyobj_aligned_img, &pyobj_face_feature) &&
        jsopencv_to_safe(info, pyobj_aligned_img, aligned_img, ArgInfo("aligned_img", 0)) &&
        jsopencv_to_safe(info, pyobj_face_feature, face_feature, ArgInfo("face_feature", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->feature(aligned_img, face_feature));
        return jsopencv_from(face_feature);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("feature");

    return NULL;
}

static Napi::Value pyopencv_cv_FaceRecognizerSF_match(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FaceRecognizerSF> * self1 = 0;
    if (!pyopencv_FaceRecognizerSF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FaceRecognizerSF' or its derivative)");
    Ptr<cv::FaceRecognizerSF> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_face_feature1 = NULL;
    Mat face_feature1;
    Napi::Value* pyobj_face_feature2 = NULL;
    Mat face_feature2;
    Napi::Value* pyobj_dis_type = NULL;
    int dis_type=FaceRecognizerSF::FR_COSINE;
    double retval;

    const char* keywords[] = { "face_feature1", "face_feature2", "dis_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FaceRecognizerSF.match", (char**)keywords, &pyobj_face_feature1, &pyobj_face_feature2, &pyobj_dis_type) &&
        jsopencv_to_safe(info, pyobj_face_feature1, face_feature1, ArgInfo("face_feature1", 0)) &&
        jsopencv_to_safe(info, pyobj_face_feature2, face_feature2, ArgInfo("face_feature2", 0)) &&
        jsopencv_to_safe(info, pyobj_dis_type, dis_type, ArgInfo("dis_type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->match(face_feature1, face_feature2, dis_type));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_face_feature1 = NULL;
    UMat face_feature1;
    Napi::Value* pyobj_face_feature2 = NULL;
    UMat face_feature2;
    Napi::Value* pyobj_dis_type = NULL;
    int dis_type=FaceRecognizerSF::FR_COSINE;
    double retval;

    const char* keywords[] = { "face_feature1", "face_feature2", "dis_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FaceRecognizerSF.match", (char**)keywords, &pyobj_face_feature1, &pyobj_face_feature2, &pyobj_dis_type) &&
        jsopencv_to_safe(info, pyobj_face_feature1, face_feature1, ArgInfo("face_feature1", 0)) &&
        jsopencv_to_safe(info, pyobj_face_feature2, face_feature2, ArgInfo("face_feature2", 0)) &&
        jsopencv_to_safe(info, pyobj_dis_type, dis_type, ArgInfo("dis_type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->match(face_feature1, face_feature2, dis_type));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("match");

    return NULL;
}



// Tables (FaceRecognizerSF)

static PyGetSetDef pyopencv_FaceRecognizerSF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FaceRecognizerSF_methods[] =
{
    {"alignCrop", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceRecognizerSF_alignCrop, 0), "alignCrop(src_img, face_box[, aligned_img]) -> aligned_img\n.   @brief Aligning image to put face on the standard position\n.        *  @param src_img input image\n.        *  @param face_box the detection result used for indicate face in input image\n.        *  @param aligned_img output aligned image"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceRecognizerSF_create_static, METH_STATIC), "create(model, config[, backend_id[, target_id]]) -> retval\n.   @brief Creates an instance of this class with given parameters\n.        *  @param model the path of the onnx model used for face recognition\n.        *  @param config the path to the config file for compability, which is not requested for ONNX models\n.        *  @param backend_id the id of backend\n.        *  @param target_id the id of target device"},
    {"feature", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceRecognizerSF_feature, 0), "feature(aligned_img[, face_feature]) -> face_feature\n.   @brief Extracting face feature from aligned image\n.        *  @param aligned_img input aligned image\n.        *  @param face_feature output face feature"},
    {"match", CV_JS_FN_WITH_KW_(pyopencv_cv_FaceRecognizerSF_match, 0), "match(face_feature1, face_feature2[, dis_type]) -> retval\n.   @brief Calculating the distance between two face features\n.        *  @param face_feature1 the first input feature\n.        *  @param face_feature2 the second input feature of the same size and the same type as face_feature1\n.        *  @param dis_type defining the similarity with optional values \"FR_OSINE\" or \"FR_NORM_L2\""},

    {NULL,          NULL}
};

// Converter (FaceRecognizerSF)

template<>
struct PyOpenCV_Converter< Ptr<cv::FaceRecognizerSF> >
{
    static PyObject* from(const Ptr<cv::FaceRecognizerSF>& r)
    {
        return pyopencv_FaceRecognizerSF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::FaceRecognizerSF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::FaceRecognizerSF> * dst_;
        if (pyopencv_FaceRecognizerSF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::FaceRecognizerSF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FarnebackOpticalFlow (Generic)
//================================================================================

// GetSet (FarnebackOpticalFlow)



// Methods (FarnebackOpticalFlow)

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_numLevels = NULL;
    int numLevels=5;
    Napi::Value* pyobj_pyrScale = NULL;
    double pyrScale=0.5;
    Napi::Value* pyobj_fastPyramids = NULL;
    bool fastPyramids=false;
    Napi::Value* pyobj_winSize = NULL;
    int winSize=13;
    Napi::Value* pyobj_numIters = NULL;
    int numIters=10;
    Napi::Value* pyobj_polyN = NULL;
    int polyN=5;
    Napi::Value* pyobj_polySigma = NULL;
    double polySigma=1.1;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Ptr<FarnebackOpticalFlow> retval;

    const char* keywords[] = { "numLevels", "pyrScale", "fastPyramids", "winSize", "numIters", "polyN", "polySigma", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOO:FarnebackOpticalFlow.create", (char**)keywords, &pyobj_numLevels, &pyobj_pyrScale, &pyobj_fastPyramids, &pyobj_winSize, &pyobj_numIters, &pyobj_polyN, &pyobj_polySigma, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_numLevels, numLevels, ArgInfo("numLevels", 0)) &&
        jsopencv_to_safe(info, pyobj_pyrScale, pyrScale, ArgInfo("pyrScale", 0)) &&
        jsopencv_to_safe(info, pyobj_fastPyramids, fastPyramids, ArgInfo("fastPyramids", 0)) &&
        jsopencv_to_safe(info, pyobj_winSize, winSize, ArgInfo("winSize", 0)) &&
        jsopencv_to_safe(info, pyobj_numIters, numIters, ArgInfo("numIters", 0)) &&
        jsopencv_to_safe(info, pyobj_polyN, polyN, ArgInfo("polyN", 0)) &&
        jsopencv_to_safe(info, pyobj_polySigma, polySigma, ArgInfo("polySigma", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::FarnebackOpticalFlow::create(numLevels, pyrScale, fastPyramids, winSize, numIters, polyN, polySigma, flags));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getFastPyramids(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFastPyramids());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getFlags(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFlags());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getNumIters(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumIters());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getNumLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumLevels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getPolyN(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPolyN());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getPolySigma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPolySigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getPyrScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPyrScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_getWinSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWinSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setFastPyramids(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_fastPyramids = NULL;
    bool fastPyramids=0;

    const char* keywords[] = { "fastPyramids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setFastPyramids", (char**)keywords, &pyobj_fastPyramids) &&
        jsopencv_to_safe(info, pyobj_fastPyramids, fastPyramids, ArgInfo("fastPyramids", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFastPyramids(fastPyramids));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setFlags(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_flags = NULL;
    int flags=0;

    const char* keywords[] = { "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setFlags", (char**)keywords, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFlags(flags));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setNumIters(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_numIters = NULL;
    int numIters=0;

    const char* keywords[] = { "numIters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setNumIters", (char**)keywords, &pyobj_numIters) &&
        jsopencv_to_safe(info, pyobj_numIters, numIters, ArgInfo("numIters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumIters(numIters));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setNumLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_numLevels = NULL;
    int numLevels=0;

    const char* keywords[] = { "numLevels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setNumLevels", (char**)keywords, &pyobj_numLevels) &&
        jsopencv_to_safe(info, pyobj_numLevels, numLevels, ArgInfo("numLevels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumLevels(numLevels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setPolyN(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_polyN = NULL;
    int polyN=0;

    const char* keywords[] = { "polyN", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setPolyN", (char**)keywords, &pyobj_polyN) &&
        jsopencv_to_safe(info, pyobj_polyN, polyN, ArgInfo("polyN", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPolyN(polyN));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setPolySigma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_polySigma = NULL;
    double polySigma=0;

    const char* keywords[] = { "polySigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setPolySigma", (char**)keywords, &pyobj_polySigma) &&
        jsopencv_to_safe(info, pyobj_polySigma, polySigma, ArgInfo("polySigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPolySigma(polySigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setPyrScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_pyrScale = NULL;
    double pyrScale=0;

    const char* keywords[] = { "pyrScale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setPyrScale", (char**)keywords, &pyobj_pyrScale) &&
        jsopencv_to_safe(info, pyobj_pyrScale, pyrScale, ArgInfo("pyrScale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPyrScale(pyrScale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FarnebackOpticalFlow_setWinSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FarnebackOpticalFlow> * self1 = 0;
    if (!pyopencv_FarnebackOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    Ptr<cv::FarnebackOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_winSize = NULL;
    int winSize=0;

    const char* keywords[] = { "winSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FarnebackOpticalFlow.setWinSize", (char**)keywords, &pyobj_winSize) &&
        jsopencv_to_safe(info, pyobj_winSize, winSize, ArgInfo("winSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWinSize(winSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (FarnebackOpticalFlow)

static PyGetSetDef pyopencv_FarnebackOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FarnebackOpticalFlow_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_create_static, METH_STATIC), "create([, numLevels[, pyrScale[, fastPyramids[, winSize[, numIters[, polyN[, polySigma[, flags]]]]]]]]) -> retval\n."},
    {"getFastPyramids", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getFastPyramids, 0), "getFastPyramids() -> retval\n."},
    {"getFlags", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getFlags, 0), "getFlags() -> retval\n."},
    {"getNumIters", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getNumIters, 0), "getNumIters() -> retval\n."},
    {"getNumLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getNumLevels, 0), "getNumLevels() -> retval\n."},
    {"getPolyN", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getPolyN, 0), "getPolyN() -> retval\n."},
    {"getPolySigma", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getPolySigma, 0), "getPolySigma() -> retval\n."},
    {"getPyrScale", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getPyrScale, 0), "getPyrScale() -> retval\n."},
    {"getWinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getWinSize, 0), "getWinSize() -> retval\n."},
    {"setFastPyramids", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setFastPyramids, 0), "setFastPyramids(fastPyramids) -> None\n."},
    {"setFlags", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setFlags, 0), "setFlags(flags) -> None\n."},
    {"setNumIters", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setNumIters, 0), "setNumIters(numIters) -> None\n."},
    {"setNumLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setNumLevels, 0), "setNumLevels(numLevels) -> None\n."},
    {"setPolyN", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setPolyN, 0), "setPolyN(polyN) -> None\n."},
    {"setPolySigma", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setPolySigma, 0), "setPolySigma(polySigma) -> None\n."},
    {"setPyrScale", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setPyrScale, 0), "setPyrScale(pyrScale) -> None\n."},
    {"setWinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setWinSize, 0), "setWinSize(winSize) -> None\n."},

    {NULL,          NULL}
};

// Converter (FarnebackOpticalFlow)

template<>
struct PyOpenCV_Converter< Ptr<cv::FarnebackOpticalFlow> >
{
    static PyObject* from(const Ptr<cv::FarnebackOpticalFlow>& r)
    {
        return pyopencv_FarnebackOpticalFlow_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::FarnebackOpticalFlow>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::FarnebackOpticalFlow> * dst_;
        if (pyopencv_FarnebackOpticalFlow_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::FarnebackOpticalFlow> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FastFeatureDetector (Generic)
//================================================================================

// GetSet (FastFeatureDetector)



// Methods (FastFeatureDetector)

static Napi::Value pyopencv_cv_FastFeatureDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_threshold = NULL;
    int threshold=10;
    Napi::Value* pyobj_nonmaxSuppression = NULL;
    bool nonmaxSuppression=true;
    Napi::Value* pyobj_type = NULL;
    FastFeatureDetector_DetectorType type=FastFeatureDetector::TYPE_9_16;
    Ptr<FastFeatureDetector> retval;

    const char* keywords[] = { "threshold", "nonmaxSuppression", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:FastFeatureDetector.create", (char**)keywords, &pyobj_threshold, &pyobj_nonmaxSuppression, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nonmaxSuppression, nonmaxSuppression, ArgInfo("nonmaxSuppression", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::FastFeatureDetector::create(threshold, nonmaxSuppression, type));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_getNonmaxSuppression(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNonmaxSuppression());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_getType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    FastFeatureDetector::DetectorType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_setNonmaxSuppression(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_f = NULL;
    bool f=0;

    const char* keywords[] = { "f", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FastFeatureDetector.setNonmaxSuppression", (char**)keywords, &pyobj_f) &&
        jsopencv_to_safe(info, pyobj_f, f, ArgInfo("f", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNonmaxSuppression(f));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    int threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FastFeatureDetector.setThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FastFeatureDetector_setType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FastFeatureDetector> * self1 = 0;
    if (!pyopencv_FastFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    Ptr<cv::FastFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_type = NULL;
    FastFeatureDetector_DetectorType type=static_cast<FastFeatureDetector_DetectorType>(0);

    const char* keywords[] = { "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FastFeatureDetector.setType", (char**)keywords, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setType(type));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (FastFeatureDetector)

static PyGetSetDef pyopencv_FastFeatureDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FastFeatureDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_create_static, METH_STATIC), "create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getNonmaxSuppression", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getNonmaxSuppression, 0), "getNonmaxSuppression() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getThreshold, 0), "getThreshold() -> retval\n."},
    {"getType", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getType, 0), "getType() -> retval\n."},
    {"setNonmaxSuppression", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_setNonmaxSuppression, 0), "setNonmaxSuppression(f) -> None\n."},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_setThreshold, 0), "setThreshold(threshold) -> None\n."},
    {"setType", CV_JS_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_setType, 0), "setType(type) -> None\n."},

    {NULL,          NULL}
};

// Converter (FastFeatureDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::FastFeatureDetector> >
{
    static PyObject* from(const Ptr<cv::FastFeatureDetector>& r)
    {
        return pyopencv_FastFeatureDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::FastFeatureDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::FastFeatureDetector> * dst_;
        if (pyopencv_FastFeatureDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::FastFeatureDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// Feature2D (Generic)
//================================================================================

// GetSet (Feature2D)



// Methods (Feature2D)

static Napi::Value pyopencv_cv_Feature2D_compute(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    Mat descriptors;

    const char* keywords[] = { "image", "keypoints", "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(image, keypoints, descriptors));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    UMat descriptors;

    const char* keywords[] = { "image", "keypoints", "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(image, keypoints, descriptors));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_images = NULL;
    vector_Mat images;
    Napi::Value* pyobj_keypoints = NULL;
    vector_vector_KeyPoint keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    vector_Mat descriptors;

    const char* keywords[] = { "images", "keypoints", "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_images, &pyobj_keypoints, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(images, keypoints, descriptors));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_images = NULL;
    vector_UMat images;
    Napi::Value* pyobj_keypoints = NULL;
    vector_vector_KeyPoint keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    vector_UMat descriptors;

    const char* keywords[] = { "images", "keypoints", "descriptors", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_images, &pyobj_keypoints, &pyobj_descriptors) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(images, keypoints, descriptors));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_defaultNorm(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->defaultNorm());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_descriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->descriptorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_descriptorType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->descriptorType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_detect(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "image", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Feature2D.detect", (char**)keywords, &pyobj_image, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, keypoints, mask));
        return jsopencv_from(keypoints);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "image", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Feature2D.detect", (char**)keywords, &pyobj_image, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, keypoints, mask));
        return jsopencv_from(keypoints);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_images = NULL;
    vector_Mat images;
    vector_vector_KeyPoint keypoints;
    Napi::Value* pyobj_masks = NULL;
    vector_Mat masks;

    const char* keywords[] = { "images", "masks", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Feature2D.detect", (char**)keywords, &pyobj_images, &pyobj_masks) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(images, keypoints, masks));
        return jsopencv_from(keypoints);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_images = NULL;
    vector_UMat images;
    vector_vector_KeyPoint keypoints;
    Napi::Value* pyobj_masks = NULL;
    vector_UMat masks;

    const char* keywords[] = { "images", "masks", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Feature2D.detect", (char**)keywords, &pyobj_images, &pyobj_masks) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)) &&
        jsopencv_to_safe(info, pyobj_masks, masks, ArgInfo("masks", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(images, keypoints, masks));
        return jsopencv_from(keypoints);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_detectAndCompute(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    Mat descriptors;
    Napi::Value* pyobj_useProvidedKeypoints = NULL;
    bool useProvidedKeypoints=false;

    const char* keywords[] = { "image", "mask", "descriptors", "useProvidedKeypoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:Feature2D.detectAndCompute", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_descriptors, &pyobj_useProvidedKeypoints) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) &&
        jsopencv_to_safe(info, pyobj_useProvidedKeypoints, useProvidedKeypoints, ArgInfo("useProvidedKeypoints", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectAndCompute(image, mask, keypoints, descriptors, useProvidedKeypoints));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    UMat descriptors;
    Napi::Value* pyobj_useProvidedKeypoints = NULL;
    bool useProvidedKeypoints=false;

    const char* keywords[] = { "image", "mask", "descriptors", "useProvidedKeypoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:Feature2D.detectAndCompute", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_descriptors, &pyobj_useProvidedKeypoints) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) &&
        jsopencv_to_safe(info, pyobj_useProvidedKeypoints, useProvidedKeypoints, ArgInfo("useProvidedKeypoints", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectAndCompute(image, mask, keypoints, descriptors, useProvidedKeypoints));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectAndCompute");

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_empty(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_read(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Feature2D.read", (char**)keywords, &pyobj_fileName) &&
        jsopencv_to_safe(info, pyobj_fileName, fileName, ArgInfo("fileName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(fileName));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arg1 = NULL;
    cv::FileNode arg1;

    const char* keywords[] = { "arg1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Feature2D.read", (char**)keywords, &pyobj_arg1) &&
        jsopencv_to_safe(info, pyobj_arg1, arg1, ArgInfo("arg1", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(arg1));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("read");

    return NULL;
}

static Napi::Value pyopencv_cv_Feature2D_write(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Feature2D> * self1 = 0;
    if (!pyopencv_Feature2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    Ptr<cv::Feature2D> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Feature2D.write", (char**)keywords, &pyobj_fileName) &&
        jsopencv_to_safe(info, pyobj_fileName, fileName, ArgInfo("fileName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(fileName));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:Feature2D.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(*fs, name));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("write");

    return NULL;
}



// Tables (Feature2D)

static PyGetSetDef pyopencv_Feature2D_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_Feature2D_methods[] =
{
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_compute, 0), "compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n.   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n.       (second variant).\n.   \n.       @param image Image.\n.       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n.       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n.       with several dominant orientations (for each orientation).\n.       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n.       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n.       descriptor for keypoint j-th keypoint.\n\n\n\ncompute(images, keypoints[, descriptors]) -> keypoints, descriptors\n.   @overload\n.   \n.       @param images Image set.\n.       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n.       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n.       with several dominant orientations (for each orientation).\n.       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n.       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n.       descriptor for keypoint j-th keypoint."},
    {"defaultNorm", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_defaultNorm, 0), "defaultNorm() -> retval\n."},
    {"descriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_descriptorSize, 0), "descriptorSize() -> retval\n."},
    {"descriptorType", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_descriptorType, 0), "descriptorType() -> retval\n."},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_detect, 0), "detect(image[, mask]) -> keypoints\n.   @brief Detects keypoints in an image (first variant) or image set (second variant).\n.   \n.       @param image Image.\n.       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n.       of keypoints detected in images[i] .\n.       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n.       matrix with non-zero values in the region of interest.\n\n\n\ndetect(images[, masks]) -> keypoints\n.   @overload\n.       @param images Image set.\n.       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n.       of keypoints detected in images[i] .\n.       @param masks Masks for each input image specifying where to look for keypoints (optional).\n.       masks[i] is a mask for images[i]."},
    {"detectAndCompute", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_detectAndCompute, 0), "detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n.   Detects keypoints and computes the descriptors"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_empty, 0), "empty() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_read, 0), "read(fileName) -> None\n.   \n\n\n\nread(arg1) -> None\n."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_Feature2D_write, 0), "write(fileName) -> None\n.   \n\n\n\nwrite(fs, name) -> None\n."},

    {NULL,          NULL}
};

// Converter (Feature2D)

template<>
struct PyOpenCV_Converter< Ptr<cv::Feature2D> >
{
    static PyObject* from(const Ptr<cv::Feature2D>& r)
    {
        return pyopencv_Feature2D_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::Feature2D>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::Feature2D> * dst_;
        if (pyopencv_Feature2D_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::Feature2D> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FileNode (Generic)
//================================================================================

// GetSet (FileNode)



// Methods (FileNode)

static int pyopencv_cv_FileNode_FileNode(pyopencv_FileNode_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::FileNode());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_FileNode_at(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    Napi::Value* pyobj_i = NULL;
    int i=0;
    FileNode retval;

    const char* keywords[] = { "i", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:FileNode.at", (char**)keywords, &pyobj_i) &&
        jsopencv_to_safe(info, pyobj_i, i, ArgInfo("i", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->operator[](i));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_empty(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_getNode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    char* nodename=(char*)"";
    FileNode retval;

    const char* keywords[] = { "nodename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "s:FileNode.getNode", (char**)keywords, &nodename))
    {
        ERRWRAP2_NAPI(info, retval = _self_->operator[](nodename));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isInt(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isInt());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isMap(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isMap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isNamed(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isNamed());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isNone(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isNone());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isReal(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isReal());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isSeq(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isSeq());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_isString(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isString());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_keys(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    std::vector<String> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->keys());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_mat(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->mat());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_name(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    std::string retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->name());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_rawSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->rawSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_real(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->real());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_size(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->size());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_string(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    std::string retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->string());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileNode_type(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::FileNode * self1 = 0;
    if (!pyopencv_FileNode_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    cv::FileNode* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->type());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (FileNode)

static PyGetSetDef pyopencv_FileNode_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FileNode_methods[] =
{
    {"at", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_at, 0), "at(i) -> retval\n.   @overload\n.        @param i Index of an element in the sequence node."},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_empty, 0), "empty() -> retval\n."},
    {"getNode", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_getNode, 0), "getNode(nodename) -> retval\n.   @overload\n.        @param nodename Name of an element in the mapping node."},
    {"isInt", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isInt, 0), "isInt() -> retval\n."},
    {"isMap", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isMap, 0), "isMap() -> retval\n."},
    {"isNamed", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isNamed, 0), "isNamed() -> retval\n."},
    {"isNone", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isNone, 0), "isNone() -> retval\n."},
    {"isReal", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isReal, 0), "isReal() -> retval\n."},
    {"isSeq", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isSeq, 0), "isSeq() -> retval\n."},
    {"isString", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_isString, 0), "isString() -> retval\n."},
    {"keys", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_keys, 0), "keys() -> retval\n.   @brief Returns keys of a mapping node.\n.        @returns Keys of a mapping node."},
    {"mat", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_mat, 0), "mat() -> retval\n."},
    {"name", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_name, 0), "name() -> retval\n."},
    {"rawSize", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_rawSize, 0), "rawSize() -> retval\n."},
    {"real", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_real, 0), "real() -> retval\n.   Internal method used when reading FileStorage.\n.        Sets the type (int, real or string) and value of the previously created node."},
    {"size", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_size, 0), "size() -> retval\n."},
    {"string", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_string, 0), "string() -> retval\n."},
    {"type", CV_JS_FN_WITH_KW_(pyopencv_cv_FileNode_type, 0), "type() -> retval\n.   @brief Returns type of the node.\n.        @returns Type of the node. See FileNode::Type"},

    {NULL,          NULL}
};

// Converter (FileNode)

template<>
struct PyOpenCV_Converter< cv::FileNode >
{
    static PyObject* from(const cv::FileNode& r)
    {
        return pyopencv_FileNode_Instance(r);
    }
    static bool to(PyObject* src, cv::FileNode& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::FileNode * dst_;
        if (pyopencv_FileNode_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::FileNode for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FileStorage (Generic)
//================================================================================

// GetSet (FileStorage)



// Methods (FileStorage)

static int pyopencv_cv_FileStorage_FileStorage(pyopencv_FileStorage_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::FileStorage>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::FileStorage()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_encoding = NULL;
    String encoding;

    const char* keywords[] = { "filename", "flags", "encoding", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FileStorage", (char**)keywords, &pyobj_filename, &pyobj_flags, &pyobj_encoding) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_encoding, encoding, ArgInfo("encoding", 0)))
    {
        new (&(self->v)) Ptr<cv::FileStorage>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::FileStorage(filename, flags, encoding)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("FileStorage");

    return -1;
}

static Napi::Value pyopencv_cv_FileStorage_endWriteStruct(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->endWriteStruct());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_getFirstTopLevelNode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    FileNode retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFirstTopLevelNode());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_getFormat(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFormat());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_getNode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    char* nodename=(char*)"";
    FileNode retval;

    const char* keywords[] = { "nodename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "s:FileStorage.getNode", (char**)keywords, &nodename))
    {
        ERRWRAP2_NAPI(info, retval = _self_->operator[](nodename));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_isOpened(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isOpened());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_open(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_encoding = NULL;
    String encoding;
    bool retval;

    const char* keywords[] = { "filename", "flags", "encoding", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FileStorage.open", (char**)keywords, &pyobj_filename, &pyobj_flags, &pyobj_encoding) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_encoding, encoding, ArgInfo("encoding", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, flags, encoding));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_release(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_releaseAndGetString(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->releaseAndGetString());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_root(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    Napi::Value* pyobj_streamidx = NULL;
    int streamidx=0;
    FileNode retval;

    const char* keywords[] = { "streamidx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:FileStorage.root", (char**)keywords, &pyobj_streamidx) &&
        jsopencv_to_safe(info, pyobj_streamidx, streamidx, ArgInfo("streamidx", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->root(streamidx));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_startWriteStruct(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    Napi::Value* pyobj_name = NULL;
    String name;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_typeName = NULL;
    String typeName;

    const char* keywords[] = { "name", "flags", "typeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:FileStorage.startWriteStruct", (char**)keywords, &pyobj_name, &pyobj_flags, &pyobj_typeName) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_typeName, typeName, ArgInfo("typeName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->startWriteStruct(name, flags, typeName));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_write(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(5);

    {
    Napi::Value* pyobj_name = NULL;
    String name;
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "name", "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(name, val));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_name = NULL;
    String name;
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "name", "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(name, val));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_name = NULL;
    String name;
    Napi::Value* pyobj_val = NULL;
    String val;

    const char* keywords[] = { "name", "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(name, val));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_name = NULL;
    String name;
    Napi::Value* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "name", "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(name, val));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_name = NULL;
    String name;
    Napi::Value* pyobj_val = NULL;
    vector_String val;

    const char* keywords[] = { "name", "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(name, val));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("write");

    return NULL;
}

static Napi::Value pyopencv_cv_FileStorage_writeComment(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::FileStorage> * self1 = 0;
    if (!pyopencv_FileStorage_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    Ptr<cv::FileStorage> _self_ = *(self1);
    Napi::Value* pyobj_comment = NULL;
    String comment;
    Napi::Value* pyobj_append = NULL;
    bool append=false;

    const char* keywords[] = { "comment", "append", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:FileStorage.writeComment", (char**)keywords, &pyobj_comment, &pyobj_append) &&
        jsopencv_to_safe(info, pyobj_comment, comment, ArgInfo("comment", 0)) &&
        jsopencv_to_safe(info, pyobj_append, append, ArgInfo("append", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->writeComment(comment, append));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (FileStorage)

static PyGetSetDef pyopencv_FileStorage_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FileStorage_methods[] =
{
    {"endWriteStruct", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_endWriteStruct, 0), "endWriteStruct() -> None\n.   @brief Finishes writing nested structure (should pair startWriteStruct())"},
    {"getFirstTopLevelNode", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_getFirstTopLevelNode, 0), "getFirstTopLevelNode() -> retval\n.   @brief Returns the first element of the top-level mapping.\n.        @returns The first element of the top-level mapping."},
    {"getFormat", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_getFormat, 0), "getFormat() -> retval\n.   @brief Returns the current format.\n.        * @returns The current format, see FileStorage::Mode"},
    {"getNode", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_getNode, 0), "getNode(nodename) -> retval\n.   @overload"},
    {"isOpened", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_isOpened, 0), "isOpened() -> retval\n.   @brief Checks whether the file is opened.\n.   \n.        @returns true if the object is associated with the current file and false otherwise. It is a\n.        good practice to call this method after you tried to open a file."},
    {"open", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_open, 0), "open(filename, flags[, encoding]) -> retval\n.   @brief Opens a file.\n.   \n.        See description of parameters in FileStorage::FileStorage. The method calls FileStorage::release\n.        before opening the file.\n.        @param filename Name of the file to open or the text string to read the data from.\n.        Extension of the file (.xml, .yml/.yaml or .json) determines its format (XML, YAML or JSON\n.        respectively). Also you can append .gz to work with compressed files, for example myHugeMatrix.xml.gz. If both\n.        FileStorage::WRITE and FileStorage::MEMORY flags are specified, source is used just to specify\n.        the output file format (e.g. mydata.xml, .yml etc.). A file name can also contain parameters.\n.        You can use this format, \"*?base64\" (e.g. \"file.json?base64\" (case sensitive)), as an alternative to\n.        FileStorage::BASE64 flag.\n.        @param flags Mode of operation. One of FileStorage::Mode\n.        @param encoding Encoding of the file. Note that UTF-16 XML encoding is not supported currently and\n.        you should use 8-bit encoding instead of it."},
    {"release", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_release, 0), "release() -> None\n.   @brief Closes the file and releases all the memory buffers.\n.   \n.        Call this method after all I/O operations with the storage are finished."},
    {"releaseAndGetString", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_releaseAndGetString, 0), "releaseAndGetString() -> retval\n.   @brief Closes the file and releases all the memory buffers.\n.   \n.        Call this method after all I/O operations with the storage are finished. If the storage was\n.        opened for writing data and FileStorage::WRITE was specified"},
    {"root", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_root, 0), "root([, streamidx]) -> retval\n.   @brief Returns the top-level mapping\n.        @param streamidx Zero-based index of the stream. In most cases there is only one stream in the file.\n.        However, YAML supports multiple streams and so there can be several.\n.        @returns The top-level mapping."},
    {"startWriteStruct", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_startWriteStruct, 0), "startWriteStruct(name, flags[, typeName]) -> None\n.   @brief Starts to write a nested structure (sequence or a mapping).\n.       @param name name of the structure. When writing to sequences (a.k.a. \"arrays\"), pass an empty string.\n.       @param flags type of the structure (FileNode::MAP or FileNode::SEQ (both with optional FileNode::FLOW)).\n.       @param typeName optional name of the type you store. The effect of setting this depends on the storage format.\n.       I.e. if the format has a specification for storing type information, this parameter is used."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_write, 0), "write(name, val) -> None\n.   * @brief Simplified writing API to use with bindings.\n.        * @param name Name of the written object. When writing to sequences (a.k.a. \"arrays\"), pass an empty string.\n.        * @param val Value of the written object."},
    {"writeComment", CV_JS_FN_WITH_KW_(pyopencv_cv_FileStorage_writeComment, 0), "writeComment(comment[, append]) -> None\n.   @brief Writes a comment.\n.   \n.        The function writes a comment into file storage. The comments are skipped when the storage is read.\n.        @param comment The written comment, single-line or multi-line\n.        @param append If true, the function tries to put the comment at the end of current line.\n.        Else if the comment is multi-line, or if it does not fit at the end of the current\n.        line, the comment starts a new line."},

    {NULL,          NULL}
};

// Converter (FileStorage)

template<>
struct PyOpenCV_Converter< Ptr<cv::FileStorage> >
{
    static PyObject* from(const Ptr<cv::FileStorage>& r)
    {
        return pyopencv_FileStorage_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::FileStorage>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::FileStorage> * dst_;
        if (pyopencv_FileStorage_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::FileStorage> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// FlannBasedMatcher (Generic)
//================================================================================

// GetSet (FlannBasedMatcher)



// Methods (FlannBasedMatcher)

static int pyopencv_cv_FlannBasedMatcher_FlannBasedMatcher(pyopencv_FlannBasedMatcher_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    Napi::Value* pyobj_indexParams = NULL;
    Ptr<flann::IndexParams> indexParams=makePtr<flann::KDTreeIndexParams>();
    Napi::Value* pyobj_searchParams = NULL;
    Ptr<flann::SearchParams> searchParams=makePtr<flann::SearchParams>();

    const char* keywords[] = { "indexParams", "searchParams", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:FlannBasedMatcher", (char**)keywords, &pyobj_indexParams, &pyobj_searchParams) &&
        jsopencv_to_safe(info, pyobj_indexParams, indexParams, ArgInfo("indexParams", 0)) &&
        jsopencv_to_safe(info, pyobj_searchParams, searchParams, ArgInfo("searchParams", 0)))
    {
        new (&(self->v)) Ptr<cv::FlannBasedMatcher>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::FlannBasedMatcher(indexParams, searchParams)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_FlannBasedMatcher_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Ptr<FlannBasedMatcher> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::FlannBasedMatcher::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (FlannBasedMatcher)

static PyGetSetDef pyopencv_FlannBasedMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_FlannBasedMatcher_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_FlannBasedMatcher_create_static, METH_STATIC), "create() -> retval\n."},

    {NULL,          NULL}
};

// Converter (FlannBasedMatcher)

template<>
struct PyOpenCV_Converter< Ptr<cv::FlannBasedMatcher> >
{
    static PyObject* from(const Ptr<cv::FlannBasedMatcher>& r)
    {
        return pyopencv_FlannBasedMatcher_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::FlannBasedMatcher>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::FlannBasedMatcher> * dst_;
        if (pyopencv_FlannBasedMatcher_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::FlannBasedMatcher> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GArrayDesc (Generic)
//================================================================================

// GetSet (GArrayDesc)



// Methods (GArrayDesc)



// Tables (GArrayDesc)

static PyGetSetDef pyopencv_GArrayDesc_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GArrayDesc_methods[] =
{

    {NULL,          NULL}
};

// Converter (GArrayDesc)

template<>
struct PyOpenCV_Converter< cv::GArrayDesc >
{
    static PyObject* from(const cv::GArrayDesc& r)
    {
        return pyopencv_GArrayDesc_Instance(r);
    }
    static bool to(PyObject* src, cv::GArrayDesc& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GArrayDesc * dst_;
        if (pyopencv_GArrayDesc_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GArrayDesc for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GArrayT (Generic)
//================================================================================

// GetSet (GArrayT)



// Methods (GArrayT)

static int pyopencv_cv_GArrayT_GArrayT(pyopencv_GArrayT_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    Napi::Value* pyobj_type = NULL;
    gapi_ArgType type=static_cast<gapi_ArgType>(0);

    const char* keywords[] = { "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GArrayT", (char**)keywords, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GArrayT(type));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GArrayT_type(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GArrayT * self1 = 0;
    if (!pyopencv_GArrayT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GArrayT' or its derivative)");
    cv::GArrayT* _self_ = (self1);
    gapi::ArgType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->type());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (GArrayT)

static PyGetSetDef pyopencv_GArrayT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GArrayT_methods[] =
{
    {"type", CV_JS_FN_WITH_KW_(pyopencv_cv_GArrayT_type, 0), "type() -> retval\n."},

    {NULL,          NULL}
};

// Converter (GArrayT)

template<>
struct PyOpenCV_Converter< cv::GArrayT >
{
    static PyObject* from(const cv::GArrayT& r)
    {
        return pyopencv_GArrayT_Instance(r);
    }
    static bool to(PyObject* src, cv::GArrayT& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GArrayT * dst_;
        if (pyopencv_GArrayT_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GArrayT for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GCompileArg (Generic)
//================================================================================

// GetSet (GCompileArg)



// Methods (GCompileArg)

static int pyopencv_cv_GCompileArg_GCompileArg(pyopencv_GCompileArg_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_arg = NULL;
    cv::GKernelPackage arg;

    const char* keywords[] = { "arg", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GCompileArg", (char**)keywords, &pyobj_arg) &&
        jsopencv_to_safe(info, pyobj_arg, arg, ArgInfo("arg", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GCompileArg(arg));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arg = NULL;
    cv::gapi::GNetPackage arg;

    const char* keywords[] = { "arg", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GCompileArg", (char**)keywords, &pyobj_arg) &&
        jsopencv_to_safe(info, pyobj_arg, arg, ArgInfo("arg", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GCompileArg(arg));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arg = NULL;
    cv::gapi::streaming::queue_capacity arg;

    const char* keywords[] = { "arg", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GCompileArg", (char**)keywords, &pyobj_arg) &&
        jsopencv_to_safe(info, pyobj_arg, arg, ArgInfo("arg", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GCompileArg(arg));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GCompileArg");

    return -1;
}



// Tables (GCompileArg)

static PyGetSetDef pyopencv_GCompileArg_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GCompileArg_methods[] =
{

    {NULL,          NULL}
};

// Converter (GCompileArg)

template<>
struct PyOpenCV_Converter< cv::GCompileArg >
{
    static PyObject* from(const cv::GCompileArg& r)
    {
        return pyopencv_GCompileArg_Instance(r);
    }
    static bool to(PyObject* src, cv::GCompileArg& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GCompileArg * dst_;
        if (pyopencv_GCompileArg_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GCompileArg for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GComputation (Generic)
//================================================================================

// GetSet (GComputation)



// Methods (GComputation)

static int pyopencv_cv_GComputation_GComputation(pyopencv_GComputation_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_ins = NULL;
    GProtoInputArgs ins;
    Napi::Value* pyobj_outs = NULL;
    GProtoOutputArgs outs;

    const char* keywords[] = { "ins", "outs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GComputation", (char**)keywords, &pyobj_ins, &pyobj_outs) &&
        jsopencv_to_safe(info, pyobj_ins, ins, ArgInfo("ins", 0)) &&
        jsopencv_to_safe(info, pyobj_outs, outs, ArgInfo("outs", 0)))
    {
        new (&(self->v)) Ptr<cv::GComputation>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::GComputation(std::move(ins), std::move(outs))));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_in = NULL;
    cv::GMat in;
    Napi::Value* pyobj_out = NULL;
    cv::GMat out;

    const char* keywords[] = { "in_", "out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GComputation", (char**)keywords, &pyobj_in, &pyobj_out) &&
        jsopencv_to_safe(info, pyobj_in, in, ArgInfo("in", 0)) &&
        jsopencv_to_safe(info, pyobj_out, out, ArgInfo("out", 0)))
    {
        new (&(self->v)) Ptr<cv::GComputation>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::GComputation(in, out)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_in = NULL;
    cv::GMat in;
    Napi::Value* pyobj_out = NULL;
    cv::GScalar out;

    const char* keywords[] = { "in_", "out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GComputation", (char**)keywords, &pyobj_in, &pyobj_out) &&
        jsopencv_to_safe(info, pyobj_in, in, ArgInfo("in", 0)) &&
        jsopencv_to_safe(info, pyobj_out, out, ArgInfo("out", 0)))
    {
        new (&(self->v)) Ptr<cv::GComputation>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::GComputation(in, out)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_in1 = NULL;
    cv::GMat in1;
    Napi::Value* pyobj_in2 = NULL;
    cv::GMat in2;
    Napi::Value* pyobj_out = NULL;
    cv::GMat out;

    const char* keywords[] = { "in1", "in2", "out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:GComputation", (char**)keywords, &pyobj_in1, &pyobj_in2, &pyobj_out) &&
        jsopencv_to_safe(info, pyobj_in1, in1, ArgInfo("in1", 0)) &&
        jsopencv_to_safe(info, pyobj_in2, in2, ArgInfo("in2", 0)) &&
        jsopencv_to_safe(info, pyobj_out, out, ArgInfo("out", 0)))
    {
        new (&(self->v)) Ptr<cv::GComputation>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::GComputation(in1, in2, out)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GComputation");

    return -1;
}

static Napi::Value pyopencv_cv_GComputation_apply(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GComputation> * self1 = 0;
    if (!pyopencv_GComputation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GComputation' or its derivative)");
    Ptr<cv::GComputation> _self_ = *(self1);
    Napi::Value* pyobj_callback = NULL;
    detail_ExtractArgsCallback callback;
    Napi::Value* pyobj_args = NULL;
    GCompileArgs args={};
    GRunArgs retval;

    const char* keywords[] = { "callback", "args", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GComputation.apply", (char**)keywords, &pyobj_callback, &pyobj_args) &&
        jsopencv_to_safe(info, pyobj_callback, callback, ArgInfo("callback", 0)) &&
        jsopencv_to_safe(info, pyobj_args, args, ArgInfo("args", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->apply(callback, std::move(args)));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GComputation_compileStreaming(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GComputation> * self1 = 0;
    if (!pyopencv_GComputation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GComputation' or its derivative)");
    Ptr<cv::GComputation> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_in_metas = NULL;
    GMetaArgs in_metas;
    Napi::Value* pyobj_args = NULL;
    GCompileArgs args={};
    GStreamingCompiled retval;

    const char* keywords[] = { "in_metas", "args", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GComputation.compileStreaming", (char**)keywords, &pyobj_in_metas, &pyobj_args) &&
        jsopencv_to_safe(info, pyobj_in_metas, in_metas, ArgInfo("in_metas", 0)) &&
        jsopencv_to_safe(info, pyobj_args, args, ArgInfo("args", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compileStreaming(std::move(in_metas), std::move(args)));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_args = NULL;
    GCompileArgs args={};
    GStreamingCompiled retval;

    const char* keywords[] = { "args", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:GComputation.compileStreaming", (char**)keywords, &pyobj_args) &&
        jsopencv_to_safe(info, pyobj_args, args, ArgInfo("args", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compileStreaming(std::move(args)));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_callback = NULL;
    detail_ExtractMetaCallback callback;
    Napi::Value* pyobj_args = NULL;
    GCompileArgs args={};
    GStreamingCompiled retval;

    const char* keywords[] = { "callback", "args", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GComputation.compileStreaming", (char**)keywords, &pyobj_callback, &pyobj_args) &&
        jsopencv_to_safe(info, pyobj_callback, callback, ArgInfo("callback", 0)) &&
        jsopencv_to_safe(info, pyobj_args, args, ArgInfo("args", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compileStreaming(callback, std::move(args)));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compileStreaming");

    return NULL;
}



// Tables (GComputation)

static PyGetSetDef pyopencv_GComputation_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GComputation_methods[] =
{
    {"apply", CV_JS_FN_WITH_KW_(pyopencv_cv_GComputation_apply, 0), "apply(callback[, args]) -> retval\n.   * @brief Compile graph on-the-fly and immediately execute it on\n.        * the inputs data vectors.\n.        *\n.        * Number of input/output data objects must match GComputation's\n.        * protocol, also types of host data objects (cv::Mat, cv::Scalar)\n.        * must match the shapes of data objects from protocol (cv::GMat,\n.        * cv::GScalar). If there's a mismatch, a run-time exception will\n.        * be generated.\n.        *\n.        * Internally, a cv::GCompiled object is created for the given\n.        * input format configuration, which then is executed on the input\n.        * data immediately. cv::GComputation caches compiled objects\n.        * produced within apply() -- if this method would be called next\n.        * time with the same input parameters (image formats, image\n.        * resolution, etc), the underlying compiled graph will be reused\n.        * without recompilation. If new metadata doesn't match the cached\n.        * one, the underlying compiled graph is regenerated.\n.        *\n.        * @note compile() always triggers a compilation process and\n.        * produces a new GCompiled object regardless if a similar one has\n.        * been cached via apply() or not.\n.        *\n.        * @param ins vector of input data to process. Don't create\n.        * GRunArgs object manually, use cv::gin() wrapper instead.\n.        * @param outs vector of output data to fill results in. cv::Mat\n.        * objects may be empty in this vector, G-API will automatically\n.        * initialize it with the required format & dimensions. Don't\n.        * create GRunArgsP object manually, use cv::gout() wrapper instead.\n.        * @param args a list of compilation arguments to pass to the\n.        * underlying compilation process. Don't create GCompileArgs\n.        * object manually, use cv::compile_args() wrapper instead.\n.        *\n.        * @sa @ref gapi_data_objects, @ref gapi_compile_args"},
    {"compileStreaming", CV_JS_FN_WITH_KW_(pyopencv_cv_GComputation_compileStreaming, 0), "compileStreaming(in_metas[, args]) -> retval\n.   * @brief Compile the computation for streaming mode.\n.        *\n.        * This method triggers compilation process and produces a new\n.        * GStreamingCompiled object which then can process video stream\n.        * data of the given format. Passing a stream in a different\n.        * format to the compiled computation will generate a run-time\n.        * exception.\n.        *\n.        * @param in_metas vector of input metadata configuration. Grab\n.        * metadata from real data objects (like cv::Mat or cv::Scalar)\n.        * using cv::descr_of(), or create it on your own.\n.        *\n.        * @param args compilation arguments for this compilation\n.        * process. Compilation arguments directly affect what kind of\n.        * executable object would be produced, e.g. which kernels (and\n.        * thus, devices) would be used to execute computation.\n.        *\n.        * @return GStreamingCompiled, a streaming-oriented executable\n.        * computation compiled specifically for the given input\n.        * parameters.\n.        *\n.        * @sa @ref gapi_compile_args\n\n\n\ncompileStreaming([, args]) -> retval\n.   * @brief Compile the computation for streaming mode.\n.        *\n.        * This method triggers compilation process and produces a new\n.        * GStreamingCompiled object which then can process video stream\n.        * data in any format. Underlying mechanisms will be adjusted to\n.        * every new input video stream automatically, but please note that\n.        * _not all_ existing backends support this (see reshape()).\n.        *\n.        * @param args compilation arguments for this compilation\n.        * process. Compilation arguments directly affect what kind of\n.        * executable object would be produced, e.g. which kernels (and\n.        * thus, devices) would be used to execute computation.\n.        *\n.        * @return GStreamingCompiled, a streaming-oriented executable\n.        * computation compiled for any input image format.\n.        *\n.        * @sa @ref gapi_compile_args\n\n\n\ncompileStreaming(callback[, args]) -> retval\n."},

    {NULL,          NULL}
};

// Converter (GComputation)

template<>
struct PyOpenCV_Converter< Ptr<cv::GComputation> >
{
    static PyObject* from(const Ptr<cv::GComputation>& r)
    {
        return pyopencv_GComputation_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::GComputation>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::GComputation> * dst_;
        if (pyopencv_GComputation_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::GComputation> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GFTTDetector (Generic)
//================================================================================

// GetSet (GFTTDetector)



// Methods (GFTTDetector)

static Napi::Value pyopencv_cv_GFTTDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_maxCorners = NULL;
    int maxCorners=1000;
    Napi::Value* pyobj_qualityLevel = NULL;
    double qualityLevel=0.01;
    Napi::Value* pyobj_minDistance = NULL;
    double minDistance=1;
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=3;
    Napi::Value* pyobj_useHarrisDetector = NULL;
    bool useHarrisDetector=false;
    Napi::Value* pyobj_k = NULL;
    double k=0.04;
    Ptr<GFTTDetector> retval;

    const char* keywords[] = { "maxCorners", "qualityLevel", "minDistance", "blockSize", "useHarrisDetector", "k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOO:GFTTDetector.create", (char**)keywords, &pyobj_maxCorners, &pyobj_qualityLevel, &pyobj_minDistance, &pyobj_blockSize, &pyobj_useHarrisDetector, &pyobj_k) &&
        jsopencv_to_safe(info, pyobj_maxCorners, maxCorners, ArgInfo("maxCorners", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityLevel, qualityLevel, ArgInfo("qualityLevel", 0)) &&
        jsopencv_to_safe(info, pyobj_minDistance, minDistance, ArgInfo("minDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)) &&
        jsopencv_to_safe(info, pyobj_useHarrisDetector, useHarrisDetector, ArgInfo("useHarrisDetector", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::GFTTDetector::create(maxCorners, qualityLevel, minDistance, blockSize, useHarrisDetector, k));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_maxCorners = NULL;
    int maxCorners=0;
    Napi::Value* pyobj_qualityLevel = NULL;
    double qualityLevel=0;
    Napi::Value* pyobj_minDistance = NULL;
    double minDistance=0;
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=0;
    Napi::Value* pyobj_gradiantSize = NULL;
    int gradiantSize=0;
    Napi::Value* pyobj_useHarrisDetector = NULL;
    bool useHarrisDetector=false;
    Napi::Value* pyobj_k = NULL;
    double k=0.04;
    Ptr<GFTTDetector> retval;

    const char* keywords[] = { "maxCorners", "qualityLevel", "minDistance", "blockSize", "gradiantSize", "useHarrisDetector", "k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|OO:GFTTDetector.create", (char**)keywords, &pyobj_maxCorners, &pyobj_qualityLevel, &pyobj_minDistance, &pyobj_blockSize, &pyobj_gradiantSize, &pyobj_useHarrisDetector, &pyobj_k) &&
        jsopencv_to_safe(info, pyobj_maxCorners, maxCorners, ArgInfo("maxCorners", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityLevel, qualityLevel, ArgInfo("qualityLevel", 0)) &&
        jsopencv_to_safe(info, pyobj_minDistance, minDistance, ArgInfo("minDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)) &&
        jsopencv_to_safe(info, pyobj_gradiantSize, gradiantSize, ArgInfo("gradiantSize", 0)) &&
        jsopencv_to_safe(info, pyobj_useHarrisDetector, useHarrisDetector, ArgInfo("useHarrisDetector", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::GFTTDetector::create(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize, useHarrisDetector, k));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getBlockSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBlockSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getGradientSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGradientSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getHarrisDetector(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getHarrisDetector());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getK(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getK());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getMaxFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxFeatures());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getMinDistance(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinDistance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_getQualityLevel(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getQualityLevel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setBlockSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=0;

    const char* keywords[] = { "blockSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setBlockSize", (char**)keywords, &pyobj_blockSize) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBlockSize(blockSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setGradientSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_gradientSize_ = NULL;
    int gradientSize_=0;

    const char* keywords[] = { "gradientSize_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setGradientSize", (char**)keywords, &pyobj_gradientSize_) &&
        jsopencv_to_safe(info, pyobj_gradientSize_, gradientSize_, ArgInfo("gradientSize_", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGradientSize(gradientSize_));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setHarrisDetector(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setHarrisDetector", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setHarrisDetector(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setK(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_k = NULL;
    double k=0;

    const char* keywords[] = { "k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setK", (char**)keywords, &pyobj_k) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setK(k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setMaxFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_maxFeatures = NULL;
    int maxFeatures=0;

    const char* keywords[] = { "maxFeatures", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setMaxFeatures", (char**)keywords, &pyobj_maxFeatures) &&
        jsopencv_to_safe(info, pyobj_maxFeatures, maxFeatures, ArgInfo("maxFeatures", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxFeatures(maxFeatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setMinDistance(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_minDistance = NULL;
    double minDistance=0;

    const char* keywords[] = { "minDistance", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setMinDistance", (char**)keywords, &pyobj_minDistance) &&
        jsopencv_to_safe(info, pyobj_minDistance, minDistance, ArgInfo("minDistance", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinDistance(minDistance));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GFTTDetector_setQualityLevel(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GFTTDetector> * self1 = 0;
    if (!pyopencv_GFTTDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    Ptr<cv::GFTTDetector> _self_ = *(self1);
    Napi::Value* pyobj_qlevel = NULL;
    double qlevel=0;

    const char* keywords[] = { "qlevel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GFTTDetector.setQualityLevel", (char**)keywords, &pyobj_qlevel) &&
        jsopencv_to_safe(info, pyobj_qlevel, qlevel, ArgInfo("qlevel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setQualityLevel(qlevel));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (GFTTDetector)

static PyGetSetDef pyopencv_GFTTDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GFTTDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_create_static, METH_STATIC), "create([, maxCorners[, qualityLevel[, minDistance[, blockSize[, useHarrisDetector[, k]]]]]]) -> retval\n.   \n\n\n\ncreate(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize[, useHarrisDetector[, k]]) -> retval\n."},
    {"getBlockSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getBlockSize, 0), "getBlockSize() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getGradientSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getGradientSize, 0), "getGradientSize() -> retval\n."},
    {"getHarrisDetector", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getHarrisDetector, 0), "getHarrisDetector() -> retval\n."},
    {"getK", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getK, 0), "getK() -> retval\n."},
    {"getMaxFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getMaxFeatures, 0), "getMaxFeatures() -> retval\n."},
    {"getMinDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getMinDistance, 0), "getMinDistance() -> retval\n."},
    {"getQualityLevel", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getQualityLevel, 0), "getQualityLevel() -> retval\n."},
    {"setBlockSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setBlockSize, 0), "setBlockSize(blockSize) -> None\n."},
    {"setGradientSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setGradientSize, 0), "setGradientSize(gradientSize_) -> None\n."},
    {"setHarrisDetector", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setHarrisDetector, 0), "setHarrisDetector(val) -> None\n."},
    {"setK", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setK, 0), "setK(k) -> None\n."},
    {"setMaxFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setMaxFeatures, 0), "setMaxFeatures(maxFeatures) -> None\n."},
    {"setMinDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setMinDistance, 0), "setMinDistance(minDistance) -> None\n."},
    {"setQualityLevel", CV_JS_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setQualityLevel, 0), "setQualityLevel(qlevel) -> None\n."},

    {NULL,          NULL}
};

// Converter (GFTTDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::GFTTDetector> >
{
    static PyObject* from(const Ptr<cv::GFTTDetector>& r)
    {
        return pyopencv_GFTTDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::GFTTDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::GFTTDetector> * dst_;
        if (pyopencv_GFTTDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::GFTTDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GFrame (Generic)
//================================================================================

// GetSet (GFrame)



// Methods (GFrame)

static int pyopencv_cv_GFrame_GFrame(pyopencv_GFrame_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GFrame());
        return 0;
    }

    return -1;
}



// Tables (GFrame)

static PyGetSetDef pyopencv_GFrame_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GFrame_methods[] =
{

    {NULL,          NULL}
};

// Converter (GFrame)

template<>
struct PyOpenCV_Converter< cv::GFrame >
{
    static PyObject* from(const cv::GFrame& r)
    {
        return pyopencv_GFrame_Instance(r);
    }
    static bool to(PyObject* src, cv::GFrame& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GFrame * dst_;
        if (pyopencv_GFrame_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GFrame for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GInferInputs (Generic)
//================================================================================

// GetSet (GInferInputs)



// Methods (GInferInputs)

static int pyopencv_cv_GInferInputs_GInferInputs(pyopencv_GInferInputs_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GInferInputs());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GInferInputs_setInput(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GInferInputs * self1 = 0;
    if (!pyopencv_GInferInputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GInferInputs' or its derivative)");
    cv::GInferInputs* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_name = NULL;
    std::string name;
    Napi::Value* pyobj_value = NULL;
    cv::GMat value;
    GInferInputs retval;

    const char* keywords[] = { "name", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GInferInputs.setInput", (char**)keywords, &pyobj_name, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInput(name, value));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_name = NULL;
    std::string name;
    Napi::Value* pyobj_value = NULL;
    cv::GFrame value;
    GInferInputs retval;

    const char* keywords[] = { "name", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GInferInputs.setInput", (char**)keywords, &pyobj_name, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInput(name, value));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setInput");

    return NULL;
}



// Tables (GInferInputs)

static PyGetSetDef pyopencv_GInferInputs_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GInferInputs_methods[] =
{
    {"setInput", CV_JS_FN_WITH_KW_(pyopencv_cv_GInferInputs_setInput, 0), "setInput(name, value) -> retval\n."},

    {NULL,          NULL}
};

// Converter (GInferInputs)

template<>
struct PyOpenCV_Converter< cv::GInferInputs >
{
    static PyObject* from(const cv::GInferInputs& r)
    {
        return pyopencv_GInferInputs_Instance(r);
    }
    static bool to(PyObject* src, cv::GInferInputs& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GInferInputs * dst_;
        if (pyopencv_GInferInputs_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GInferInputs for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GInferListInputs (Generic)
//================================================================================

// GetSet (GInferListInputs)



// Methods (GInferListInputs)

static int pyopencv_cv_GInferListInputs_GInferListInputs(pyopencv_GInferListInputs_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GInferListInputs());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GInferListInputs_setInput(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GInferListInputs * self1 = 0;
    if (!pyopencv_GInferListInputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GInferListInputs' or its derivative)");
    cv::GInferListInputs* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_name = NULL;
    std::string name;
    Napi::Value* pyobj_value = NULL;
    GArray_GMat value;
    GInferListInputs retval;

    const char* keywords[] = { "name", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GInferListInputs.setInput", (char**)keywords, &pyobj_name, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInput(name, value));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_name = NULL;
    std::string name;
    Napi::Value* pyobj_value = NULL;
    GArray_Rect value;
    GInferListInputs retval;

    const char* keywords[] = { "name", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GInferListInputs.setInput", (char**)keywords, &pyobj_name, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInput(name, value));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setInput");

    return NULL;
}



// Tables (GInferListInputs)

static PyGetSetDef pyopencv_GInferListInputs_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GInferListInputs_methods[] =
{
    {"setInput", CV_JS_FN_WITH_KW_(pyopencv_cv_GInferListInputs_setInput, 0), "setInput(name, value) -> retval\n."},

    {NULL,          NULL}
};

// Converter (GInferListInputs)

template<>
struct PyOpenCV_Converter< cv::GInferListInputs >
{
    static PyObject* from(const cv::GInferListInputs& r)
    {
        return pyopencv_GInferListInputs_Instance(r);
    }
    static bool to(PyObject* src, cv::GInferListInputs& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GInferListInputs * dst_;
        if (pyopencv_GInferListInputs_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GInferListInputs for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GInferListOutputs (Generic)
//================================================================================

// GetSet (GInferListOutputs)



// Methods (GInferListOutputs)

static int pyopencv_cv_GInferListOutputs_GInferListOutputs(pyopencv_GInferListOutputs_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GInferListOutputs());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GInferListOutputs_at(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GInferListOutputs * self1 = 0;
    if (!pyopencv_GInferListOutputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GInferListOutputs' or its derivative)");
    cv::GInferListOutputs* _self_ = (self1);
    Napi::Value* pyobj_name = NULL;
    std::string name;
    cv::GArray<cv::GMat> retval;

    const char* keywords[] = { "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GInferListOutputs.at", (char**)keywords, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->at(name));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (GInferListOutputs)

static PyGetSetDef pyopencv_GInferListOutputs_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GInferListOutputs_methods[] =
{
    {"at", CV_JS_FN_WITH_KW_(pyopencv_cv_GInferListOutputs_at, 0), "at(name) -> retval\n."},

    {NULL,          NULL}
};

// Converter (GInferListOutputs)

template<>
struct PyOpenCV_Converter< cv::GInferListOutputs >
{
    static PyObject* from(const cv::GInferListOutputs& r)
    {
        return pyopencv_GInferListOutputs_Instance(r);
    }
    static bool to(PyObject* src, cv::GInferListOutputs& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GInferListOutputs * dst_;
        if (pyopencv_GInferListOutputs_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GInferListOutputs for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GInferOutputs (Generic)
//================================================================================

// GetSet (GInferOutputs)



// Methods (GInferOutputs)

static int pyopencv_cv_GInferOutputs_GInferOutputs(pyopencv_GInferOutputs_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GInferOutputs());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GInferOutputs_at(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GInferOutputs * self1 = 0;
    if (!pyopencv_GInferOutputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GInferOutputs' or its derivative)");
    cv::GInferOutputs* _self_ = (self1);
    Napi::Value* pyobj_name = NULL;
    std::string name;
    cv::GMat retval;

    const char* keywords[] = { "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GInferOutputs.at", (char**)keywords, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->at(name));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (GInferOutputs)

static PyGetSetDef pyopencv_GInferOutputs_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GInferOutputs_methods[] =
{
    {"at", CV_JS_FN_WITH_KW_(pyopencv_cv_GInferOutputs_at, 0), "at(name) -> retval\n."},

    {NULL,          NULL}
};

// Converter (GInferOutputs)

template<>
struct PyOpenCV_Converter< cv::GInferOutputs >
{
    static PyObject* from(const cv::GInferOutputs& r)
    {
        return pyopencv_GInferOutputs_Instance(r);
    }
    static bool to(PyObject* src, cv::GInferOutputs& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GInferOutputs * dst_;
        if (pyopencv_GInferOutputs_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GInferOutputs for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GKernelPackage (Generic)
//================================================================================

// GetSet (GKernelPackage)



// Methods (GKernelPackage)



// Tables (GKernelPackage)

static PyGetSetDef pyopencv_GKernelPackage_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GKernelPackage_methods[] =
{

    {NULL,          NULL}
};

// Converter (GKernelPackage)

template<>
struct PyOpenCV_Converter< cv::GKernelPackage >
{
    static PyObject* from(const cv::GKernelPackage& r)
    {
        return pyopencv_GKernelPackage_Instance(r);
    }
    static bool to(PyObject* src, cv::GKernelPackage& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GKernelPackage * dst_;
        if (pyopencv_GKernelPackage_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GKernelPackage for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GMat (Generic)
//================================================================================

// GetSet (GMat)



// Methods (GMat)

static int pyopencv_cv_GMat_GMat(pyopencv_GMat_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GMat());
        return 0;
    }

    return -1;
}



// Tables (GMat)

static PyGetSetDef pyopencv_GMat_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GMat_methods[] =
{

    {NULL,          NULL}
};

// Converter (GMat)

template<>
struct PyOpenCV_Converter< cv::GMat >
{
    static PyObject* from(const cv::GMat& r)
    {
        return pyopencv_GMat_Instance(r);
    }
    static bool to(PyObject* src, cv::GMat& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GMat * dst_;
        if (pyopencv_GMat_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GMat for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GMatDesc (Generic)
//================================================================================

// GetSet (GMatDesc)


static PyObject* pyopencv_GMatDesc_get_chan(pyopencv_GMatDesc_t* p, void *closure)
{
    return jsopencv_from(p->v.chan);
}

static PyObject* pyopencv_GMatDesc_get_depth(pyopencv_GMatDesc_t* p, void *closure)
{
    return jsopencv_from(p->v.depth);
}

static PyObject* pyopencv_GMatDesc_get_dims(pyopencv_GMatDesc_t* p, void *closure)
{
    return jsopencv_from(p->v.dims);
}

static PyObject* pyopencv_GMatDesc_get_planar(pyopencv_GMatDesc_t* p, void *closure)
{
    return jsopencv_from(p->v.planar);
}

static PyObject* pyopencv_GMatDesc_get_size(pyopencv_GMatDesc_t* p, void *closure)
{
    return jsopencv_from(p->v.size);
}


// Methods (GMatDesc)

static int pyopencv_cv_GMatDesc_GMatDesc(pyopencv_GMatDesc_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_d = NULL;
    int d=0;
    Napi::Value* pyobj_c = NULL;
    int c=0;
    Napi::Value* pyobj_s = NULL;
    Size s;
    Napi::Value* pyobj_p = NULL;
    bool p=false;

    const char* keywords[] = { "d", "c", "s", "p", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:GMatDesc", (char**)keywords, &pyobj_d, &pyobj_c, &pyobj_s, &pyobj_p) &&
        jsopencv_to_safe(info, pyobj_d, d, ArgInfo("d", 0)) &&
        jsopencv_to_safe(info, pyobj_c, c, ArgInfo("c", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_p, p, ArgInfo("p", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GMatDesc(d, c, s, p));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_d = NULL;
    int d=0;
    Napi::Value* pyobj_dd = NULL;
    vector_int dd;

    const char* keywords[] = { "d", "dd", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GMatDesc", (char**)keywords, &pyobj_d, &pyobj_dd) &&
        jsopencv_to_safe(info, pyobj_d, d, ArgInfo("d", 0)) &&
        jsopencv_to_safe(info, pyobj_dd, dd, ArgInfo("dd", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GMatDesc(d, dd));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_d = NULL;
    int d=0;
    Napi::Value* pyobj_dd = NULL;
    vector_int dd;

    const char* keywords[] = { "d", "dd", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GMatDesc", (char**)keywords, &pyobj_d, &pyobj_dd) &&
        jsopencv_to_safe(info, pyobj_d, d, ArgInfo("d", 0)) &&
        jsopencv_to_safe(info, pyobj_dd, dd, ArgInfo("dd", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GMatDesc(d, std::move(dd)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GMatDesc());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GMatDesc");

    return -1;
}

static Napi::Value pyopencv_cv_GMatDesc_asInterleaved(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GMatDesc * self1 = 0;
    if (!pyopencv_GMatDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GMatDesc' or its derivative)");
    cv::GMatDesc* _self_ = (self1);
    GMatDesc retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->asInterleaved());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GMatDesc_asPlanar(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GMatDesc * self1 = 0;
    if (!pyopencv_GMatDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GMatDesc' or its derivative)");
    cv::GMatDesc* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    GMatDesc retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->asPlanar());
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_planes = NULL;
    int planes=0;
    GMatDesc retval;

    const char* keywords[] = { "planes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GMatDesc.asPlanar", (char**)keywords, &pyobj_planes) &&
        jsopencv_to_safe(info, pyobj_planes, planes, ArgInfo("planes", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->asPlanar(planes));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("asPlanar");

    return NULL;
}

static Napi::Value pyopencv_cv_GMatDesc_withDepth(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GMatDesc * self1 = 0;
    if (!pyopencv_GMatDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GMatDesc' or its derivative)");
    cv::GMatDesc* _self_ = (self1);
    Napi::Value* pyobj_ddepth = NULL;
    int ddepth=0;
    GMatDesc retval;

    const char* keywords[] = { "ddepth", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GMatDesc.withDepth", (char**)keywords, &pyobj_ddepth) &&
        jsopencv_to_safe(info, pyobj_ddepth, ddepth, ArgInfo("ddepth", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->withDepth(ddepth));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GMatDesc_withSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GMatDesc * self1 = 0;
    if (!pyopencv_GMatDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GMatDesc' or its derivative)");
    cv::GMatDesc* _self_ = (self1);
    Napi::Value* pyobj_sz = NULL;
    Size sz;
    GMatDesc retval;

    const char* keywords[] = { "sz", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GMatDesc.withSize", (char**)keywords, &pyobj_sz) &&
        jsopencv_to_safe(info, pyobj_sz, sz, ArgInfo("sz", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->withSize(sz));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GMatDesc_withSizeDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GMatDesc * self1 = 0;
    if (!pyopencv_GMatDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GMatDesc' or its derivative)");
    cv::GMatDesc* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_delta = NULL;
    Size delta;
    GMatDesc retval;

    const char* keywords[] = { "delta", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GMatDesc.withSizeDelta", (char**)keywords, &pyobj_delta) &&
        jsopencv_to_safe(info, pyobj_delta, delta, ArgInfo("delta", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->withSizeDelta(delta));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dx = NULL;
    int dx=0;
    Napi::Value* pyobj_dy = NULL;
    int dy=0;
    GMatDesc retval;

    const char* keywords[] = { "dx", "dy", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GMatDesc.withSizeDelta", (char**)keywords, &pyobj_dx, &pyobj_dy) &&
        jsopencv_to_safe(info, pyobj_dx, dx, ArgInfo("dx", 0)) &&
        jsopencv_to_safe(info, pyobj_dy, dy, ArgInfo("dy", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->withSizeDelta(dx, dy));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("withSizeDelta");

    return NULL;
}

static Napi::Value pyopencv_cv_GMatDesc_withType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GMatDesc * self1 = 0;
    if (!pyopencv_GMatDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GMatDesc' or its derivative)");
    cv::GMatDesc* _self_ = (self1);
    Napi::Value* pyobj_ddepth = NULL;
    int ddepth=0;
    Napi::Value* pyobj_dchan = NULL;
    int dchan=0;
    GMatDesc retval;

    const char* keywords[] = { "ddepth", "dchan", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GMatDesc.withType", (char**)keywords, &pyobj_ddepth, &pyobj_dchan) &&
        jsopencv_to_safe(info, pyobj_ddepth, ddepth, ArgInfo("ddepth", 0)) &&
        jsopencv_to_safe(info, pyobj_dchan, dchan, ArgInfo("dchan", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->withType(ddepth, dchan));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (GMatDesc)

static PyGetSetDef pyopencv_GMatDesc_getseters[] =
{
    {(char*)"chan", (getter)pyopencv_GMatDesc_get_chan, NULL, (char*)"chan", NULL},
    {(char*)"depth", (getter)pyopencv_GMatDesc_get_depth, NULL, (char*)"depth", NULL},
    {(char*)"dims", (getter)pyopencv_GMatDesc_get_dims, NULL, (char*)"dims", NULL},
    {(char*)"planar", (getter)pyopencv_GMatDesc_get_planar, NULL, (char*)"planar", NULL},
    {(char*)"size", (getter)pyopencv_GMatDesc_get_size, NULL, (char*)"size", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GMatDesc_methods[] =
{
    {"asInterleaved", CV_JS_FN_WITH_KW_(pyopencv_cv_GMatDesc_asInterleaved, 0), "asInterleaved() -> retval\n."},
    {"asPlanar", CV_JS_FN_WITH_KW_(pyopencv_cv_GMatDesc_asPlanar, 0), "asPlanar() -> retval\n.   \n\n\n\nasPlanar(planes) -> retval\n."},
    {"withDepth", CV_JS_FN_WITH_KW_(pyopencv_cv_GMatDesc_withDepth, 0), "withDepth(ddepth) -> retval\n."},
    {"withSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GMatDesc_withSize, 0), "withSize(sz) -> retval\n."},
    {"withSizeDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_GMatDesc_withSizeDelta, 0), "withSizeDelta(delta) -> retval\n.   \n\n\n\nwithSizeDelta(dx, dy) -> retval\n."},
    {"withType", CV_JS_FN_WITH_KW_(pyopencv_cv_GMatDesc_withType, 0), "withType(ddepth, dchan) -> retval\n."},

    {NULL,          NULL}
};

// Converter (GMatDesc)

template<>
struct PyOpenCV_Converter< cv::GMatDesc >
{
    static PyObject* from(const cv::GMatDesc& r)
    {
        return pyopencv_GMatDesc_Instance(r);
    }
    static bool to(PyObject* src, cv::GMatDesc& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GMatDesc * dst_;
        if (pyopencv_GMatDesc_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GMatDesc for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GOpaqueDesc (Generic)
//================================================================================

// GetSet (GOpaqueDesc)



// Methods (GOpaqueDesc)



// Tables (GOpaqueDesc)

static PyGetSetDef pyopencv_GOpaqueDesc_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GOpaqueDesc_methods[] =
{

    {NULL,          NULL}
};

// Converter (GOpaqueDesc)

template<>
struct PyOpenCV_Converter< cv::GOpaqueDesc >
{
    static PyObject* from(const cv::GOpaqueDesc& r)
    {
        return pyopencv_GOpaqueDesc_Instance(r);
    }
    static bool to(PyObject* src, cv::GOpaqueDesc& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GOpaqueDesc * dst_;
        if (pyopencv_GOpaqueDesc_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GOpaqueDesc for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GOpaqueT (Generic)
//================================================================================

// GetSet (GOpaqueT)



// Methods (GOpaqueT)

static int pyopencv_cv_GOpaqueT_GOpaqueT(pyopencv_GOpaqueT_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    Napi::Value* pyobj_type = NULL;
    gapi_ArgType type=static_cast<gapi_ArgType>(0);

    const char* keywords[] = { "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GOpaqueT", (char**)keywords, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GOpaqueT(type));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GOpaqueT_type(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GOpaqueT * self1 = 0;
    if (!pyopencv_GOpaqueT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GOpaqueT' or its derivative)");
    cv::GOpaqueT* _self_ = (self1);
    gapi::ArgType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->type());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (GOpaqueT)

static PyGetSetDef pyopencv_GOpaqueT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GOpaqueT_methods[] =
{
    {"type", CV_JS_FN_WITH_KW_(pyopencv_cv_GOpaqueT_type, 0), "type() -> retval\n."},

    {NULL,          NULL}
};

// Converter (GOpaqueT)

template<>
struct PyOpenCV_Converter< cv::GOpaqueT >
{
    static PyObject* from(const cv::GOpaqueT& r)
    {
        return pyopencv_GOpaqueT_Instance(r);
    }
    static bool to(PyObject* src, cv::GOpaqueT& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GOpaqueT * dst_;
        if (pyopencv_GOpaqueT_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GOpaqueT for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GScalar (Generic)
//================================================================================

// GetSet (GScalar)



// Methods (GScalar)

static int pyopencv_cv_GScalar_GScalar(pyopencv_GScalar_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GScalar());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;

    const char* keywords[] = { "s", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GScalar", (char**)keywords, &pyobj_s) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GScalar(s));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GScalar");

    return -1;
}



// Tables (GScalar)

static PyGetSetDef pyopencv_GScalar_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GScalar_methods[] =
{

    {NULL,          NULL}
};

// Converter (GScalar)

template<>
struct PyOpenCV_Converter< cv::GScalar >
{
    static PyObject* from(const cv::GScalar& r)
    {
        return pyopencv_GScalar_Instance(r);
    }
    static bool to(PyObject* src, cv::GScalar& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GScalar * dst_;
        if (pyopencv_GScalar_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GScalar for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GScalarDesc (Generic)
//================================================================================

// GetSet (GScalarDesc)



// Methods (GScalarDesc)



// Tables (GScalarDesc)

static PyGetSetDef pyopencv_GScalarDesc_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GScalarDesc_methods[] =
{

    {NULL,          NULL}
};

// Converter (GScalarDesc)

template<>
struct PyOpenCV_Converter< cv::GScalarDesc >
{
    static PyObject* from(const cv::GScalarDesc& r)
    {
        return pyopencv_GScalarDesc_Instance(r);
    }
    static bool to(PyObject* src, cv::GScalarDesc& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GScalarDesc * dst_;
        if (pyopencv_GScalarDesc_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GScalarDesc for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GStreamingCompiled (Generic)
//================================================================================

// GetSet (GStreamingCompiled)



// Methods (GStreamingCompiled)

static int pyopencv_cv_GStreamingCompiled_GStreamingCompiled(pyopencv_GStreamingCompiled_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::GStreamingCompiled());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_GStreamingCompiled_pull(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GStreamingCompiled * self1 = 0;
    if (!pyopencv_GStreamingCompiled_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GStreamingCompiled' or its derivative)");
    cv::GStreamingCompiled* _self_ = (self1);
    std::tuple<bool, cv::util::variant<cv::GRunArgs, cv::GOptRunArgs>> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->pull());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GStreamingCompiled_running(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GStreamingCompiled * self1 = 0;
    if (!pyopencv_GStreamingCompiled_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GStreamingCompiled' or its derivative)");
    cv::GStreamingCompiled* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->running());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GStreamingCompiled_setSource(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GStreamingCompiled * self1 = 0;
    if (!pyopencv_GStreamingCompiled_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GStreamingCompiled' or its derivative)");
    cv::GStreamingCompiled* _self_ = (self1);
    Napi::Value* pyobj_callback = NULL;
    detail_ExtractArgsCallback callback;

    const char* keywords[] = { "callback", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GStreamingCompiled.setSource", (char**)keywords, &pyobj_callback) &&
        jsopencv_to_safe(info, pyobj_callback, callback, ArgInfo("callback", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSource(callback));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GStreamingCompiled_start(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GStreamingCompiled * self1 = 0;
    if (!pyopencv_GStreamingCompiled_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GStreamingCompiled' or its derivative)");
    cv::GStreamingCompiled* _self_ = (self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->start());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GStreamingCompiled_stop(const Napi::CallbackInfo &info)
{
    using namespace cv;


    cv::GStreamingCompiled * self1 = 0;
    if (!pyopencv_GStreamingCompiled_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GStreamingCompiled' or its derivative)");
    cv::GStreamingCompiled* _self_ = (self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->stop());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (GStreamingCompiled)

static PyGetSetDef pyopencv_GStreamingCompiled_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GStreamingCompiled_methods[] =
{
    {"pull", CV_JS_FN_WITH_KW_(pyopencv_cv_GStreamingCompiled_pull, 0), "pull() -> retval\n.   * @brief Get the next processed frame from the pipeline.\n.        *\n.        * Use gout() to create an output parameter vector.\n.        *\n.        * Output vectors must have the same number of elements as defined\n.        * in the cv::GComputation protocol (at the moment of its\n.        * construction). Shapes of elements also must conform to protocol\n.        * (e.g. cv::Mat needs to be passed where cv::GMat has been\n.        * declared as output, and so on). Run-time exception is generated\n.        * on type mismatch.\n.        *\n.        * This method writes new data into objects passed via output\n.        * vector.  If there is no data ready yet, this method blocks. Use\n.        * try_pull() if you need a non-blocking version.\n.        *\n.        * @param outs vector of output parameters to obtain.\n.        * @return true if next result has been obtained,\n.        *    false marks end of the stream."},
    {"running", CV_JS_FN_WITH_KW_(pyopencv_cv_GStreamingCompiled_running, 0), "running() -> retval\n.   * @brief Test if the pipeline is running.\n.        *\n.        * @note This method is not thread-safe (with respect to the user\n.        * side) at the moment. Protect the access if\n.        * start()/stop()/setSource() may be called on the same object in\n.        * multiple threads in your application.\n.        *\n.        * @return true if the current stream is not over yet."},
    {"setSource", CV_JS_FN_WITH_KW_(pyopencv_cv_GStreamingCompiled_setSource, 0), "setSource(callback) -> None\n.   * @brief Specify the input data to GStreamingCompiled for\n.        * processing, a generic version.\n.        *\n.        * Use gin() to create an input parameter vector.\n.        *\n.        * Input vectors must have the same number of elements as defined\n.        * in the cv::GComputation protocol (at the moment of its\n.        * construction). Shapes of elements also must conform to protocol\n.        * (e.g. cv::Mat needs to be passed where cv::GMat has been\n.        * declared as input, and so on). Run-time exception is generated\n.        * on type mismatch.\n.        *\n.        * In contrast with regular GCompiled, user can also pass an\n.        * object of type GVideoCapture for a GMat parameter of the parent\n.        * GComputation.  The compiled pipeline will start fetching data\n.        * from that GVideoCapture and feeding it into the\n.        * pipeline. Pipeline stops when a GVideoCapture marks end of the\n.        * stream (or when stop() is called).\n.        *\n.        * Passing a regular Mat for a GMat parameter makes it \"infinite\"\n.        * source -- pipeline may run forever feeding with this Mat until\n.        * stopped explicitly.\n.        *\n.        * Currently only a single GVideoCapture is supported as input. If\n.        * the parent GComputation is declared with multiple input GMat's,\n.        * one of those can be specified as GVideoCapture but all others\n.        * must be regular Mat objects.\n.        *\n.        * Throws if pipeline is already running. Use stop() and then\n.        * setSource() to run the graph on a new video stream.\n.        *\n.        * @note This method is not thread-safe (with respect to the user\n.        * side) at the moment. Protect the access if\n.        * start()/stop()/setSource() may be called on the same object in\n.        * multiple threads in your application.\n.        *\n.        * @param ins vector of inputs to process.\n.        * @sa gin"},
    {"start", CV_JS_FN_WITH_KW_(pyopencv_cv_GStreamingCompiled_start, 0), "start() -> None\n.   * @brief Start the pipeline execution.\n.        *\n.        * Use pull()/try_pull() to obtain data. Throws an exception if\n.        * a video source was not specified.\n.        *\n.        * setSource() must be called first, even if the pipeline has been\n.        * working already and then stopped (explicitly via stop() or due\n.        * stream completion)\n.        *\n.        * @note This method is not thread-safe (with respect to the user\n.        * side) at the moment. Protect the access if\n.        * start()/stop()/setSource() may be called on the same object in\n.        * multiple threads in your application."},
    {"stop", CV_JS_FN_WITH_KW_(pyopencv_cv_GStreamingCompiled_stop, 0), "stop() -> None\n.   * @brief Stop (abort) processing the pipeline.\n.        *\n.        * Note - it is not pause but a complete stop. Calling start()\n.        * will cause G-API to start processing the stream from the early beginning.\n.        *\n.        * Throws if the pipeline is not running."},

    {NULL,          NULL}
};

// Converter (GStreamingCompiled)

template<>
struct PyOpenCV_Converter< cv::GStreamingCompiled >
{
    static PyObject* from(const cv::GStreamingCompiled& r)
    {
        return pyopencv_GStreamingCompiled_Instance(r);
    }
    static bool to(PyObject* src, cv::GStreamingCompiled& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::GStreamingCompiled * dst_;
        if (pyopencv_GStreamingCompiled_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::GStreamingCompiled for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GeneralizedHough (Generic)
//================================================================================

// GetSet (GeneralizedHough)



// Methods (GeneralizedHough)

static Napi::Value pyopencv_cv_GeneralizedHough_detect(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_positions = NULL;
    Mat positions;
    Napi::Value* pyobj_votes = NULL;
    Mat votes;

    const char* keywords[] = { "image", "positions", "votes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:GeneralizedHough.detect", (char**)keywords, &pyobj_image, &pyobj_positions, &pyobj_votes) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_positions, positions, ArgInfo("positions", 1)) &&
        jsopencv_to_safe(info, pyobj_votes, votes, ArgInfo("votes", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, positions, votes));
        return Py_BuildValue("(NN)", jsopencv_from(positions), jsopencv_from(votes));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_positions = NULL;
    UMat positions;
    Napi::Value* pyobj_votes = NULL;
    UMat votes;

    const char* keywords[] = { "image", "positions", "votes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:GeneralizedHough.detect", (char**)keywords, &pyobj_image, &pyobj_positions, &pyobj_votes) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_positions, positions, ArgInfo("positions", 1)) &&
        jsopencv_to_safe(info, pyobj_votes, votes, ArgInfo("votes", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, positions, votes));
        return Py_BuildValue("(NN)", jsopencv_from(positions), jsopencv_from(votes));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_edges = NULL;
    Mat edges;
    Napi::Value* pyobj_dx = NULL;
    Mat dx;
    Napi::Value* pyobj_dy = NULL;
    Mat dy;
    Napi::Value* pyobj_positions = NULL;
    Mat positions;
    Napi::Value* pyobj_votes = NULL;
    Mat votes;

    const char* keywords[] = { "edges", "dx", "dy", "positions", "votes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:GeneralizedHough.detect", (char**)keywords, &pyobj_edges, &pyobj_dx, &pyobj_dy, &pyobj_positions, &pyobj_votes) &&
        jsopencv_to_safe(info, pyobj_edges, edges, ArgInfo("edges", 0)) &&
        jsopencv_to_safe(info, pyobj_dx, dx, ArgInfo("dx", 0)) &&
        jsopencv_to_safe(info, pyobj_dy, dy, ArgInfo("dy", 0)) &&
        jsopencv_to_safe(info, pyobj_positions, positions, ArgInfo("positions", 1)) &&
        jsopencv_to_safe(info, pyobj_votes, votes, ArgInfo("votes", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(edges, dx, dy, positions, votes));
        return Py_BuildValue("(NN)", jsopencv_from(positions), jsopencv_from(votes));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_edges = NULL;
    UMat edges;
    Napi::Value* pyobj_dx = NULL;
    UMat dx;
    Napi::Value* pyobj_dy = NULL;
    UMat dy;
    Napi::Value* pyobj_positions = NULL;
    UMat positions;
    Napi::Value* pyobj_votes = NULL;
    UMat votes;

    const char* keywords[] = { "edges", "dx", "dy", "positions", "votes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:GeneralizedHough.detect", (char**)keywords, &pyobj_edges, &pyobj_dx, &pyobj_dy, &pyobj_positions, &pyobj_votes) &&
        jsopencv_to_safe(info, pyobj_edges, edges, ArgInfo("edges", 0)) &&
        jsopencv_to_safe(info, pyobj_dx, dx, ArgInfo("dx", 0)) &&
        jsopencv_to_safe(info, pyobj_dy, dy, ArgInfo("dy", 0)) &&
        jsopencv_to_safe(info, pyobj_positions, positions, ArgInfo("positions", 1)) &&
        jsopencv_to_safe(info, pyobj_votes, votes, ArgInfo("votes", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(edges, dx, dy, positions, votes));
        return Py_BuildValue("(NN)", jsopencv_from(positions), jsopencv_from(votes));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_getCannyHighThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCannyHighThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_getCannyLowThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCannyLowThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_getDp(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDp());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_getMaxBufferSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxBufferSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_getMinDist(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinDist());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_setCannyHighThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    Napi::Value* pyobj_cannyHighThresh = NULL;
    int cannyHighThresh=0;

    const char* keywords[] = { "cannyHighThresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHough.setCannyHighThresh", (char**)keywords, &pyobj_cannyHighThresh) &&
        jsopencv_to_safe(info, pyobj_cannyHighThresh, cannyHighThresh, ArgInfo("cannyHighThresh", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCannyHighThresh(cannyHighThresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_setCannyLowThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    Napi::Value* pyobj_cannyLowThresh = NULL;
    int cannyLowThresh=0;

    const char* keywords[] = { "cannyLowThresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHough.setCannyLowThresh", (char**)keywords, &pyobj_cannyLowThresh) &&
        jsopencv_to_safe(info, pyobj_cannyLowThresh, cannyLowThresh, ArgInfo("cannyLowThresh", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCannyLowThresh(cannyLowThresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_setDp(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    Napi::Value* pyobj_dp = NULL;
    double dp=0;

    const char* keywords[] = { "dp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHough.setDp", (char**)keywords, &pyobj_dp) &&
        jsopencv_to_safe(info, pyobj_dp, dp, ArgInfo("dp", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDp(dp));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_setMaxBufferSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    Napi::Value* pyobj_maxBufferSize = NULL;
    int maxBufferSize=0;

    const char* keywords[] = { "maxBufferSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHough.setMaxBufferSize", (char**)keywords, &pyobj_maxBufferSize) &&
        jsopencv_to_safe(info, pyobj_maxBufferSize, maxBufferSize, ArgInfo("maxBufferSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxBufferSize(maxBufferSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_setMinDist(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    Napi::Value* pyobj_minDist = NULL;
    double minDist=0;

    const char* keywords[] = { "minDist", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHough.setMinDist", (char**)keywords, &pyobj_minDist) &&
        jsopencv_to_safe(info, pyobj_minDist, minDist, ArgInfo("minDist", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinDist(minDist));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHough_setTemplate(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHough> * self1 = 0;
    if (!pyopencv_GeneralizedHough_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHough' or its derivative)");
    Ptr<cv::GeneralizedHough> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_templ = NULL;
    Mat templ;
    Napi::Value* pyobj_templCenter = NULL;
    Point templCenter=Point(-1, -1);

    const char* keywords[] = { "templ", "templCenter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GeneralizedHough.setTemplate", (char**)keywords, &pyobj_templ, &pyobj_templCenter) &&
        jsopencv_to_safe(info, pyobj_templ, templ, ArgInfo("templ", 0)) &&
        jsopencv_to_safe(info, pyobj_templCenter, templCenter, ArgInfo("templCenter", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTemplate(templ, templCenter));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_templ = NULL;
    UMat templ;
    Napi::Value* pyobj_templCenter = NULL;
    Point templCenter=Point(-1, -1);

    const char* keywords[] = { "templ", "templCenter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GeneralizedHough.setTemplate", (char**)keywords, &pyobj_templ, &pyobj_templCenter) &&
        jsopencv_to_safe(info, pyobj_templ, templ, ArgInfo("templ", 0)) &&
        jsopencv_to_safe(info, pyobj_templCenter, templCenter, ArgInfo("templCenter", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTemplate(templ, templCenter));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_edges = NULL;
    Mat edges;
    Napi::Value* pyobj_dx = NULL;
    Mat dx;
    Napi::Value* pyobj_dy = NULL;
    Mat dy;
    Napi::Value* pyobj_templCenter = NULL;
    Point templCenter=Point(-1, -1);

    const char* keywords[] = { "edges", "dx", "dy", "templCenter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:GeneralizedHough.setTemplate", (char**)keywords, &pyobj_edges, &pyobj_dx, &pyobj_dy, &pyobj_templCenter) &&
        jsopencv_to_safe(info, pyobj_edges, edges, ArgInfo("edges", 0)) &&
        jsopencv_to_safe(info, pyobj_dx, dx, ArgInfo("dx", 0)) &&
        jsopencv_to_safe(info, pyobj_dy, dy, ArgInfo("dy", 0)) &&
        jsopencv_to_safe(info, pyobj_templCenter, templCenter, ArgInfo("templCenter", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTemplate(edges, dx, dy, templCenter));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_edges = NULL;
    UMat edges;
    Napi::Value* pyobj_dx = NULL;
    UMat dx;
    Napi::Value* pyobj_dy = NULL;
    UMat dy;
    Napi::Value* pyobj_templCenter = NULL;
    Point templCenter=Point(-1, -1);

    const char* keywords[] = { "edges", "dx", "dy", "templCenter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:GeneralizedHough.setTemplate", (char**)keywords, &pyobj_edges, &pyobj_dx, &pyobj_dy, &pyobj_templCenter) &&
        jsopencv_to_safe(info, pyobj_edges, edges, ArgInfo("edges", 0)) &&
        jsopencv_to_safe(info, pyobj_dx, dx, ArgInfo("dx", 0)) &&
        jsopencv_to_safe(info, pyobj_dy, dy, ArgInfo("dy", 0)) &&
        jsopencv_to_safe(info, pyobj_templCenter, templCenter, ArgInfo("templCenter", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTemplate(edges, dx, dy, templCenter));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setTemplate");

    return NULL;
}



// Tables (GeneralizedHough)

static PyGetSetDef pyopencv_GeneralizedHough_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GeneralizedHough_methods[] =
{
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_detect, 0), "detect(image[, positions[, votes]]) -> positions, votes\n.   \n\n\n\ndetect(edges, dx, dy[, positions[, votes]]) -> positions, votes\n."},
    {"getCannyHighThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_getCannyHighThresh, 0), "getCannyHighThresh() -> retval\n."},
    {"getCannyLowThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_getCannyLowThresh, 0), "getCannyLowThresh() -> retval\n."},
    {"getDp", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_getDp, 0), "getDp() -> retval\n."},
    {"getMaxBufferSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_getMaxBufferSize, 0), "getMaxBufferSize() -> retval\n."},
    {"getMinDist", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_getMinDist, 0), "getMinDist() -> retval\n."},
    {"setCannyHighThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_setCannyHighThresh, 0), "setCannyHighThresh(cannyHighThresh) -> None\n."},
    {"setCannyLowThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_setCannyLowThresh, 0), "setCannyLowThresh(cannyLowThresh) -> None\n."},
    {"setDp", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_setDp, 0), "setDp(dp) -> None\n."},
    {"setMaxBufferSize", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_setMaxBufferSize, 0), "setMaxBufferSize(maxBufferSize) -> None\n."},
    {"setMinDist", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_setMinDist, 0), "setMinDist(minDist) -> None\n."},
    {"setTemplate", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHough_setTemplate, 0), "setTemplate(templ[, templCenter]) -> None\n.   \n\n\n\nsetTemplate(edges, dx, dy[, templCenter]) -> None\n."},

    {NULL,          NULL}
};

// Converter (GeneralizedHough)

template<>
struct PyOpenCV_Converter< Ptr<cv::GeneralizedHough> >
{
    static PyObject* from(const Ptr<cv::GeneralizedHough>& r)
    {
        return pyopencv_GeneralizedHough_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::GeneralizedHough>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::GeneralizedHough> * dst_;
        if (pyopencv_GeneralizedHough_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::GeneralizedHough> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GeneralizedHoughBallard (Generic)
//================================================================================

// GetSet (GeneralizedHoughBallard)



// Methods (GeneralizedHoughBallard)

static Napi::Value pyopencv_cv_GeneralizedHoughBallard_getLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughBallard> * self1 = 0;
    if (!pyopencv_GeneralizedHoughBallard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughBallard' or its derivative)");
    Ptr<cv::GeneralizedHoughBallard> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLevels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughBallard_getVotesThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughBallard> * self1 = 0;
    if (!pyopencv_GeneralizedHoughBallard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughBallard' or its derivative)");
    Ptr<cv::GeneralizedHoughBallard> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVotesThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughBallard_setLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughBallard> * self1 = 0;
    if (!pyopencv_GeneralizedHoughBallard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughBallard' or its derivative)");
    Ptr<cv::GeneralizedHoughBallard> _self_ = *(self1);
    Napi::Value* pyobj_levels = NULL;
    int levels=0;

    const char* keywords[] = { "levels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughBallard.setLevels", (char**)keywords, &pyobj_levels) &&
        jsopencv_to_safe(info, pyobj_levels, levels, ArgInfo("levels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLevels(levels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughBallard_setVotesThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughBallard> * self1 = 0;
    if (!pyopencv_GeneralizedHoughBallard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughBallard' or its derivative)");
    Ptr<cv::GeneralizedHoughBallard> _self_ = *(self1);
    Napi::Value* pyobj_votesThreshold = NULL;
    int votesThreshold=0;

    const char* keywords[] = { "votesThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughBallard.setVotesThreshold", (char**)keywords, &pyobj_votesThreshold) &&
        jsopencv_to_safe(info, pyobj_votesThreshold, votesThreshold, ArgInfo("votesThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setVotesThreshold(votesThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (GeneralizedHoughBallard)

static PyGetSetDef pyopencv_GeneralizedHoughBallard_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GeneralizedHoughBallard_methods[] =
{
    {"getLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughBallard_getLevels, 0), "getLevels() -> retval\n."},
    {"getVotesThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughBallard_getVotesThreshold, 0), "getVotesThreshold() -> retval\n."},
    {"setLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughBallard_setLevels, 0), "setLevels(levels) -> None\n."},
    {"setVotesThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughBallard_setVotesThreshold, 0), "setVotesThreshold(votesThreshold) -> None\n."},

    {NULL,          NULL}
};

// Converter (GeneralizedHoughBallard)

template<>
struct PyOpenCV_Converter< Ptr<cv::GeneralizedHoughBallard> >
{
    static PyObject* from(const Ptr<cv::GeneralizedHoughBallard>& r)
    {
        return pyopencv_GeneralizedHoughBallard_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::GeneralizedHoughBallard>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::GeneralizedHoughBallard> * dst_;
        if (pyopencv_GeneralizedHoughBallard_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::GeneralizedHoughBallard> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// GeneralizedHoughGuil (Generic)
//================================================================================

// GetSet (GeneralizedHoughGuil)



// Methods (GeneralizedHoughGuil)

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getAngleEpsilon(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAngleEpsilon());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getAngleStep(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAngleStep());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getAngleThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAngleThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLevels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getMaxAngle(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxAngle());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getMaxScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getMinAngle(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinAngle());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getMinScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getPosThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPosThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getScaleStep(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleStep());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getScaleThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_getXi(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getXi());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setAngleEpsilon(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_angleEpsilon = NULL;
    double angleEpsilon=0;

    const char* keywords[] = { "angleEpsilon", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setAngleEpsilon", (char**)keywords, &pyobj_angleEpsilon) &&
        jsopencv_to_safe(info, pyobj_angleEpsilon, angleEpsilon, ArgInfo("angleEpsilon", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAngleEpsilon(angleEpsilon));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setAngleStep(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_angleStep = NULL;
    double angleStep=0;

    const char* keywords[] = { "angleStep", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setAngleStep", (char**)keywords, &pyobj_angleStep) &&
        jsopencv_to_safe(info, pyobj_angleStep, angleStep, ArgInfo("angleStep", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAngleStep(angleStep));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setAngleThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_angleThresh = NULL;
    int angleThresh=0;

    const char* keywords[] = { "angleThresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setAngleThresh", (char**)keywords, &pyobj_angleThresh) &&
        jsopencv_to_safe(info, pyobj_angleThresh, angleThresh, ArgInfo("angleThresh", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAngleThresh(angleThresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_levels = NULL;
    int levels=0;

    const char* keywords[] = { "levels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setLevels", (char**)keywords, &pyobj_levels) &&
        jsopencv_to_safe(info, pyobj_levels, levels, ArgInfo("levels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLevels(levels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setMaxAngle(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_maxAngle = NULL;
    double maxAngle=0;

    const char* keywords[] = { "maxAngle", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setMaxAngle", (char**)keywords, &pyobj_maxAngle) &&
        jsopencv_to_safe(info, pyobj_maxAngle, maxAngle, ArgInfo("maxAngle", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxAngle(maxAngle));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setMaxScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_maxScale = NULL;
    double maxScale=0;

    const char* keywords[] = { "maxScale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setMaxScale", (char**)keywords, &pyobj_maxScale) &&
        jsopencv_to_safe(info, pyobj_maxScale, maxScale, ArgInfo("maxScale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxScale(maxScale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setMinAngle(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_minAngle = NULL;
    double minAngle=0;

    const char* keywords[] = { "minAngle", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setMinAngle", (char**)keywords, &pyobj_minAngle) &&
        jsopencv_to_safe(info, pyobj_minAngle, minAngle, ArgInfo("minAngle", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinAngle(minAngle));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setMinScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_minScale = NULL;
    double minScale=0;

    const char* keywords[] = { "minScale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setMinScale", (char**)keywords, &pyobj_minScale) &&
        jsopencv_to_safe(info, pyobj_minScale, minScale, ArgInfo("minScale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinScale(minScale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setPosThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_posThresh = NULL;
    int posThresh=0;

    const char* keywords[] = { "posThresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setPosThresh", (char**)keywords, &pyobj_posThresh) &&
        jsopencv_to_safe(info, pyobj_posThresh, posThresh, ArgInfo("posThresh", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPosThresh(posThresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setScaleStep(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_scaleStep = NULL;
    double scaleStep=0;

    const char* keywords[] = { "scaleStep", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setScaleStep", (char**)keywords, &pyobj_scaleStep) &&
        jsopencv_to_safe(info, pyobj_scaleStep, scaleStep, ArgInfo("scaleStep", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleStep(scaleStep));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setScaleThresh(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_scaleThresh = NULL;
    int scaleThresh=0;

    const char* keywords[] = { "scaleThresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setScaleThresh", (char**)keywords, &pyobj_scaleThresh) &&
        jsopencv_to_safe(info, pyobj_scaleThresh, scaleThresh, ArgInfo("scaleThresh", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleThresh(scaleThresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_GeneralizedHoughGuil_setXi(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::GeneralizedHoughGuil> * self1 = 0;
    if (!pyopencv_GeneralizedHoughGuil_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'GeneralizedHoughGuil' or its derivative)");
    Ptr<cv::GeneralizedHoughGuil> _self_ = *(self1);
    Napi::Value* pyobj_xi = NULL;
    double xi=0;

    const char* keywords[] = { "xi", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GeneralizedHoughGuil.setXi", (char**)keywords, &pyobj_xi) &&
        jsopencv_to_safe(info, pyobj_xi, xi, ArgInfo("xi", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setXi(xi));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (GeneralizedHoughGuil)

static PyGetSetDef pyopencv_GeneralizedHoughGuil_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_GeneralizedHoughGuil_methods[] =
{
    {"getAngleEpsilon", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getAngleEpsilon, 0), "getAngleEpsilon() -> retval\n."},
    {"getAngleStep", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getAngleStep, 0), "getAngleStep() -> retval\n."},
    {"getAngleThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getAngleThresh, 0), "getAngleThresh() -> retval\n."},
    {"getLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getLevels, 0), "getLevels() -> retval\n."},
    {"getMaxAngle", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getMaxAngle, 0), "getMaxAngle() -> retval\n."},
    {"getMaxScale", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getMaxScale, 0), "getMaxScale() -> retval\n."},
    {"getMinAngle", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getMinAngle, 0), "getMinAngle() -> retval\n."},
    {"getMinScale", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getMinScale, 0), "getMinScale() -> retval\n."},
    {"getPosThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getPosThresh, 0), "getPosThresh() -> retval\n."},
    {"getScaleStep", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getScaleStep, 0), "getScaleStep() -> retval\n."},
    {"getScaleThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getScaleThresh, 0), "getScaleThresh() -> retval\n."},
    {"getXi", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_getXi, 0), "getXi() -> retval\n."},
    {"setAngleEpsilon", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setAngleEpsilon, 0), "setAngleEpsilon(angleEpsilon) -> None\n."},
    {"setAngleStep", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setAngleStep, 0), "setAngleStep(angleStep) -> None\n."},
    {"setAngleThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setAngleThresh, 0), "setAngleThresh(angleThresh) -> None\n."},
    {"setLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setLevels, 0), "setLevels(levels) -> None\n."},
    {"setMaxAngle", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setMaxAngle, 0), "setMaxAngle(maxAngle) -> None\n."},
    {"setMaxScale", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setMaxScale, 0), "setMaxScale(maxScale) -> None\n."},
    {"setMinAngle", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setMinAngle, 0), "setMinAngle(minAngle) -> None\n."},
    {"setMinScale", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setMinScale, 0), "setMinScale(minScale) -> None\n."},
    {"setPosThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setPosThresh, 0), "setPosThresh(posThresh) -> None\n."},
    {"setScaleStep", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setScaleStep, 0), "setScaleStep(scaleStep) -> None\n."},
    {"setScaleThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setScaleThresh, 0), "setScaleThresh(scaleThresh) -> None\n."},
    {"setXi", CV_JS_FN_WITH_KW_(pyopencv_cv_GeneralizedHoughGuil_setXi, 0), "setXi(xi) -> None\n."},

    {NULL,          NULL}
};

// Converter (GeneralizedHoughGuil)

template<>
struct PyOpenCV_Converter< Ptr<cv::GeneralizedHoughGuil> >
{
    static PyObject* from(const Ptr<cv::GeneralizedHoughGuil>& r)
    {
        return pyopencv_GeneralizedHoughGuil_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::GeneralizedHoughGuil>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::GeneralizedHoughGuil> * dst_;
        if (pyopencv_GeneralizedHoughGuil_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::GeneralizedHoughGuil> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// HOGDescriptor (Generic)
//================================================================================

// GetSet (HOGDescriptor)


static PyObject* pyopencv_HOGDescriptor_get_L2HysThreshold(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->L2HysThreshold);
}

static PyObject* pyopencv_HOGDescriptor_get_blockSize(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->blockSize);
}

static PyObject* pyopencv_HOGDescriptor_get_blockStride(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->blockStride);
}

static PyObject* pyopencv_HOGDescriptor_get_cellSize(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->cellSize);
}

static PyObject* pyopencv_HOGDescriptor_get_derivAperture(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->derivAperture);
}

static PyObject* pyopencv_HOGDescriptor_get_gammaCorrection(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->gammaCorrection);
}

static PyObject* pyopencv_HOGDescriptor_get_histogramNormType(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->histogramNormType);
}

static PyObject* pyopencv_HOGDescriptor_get_nbins(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->nbins);
}

static PyObject* pyopencv_HOGDescriptor_get_nlevels(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->nlevels);
}

static PyObject* pyopencv_HOGDescriptor_get_signedGradient(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->signedGradient);
}

static PyObject* pyopencv_HOGDescriptor_get_svmDetector(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->svmDetector);
}

static PyObject* pyopencv_HOGDescriptor_get_winSigma(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->winSigma);
}

static PyObject* pyopencv_HOGDescriptor_get_winSize(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return jsopencv_from(p->v->winSize);
}


// Methods (HOGDescriptor)

static int pyopencv_cv_HOGDescriptor_HOGDescriptor(pyopencv_HOGDescriptor_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(3);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::HOGDescriptor()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__winSize = NULL;
    Size _winSize;
    Napi::Value* pyobj__blockSize = NULL;
    Size _blockSize;
    Napi::Value* pyobj__blockStride = NULL;
    Size _blockStride;
    Napi::Value* pyobj__cellSize = NULL;
    Size _cellSize;
    Napi::Value* pyobj__nbins = NULL;
    int _nbins=0;
    Napi::Value* pyobj__derivAperture = NULL;
    int _derivAperture=1;
    Napi::Value* pyobj__winSigma = NULL;
    double _winSigma=-1;
    Napi::Value* pyobj__histogramNormType = NULL;
    HOGDescriptor_HistogramNormType _histogramNormType=HOGDescriptor::L2Hys;
    Napi::Value* pyobj__L2HysThreshold = NULL;
    double _L2HysThreshold=0.2;
    Napi::Value* pyobj__gammaCorrection = NULL;
    bool _gammaCorrection=false;
    Napi::Value* pyobj__nlevels = NULL;
    int _nlevels=HOGDescriptor::DEFAULT_NLEVELS;
    Napi::Value* pyobj__signedGradient = NULL;
    bool _signedGradient=false;

    const char* keywords[] = { "_winSize", "_blockSize", "_blockStride", "_cellSize", "_nbins", "_derivAperture", "_winSigma", "_histogramNormType", "_L2HysThreshold", "_gammaCorrection", "_nlevels", "_signedGradient", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|OOOOOOO:HOGDescriptor", (char**)keywords, &pyobj__winSize, &pyobj__blockSize, &pyobj__blockStride, &pyobj__cellSize, &pyobj__nbins, &pyobj__derivAperture, &pyobj__winSigma, &pyobj__histogramNormType, &pyobj__L2HysThreshold, &pyobj__gammaCorrection, &pyobj__nlevels, &pyobj__signedGradient) &&
        jsopencv_to_safe(info, pyobj__winSize, _winSize, ArgInfo("_winSize", 0)) &&
        jsopencv_to_safe(info, pyobj__blockSize, _blockSize, ArgInfo("_blockSize", 0)) &&
        jsopencv_to_safe(info, pyobj__blockStride, _blockStride, ArgInfo("_blockStride", 0)) &&
        jsopencv_to_safe(info, pyobj__cellSize, _cellSize, ArgInfo("_cellSize", 0)) &&
        jsopencv_to_safe(info, pyobj__nbins, _nbins, ArgInfo("_nbins", 0)) &&
        jsopencv_to_safe(info, pyobj__derivAperture, _derivAperture, ArgInfo("_derivAperture", 0)) &&
        jsopencv_to_safe(info, pyobj__winSigma, _winSigma, ArgInfo("_winSigma", 0)) &&
        jsopencv_to_safe(info, pyobj__histogramNormType, _histogramNormType, ArgInfo("_histogramNormType", 0)) &&
        jsopencv_to_safe(info, pyobj__L2HysThreshold, _L2HysThreshold, ArgInfo("_L2HysThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj__gammaCorrection, _gammaCorrection, ArgInfo("_gammaCorrection", 0)) &&
        jsopencv_to_safe(info, pyobj__nlevels, _nlevels, ArgInfo("_nlevels", 0)) &&
        jsopencv_to_safe(info, pyobj__signedGradient, _signedGradient, ArgInfo("_signedGradient", 0)))
    {
        new (&(self->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::HOGDescriptor(_winSize, _blockSize, _blockStride, _cellSize, _nbins, _derivAperture, _winSigma, _histogramNormType, _L2HysThreshold, _gammaCorrection, _nlevels, _signedGradient)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:HOGDescriptor", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        new (&(self->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::HOGDescriptor(filename)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("HOGDescriptor");

    return -1;
}

static Napi::Value pyopencv_cv_HOGDescriptor_checkDetectorSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->checkDetectorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_compute(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    vector_float descriptors;
    Napi::Value* pyobj_winStride = NULL;
    Size winStride;
    Napi::Value* pyobj_padding = NULL;
    Size padding;
    Napi::Value* pyobj_locations = NULL;
    vector_Point locations=std::vector<Point>();

    const char* keywords[] = { "img", "winStride", "padding", "locations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:HOGDescriptor.compute", (char**)keywords, &pyobj_img, &pyobj_winStride, &pyobj_padding, &pyobj_locations) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        jsopencv_to_safe(info, pyobj_padding, padding, ArgInfo("padding", 0)) &&
        jsopencv_to_safe(info, pyobj_locations, locations, ArgInfo("locations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(img, descriptors, winStride, padding, locations));
        return jsopencv_from(descriptors);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    vector_float descriptors;
    Napi::Value* pyobj_winStride = NULL;
    Size winStride;
    Napi::Value* pyobj_padding = NULL;
    Size padding;
    Napi::Value* pyobj_locations = NULL;
    vector_Point locations=std::vector<Point>();

    const char* keywords[] = { "img", "winStride", "padding", "locations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:HOGDescriptor.compute", (char**)keywords, &pyobj_img, &pyobj_winStride, &pyobj_padding, &pyobj_locations) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        jsopencv_to_safe(info, pyobj_padding, padding, ArgInfo("padding", 0)) &&
        jsopencv_to_safe(info, pyobj_locations, locations, ArgInfo("locations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(img, descriptors, winStride, padding, locations));
        return jsopencv_from(descriptors);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_computeGradient(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_grad = NULL;
    Mat grad;
    Napi::Value* pyobj_angleOfs = NULL;
    Mat angleOfs;
    Napi::Value* pyobj_paddingTL = NULL;
    Size paddingTL;
    Napi::Value* pyobj_paddingBR = NULL;
    Size paddingBR;

    const char* keywords[] = { "img", "grad", "angleOfs", "paddingTL", "paddingBR", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:HOGDescriptor.computeGradient", (char**)keywords, &pyobj_img, &pyobj_grad, &pyobj_angleOfs, &pyobj_paddingTL, &pyobj_paddingBR) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_grad, grad, ArgInfo("grad", 1)) &&
        jsopencv_to_safe(info, pyobj_angleOfs, angleOfs, ArgInfo("angleOfs", 1)) &&
        jsopencv_to_safe(info, pyobj_paddingTL, paddingTL, ArgInfo("paddingTL", 0)) &&
        jsopencv_to_safe(info, pyobj_paddingBR, paddingBR, ArgInfo("paddingBR", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->computeGradient(img, grad, angleOfs, paddingTL, paddingBR));
        return Py_BuildValue("(NN)", jsopencv_from(grad), jsopencv_from(angleOfs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_grad = NULL;
    UMat grad;
    Napi::Value* pyobj_angleOfs = NULL;
    UMat angleOfs;
    Napi::Value* pyobj_paddingTL = NULL;
    Size paddingTL;
    Napi::Value* pyobj_paddingBR = NULL;
    Size paddingBR;

    const char* keywords[] = { "img", "grad", "angleOfs", "paddingTL", "paddingBR", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OO:HOGDescriptor.computeGradient", (char**)keywords, &pyobj_img, &pyobj_grad, &pyobj_angleOfs, &pyobj_paddingTL, &pyobj_paddingBR) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_grad, grad, ArgInfo("grad", 1)) &&
        jsopencv_to_safe(info, pyobj_angleOfs, angleOfs, ArgInfo("angleOfs", 1)) &&
        jsopencv_to_safe(info, pyobj_paddingTL, paddingTL, ArgInfo("paddingTL", 0)) &&
        jsopencv_to_safe(info, pyobj_paddingBR, paddingBR, ArgInfo("paddingBR", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->computeGradient(img, grad, angleOfs, paddingTL, paddingBR));
        return Py_BuildValue("(NN)", jsopencv_from(grad), jsopencv_from(angleOfs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("computeGradient");

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_detect(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    vector_Point foundLocations;
    vector_double weights;
    Napi::Value* pyobj_hitThreshold = NULL;
    double hitThreshold=0;
    Napi::Value* pyobj_winStride = NULL;
    Size winStride;
    Napi::Value* pyobj_padding = NULL;
    Size padding;
    Napi::Value* pyobj_searchLocations = NULL;
    vector_Point searchLocations=std::vector<Point>();

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "searchLocations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:HOGDescriptor.detect", (char**)keywords, &pyobj_img, &pyobj_hitThreshold, &pyobj_winStride, &pyobj_padding, &pyobj_searchLocations) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_hitThreshold, hitThreshold, ArgInfo("hitThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        jsopencv_to_safe(info, pyobj_padding, padding, ArgInfo("padding", 0)) &&
        jsopencv_to_safe(info, pyobj_searchLocations, searchLocations, ArgInfo("searchLocations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(img, foundLocations, weights, hitThreshold, winStride, padding, searchLocations));
        return Py_BuildValue("(NN)", jsopencv_from(foundLocations), jsopencv_from(weights));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    vector_Point foundLocations;
    vector_double weights;
    Napi::Value* pyobj_hitThreshold = NULL;
    double hitThreshold=0;
    Napi::Value* pyobj_winStride = NULL;
    Size winStride;
    Napi::Value* pyobj_padding = NULL;
    Size padding;
    Napi::Value* pyobj_searchLocations = NULL;
    vector_Point searchLocations=std::vector<Point>();

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "searchLocations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:HOGDescriptor.detect", (char**)keywords, &pyobj_img, &pyobj_hitThreshold, &pyobj_winStride, &pyobj_padding, &pyobj_searchLocations) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_hitThreshold, hitThreshold, ArgInfo("hitThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        jsopencv_to_safe(info, pyobj_padding, padding, ArgInfo("padding", 0)) &&
        jsopencv_to_safe(info, pyobj_searchLocations, searchLocations, ArgInfo("searchLocations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(img, foundLocations, weights, hitThreshold, winStride, padding, searchLocations));
        return Py_BuildValue("(NN)", jsopencv_from(foundLocations), jsopencv_from(weights));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_detectMultiScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    vector_Rect foundLocations;
    vector_double foundWeights;
    Napi::Value* pyobj_hitThreshold = NULL;
    double hitThreshold=0;
    Napi::Value* pyobj_winStride = NULL;
    Size winStride;
    Napi::Value* pyobj_padding = NULL;
    Size padding;
    Napi::Value* pyobj_scale = NULL;
    double scale=1.05;
    Napi::Value* pyobj_groupThreshold = NULL;
    double groupThreshold=2.0;
    Napi::Value* pyobj_useMeanshiftGrouping = NULL;
    bool useMeanshiftGrouping=false;

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "scale", "groupThreshold", "useMeanshiftGrouping", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOOO:HOGDescriptor.detectMultiScale", (char**)keywords, &pyobj_img, &pyobj_hitThreshold, &pyobj_winStride, &pyobj_padding, &pyobj_scale, &pyobj_groupThreshold, &pyobj_useMeanshiftGrouping) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_hitThreshold, hitThreshold, ArgInfo("hitThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        jsopencv_to_safe(info, pyobj_padding, padding, ArgInfo("padding", 0)) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)) &&
        jsopencv_to_safe(info, pyobj_groupThreshold, groupThreshold, ArgInfo("groupThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_useMeanshiftGrouping, useMeanshiftGrouping, ArgInfo("useMeanshiftGrouping", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(img, foundLocations, foundWeights, hitThreshold, winStride, padding, scale, groupThreshold, useMeanshiftGrouping));
        return Py_BuildValue("(NN)", jsopencv_from(foundLocations), jsopencv_from(foundWeights));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    vector_Rect foundLocations;
    vector_double foundWeights;
    Napi::Value* pyobj_hitThreshold = NULL;
    double hitThreshold=0;
    Napi::Value* pyobj_winStride = NULL;
    Size winStride;
    Napi::Value* pyobj_padding = NULL;
    Size padding;
    Napi::Value* pyobj_scale = NULL;
    double scale=1.05;
    Napi::Value* pyobj_groupThreshold = NULL;
    double groupThreshold=2.0;
    Napi::Value* pyobj_useMeanshiftGrouping = NULL;
    bool useMeanshiftGrouping=false;

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "scale", "groupThreshold", "useMeanshiftGrouping", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOOO:HOGDescriptor.detectMultiScale", (char**)keywords, &pyobj_img, &pyobj_hitThreshold, &pyobj_winStride, &pyobj_padding, &pyobj_scale, &pyobj_groupThreshold, &pyobj_useMeanshiftGrouping) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_hitThreshold, hitThreshold, ArgInfo("hitThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        jsopencv_to_safe(info, pyobj_padding, padding, ArgInfo("padding", 0)) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)) &&
        jsopencv_to_safe(info, pyobj_groupThreshold, groupThreshold, ArgInfo("groupThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_useMeanshiftGrouping, useMeanshiftGrouping, ArgInfo("useMeanshiftGrouping", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMultiScale(img, foundLocations, foundWeights, hitThreshold, winStride, padding, scale, groupThreshold, useMeanshiftGrouping));
        return Py_BuildValue("(NN)", jsopencv_from(foundLocations), jsopencv_from(foundWeights));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectMultiScale");

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_getDaimlerPeopleDetector_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    std::vector<float> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::HOGDescriptor::getDaimlerPeopleDetector());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_getDefaultPeopleDetector_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    std::vector<float> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::HOGDescriptor::getDefaultPeopleDetector());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_getDescriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDescriptorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_getWinSigma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWinSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_load(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_objname = NULL;
    String objname;
    bool retval;

    const char* keywords[] = { "filename", "objname", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:HOGDescriptor.load", (char**)keywords, &pyobj_filename, &pyobj_objname) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_objname, objname, ArgInfo("objname", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->load(filename, objname));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_save(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_objname = NULL;
    String objname;

    const char* keywords[] = { "filename", "objname", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:HOGDescriptor.save", (char**)keywords, &pyobj_filename, &pyobj_objname) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_objname, objname, ArgInfo("objname", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->save(filename, objname));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_HOGDescriptor_setSVMDetector(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::HOGDescriptor> * self1 = 0;
    if (!pyopencv_HOGDescriptor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    Ptr<cv::HOGDescriptor> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_svmdetector = NULL;
    Mat svmdetector;

    const char* keywords[] = { "svmdetector", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:HOGDescriptor.setSVMDetector", (char**)keywords, &pyobj_svmdetector) &&
        jsopencv_to_safe(info, pyobj_svmdetector, svmdetector, ArgInfo("svmdetector", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSVMDetector(svmdetector));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_svmdetector = NULL;
    UMat svmdetector;

    const char* keywords[] = { "svmdetector", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:HOGDescriptor.setSVMDetector", (char**)keywords, &pyobj_svmdetector) &&
        jsopencv_to_safe(info, pyobj_svmdetector, svmdetector, ArgInfo("svmdetector", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSVMDetector(svmdetector));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setSVMDetector");

    return NULL;
}



// Tables (HOGDescriptor)

static PyGetSetDef pyopencv_HOGDescriptor_getseters[] =
{
    {(char*)"L2HysThreshold", (getter)pyopencv_HOGDescriptor_get_L2HysThreshold, NULL, (char*)"L2HysThreshold", NULL},
    {(char*)"blockSize", (getter)pyopencv_HOGDescriptor_get_blockSize, NULL, (char*)"blockSize", NULL},
    {(char*)"blockStride", (getter)pyopencv_HOGDescriptor_get_blockStride, NULL, (char*)"blockStride", NULL},
    {(char*)"cellSize", (getter)pyopencv_HOGDescriptor_get_cellSize, NULL, (char*)"cellSize", NULL},
    {(char*)"derivAperture", (getter)pyopencv_HOGDescriptor_get_derivAperture, NULL, (char*)"derivAperture", NULL},
    {(char*)"gammaCorrection", (getter)pyopencv_HOGDescriptor_get_gammaCorrection, NULL, (char*)"gammaCorrection", NULL},
    {(char*)"histogramNormType", (getter)pyopencv_HOGDescriptor_get_histogramNormType, NULL, (char*)"histogramNormType", NULL},
    {(char*)"nbins", (getter)pyopencv_HOGDescriptor_get_nbins, NULL, (char*)"nbins", NULL},
    {(char*)"nlevels", (getter)pyopencv_HOGDescriptor_get_nlevels, NULL, (char*)"nlevels", NULL},
    {(char*)"signedGradient", (getter)pyopencv_HOGDescriptor_get_signedGradient, NULL, (char*)"signedGradient", NULL},
    {(char*)"svmDetector", (getter)pyopencv_HOGDescriptor_get_svmDetector, NULL, (char*)"svmDetector", NULL},
    {(char*)"winSigma", (getter)pyopencv_HOGDescriptor_get_winSigma, NULL, (char*)"winSigma", NULL},
    {(char*)"winSize", (getter)pyopencv_HOGDescriptor_get_winSize, NULL, (char*)"winSize", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_HOGDescriptor_methods[] =
{
    {"checkDetectorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_checkDetectorSize, 0), "checkDetectorSize() -> retval\n.   @brief Checks if detector size equal to descriptor size."},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_compute, 0), "compute(img[, winStride[, padding[, locations]]]) -> descriptors\n.   @brief Computes HOG descriptors of given image.\n.       @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.\n.       @param descriptors Matrix of the type CV_32F\n.       @param winStride Window stride. It must be a multiple of block stride.\n.       @param padding Padding\n.       @param locations Vector of Point"},
    {"computeGradient", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_computeGradient, 0), "computeGradient(img, grad, angleOfs[, paddingTL[, paddingBR]]) -> grad, angleOfs\n.   @brief  Computes gradients and quantized gradient orientations.\n.       @param img Matrix contains the image to be computed\n.       @param grad Matrix of type CV_32FC2 contains computed gradients\n.       @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations\n.       @param paddingTL Padding from top-left\n.       @param paddingBR Padding from bottom-right"},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_detect, 0), "detect(img[, hitThreshold[, winStride[, padding[, searchLocations]]]]) -> foundLocations, weights\n.   @brief Performs object detection without a multi-scale window.\n.       @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n.       @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.\n.       @param weights Vector that will contain confidence values for each detected object.\n.       @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n.       Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n.       But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n.       @param winStride Window stride. It must be a multiple of block stride.\n.       @param padding Padding\n.       @param searchLocations Vector of Point includes set of requested locations to be evaluated."},
    {"detectMultiScale", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_detectMultiScale, 0), "detectMultiScale(img[, hitThreshold[, winStride[, padding[, scale[, groupThreshold[, useMeanshiftGrouping]]]]]]) -> foundLocations, foundWeights\n.   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n.       of rectangles.\n.       @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n.       @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n.       @param foundWeights Vector that will contain confidence values for each detected object.\n.       @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n.       Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n.       But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n.       @param winStride Window stride. It must be a multiple of block stride.\n.       @param padding Padding\n.       @param scale Coefficient of the detection window increase.\n.       @param groupThreshold Coefficient to regulate the similarity threshold. When detected, some objects can be covered\n.       by many rectangles. 0 means not to perform grouping.\n.       @param useMeanshiftGrouping indicates grouping algorithm"},
    {"getDaimlerPeopleDetector", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getDaimlerPeopleDetector_static, METH_STATIC), "getDaimlerPeopleDetector() -> retval\n.   @brief Returns coefficients of the classifier trained for people detection (for 48x96 windows)."},
    {"getDefaultPeopleDetector", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getDefaultPeopleDetector_static, METH_STATIC), "getDefaultPeopleDetector() -> retval\n.   @brief Returns coefficients of the classifier trained for people detection (for 64x128 windows)."},
    {"getDescriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getDescriptorSize, 0), "getDescriptorSize() -> retval\n.   @brief Returns the number of coefficients required for the classification."},
    {"getWinSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getWinSigma, 0), "getWinSigma() -> retval\n.   @brief Returns winSigma value"},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_load, 0), "load(filename[, objname]) -> retval\n.   @brief loads HOGDescriptor parameters and coefficients for the linear SVM classifier from a file\n.       @param filename Name of the file to read.\n.       @param objname The optional name of the node to read (if empty, the first top-level node will be used)."},
    {"save", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_save, 0), "save(filename[, objname]) -> None\n.   @brief saves HOGDescriptor parameters and coefficients for the linear SVM classifier to a file\n.       @param filename File name\n.       @param objname Object name"},
    {"setSVMDetector", CV_JS_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_setSVMDetector, 0), "setSVMDetector(svmdetector) -> None\n.   @brief Sets coefficients for the linear SVM classifier.\n.       @param svmdetector coefficients for the linear SVM classifier."},

    {NULL,          NULL}
};

// Converter (HOGDescriptor)

template<>
struct PyOpenCV_Converter< Ptr<cv::HOGDescriptor> >
{
    static PyObject* from(const Ptr<cv::HOGDescriptor>& r)
    {
        return pyopencv_HOGDescriptor_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::HOGDescriptor>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::HOGDescriptor> * dst_;
        if (pyopencv_HOGDescriptor_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::HOGDescriptor> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// KAZE (Generic)
//================================================================================

// GetSet (KAZE)



// Methods (KAZE)

static Napi::Value pyopencv_cv_KAZE_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_extended = NULL;
    bool extended=false;
    Napi::Value* pyobj_upright = NULL;
    bool upright=false;
    Napi::Value* pyobj_threshold = NULL;
    float threshold=0.001f;
    Napi::Value* pyobj_nOctaves = NULL;
    int nOctaves=4;
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=4;
    Napi::Value* pyobj_diffusivity = NULL;
    KAZE_DiffusivityType diffusivity=KAZE::DIFF_PM_G2;
    Ptr<KAZE> retval;

    const char* keywords[] = { "extended", "upright", "threshold", "nOctaves", "nOctaveLayers", "diffusivity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOO:KAZE.create", (char**)keywords, &pyobj_extended, &pyobj_upright, &pyobj_threshold, &pyobj_nOctaves, &pyobj_nOctaveLayers, &pyobj_diffusivity) &&
        jsopencv_to_safe(info, pyobj_extended, extended, ArgInfo("extended", 0)) &&
        jsopencv_to_safe(info, pyobj_upright, upright, ArgInfo("upright", 0)) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaves, nOctaves, ArgInfo("nOctaves", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)) &&
        jsopencv_to_safe(info, pyobj_diffusivity, diffusivity, ArgInfo("diffusivity", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::KAZE::create(extended, upright, threshold, nOctaves, nOctaveLayers, diffusivity));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getDiffusivity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    KAZE::DiffusivityType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDiffusivity());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getExtended(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getExtended());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaveLayers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaves());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_getUpright(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUpright());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_setDiffusivity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    Napi::Value* pyobj_diff = NULL;
    KAZE_DiffusivityType diff=static_cast<KAZE_DiffusivityType>(0);

    const char* keywords[] = { "diff", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KAZE.setDiffusivity", (char**)keywords, &pyobj_diff) &&
        jsopencv_to_safe(info, pyobj_diff, diff, ArgInfo("diff", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDiffusivity(diff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_setExtended(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    Napi::Value* pyobj_extended = NULL;
    bool extended=0;

    const char* keywords[] = { "extended", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KAZE.setExtended", (char**)keywords, &pyobj_extended) &&
        jsopencv_to_safe(info, pyobj_extended, extended, ArgInfo("extended", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setExtended(extended));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_setNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    Napi::Value* pyobj_octaveLayers = NULL;
    int octaveLayers=0;

    const char* keywords[] = { "octaveLayers", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KAZE.setNOctaveLayers", (char**)keywords, &pyobj_octaveLayers) &&
        jsopencv_to_safe(info, pyobj_octaveLayers, octaveLayers, ArgInfo("octaveLayers", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaveLayers(octaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_setNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    Napi::Value* pyobj_octaves = NULL;
    int octaves=0;

    const char* keywords[] = { "octaves", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KAZE.setNOctaves", (char**)keywords, &pyobj_octaves) &&
        jsopencv_to_safe(info, pyobj_octaves, octaves, ArgInfo("octaves", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaves(octaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    Napi::Value* pyobj_threshold = NULL;
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KAZE.setThreshold", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KAZE_setUpright(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KAZE> * self1 = 0;
    if (!pyopencv_KAZE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    Ptr<cv::KAZE> _self_ = *(self1);
    Napi::Value* pyobj_upright = NULL;
    bool upright=0;

    const char* keywords[] = { "upright", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KAZE.setUpright", (char**)keywords, &pyobj_upright) &&
        jsopencv_to_safe(info, pyobj_upright, upright, ArgInfo("upright", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUpright(upright));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (KAZE)

static PyGetSetDef pyopencv_KAZE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_KAZE_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_create_static, METH_STATIC), "create([, extended[, upright[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]) -> retval\n.   @brief The KAZE constructor\n.   \n.       @param extended Set to enable extraction of extended (128-byte) descriptor.\n.       @param upright Set to enable use of upright descriptors (non rotation-invariant).\n.       @param threshold Detector response threshold to accept point\n.       @param nOctaves Maximum octave evolution of the image\n.       @param nOctaveLayers Default number of sublevels per scale level\n.       @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n.       DIFF_CHARBONNIER"},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDiffusivity", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getDiffusivity, 0), "getDiffusivity() -> retval\n."},
    {"getExtended", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getExtended, 0), "getExtended() -> retval\n."},
    {"getNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getThreshold, 0), "getThreshold() -> retval\n."},
    {"getUpright", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_getUpright, 0), "getUpright() -> retval\n."},
    {"setDiffusivity", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_setDiffusivity, 0), "setDiffusivity(diff) -> None\n."},
    {"setExtended", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_setExtended, 0), "setExtended(extended) -> None\n."},
    {"setNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_setNOctaveLayers, 0), "setNOctaveLayers(octaveLayers) -> None\n."},
    {"setNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_setNOctaves, 0), "setNOctaves(octaves) -> None\n."},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_setThreshold, 0), "setThreshold(threshold) -> None\n."},
    {"setUpright", CV_JS_FN_WITH_KW_(pyopencv_cv_KAZE_setUpright, 0), "setUpright(upright) -> None\n."},

    {NULL,          NULL}
};

// Converter (KAZE)

template<>
struct PyOpenCV_Converter< Ptr<cv::KAZE> >
{
    static PyObject* from(const Ptr<cv::KAZE>& r)
    {
        return pyopencv_KAZE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::KAZE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::KAZE> * dst_;
        if (pyopencv_KAZE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::KAZE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// KalmanFilter (Generic)
//================================================================================

// GetSet (KalmanFilter)


static PyObject* pyopencv_KalmanFilter_get_controlMatrix(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->controlMatrix);
}

static int pyopencv_KalmanFilter_set_controlMatrix(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the controlMatrix attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->controlMatrix, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_errorCovPost(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->errorCovPost);
}

static int pyopencv_KalmanFilter_set_errorCovPost(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCovPost attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->errorCovPost, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_errorCovPre(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->errorCovPre);
}

static int pyopencv_KalmanFilter_set_errorCovPre(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCovPre attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->errorCovPre, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_gain(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->gain);
}

static int pyopencv_KalmanFilter_set_gain(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the gain attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->gain, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_measurementMatrix(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->measurementMatrix);
}

static int pyopencv_KalmanFilter_set_measurementMatrix(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the measurementMatrix attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->measurementMatrix, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_measurementNoiseCov(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->measurementNoiseCov);
}

static int pyopencv_KalmanFilter_set_measurementNoiseCov(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the measurementNoiseCov attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->measurementNoiseCov, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_processNoiseCov(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->processNoiseCov);
}

static int pyopencv_KalmanFilter_set_processNoiseCov(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the processNoiseCov attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->processNoiseCov, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_statePost(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->statePost);
}

static int pyopencv_KalmanFilter_set_statePost(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the statePost attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->statePost, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_statePre(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->statePre);
}

static int pyopencv_KalmanFilter_set_statePre(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the statePre attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->statePre, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_transitionMatrix(pyopencv_KalmanFilter_t* p, void *closure)
{
    return jsopencv_from(p->v->transitionMatrix);
}

static int pyopencv_KalmanFilter_set_transitionMatrix(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the transitionMatrix attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->transitionMatrix, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (KalmanFilter)

static int pyopencv_cv_KalmanFilter_KalmanFilter(pyopencv_KalmanFilter_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::KalmanFilter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::KalmanFilter()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dynamParams = NULL;
    int dynamParams=0;
    Napi::Value* pyobj_measureParams = NULL;
    int measureParams=0;
    Napi::Value* pyobj_controlParams = NULL;
    int controlParams=0;
    Napi::Value* pyobj_type = NULL;
    int type=CV_32F;

    const char* keywords[] = { "dynamParams", "measureParams", "controlParams", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:KalmanFilter", (char**)keywords, &pyobj_dynamParams, &pyobj_measureParams, &pyobj_controlParams, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_dynamParams, dynamParams, ArgInfo("dynamParams", 0)) &&
        jsopencv_to_safe(info, pyobj_measureParams, measureParams, ArgInfo("measureParams", 0)) &&
        jsopencv_to_safe(info, pyobj_controlParams, controlParams, ArgInfo("controlParams", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        new (&(self->v)) Ptr<cv::KalmanFilter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::KalmanFilter(dynamParams, measureParams, controlParams, type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("KalmanFilter");

    return -1;
}

static Napi::Value pyopencv_cv_KalmanFilter_correct(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KalmanFilter> * self1 = 0;
    if (!pyopencv_KalmanFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KalmanFilter' or its derivative)");
    Ptr<cv::KalmanFilter> _self_ = *(self1);
    Napi::Value* pyobj_measurement = NULL;
    Mat measurement;
    Mat retval;

    const char* keywords[] = { "measurement", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KalmanFilter.correct", (char**)keywords, &pyobj_measurement) &&
        jsopencv_to_safe(info, pyobj_measurement, measurement, ArgInfo("measurement", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->correct(measurement));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_KalmanFilter_predict(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::KalmanFilter> * self1 = 0;
    if (!pyopencv_KalmanFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'KalmanFilter' or its derivative)");
    Ptr<cv::KalmanFilter> _self_ = *(self1);
    Napi::Value* pyobj_control = NULL;
    Mat control;
    Mat retval;

    const char* keywords[] = { "control", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:KalmanFilter.predict", (char**)keywords, &pyobj_control) &&
        jsopencv_to_safe(info, pyobj_control, control, ArgInfo("control", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(control));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (KalmanFilter)

static PyGetSetDef pyopencv_KalmanFilter_getseters[] =
{
    {(char*)"controlMatrix", (getter)pyopencv_KalmanFilter_get_controlMatrix, (setter)pyopencv_KalmanFilter_set_controlMatrix, (char*)"controlMatrix", NULL},
    {(char*)"errorCovPost", (getter)pyopencv_KalmanFilter_get_errorCovPost, (setter)pyopencv_KalmanFilter_set_errorCovPost, (char*)"errorCovPost", NULL},
    {(char*)"errorCovPre", (getter)pyopencv_KalmanFilter_get_errorCovPre, (setter)pyopencv_KalmanFilter_set_errorCovPre, (char*)"errorCovPre", NULL},
    {(char*)"gain", (getter)pyopencv_KalmanFilter_get_gain, (setter)pyopencv_KalmanFilter_set_gain, (char*)"gain", NULL},
    {(char*)"measurementMatrix", (getter)pyopencv_KalmanFilter_get_measurementMatrix, (setter)pyopencv_KalmanFilter_set_measurementMatrix, (char*)"measurementMatrix", NULL},
    {(char*)"measurementNoiseCov", (getter)pyopencv_KalmanFilter_get_measurementNoiseCov, (setter)pyopencv_KalmanFilter_set_measurementNoiseCov, (char*)"measurementNoiseCov", NULL},
    {(char*)"processNoiseCov", (getter)pyopencv_KalmanFilter_get_processNoiseCov, (setter)pyopencv_KalmanFilter_set_processNoiseCov, (char*)"processNoiseCov", NULL},
    {(char*)"statePost", (getter)pyopencv_KalmanFilter_get_statePost, (setter)pyopencv_KalmanFilter_set_statePost, (char*)"statePost", NULL},
    {(char*)"statePre", (getter)pyopencv_KalmanFilter_get_statePre, (setter)pyopencv_KalmanFilter_set_statePre, (char*)"statePre", NULL},
    {(char*)"transitionMatrix", (getter)pyopencv_KalmanFilter_get_transitionMatrix, (setter)pyopencv_KalmanFilter_set_transitionMatrix, (char*)"transitionMatrix", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_KalmanFilter_methods[] =
{
    {"correct", CV_JS_FN_WITH_KW_(pyopencv_cv_KalmanFilter_correct, 0), "correct(measurement) -> retval\n.   @brief Updates the predicted state from the measurement.\n.   \n.       @param measurement The measured system parameters"},
    {"predict", CV_JS_FN_WITH_KW_(pyopencv_cv_KalmanFilter_predict, 0), "predict([, control]) -> retval\n.   @brief Computes a predicted state.\n.   \n.       @param control The optional input control"},

    {NULL,          NULL}
};

// Converter (KalmanFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::KalmanFilter> >
{
    static PyObject* from(const Ptr<cv::KalmanFilter>& r)
    {
        return pyopencv_KalmanFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::KalmanFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::KalmanFilter> * dst_;
        if (pyopencv_KalmanFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::KalmanFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// KeyPoint (Generic)
//================================================================================

// GetSet (KeyPoint)


static PyObject* pyopencv_KeyPoint_get_angle(pyopencv_KeyPoint_t* p, void *closure)
{
    return jsopencv_from(p->v.angle);
}

static int pyopencv_KeyPoint_set_angle(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the angle attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.angle, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_class_id(pyopencv_KeyPoint_t* p, void *closure)
{
    return jsopencv_from(p->v.class_id);
}

static int pyopencv_KeyPoint_set_class_id(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the class_id attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.class_id, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_octave(pyopencv_KeyPoint_t* p, void *closure)
{
    return jsopencv_from(p->v.octave);
}

static int pyopencv_KeyPoint_set_octave(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the octave attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.octave, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_pt(pyopencv_KeyPoint_t* p, void *closure)
{
    return jsopencv_from(p->v.pt);
}

static int pyopencv_KeyPoint_set_pt(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the pt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.pt, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_response(pyopencv_KeyPoint_t* p, void *closure)
{
    return jsopencv_from(p->v.response);
}

static int pyopencv_KeyPoint_set_response(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the response attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.response, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_size(pyopencv_KeyPoint_t* p, void *closure)
{
    return jsopencv_from(p->v.size);
}

static int pyopencv_KeyPoint_set_size(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the size attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.size, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (KeyPoint)

static int pyopencv_cv_KeyPoint_KeyPoint(pyopencv_KeyPoint_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::KeyPoint());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_x = NULL;
    float x=0.f;
    Napi::Value* pyobj_y = NULL;
    float y=0.f;
    Napi::Value* pyobj_size = NULL;
    float size=0.f;
    Napi::Value* pyobj_angle = NULL;
    float angle=-1;
    Napi::Value* pyobj_response = NULL;
    float response=0;
    Napi::Value* pyobj_octave = NULL;
    int octave=0;
    Napi::Value* pyobj_class_id = NULL;
    int class_id=-1;

    const char* keywords[] = { "x", "y", "size", "angle", "response", "octave", "class_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOOO:KeyPoint", (char**)keywords, &pyobj_x, &pyobj_y, &pyobj_size, &pyobj_angle, &pyobj_response, &pyobj_octave, &pyobj_class_id) &&
        jsopencv_to_safe(info, pyobj_x, x, ArgInfo("x", 0)) &&
        jsopencv_to_safe(info, pyobj_y, y, ArgInfo("y", 0)) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_angle, angle, ArgInfo("angle", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)) &&
        jsopencv_to_safe(info, pyobj_octave, octave, ArgInfo("octave", 0)) &&
        jsopencv_to_safe(info, pyobj_class_id, class_id, ArgInfo("class_id", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::KeyPoint(x, y, size, angle, response, octave, class_id));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("KeyPoint");

    return -1;
}

static Napi::Value pyopencv_cv_KeyPoint_convert_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    vector_Point2f points2f;
    Napi::Value* pyobj_keypointIndexes = NULL;
    vector_int keypointIndexes=std::vector<int>();

    const char* keywords[] = { "keypoints", "keypointIndexes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:KeyPoint.convert", (char**)keywords, &pyobj_keypoints, &pyobj_keypointIndexes) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 0)) &&
        jsopencv_to_safe(info, pyobj_keypointIndexes, keypointIndexes, ArgInfo("keypointIndexes", 0)))
    {
        ERRWRAP2_NAPI(info, cv::KeyPoint::convert(keypoints, points2f, keypointIndexes));
        return jsopencv_from(points2f);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_points2f = NULL;
    vector_Point2f points2f;
    vector_KeyPoint keypoints;
    Napi::Value* pyobj_size = NULL;
    float size=1;
    Napi::Value* pyobj_response = NULL;
    float response=1;
    Napi::Value* pyobj_octave = NULL;
    int octave=0;
    Napi::Value* pyobj_class_id = NULL;
    int class_id=-1;

    const char* keywords[] = { "points2f", "size", "response", "octave", "class_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:KeyPoint.convert", (char**)keywords, &pyobj_points2f, &pyobj_size, &pyobj_response, &pyobj_octave, &pyobj_class_id) &&
        jsopencv_to_safe(info, pyobj_points2f, points2f, ArgInfo("points2f", 0)) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)) &&
        jsopencv_to_safe(info, pyobj_octave, octave, ArgInfo("octave", 0)) &&
        jsopencv_to_safe(info, pyobj_class_id, class_id, ArgInfo("class_id", 0)))
    {
        ERRWRAP2_NAPI(info, cv::KeyPoint::convert(points2f, keypoints, size, response, octave, class_id));
        return jsopencv_from(keypoints);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("convert");

    return NULL;
}

static Napi::Value pyopencv_cv_KeyPoint_overlap_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_kp1 = NULL;
    cv::KeyPoint kp1;
    Napi::Value* pyobj_kp2 = NULL;
    cv::KeyPoint kp2;
    float retval;

    const char* keywords[] = { "kp1", "kp2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:KeyPoint.overlap", (char**)keywords, &pyobj_kp1, &pyobj_kp2) &&
        jsopencv_to_safe(info, pyobj_kp1, kp1, ArgInfo("kp1", 0)) &&
        jsopencv_to_safe(info, pyobj_kp2, kp2, ArgInfo("kp2", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::KeyPoint::overlap(kp1, kp2));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (KeyPoint)

static PyGetSetDef pyopencv_KeyPoint_getseters[] =
{
    {(char*)"angle", (getter)pyopencv_KeyPoint_get_angle, (setter)pyopencv_KeyPoint_set_angle, (char*)"angle", NULL},
    {(char*)"class_id", (getter)pyopencv_KeyPoint_get_class_id, (setter)pyopencv_KeyPoint_set_class_id, (char*)"class_id", NULL},
    {(char*)"octave", (getter)pyopencv_KeyPoint_get_octave, (setter)pyopencv_KeyPoint_set_octave, (char*)"octave", NULL},
    {(char*)"pt", (getter)pyopencv_KeyPoint_get_pt, (setter)pyopencv_KeyPoint_set_pt, (char*)"pt", NULL},
    {(char*)"response", (getter)pyopencv_KeyPoint_get_response, (setter)pyopencv_KeyPoint_set_response, (char*)"response", NULL},
    {(char*)"size", (getter)pyopencv_KeyPoint_get_size, (setter)pyopencv_KeyPoint_set_size, (char*)"size", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_KeyPoint_methods[] =
{
    {"convert", CV_JS_FN_WITH_KW_(pyopencv_cv_KeyPoint_convert_static, METH_STATIC), "convert(keypoints[, keypointIndexes]) -> points2f\n.   This method converts vector of keypoints to vector of points or the reverse, where each keypoint is\n.       assigned the same size and the same orientation.\n.   \n.       @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n.       @param points2f Array of (x,y) coordinates of each keypoint\n.       @param keypointIndexes Array of indexes of keypoints to be converted to points. (Acts like a mask to\n.       convert only specified keypoints)\n\n\n\nconvert(points2f[, size[, response[, octave[, class_id]]]]) -> keypoints\n.   @overload\n.       @param points2f Array of (x,y) coordinates of each keypoint\n.       @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n.       @param size keypoint diameter\n.       @param response keypoint detector response on the keypoint (that is, strength of the keypoint)\n.       @param octave pyramid octave in which the keypoint has been detected\n.       @param class_id object id"},
    {"overlap", CV_JS_FN_WITH_KW_(pyopencv_cv_KeyPoint_overlap_static, METH_STATIC), "overlap(kp1, kp2) -> retval\n.   This method computes overlap for pair of keypoints. Overlap is the ratio between area of keypoint\n.       regions' intersection and area of keypoint regions' union (considering keypoint region as circle).\n.       If they don't overlap, we get zero. If they coincide at same location with same size, we get 1.\n.       @param kp1 First keypoint\n.       @param kp2 Second keypoint"},

    {NULL,          NULL}
};

// Converter (KeyPoint)

template<>
struct PyOpenCV_Converter< cv::KeyPoint >
{
    static PyObject* from(const cv::KeyPoint& r)
    {
        return pyopencv_KeyPoint_Instance(r);
    }
    static bool to(PyObject* src, cv::KeyPoint& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::KeyPoint * dst_;
        if (pyopencv_KeyPoint_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::KeyPoint for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// LineSegmentDetector (Generic)
//================================================================================

// GetSet (LineSegmentDetector)



// Methods (LineSegmentDetector)

static Napi::Value pyopencv_cv_LineSegmentDetector_compareSegments(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::LineSegmentDetector> * self1 = 0;
    if (!pyopencv_LineSegmentDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'LineSegmentDetector' or its derivative)");
    Ptr<cv::LineSegmentDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_lines1 = NULL;
    Mat lines1;
    Napi::Value* pyobj_lines2 = NULL;
    Mat lines2;
    Napi::Value* pyobj_image = NULL;
    Mat image;
    int retval;

    const char* keywords[] = { "size", "lines1", "lines2", "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:LineSegmentDetector.compareSegments", (char**)keywords, &pyobj_size, &pyobj_lines1, &pyobj_lines2, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_lines1, lines1, ArgInfo("lines1", 0)) &&
        jsopencv_to_safe(info, pyobj_lines2, lines2, ArgInfo("lines2", 0)) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compareSegments(size, lines1, lines2, image));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(image));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_lines1 = NULL;
    UMat lines1;
    Napi::Value* pyobj_lines2 = NULL;
    UMat lines2;
    Napi::Value* pyobj_image = NULL;
    UMat image;
    int retval;

    const char* keywords[] = { "size", "lines1", "lines2", "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:LineSegmentDetector.compareSegments", (char**)keywords, &pyobj_size, &pyobj_lines1, &pyobj_lines2, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_lines1, lines1, ArgInfo("lines1", 0)) &&
        jsopencv_to_safe(info, pyobj_lines2, lines2, ArgInfo("lines2", 0)) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compareSegments(size, lines1, lines2, image));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(image));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compareSegments");

    return NULL;
}

static Napi::Value pyopencv_cv_LineSegmentDetector_detect(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::LineSegmentDetector> * self1 = 0;
    if (!pyopencv_LineSegmentDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'LineSegmentDetector' or its derivative)");
    Ptr<cv::LineSegmentDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_lines = NULL;
    Mat lines;
    Napi::Value* pyobj_width = NULL;
    Mat width;
    Napi::Value* pyobj_prec = NULL;
    Mat prec;
    Napi::Value* pyobj_nfa = NULL;
    Mat nfa;

    const char* keywords[] = { "image", "lines", "width", "prec", "nfa", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:LineSegmentDetector.detect", (char**)keywords, &pyobj_image, &pyobj_lines, &pyobj_width, &pyobj_prec, &pyobj_nfa) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 1)) &&
        jsopencv_to_safe(info, pyobj_width, width, ArgInfo("width", 1)) &&
        jsopencv_to_safe(info, pyobj_prec, prec, ArgInfo("prec", 1)) &&
        jsopencv_to_safe(info, pyobj_nfa, nfa, ArgInfo("nfa", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, lines, width, prec, nfa));
        return Py_BuildValue("(NNNN)", jsopencv_from(lines), jsopencv_from(width), jsopencv_from(prec), jsopencv_from(nfa));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_lines = NULL;
    UMat lines;
    Napi::Value* pyobj_width = NULL;
    UMat width;
    Napi::Value* pyobj_prec = NULL;
    UMat prec;
    Napi::Value* pyobj_nfa = NULL;
    UMat nfa;

    const char* keywords[] = { "image", "lines", "width", "prec", "nfa", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:LineSegmentDetector.detect", (char**)keywords, &pyobj_image, &pyobj_lines, &pyobj_width, &pyobj_prec, &pyobj_nfa) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 1)) &&
        jsopencv_to_safe(info, pyobj_width, width, ArgInfo("width", 1)) &&
        jsopencv_to_safe(info, pyobj_prec, prec, ArgInfo("prec", 1)) &&
        jsopencv_to_safe(info, pyobj_nfa, nfa, ArgInfo("nfa", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, lines, width, prec, nfa));
        return Py_BuildValue("(NNNN)", jsopencv_from(lines), jsopencv_from(width), jsopencv_from(prec), jsopencv_from(nfa));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_LineSegmentDetector_drawSegments(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::LineSegmentDetector> * self1 = 0;
    if (!pyopencv_LineSegmentDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'LineSegmentDetector' or its derivative)");
    Ptr<cv::LineSegmentDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_lines = NULL;
    Mat lines;

    const char* keywords[] = { "image", "lines", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:LineSegmentDetector.drawSegments", (char**)keywords, &pyobj_image, &pyobj_lines) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->drawSegments(image, lines));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_lines = NULL;
    UMat lines;

    const char* keywords[] = { "image", "lines", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:LineSegmentDetector.drawSegments", (char**)keywords, &pyobj_image, &pyobj_lines) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->drawSegments(image, lines));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("drawSegments");

    return NULL;
}



// Tables (LineSegmentDetector)

static PyGetSetDef pyopencv_LineSegmentDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_LineSegmentDetector_methods[] =
{
    {"compareSegments", CV_JS_FN_WITH_KW_(pyopencv_cv_LineSegmentDetector_compareSegments, 0), "compareSegments(size, lines1, lines2[, image]) -> retval, image\n.   @brief Draws two groups of lines in blue and red, counting the non overlapping (mismatching) pixels.\n.   \n.       @param size The size of the image, where lines1 and lines2 were found.\n.       @param lines1 The first group of lines that needs to be drawn. It is visualized in blue color.\n.       @param lines2 The second group of lines. They visualized in red color.\n.       @param image Optional image, where the lines will be drawn. The image should be color(3-channel)\n.       in order for lines1 and lines2 to be drawn in the above mentioned colors."},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_LineSegmentDetector_detect, 0), "detect(image[, lines[, width[, prec[, nfa]]]]) -> lines, width, prec, nfa\n.   @brief Finds lines in the input image.\n.   \n.       This is the output of the default parameters of the algorithm on the above shown image.\n.   \n.       ![image](pics/building_lsd.png)\n.   \n.       @param image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:\n.       `lsd_ptr-\\>detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);`\n.       @param lines A vector of Vec4f elements specifying the beginning and ending point of a line. Where\n.       Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly\n.       oriented depending on the gradient.\n.       @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.\n.       @param prec Vector of precisions with which the lines are found.\n.       @param nfa Vector containing number of false alarms in the line region, with precision of 10%. The\n.       bigger the value, logarithmically better the detection.\n.       - -1 corresponds to 10 mean false alarms\n.       - 0 corresponds to 1 mean false alarm\n.       - 1 corresponds to 0.1 mean false alarms\n.       This vector will be calculated only when the objects type is #LSD_REFINE_ADV."},
    {"drawSegments", CV_JS_FN_WITH_KW_(pyopencv_cv_LineSegmentDetector_drawSegments, 0), "drawSegments(image, lines) -> image\n.   @brief Draws the line segments on a given image.\n.       @param image The image, where the lines will be drawn. Should be bigger or equal to the image,\n.       where the lines were found.\n.       @param lines A vector of the lines that needed to be drawn."},

    {NULL,          NULL}
};

// Converter (LineSegmentDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::LineSegmentDetector> >
{
    static PyObject* from(const Ptr<cv::LineSegmentDetector>& r)
    {
        return pyopencv_LineSegmentDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::LineSegmentDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::LineSegmentDetector> * dst_;
        if (pyopencv_LineSegmentDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::LineSegmentDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// MSER (Generic)
//================================================================================

// GetSet (MSER)



// Methods (MSER)

static Napi::Value pyopencv_cv_MSER_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_delta = NULL;
    int delta=5;
    Napi::Value* pyobj_min_area = NULL;
    int min_area=60;
    Napi::Value* pyobj_max_area = NULL;
    int max_area=14400;
    Napi::Value* pyobj_max_variation = NULL;
    double max_variation=0.25;
    Napi::Value* pyobj_min_diversity = NULL;
    double min_diversity=.2;
    Napi::Value* pyobj_max_evolution = NULL;
    int max_evolution=200;
    Napi::Value* pyobj_area_threshold = NULL;
    double area_threshold=1.01;
    Napi::Value* pyobj_min_margin = NULL;
    double min_margin=0.003;
    Napi::Value* pyobj_edge_blur_size = NULL;
    int edge_blur_size=5;
    Ptr<MSER> retval;

    const char* keywords[] = { "delta", "min_area", "max_area", "max_variation", "min_diversity", "max_evolution", "area_threshold", "min_margin", "edge_blur_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOOO:MSER.create", (char**)keywords, &pyobj_delta, &pyobj_min_area, &pyobj_max_area, &pyobj_max_variation, &pyobj_min_diversity, &pyobj_max_evolution, &pyobj_area_threshold, &pyobj_min_margin, &pyobj_edge_blur_size) &&
        jsopencv_to_safe(info, pyobj_delta, delta, ArgInfo("delta", 0)) &&
        jsopencv_to_safe(info, pyobj_min_area, min_area, ArgInfo("min_area", 0)) &&
        jsopencv_to_safe(info, pyobj_max_area, max_area, ArgInfo("max_area", 0)) &&
        jsopencv_to_safe(info, pyobj_max_variation, max_variation, ArgInfo("max_variation", 0)) &&
        jsopencv_to_safe(info, pyobj_min_diversity, min_diversity, ArgInfo("min_diversity", 0)) &&
        jsopencv_to_safe(info, pyobj_max_evolution, max_evolution, ArgInfo("max_evolution", 0)) &&
        jsopencv_to_safe(info, pyobj_area_threshold, area_threshold, ArgInfo("area_threshold", 0)) &&
        jsopencv_to_safe(info, pyobj_min_margin, min_margin, ArgInfo("min_margin", 0)) &&
        jsopencv_to_safe(info, pyobj_edge_blur_size, edge_blur_size, ArgInfo("edge_blur_size", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::MSER::create(delta, min_area, max_area, max_variation, min_diversity, max_evolution, area_threshold, min_margin, edge_blur_size));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_detectRegions(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    vector_vector_Point msers;
    vector_Rect bboxes;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.detectRegions", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectRegions(image, msers, bboxes));
        return Py_BuildValue("(NN)", jsopencv_from(msers), jsopencv_from(bboxes));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    vector_vector_Point msers;
    vector_Rect bboxes;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.detectRegions", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectRegions(image, msers, bboxes));
        return Py_BuildValue("(NN)", jsopencv_from(msers), jsopencv_from(bboxes));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectRegions");

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getAreaThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAreaThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDelta());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getEdgeBlurSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEdgeBlurSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getMaxArea(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxArea());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getMaxEvolution(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxEvolution());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getMaxVariation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxVariation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getMinArea(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinArea());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getMinDiversity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinDiversity());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getMinMargin(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinMargin());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_getPass2Only(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPass2Only());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setAreaThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_areaThreshold = NULL;
    double areaThreshold=0;

    const char* keywords[] = { "areaThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setAreaThreshold", (char**)keywords, &pyobj_areaThreshold) &&
        jsopencv_to_safe(info, pyobj_areaThreshold, areaThreshold, ArgInfo("areaThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAreaThreshold(areaThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_delta = NULL;
    int delta=0;

    const char* keywords[] = { "delta", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setDelta", (char**)keywords, &pyobj_delta) &&
        jsopencv_to_safe(info, pyobj_delta, delta, ArgInfo("delta", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDelta(delta));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setEdgeBlurSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_edge_blur_size = NULL;
    int edge_blur_size=0;

    const char* keywords[] = { "edge_blur_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setEdgeBlurSize", (char**)keywords, &pyobj_edge_blur_size) &&
        jsopencv_to_safe(info, pyobj_edge_blur_size, edge_blur_size, ArgInfo("edge_blur_size", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEdgeBlurSize(edge_blur_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setMaxArea(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_maxArea = NULL;
    int maxArea=0;

    const char* keywords[] = { "maxArea", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setMaxArea", (char**)keywords, &pyobj_maxArea) &&
        jsopencv_to_safe(info, pyobj_maxArea, maxArea, ArgInfo("maxArea", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxArea(maxArea));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setMaxEvolution(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_maxEvolution = NULL;
    int maxEvolution=0;

    const char* keywords[] = { "maxEvolution", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setMaxEvolution", (char**)keywords, &pyobj_maxEvolution) &&
        jsopencv_to_safe(info, pyobj_maxEvolution, maxEvolution, ArgInfo("maxEvolution", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxEvolution(maxEvolution));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setMaxVariation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_maxVariation = NULL;
    double maxVariation=0;

    const char* keywords[] = { "maxVariation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setMaxVariation", (char**)keywords, &pyobj_maxVariation) &&
        jsopencv_to_safe(info, pyobj_maxVariation, maxVariation, ArgInfo("maxVariation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxVariation(maxVariation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setMinArea(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_minArea = NULL;
    int minArea=0;

    const char* keywords[] = { "minArea", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setMinArea", (char**)keywords, &pyobj_minArea) &&
        jsopencv_to_safe(info, pyobj_minArea, minArea, ArgInfo("minArea", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinArea(minArea));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setMinDiversity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_minDiversity = NULL;
    double minDiversity=0;

    const char* keywords[] = { "minDiversity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setMinDiversity", (char**)keywords, &pyobj_minDiversity) &&
        jsopencv_to_safe(info, pyobj_minDiversity, minDiversity, ArgInfo("minDiversity", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinDiversity(minDiversity));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setMinMargin(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_min_margin = NULL;
    double min_margin=0;

    const char* keywords[] = { "min_margin", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setMinMargin", (char**)keywords, &pyobj_min_margin) &&
        jsopencv_to_safe(info, pyobj_min_margin, min_margin, ArgInfo("min_margin", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinMargin(min_margin));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MSER_setPass2Only(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MSER> * self1 = 0;
    if (!pyopencv_MSER_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    Ptr<cv::MSER> _self_ = *(self1);
    Napi::Value* pyobj_f = NULL;
    bool f=0;

    const char* keywords[] = { "f", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MSER.setPass2Only", (char**)keywords, &pyobj_f) &&
        jsopencv_to_safe(info, pyobj_f, f, ArgInfo("f", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPass2Only(f));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (MSER)

static PyGetSetDef pyopencv_MSER_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_MSER_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_create_static, METH_STATIC), "create([, delta[, min_area[, max_area[, max_variation[, min_diversity[, max_evolution[, area_threshold[, min_margin[, edge_blur_size]]]]]]]]]) -> retval\n.   @brief Full constructor for %MSER detector\n.   \n.       @param delta it compares \\f$(size_{i}-size_{i-delta})/size_{i-delta}\\f$\n.       @param min_area prune the area which smaller than minArea\n.       @param max_area prune the area which bigger than maxArea\n.       @param max_variation prune the area have similar size to its children\n.       @param min_diversity for color image, trace back to cut off mser with diversity less than min_diversity\n.       @param max_evolution  for color image, the evolution steps\n.       @param area_threshold for color image, the area threshold to cause re-initialize\n.       @param min_margin for color image, ignore too small margin\n.       @param edge_blur_size for color image, the aperture size for edge blur"},
    {"detectRegions", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_detectRegions, 0), "detectRegions(image) -> msers, bboxes\n.   @brief Detect %MSER regions\n.   \n.       @param image input image (8UC1, 8UC3 or 8UC4, must be greater or equal than 3x3)\n.       @param msers resulting list of point sets\n.       @param bboxes resulting bounding boxes"},
    {"getAreaThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getAreaThreshold, 0), "getAreaThreshold() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getDelta, 0), "getDelta() -> retval\n."},
    {"getEdgeBlurSize", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getEdgeBlurSize, 0), "getEdgeBlurSize() -> retval\n."},
    {"getMaxArea", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getMaxArea, 0), "getMaxArea() -> retval\n."},
    {"getMaxEvolution", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getMaxEvolution, 0), "getMaxEvolution() -> retval\n."},
    {"getMaxVariation", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getMaxVariation, 0), "getMaxVariation() -> retval\n."},
    {"getMinArea", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getMinArea, 0), "getMinArea() -> retval\n."},
    {"getMinDiversity", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getMinDiversity, 0), "getMinDiversity() -> retval\n."},
    {"getMinMargin", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getMinMargin, 0), "getMinMargin() -> retval\n."},
    {"getPass2Only", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_getPass2Only, 0), "getPass2Only() -> retval\n."},
    {"setAreaThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setAreaThreshold, 0), "setAreaThreshold(areaThreshold) -> None\n."},
    {"setDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setDelta, 0), "setDelta(delta) -> None\n."},
    {"setEdgeBlurSize", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setEdgeBlurSize, 0), "setEdgeBlurSize(edge_blur_size) -> None\n."},
    {"setMaxArea", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setMaxArea, 0), "setMaxArea(maxArea) -> None\n."},
    {"setMaxEvolution", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setMaxEvolution, 0), "setMaxEvolution(maxEvolution) -> None\n."},
    {"setMaxVariation", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setMaxVariation, 0), "setMaxVariation(maxVariation) -> None\n."},
    {"setMinArea", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setMinArea, 0), "setMinArea(minArea) -> None\n."},
    {"setMinDiversity", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setMinDiversity, 0), "setMinDiversity(minDiversity) -> None\n."},
    {"setMinMargin", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setMinMargin, 0), "setMinMargin(min_margin) -> None\n."},
    {"setPass2Only", CV_JS_FN_WITH_KW_(pyopencv_cv_MSER_setPass2Only, 0), "setPass2Only(f) -> None\n."},

    {NULL,          NULL}
};

// Converter (MSER)

template<>
struct PyOpenCV_Converter< Ptr<cv::MSER> >
{
    static PyObject* from(const Ptr<cv::MSER>& r)
    {
        return pyopencv_MSER_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::MSER>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::MSER> * dst_;
        if (pyopencv_MSER_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::MSER> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// MergeDebevec (Generic)
//================================================================================

// GetSet (MergeDebevec)



// Methods (MergeDebevec)

static Napi::Value pyopencv_cv_MergeDebevec_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeDebevec> * self1 = 0;
    if (!pyopencv_MergeDebevec_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeDebevec' or its derivative)");
    Ptr<cv::MergeDebevec> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;
    Napi::Value* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;
    Napi::Value* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}



// Tables (MergeDebevec)

static PyGetSetDef pyopencv_MergeDebevec_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_MergeDebevec_methods[] =
{
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeDebevec_process, 0), "process(src, times, response[, dst]) -> dst\n.   \n\n\n\nprocess(src, times[, dst]) -> dst\n."},

    {NULL,          NULL}
};

// Converter (MergeDebevec)

template<>
struct PyOpenCV_Converter< Ptr<cv::MergeDebevec> >
{
    static PyObject* from(const Ptr<cv::MergeDebevec>& r)
    {
        return pyopencv_MergeDebevec_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::MergeDebevec>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::MergeDebevec> * dst_;
        if (pyopencv_MergeDebevec_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::MergeDebevec> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// MergeExposures (Generic)
//================================================================================

// GetSet (MergeExposures)



// Methods (MergeExposures)

static Napi::Value pyopencv_cv_MergeExposures_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeExposures> * self1 = 0;
    if (!pyopencv_MergeExposures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeExposures' or its derivative)");
    Ptr<cv::MergeExposures> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;
    Napi::Value* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeExposures.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;
    Napi::Value* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeExposures.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}



// Tables (MergeExposures)

static PyGetSetDef pyopencv_MergeExposures_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_MergeExposures_methods[] =
{
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeExposures_process, 0), "process(src, times, response[, dst]) -> dst\n.   @brief Merges images.\n.   \n.       @param src vector of input images\n.       @param dst result image\n.       @param times vector of exposure time values for each image\n.       @param response 256x1 matrix with inverse camera response function for each pixel value, it should\n.       have the same number of channels as images."},

    {NULL,          NULL}
};

// Converter (MergeExposures)

template<>
struct PyOpenCV_Converter< Ptr<cv::MergeExposures> >
{
    static PyObject* from(const Ptr<cv::MergeExposures>& r)
    {
        return pyopencv_MergeExposures_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::MergeExposures>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::MergeExposures> * dst_;
        if (pyopencv_MergeExposures_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::MergeExposures> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// MergeMertens (Generic)
//================================================================================

// GetSet (MergeMertens)



// Methods (MergeMertens)

static Napi::Value pyopencv_cv_MergeMertens_getContrastWeight(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getContrastWeight());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MergeMertens_getExposureWeight(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getExposureWeight());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MergeMertens_getSaturationWeight(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSaturationWeight());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MergeMertens_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;
    Napi::Value* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;
    Napi::Value* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}

static Napi::Value pyopencv_cv_MergeMertens_setContrastWeight(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    Napi::Value* pyobj_contrast_weiht = NULL;
    float contrast_weiht=0.f;

    const char* keywords[] = { "contrast_weiht", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MergeMertens.setContrastWeight", (char**)keywords, &pyobj_contrast_weiht) &&
        jsopencv_to_safe(info, pyobj_contrast_weiht, contrast_weiht, ArgInfo("contrast_weiht", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setContrastWeight(contrast_weiht));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MergeMertens_setExposureWeight(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    Napi::Value* pyobj_exposure_weight = NULL;
    float exposure_weight=0.f;

    const char* keywords[] = { "exposure_weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MergeMertens.setExposureWeight", (char**)keywords, &pyobj_exposure_weight) &&
        jsopencv_to_safe(info, pyobj_exposure_weight, exposure_weight, ArgInfo("exposure_weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setExposureWeight(exposure_weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_MergeMertens_setSaturationWeight(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeMertens> * self1 = 0;
    if (!pyopencv_MergeMertens_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    Ptr<cv::MergeMertens> _self_ = *(self1);
    Napi::Value* pyobj_saturation_weight = NULL;
    float saturation_weight=0.f;

    const char* keywords[] = { "saturation_weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:MergeMertens.setSaturationWeight", (char**)keywords, &pyobj_saturation_weight) &&
        jsopencv_to_safe(info, pyobj_saturation_weight, saturation_weight, ArgInfo("saturation_weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSaturationWeight(saturation_weight));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (MergeMertens)

static PyGetSetDef pyopencv_MergeMertens_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_MergeMertens_methods[] =
{
    {"getContrastWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_getContrastWeight, 0), "getContrastWeight() -> retval\n."},
    {"getExposureWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_getExposureWeight, 0), "getExposureWeight() -> retval\n."},
    {"getSaturationWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_getSaturationWeight, 0), "getSaturationWeight() -> retval\n."},
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_process, 0), "process(src, times, response[, dst]) -> dst\n.   \n\n\n\nprocess(src[, dst]) -> dst\n.   @brief Short version of process, that doesn't take extra arguments.\n.   \n.       @param src vector of input images\n.       @param dst result image"},
    {"setContrastWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_setContrastWeight, 0), "setContrastWeight(contrast_weiht) -> None\n."},
    {"setExposureWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_setExposureWeight, 0), "setExposureWeight(exposure_weight) -> None\n."},
    {"setSaturationWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeMertens_setSaturationWeight, 0), "setSaturationWeight(saturation_weight) -> None\n."},

    {NULL,          NULL}
};

// Converter (MergeMertens)

template<>
struct PyOpenCV_Converter< Ptr<cv::MergeMertens> >
{
    static PyObject* from(const Ptr<cv::MergeMertens>& r)
    {
        return pyopencv_MergeMertens_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::MergeMertens>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::MergeMertens> * dst_;
        if (pyopencv_MergeMertens_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::MergeMertens> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// MergeRobertson (Generic)
//================================================================================

// GetSet (MergeRobertson)



// Methods (MergeRobertson)

static Napi::Value pyopencv_cv_MergeRobertson_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::MergeRobertson> * self1 = 0;
    if (!pyopencv_MergeRobertson_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'MergeRobertson' or its derivative)");
    Ptr<cv::MergeRobertson> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;
    Napi::Value* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;
    Napi::Value* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)) &&
        jsopencv_to_safe(info, pyobj_response, response, ArgInfo("response", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times, response));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_times = NULL;
    Mat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_times = NULL;
    UMat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_times, times, ArgInfo("times", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst, times));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}



// Tables (MergeRobertson)

static PyGetSetDef pyopencv_MergeRobertson_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_MergeRobertson_methods[] =
{
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_MergeRobertson_process, 0), "process(src, times, response[, dst]) -> dst\n.   \n\n\n\nprocess(src, times[, dst]) -> dst\n."},

    {NULL,          NULL}
};

// Converter (MergeRobertson)

template<>
struct PyOpenCV_Converter< Ptr<cv::MergeRobertson> >
{
    static PyObject* from(const Ptr<cv::MergeRobertson>& r)
    {
        return pyopencv_MergeRobertson_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::MergeRobertson>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::MergeRobertson> * dst_;
        if (pyopencv_MergeRobertson_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::MergeRobertson> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// Moments (Map)
//================================================================================
static bool pyopencv_to(PyObject* src, cv::Moments& dst, const ArgInfo& info)
{
    PyObject* tmp;
    bool ok;

    if( PyMapping_HasKeyString(src, (char*)"m00") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m00");
        ok = tmp && pyopencv_to_safe(tmp, dst.m00, ArgInfo("m00", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m10") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m10");
        ok = tmp && pyopencv_to_safe(tmp, dst.m10, ArgInfo("m10", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m01") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m01");
        ok = tmp && pyopencv_to_safe(tmp, dst.m01, ArgInfo("m01", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m20") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m20");
        ok = tmp && pyopencv_to_safe(tmp, dst.m20, ArgInfo("m20", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m11") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m11");
        ok = tmp && pyopencv_to_safe(tmp, dst.m11, ArgInfo("m11", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m02") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m02");
        ok = tmp && pyopencv_to_safe(tmp, dst.m02, ArgInfo("m02", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m30") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m30");
        ok = tmp && pyopencv_to_safe(tmp, dst.m30, ArgInfo("m30", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m21") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m21");
        ok = tmp && pyopencv_to_safe(tmp, dst.m21, ArgInfo("m21", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m12") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m12");
        ok = tmp && pyopencv_to_safe(tmp, dst.m12, ArgInfo("m12", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m03") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m03");
        ok = tmp && pyopencv_to_safe(tmp, dst.m03, ArgInfo("m03", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu20") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu20");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu20, ArgInfo("mu20", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu11") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu11");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu11, ArgInfo("mu11", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu02") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu02");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu02, ArgInfo("mu02", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu30") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu30");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu30, ArgInfo("mu30", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu21") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu21");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu21, ArgInfo("mu21", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu12") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu12");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu12, ArgInfo("mu12", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu03") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu03");
        ok = tmp && pyopencv_to_safe(tmp, dst.mu03, ArgInfo("mu03", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu20") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu20");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu20, ArgInfo("nu20", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu11") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu11");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu11, ArgInfo("nu11", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu02") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu02");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu02, ArgInfo("nu02", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu30") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu30");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu30, ArgInfo("nu30", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu21") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu21");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu21, ArgInfo("nu21", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu12") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu12");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu12, ArgInfo("nu12", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu03") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu03");
        ok = tmp && pyopencv_to_safe(tmp, dst.nu03, ArgInfo("nu03", false));
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    return true;
}

template<> bool pyopencv_to(PyObject* src, cv::Moments& dst, const ArgInfo& info);

//================================================================================
// ORB (Generic)
//================================================================================

// GetSet (ORB)



// Methods (ORB)

static Napi::Value pyopencv_cv_ORB_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_nfeatures = NULL;
    int nfeatures=500;
    Napi::Value* pyobj_scaleFactor = NULL;
    float scaleFactor=1.2f;
    Napi::Value* pyobj_nlevels = NULL;
    int nlevels=8;
    Napi::Value* pyobj_edgeThreshold = NULL;
    int edgeThreshold=31;
    Napi::Value* pyobj_firstLevel = NULL;
    int firstLevel=0;
    Napi::Value* pyobj_WTA_K = NULL;
    int WTA_K=2;
    Napi::Value* pyobj_scoreType = NULL;
    ORB_ScoreType scoreType=ORB::HARRIS_SCORE;
    Napi::Value* pyobj_patchSize = NULL;
    int patchSize=31;
    Napi::Value* pyobj_fastThreshold = NULL;
    int fastThreshold=20;
    Ptr<ORB> retval;

    const char* keywords[] = { "nfeatures", "scaleFactor", "nlevels", "edgeThreshold", "firstLevel", "WTA_K", "scoreType", "patchSize", "fastThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOOO:ORB.create", (char**)keywords, &pyobj_nfeatures, &pyobj_scaleFactor, &pyobj_nlevels, &pyobj_edgeThreshold, &pyobj_firstLevel, &pyobj_WTA_K, &pyobj_scoreType, &pyobj_patchSize, &pyobj_fastThreshold) &&
        jsopencv_to_safe(info, pyobj_nfeatures, nfeatures, ArgInfo("nfeatures", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)) &&
        jsopencv_to_safe(info, pyobj_nlevels, nlevels, ArgInfo("nlevels", 0)) &&
        jsopencv_to_safe(info, pyobj_edgeThreshold, edgeThreshold, ArgInfo("edgeThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_firstLevel, firstLevel, ArgInfo("firstLevel", 0)) &&
        jsopencv_to_safe(info, pyobj_WTA_K, WTA_K, ArgInfo("WTA_K", 0)) &&
        jsopencv_to_safe(info, pyobj_scoreType, scoreType, ArgInfo("scoreType", 0)) &&
        jsopencv_to_safe(info, pyobj_patchSize, patchSize, ArgInfo("patchSize", 0)) &&
        jsopencv_to_safe(info, pyobj_fastThreshold, fastThreshold, ArgInfo("fastThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ORB::create(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel, WTA_K, scoreType, patchSize, fastThreshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getEdgeThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEdgeThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getFastThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFastThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getFirstLevel(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFirstLevel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getMaxFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxFeatures());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getNLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNLevels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getPatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPatchSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleFactor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getScoreType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    ORB::ScoreType retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScoreType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_getWTA_K(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWTA_K());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setEdgeThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_edgeThreshold = NULL;
    int edgeThreshold=0;

    const char* keywords[] = { "edgeThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setEdgeThreshold", (char**)keywords, &pyobj_edgeThreshold) &&
        jsopencv_to_safe(info, pyobj_edgeThreshold, edgeThreshold, ArgInfo("edgeThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEdgeThreshold(edgeThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setFastThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_fastThreshold = NULL;
    int fastThreshold=0;

    const char* keywords[] = { "fastThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setFastThreshold", (char**)keywords, &pyobj_fastThreshold) &&
        jsopencv_to_safe(info, pyobj_fastThreshold, fastThreshold, ArgInfo("fastThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFastThreshold(fastThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setFirstLevel(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_firstLevel = NULL;
    int firstLevel=0;

    const char* keywords[] = { "firstLevel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setFirstLevel", (char**)keywords, &pyobj_firstLevel) &&
        jsopencv_to_safe(info, pyobj_firstLevel, firstLevel, ArgInfo("firstLevel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFirstLevel(firstLevel));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setMaxFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_maxFeatures = NULL;
    int maxFeatures=0;

    const char* keywords[] = { "maxFeatures", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setMaxFeatures", (char**)keywords, &pyobj_maxFeatures) &&
        jsopencv_to_safe(info, pyobj_maxFeatures, maxFeatures, ArgInfo("maxFeatures", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxFeatures(maxFeatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setNLevels(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_nlevels = NULL;
    int nlevels=0;

    const char* keywords[] = { "nlevels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setNLevels", (char**)keywords, &pyobj_nlevels) &&
        jsopencv_to_safe(info, pyobj_nlevels, nlevels, ArgInfo("nlevels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNLevels(nlevels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setPatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_patchSize = NULL;
    int patchSize=0;

    const char* keywords[] = { "patchSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setPatchSize", (char**)keywords, &pyobj_patchSize) &&
        jsopencv_to_safe(info, pyobj_patchSize, patchSize, ArgInfo("patchSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPatchSize(patchSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_scaleFactor = NULL;
    double scaleFactor=0;

    const char* keywords[] = { "scaleFactor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setScaleFactor", (char**)keywords, &pyobj_scaleFactor) &&
        jsopencv_to_safe(info, pyobj_scaleFactor, scaleFactor, ArgInfo("scaleFactor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleFactor(scaleFactor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setScoreType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_scoreType = NULL;
    ORB_ScoreType scoreType=static_cast<ORB_ScoreType>(0);

    const char* keywords[] = { "scoreType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setScoreType", (char**)keywords, &pyobj_scoreType) &&
        jsopencv_to_safe(info, pyobj_scoreType, scoreType, ArgInfo("scoreType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScoreType(scoreType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ORB_setWTA_K(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::ORB> * self1 = 0;
    if (!pyopencv_ORB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    Ptr<cv::ORB> _self_ = *(self1);
    Napi::Value* pyobj_wta_k = NULL;
    int wta_k=0;

    const char* keywords[] = { "wta_k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ORB.setWTA_K", (char**)keywords, &pyobj_wta_k) &&
        jsopencv_to_safe(info, pyobj_wta_k, wta_k, ArgInfo("wta_k", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWTA_K(wta_k));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ORB)

static PyGetSetDef pyopencv_ORB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ORB_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_create_static, METH_STATIC), "create([, nfeatures[, scaleFactor[, nlevels[, edgeThreshold[, firstLevel[, WTA_K[, scoreType[, patchSize[, fastThreshold]]]]]]]]]) -> retval\n.   @brief The ORB constructor\n.   \n.       @param nfeatures The maximum number of features to retain.\n.       @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical\n.       pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor\n.       will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor\n.       will mean that to cover certain scale range you will need more pyramid levels and so the speed\n.       will suffer.\n.       @param nlevels The number of pyramid levels. The smallest level will have linear size equal to\n.       input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).\n.       @param edgeThreshold This is size of the border where the features are not detected. It should\n.       roughly match the patchSize parameter.\n.       @param firstLevel The level of pyramid to put source image to. Previous layers are filled\n.       with upscaled source image.\n.       @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The\n.       default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,\n.       so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3\n.       random points (of course, those point coordinates are random, but they are generated from the\n.       pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel\n.       rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such\n.       output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,\n.       denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each\n.       bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).\n.       @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features\n.       (the score is written to KeyPoint::score and is used to retain best nfeatures features);\n.       FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,\n.       but it is a little faster to compute.\n.       @param patchSize size of the patch used by the oriented BRIEF descriptor. Of course, on smaller\n.       pyramid layers the perceived image area covered by a feature will be larger.\n.       @param fastThreshold the fast threshold"},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getEdgeThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getEdgeThreshold, 0), "getEdgeThreshold() -> retval\n."},
    {"getFastThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getFastThreshold, 0), "getFastThreshold() -> retval\n."},
    {"getFirstLevel", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getFirstLevel, 0), "getFirstLevel() -> retval\n."},
    {"getMaxFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getMaxFeatures, 0), "getMaxFeatures() -> retval\n."},
    {"getNLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getNLevels, 0), "getNLevels() -> retval\n."},
    {"getPatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getPatchSize, 0), "getPatchSize() -> retval\n."},
    {"getScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getScoreType", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getScoreType, 0), "getScoreType() -> retval\n."},
    {"getWTA_K", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_getWTA_K, 0), "getWTA_K() -> retval\n."},
    {"setEdgeThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setEdgeThreshold, 0), "setEdgeThreshold(edgeThreshold) -> None\n."},
    {"setFastThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setFastThreshold, 0), "setFastThreshold(fastThreshold) -> None\n."},
    {"setFirstLevel", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setFirstLevel, 0), "setFirstLevel(firstLevel) -> None\n."},
    {"setMaxFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setMaxFeatures, 0), "setMaxFeatures(maxFeatures) -> None\n."},
    {"setNLevels", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setNLevels, 0), "setNLevels(nlevels) -> None\n."},
    {"setPatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setPatchSize, 0), "setPatchSize(patchSize) -> None\n."},
    {"setScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setScaleFactor, 0), "setScaleFactor(scaleFactor) -> None\n."},
    {"setScoreType", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setScoreType, 0), "setScoreType(scoreType) -> None\n."},
    {"setWTA_K", CV_JS_FN_WITH_KW_(pyopencv_cv_ORB_setWTA_K, 0), "setWTA_K(wta_k) -> None\n."},

    {NULL,          NULL}
};

// Converter (ORB)

template<>
struct PyOpenCV_Converter< Ptr<cv::ORB> >
{
    static PyObject* from(const Ptr<cv::ORB>& r)
    {
        return pyopencv_ORB_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ORB>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ORB> * dst_;
        if (pyopencv_ORB_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ORB> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// QRCodeDetector (Generic)
//================================================================================

// GetSet (QRCodeDetector)



// Methods (QRCodeDetector)

static int pyopencv_cv_QRCodeDetector_QRCodeDetector(pyopencv_QRCodeDetector_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::QRCodeDetector>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::QRCodeDetector()));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_QRCodeDetector_decode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    Mat straight_qrcode;
    std::string retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:QRCodeDetector.decode", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decode(img, points, straight_qrcode));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    UMat straight_qrcode;
    std::string retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:QRCodeDetector.decode", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decode(img, points, straight_qrcode));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("decode");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_decodeCurved(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    Mat straight_qrcode;
    cv::String retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:QRCodeDetector.decodeCurved", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decodeCurved(img, points, straight_qrcode));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    UMat straight_qrcode;
    cv::String retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:QRCodeDetector.decodeCurved", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decodeCurved(img, points, straight_qrcode));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("decodeCurved");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_decodeMulti(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    vector_string decoded_info;
    Napi::Value* pyobj_straight_qrcode = NULL;
    vector_Mat straight_qrcode;
    bool retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:QRCodeDetector.decodeMulti", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decodeMulti(img, points, decoded_info, straight_qrcode));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    vector_string decoded_info;
    Napi::Value* pyobj_straight_qrcode = NULL;
    vector_UMat straight_qrcode;
    bool retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:QRCodeDetector.decodeMulti", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decodeMulti(img, points, decoded_info, straight_qrcode));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("decodeMulti");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_detect(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeDetector.detect", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detect(img, points));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeDetector.detect", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detect(img, points));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_detectAndDecode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    Mat straight_qrcode;
    std::string retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:QRCodeDetector.detectAndDecode", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecode(img, points, straight_qrcode));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(points), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    UMat straight_qrcode;
    std::string retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:QRCodeDetector.detectAndDecode", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecode(img, points, straight_qrcode));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(points), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectAndDecode");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_detectAndDecodeCurved(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    Mat straight_qrcode;
    std::string retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:QRCodeDetector.detectAndDecodeCurved", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecodeCurved(img, points, straight_qrcode));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(points), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    UMat straight_qrcode;
    std::string retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:QRCodeDetector.detectAndDecodeCurved", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecodeCurved(img, points, straight_qrcode));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(points), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectAndDecodeCurved");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_detectAndDecodeMulti(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    vector_string decoded_info;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    vector_Mat straight_qrcode;
    bool retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:QRCodeDetector.detectAndDecodeMulti", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecodeMulti(img, decoded_info, points, straight_qrcode));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(points), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    vector_string decoded_info;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    Napi::Value* pyobj_straight_qrcode = NULL;
    vector_UMat straight_qrcode;
    bool retval;

    const char* keywords[] = { "img", "points", "straight_qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:QRCodeDetector.detectAndDecodeMulti", (char**)keywords, &pyobj_img, &pyobj_points, &pyobj_straight_qrcode) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)) &&
        jsopencv_to_safe(info, pyobj_straight_qrcode, straight_qrcode, ArgInfo("straight_qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecodeMulti(img, decoded_info, points, straight_qrcode));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(points), jsopencv_from(straight_qrcode));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectAndDecodeMulti");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_detectMulti(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeDetector.detectMulti", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectMulti(img, points));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeDetector.detectMulti", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectMulti(img, points));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectMulti");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_setEpsX(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    Napi::Value* pyobj_epsX = NULL;
    double epsX=0;

    const char* keywords[] = { "epsX", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:QRCodeDetector.setEpsX", (char**)keywords, &pyobj_epsX) &&
        jsopencv_to_safe(info, pyobj_epsX, epsX, ArgInfo("epsX", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEpsX(epsX));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_setEpsY(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    Napi::Value* pyobj_epsY = NULL;
    double epsY=0;

    const char* keywords[] = { "epsY", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:QRCodeDetector.setEpsY", (char**)keywords, &pyobj_epsY) &&
        jsopencv_to_safe(info, pyobj_epsY, epsY, ArgInfo("epsY", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEpsY(epsY));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeDetector_setUseAlignmentMarkers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeDetector> * self1 = 0;
    if (!pyopencv_QRCodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeDetector' or its derivative)");
    Ptr<cv::QRCodeDetector> _self_ = *(self1);
    Napi::Value* pyobj_useAlignmentMarkers = NULL;
    bool useAlignmentMarkers=0;

    const char* keywords[] = { "useAlignmentMarkers", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:QRCodeDetector.setUseAlignmentMarkers", (char**)keywords, &pyobj_useAlignmentMarkers) &&
        jsopencv_to_safe(info, pyobj_useAlignmentMarkers, useAlignmentMarkers, ArgInfo("useAlignmentMarkers", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseAlignmentMarkers(useAlignmentMarkers));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (QRCodeDetector)

static PyGetSetDef pyopencv_QRCodeDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_QRCodeDetector_methods[] =
{
    {"decode", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_decode, 0), "decode(img, points[, straight_qrcode]) -> retval, straight_qrcode\n.   @brief Decodes QR code in image once it's found by the detect() method.\n.   \n.        Returns UTF8-encoded output string or empty string if the code cannot be decoded.\n.        @param img grayscale or color (BGR) image containing QR code.\n.        @param points Quadrangle vertices found by detect() method (or some other algorithm).\n.        @param straight_qrcode The optional output image containing rectified and binarized QR code"},
    {"decodeCurved", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_decodeCurved, 0), "decodeCurved(img, points[, straight_qrcode]) -> retval, straight_qrcode\n.   @brief Decodes QR code on a curved surface in image once it's found by the detect() method.\n.   \n.        Returns UTF8-encoded output string or empty string if the code cannot be decoded.\n.        @param img grayscale or color (BGR) image containing QR code.\n.        @param points Quadrangle vertices found by detect() method (or some other algorithm).\n.        @param straight_qrcode The optional output image containing rectified and binarized QR code"},
    {"decodeMulti", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_decodeMulti, 0), "decodeMulti(img, points[, straight_qrcode]) -> retval, decoded_info, straight_qrcode\n.   @brief Decodes QR codes in image once it's found by the detect() method.\n.        @param img grayscale or color (BGR) image containing QR codes.\n.        @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.\n.        @param points vector of Quadrangle vertices found by detect() method (or some other algorithm).\n.        @param straight_qrcode The optional output vector of images containing rectified and binarized QR codes"},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_detect, 0), "detect(img[, points]) -> retval, points\n.   @brief Detects QR code in image and returns the quadrangle containing the code.\n.        @param img grayscale or color (BGR) image containing (or not) QR code.\n.        @param points Output vector of vertices of the minimum-area quadrangle containing the code."},
    {"detectAndDecode", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_detectAndDecode, 0), "detectAndDecode(img[, points[, straight_qrcode]]) -> retval, points, straight_qrcode\n.   @brief Both detects and decodes QR code\n.   \n.        @param img grayscale or color (BGR) image containing QR code.\n.        @param points optional output array of vertices of the found QR code quadrangle. Will be empty if not found.\n.        @param straight_qrcode The optional output image containing rectified and binarized QR code"},
    {"detectAndDecodeCurved", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_detectAndDecodeCurved, 0), "detectAndDecodeCurved(img[, points[, straight_qrcode]]) -> retval, points, straight_qrcode\n.   @brief Both detects and decodes QR code on a curved surface\n.   \n.        @param img grayscale or color (BGR) image containing QR code.\n.        @param points optional output array of vertices of the found QR code quadrangle. Will be empty if not found.\n.        @param straight_qrcode The optional output image containing rectified and binarized QR code"},
    {"detectAndDecodeMulti", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_detectAndDecodeMulti, 0), "detectAndDecodeMulti(img[, points[, straight_qrcode]]) -> retval, decoded_info, points, straight_qrcode\n.   @brief Both detects and decodes QR codes\n.       @param img grayscale or color (BGR) image containing QR codes.\n.       @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.\n.       @param points optional output vector of vertices of the found QR code quadrangles. Will be empty if not found.\n.       @param straight_qrcode The optional output vector of images containing rectified and binarized QR codes"},
    {"detectMulti", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_detectMulti, 0), "detectMulti(img[, points]) -> retval, points\n.   @brief Detects QR codes in image and returns the vector of the quadrangles containing the codes.\n.        @param img grayscale or color (BGR) image containing (or not) QR codes.\n.        @param points Output vector of vector of vertices of the minimum-area quadrangle containing the codes."},
    {"setEpsX", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_setEpsX, 0), "setEpsX(epsX) -> None\n.   @brief sets the epsilon used during the horizontal scan of QR code stop marker detection.\n.        @param epsX Epsilon neighborhood, which allows you to determine the horizontal pattern\n.        of the scheme 1:1:3:1:1 according to QR code standard."},
    {"setEpsY", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_setEpsY, 0), "setEpsY(epsY) -> None\n.   @brief sets the epsilon used during the vertical scan of QR code stop marker detection.\n.        @param epsY Epsilon neighborhood, which allows you to determine the vertical pattern\n.        of the scheme 1:1:3:1:1 according to QR code standard."},
    {"setUseAlignmentMarkers", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeDetector_setUseAlignmentMarkers, 0), "setUseAlignmentMarkers(useAlignmentMarkers) -> None\n.   @brief use markers to improve the position of the corners of the QR code\n.        *\n.        * alignmentMarkers using by default"},

    {NULL,          NULL}
};

// Converter (QRCodeDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::QRCodeDetector> >
{
    static PyObject* from(const Ptr<cv::QRCodeDetector>& r)
    {
        return pyopencv_QRCodeDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::QRCodeDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::QRCodeDetector> * dst_;
        if (pyopencv_QRCodeDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::QRCodeDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// QRCodeEncoder (Generic)
//================================================================================

// GetSet (QRCodeEncoder)



// Methods (QRCodeEncoder)

static Napi::Value pyopencv_cv_QRCodeEncoder_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::QRCodeEncoder::Params parameters=QRCodeEncoder::Params();
    Ptr<QRCodeEncoder> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:QRCodeEncoder.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::QRCodeEncoder::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeEncoder_encode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeEncoder> * self1 = 0;
    if (!pyopencv_QRCodeEncoder_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeEncoder' or its derivative)");
    Ptr<cv::QRCodeEncoder> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_encoded_info = NULL;
    String encoded_info;
    Napi::Value* pyobj_qrcode = NULL;
    Mat qrcode;

    const char* keywords[] = { "encoded_info", "qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeEncoder.encode", (char**)keywords, &pyobj_encoded_info, &pyobj_qrcode) &&
        jsopencv_to_safe(info, pyobj_encoded_info, encoded_info, ArgInfo("encoded_info", 0)) &&
        jsopencv_to_safe(info, pyobj_qrcode, qrcode, ArgInfo("qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->encode(encoded_info, qrcode));
        return jsopencv_from(qrcode);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_encoded_info = NULL;
    String encoded_info;
    Napi::Value* pyobj_qrcode = NULL;
    UMat qrcode;

    const char* keywords[] = { "encoded_info", "qrcode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeEncoder.encode", (char**)keywords, &pyobj_encoded_info, &pyobj_qrcode) &&
        jsopencv_to_safe(info, pyobj_encoded_info, encoded_info, ArgInfo("encoded_info", 0)) &&
        jsopencv_to_safe(info, pyobj_qrcode, qrcode, ArgInfo("qrcode", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->encode(encoded_info, qrcode));
        return jsopencv_from(qrcode);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("encode");

    return NULL;
}

static Napi::Value pyopencv_cv_QRCodeEncoder_encodeStructuredAppend(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::QRCodeEncoder> * self1 = 0;
    if (!pyopencv_QRCodeEncoder_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'QRCodeEncoder' or its derivative)");
    Ptr<cv::QRCodeEncoder> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_encoded_info = NULL;
    String encoded_info;
    Napi::Value* pyobj_qrcodes = NULL;
    vector_Mat qrcodes;

    const char* keywords[] = { "encoded_info", "qrcodes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeEncoder.encodeStructuredAppend", (char**)keywords, &pyobj_encoded_info, &pyobj_qrcodes) &&
        jsopencv_to_safe(info, pyobj_encoded_info, encoded_info, ArgInfo("encoded_info", 0)) &&
        jsopencv_to_safe(info, pyobj_qrcodes, qrcodes, ArgInfo("qrcodes", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->encodeStructuredAppend(encoded_info, qrcodes));
        return jsopencv_from(qrcodes);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_encoded_info = NULL;
    String encoded_info;
    Napi::Value* pyobj_qrcodes = NULL;
    vector_UMat qrcodes;

    const char* keywords[] = { "encoded_info", "qrcodes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:QRCodeEncoder.encodeStructuredAppend", (char**)keywords, &pyobj_encoded_info, &pyobj_qrcodes) &&
        jsopencv_to_safe(info, pyobj_encoded_info, encoded_info, ArgInfo("encoded_info", 0)) &&
        jsopencv_to_safe(info, pyobj_qrcodes, qrcodes, ArgInfo("qrcodes", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->encodeStructuredAppend(encoded_info, qrcodes));
        return jsopencv_from(qrcodes);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("encodeStructuredAppend");

    return NULL;
}



// Tables (QRCodeEncoder)

static PyGetSetDef pyopencv_QRCodeEncoder_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_QRCodeEncoder_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeEncoder_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Constructor\n.       @param parameters QR code encoder parameters QRCodeEncoder::Params"},
    {"encode", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeEncoder_encode, 0), "encode(encoded_info[, qrcode]) -> qrcode\n.   @brief Generates QR code from input string.\n.        @param encoded_info Input string to encode.\n.        @param qrcode Generated QR code."},
    {"encodeStructuredAppend", CV_JS_FN_WITH_KW_(pyopencv_cv_QRCodeEncoder_encodeStructuredAppend, 0), "encodeStructuredAppend(encoded_info[, qrcodes]) -> qrcodes\n.   @brief Generates QR code from input string in Structured Append mode. The encoded message is splitting over a number of QR codes.\n.        @param encoded_info Input string to encode.\n.        @param qrcodes Vector of generated QR codes."},

    {NULL,          NULL}
};

// Converter (QRCodeEncoder)

template<>
struct PyOpenCV_Converter< Ptr<cv::QRCodeEncoder> >
{
    static PyObject* from(const Ptr<cv::QRCodeEncoder>& r)
    {
        return pyopencv_QRCodeEncoder_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::QRCodeEncoder>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::QRCodeEncoder> * dst_;
        if (pyopencv_QRCodeEncoder_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::QRCodeEncoder> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// QRCodeEncoder_Params (Generic)
//================================================================================

// GetSet (QRCodeEncoder_Params)


static PyObject* pyopencv_QRCodeEncoder_Params_get_correction_level(pyopencv_QRCodeEncoder_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.correction_level);
}

static int pyopencv_QRCodeEncoder_Params_set_correction_level(pyopencv_QRCodeEncoder_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the correction_level attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.correction_level, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_QRCodeEncoder_Params_get_mode(pyopencv_QRCodeEncoder_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.mode);
}

static int pyopencv_QRCodeEncoder_Params_set_mode(pyopencv_QRCodeEncoder_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the mode attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.mode, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_QRCodeEncoder_Params_get_structure_number(pyopencv_QRCodeEncoder_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.structure_number);
}

static int pyopencv_QRCodeEncoder_Params_set_structure_number(pyopencv_QRCodeEncoder_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the structure_number attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.structure_number, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_QRCodeEncoder_Params_get_version(pyopencv_QRCodeEncoder_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.version);
}

static int pyopencv_QRCodeEncoder_Params_set_version(pyopencv_QRCodeEncoder_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the version attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.version, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (QRCodeEncoder_Params)

static int pyopencv_cv_QRCodeEncoder_Params_QRCodeEncoder_Params(pyopencv_QRCodeEncoder_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::QRCodeEncoder::Params());
        return 0;
    }

    return -1;
}



// Tables (QRCodeEncoder_Params)

static PyGetSetDef pyopencv_QRCodeEncoder_Params_getseters[] =
{
    {(char*)"correction_level", (getter)pyopencv_QRCodeEncoder_Params_get_correction_level, (setter)pyopencv_QRCodeEncoder_Params_set_correction_level, (char*)"correction_level", NULL},
    {(char*)"mode", (getter)pyopencv_QRCodeEncoder_Params_get_mode, (setter)pyopencv_QRCodeEncoder_Params_set_mode, (char*)"mode", NULL},
    {(char*)"structure_number", (getter)pyopencv_QRCodeEncoder_Params_get_structure_number, (setter)pyopencv_QRCodeEncoder_Params_set_structure_number, (char*)"structure_number", NULL},
    {(char*)"version", (getter)pyopencv_QRCodeEncoder_Params_get_version, (setter)pyopencv_QRCodeEncoder_Params_set_version, (char*)"version", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_QRCodeEncoder_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (QRCodeEncoder_Params)

template<>
struct PyOpenCV_Converter< cv::QRCodeEncoder::Params >
{
    static PyObject* from(const cv::QRCodeEncoder::Params& r)
    {
        return pyopencv_QRCodeEncoder_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::QRCodeEncoder::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::QRCodeEncoder::Params * dst_;
        if (pyopencv_QRCodeEncoder_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::QRCodeEncoder::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// SIFT (Generic)
//================================================================================

// GetSet (SIFT)



// Methods (SIFT)

static Napi::Value pyopencv_cv_SIFT_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_nfeatures = NULL;
    int nfeatures=0;
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=3;
    Napi::Value* pyobj_contrastThreshold = NULL;
    double contrastThreshold=0.04;
    Napi::Value* pyobj_edgeThreshold = NULL;
    double edgeThreshold=10;
    Napi::Value* pyobj_sigma = NULL;
    double sigma=1.6;
    Ptr<SIFT> retval;

    const char* keywords[] = { "nfeatures", "nOctaveLayers", "contrastThreshold", "edgeThreshold", "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:SIFT.create", (char**)keywords, &pyobj_nfeatures, &pyobj_nOctaveLayers, &pyobj_contrastThreshold, &pyobj_edgeThreshold, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_nfeatures, nfeatures, ArgInfo("nfeatures", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)) &&
        jsopencv_to_safe(info, pyobj_contrastThreshold, contrastThreshold, ArgInfo("contrastThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_edgeThreshold, edgeThreshold, ArgInfo("edgeThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::SIFT::create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_nfeatures = NULL;
    int nfeatures=0;
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=0;
    Napi::Value* pyobj_contrastThreshold = NULL;
    double contrastThreshold=0;
    Napi::Value* pyobj_edgeThreshold = NULL;
    double edgeThreshold=0;
    Napi::Value* pyobj_sigma = NULL;
    double sigma=0;
    Napi::Value* pyobj_descriptorType = NULL;
    int descriptorType=0;
    Ptr<SIFT> retval;

    const char* keywords[] = { "nfeatures", "nOctaveLayers", "contrastThreshold", "edgeThreshold", "sigma", "descriptorType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOOO:SIFT.create", (char**)keywords, &pyobj_nfeatures, &pyobj_nOctaveLayers, &pyobj_contrastThreshold, &pyobj_edgeThreshold, &pyobj_sigma, &pyobj_descriptorType) &&
        jsopencv_to_safe(info, pyobj_nfeatures, nfeatures, ArgInfo("nfeatures", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)) &&
        jsopencv_to_safe(info, pyobj_contrastThreshold, contrastThreshold, ArgInfo("contrastThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_edgeThreshold, edgeThreshold, ArgInfo("edgeThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)) &&
        jsopencv_to_safe(info, pyobj_descriptorType, descriptorType, ArgInfo("descriptorType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::SIFT::create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma, descriptorType));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_getContrastThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getContrastThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_getEdgeThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEdgeThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_getNFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNFeatures());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_getNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaveLayers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_getSigma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_setContrastThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    Napi::Value* pyobj_contrastThreshold = NULL;
    double contrastThreshold=0;

    const char* keywords[] = { "contrastThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SIFT.setContrastThreshold", (char**)keywords, &pyobj_contrastThreshold) &&
        jsopencv_to_safe(info, pyobj_contrastThreshold, contrastThreshold, ArgInfo("contrastThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setContrastThreshold(contrastThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_setEdgeThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    Napi::Value* pyobj_edgeThreshold = NULL;
    double edgeThreshold=0;

    const char* keywords[] = { "edgeThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SIFT.setEdgeThreshold", (char**)keywords, &pyobj_edgeThreshold) &&
        jsopencv_to_safe(info, pyobj_edgeThreshold, edgeThreshold, ArgInfo("edgeThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEdgeThreshold(edgeThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_setNFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    Napi::Value* pyobj_maxFeatures = NULL;
    int maxFeatures=0;

    const char* keywords[] = { "maxFeatures", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SIFT.setNFeatures", (char**)keywords, &pyobj_maxFeatures) &&
        jsopencv_to_safe(info, pyobj_maxFeatures, maxFeatures, ArgInfo("maxFeatures", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNFeatures(maxFeatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_setNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=0;

    const char* keywords[] = { "nOctaveLayers", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SIFT.setNOctaveLayers", (char**)keywords, &pyobj_nOctaveLayers) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaveLayers(nOctaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SIFT_setSigma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SIFT> * self1 = 0;
    if (!pyopencv_SIFT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SIFT' or its derivative)");
    Ptr<cv::SIFT> _self_ = *(self1);
    Napi::Value* pyobj_sigma = NULL;
    double sigma=0;

    const char* keywords[] = { "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SIFT.setSigma", (char**)keywords, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigma(sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (SIFT)

static PyGetSetDef pyopencv_SIFT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_SIFT_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_create_static, METH_STATIC), "create([, nfeatures[, nOctaveLayers[, contrastThreshold[, edgeThreshold[, sigma]]]]]) -> retval\n.   @param nfeatures The number of best features to retain. The features are ranked by their scores\n.       (measured in SIFT algorithm as the local contrast)\n.   \n.       @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The\n.       number of octaves is computed automatically from the image resolution.\n.   \n.       @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform\n.       (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\n.   \n.       @note The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When\n.       nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set\n.       this argument to 0.09.\n.   \n.       @param edgeThreshold The threshold used to filter out edge-like features. Note that the its meaning\n.       is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are\n.       filtered out (more features are retained).\n.   \n.       @param sigma The sigma of the Gaussian applied to the input image at the octave \\#0. If your image\n.       is captured with a weak camera with soft lenses, you might want to reduce the number.\n\n\n\ncreate(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma, descriptorType) -> retval\n.   @brief Create SIFT with specified descriptorType.\n.       @param nfeatures The number of best features to retain. The features are ranked by their scores\n.       (measured in SIFT algorithm as the local contrast)\n.   \n.       @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The\n.       number of octaves is computed automatically from the image resolution.\n.   \n.       @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform\n.       (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\n.   \n.       @note The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When\n.       nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set\n.       this argument to 0.09.\n.   \n.       @param edgeThreshold The threshold used to filter out edge-like features. Note that the its meaning\n.       is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are\n.       filtered out (more features are retained).\n.   \n.       @param sigma The sigma of the Gaussian applied to the input image at the octave \\#0. If your image\n.       is captured with a weak camera with soft lenses, you might want to reduce the number.\n.   \n.       @param descriptorType The type of descriptors. Only CV_32F and CV_8U are supported."},
    {"getContrastThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_getContrastThreshold, 0), "getContrastThreshold() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getEdgeThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_getEdgeThreshold, 0), "getEdgeThreshold() -> retval\n."},
    {"getNFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_getNFeatures, 0), "getNFeatures() -> retval\n."},
    {"getNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_getSigma, 0), "getSigma() -> retval\n."},
    {"setContrastThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_setContrastThreshold, 0), "setContrastThreshold(contrastThreshold) -> None\n."},
    {"setEdgeThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_setEdgeThreshold, 0), "setEdgeThreshold(edgeThreshold) -> None\n."},
    {"setNFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_setNFeatures, 0), "setNFeatures(maxFeatures) -> None\n."},
    {"setNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_setNOctaveLayers, 0), "setNOctaveLayers(nOctaveLayers) -> None\n."},
    {"setSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_SIFT_setSigma, 0), "setSigma(sigma) -> None\n."},

    {NULL,          NULL}
};

// Converter (SIFT)

template<>
struct PyOpenCV_Converter< Ptr<cv::SIFT> >
{
    static PyObject* from(const Ptr<cv::SIFT>& r)
    {
        return pyopencv_SIFT_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::SIFT>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::SIFT> * dst_;
        if (pyopencv_SIFT_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::SIFT> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// SimpleBlobDetector (Generic)
//================================================================================

// GetSet (SimpleBlobDetector)



// Methods (SimpleBlobDetector)

static Napi::Value pyopencv_cv_SimpleBlobDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::SimpleBlobDetector::Params parameters=SimpleBlobDetector::Params();
    Ptr<SimpleBlobDetector> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:SimpleBlobDetector.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::SimpleBlobDetector::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SimpleBlobDetector_getBlobContours(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SimpleBlobDetector> * self1 = 0;
    if (!pyopencv_SimpleBlobDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SimpleBlobDetector' or its derivative)");
    Ptr<cv::SimpleBlobDetector> _self_ = *(self1);
    std::vector<std::vector<cv::Point> > retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBlobContours());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SimpleBlobDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SimpleBlobDetector> * self1 = 0;
    if (!pyopencv_SimpleBlobDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SimpleBlobDetector' or its derivative)");
    Ptr<cv::SimpleBlobDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SimpleBlobDetector_getParams(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SimpleBlobDetector> * self1 = 0;
    if (!pyopencv_SimpleBlobDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SimpleBlobDetector' or its derivative)");
    Ptr<cv::SimpleBlobDetector> _self_ = *(self1);
    SimpleBlobDetector::Params retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getParams());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SimpleBlobDetector_setParams(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SimpleBlobDetector> * self1 = 0;
    if (!pyopencv_SimpleBlobDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SimpleBlobDetector' or its derivative)");
    Ptr<cv::SimpleBlobDetector> _self_ = *(self1);
    Napi::Value* pyobj_params = NULL;
    cv::SimpleBlobDetector::Params params;

    const char* keywords[] = { "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SimpleBlobDetector.setParams", (char**)keywords, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setParams(params));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (SimpleBlobDetector)

static PyGetSetDef pyopencv_SimpleBlobDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_SimpleBlobDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_create_static, METH_STATIC), "create([, parameters]) -> retval\n."},
    {"getBlobContours", CV_JS_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_getBlobContours, 0), "getBlobContours() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getParams", CV_JS_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_getParams, 0), "getParams() -> retval\n."},
    {"setParams", CV_JS_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_setParams, 0), "setParams(params) -> None\n."},

    {NULL,          NULL}
};

// Converter (SimpleBlobDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::SimpleBlobDetector> >
{
    static PyObject* from(const Ptr<cv::SimpleBlobDetector>& r)
    {
        return pyopencv_SimpleBlobDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::SimpleBlobDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::SimpleBlobDetector> * dst_;
        if (pyopencv_SimpleBlobDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::SimpleBlobDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// SimpleBlobDetector_Params (Generic)
//================================================================================

// GetSet (SimpleBlobDetector_Params)


static PyObject* pyopencv_SimpleBlobDetector_Params_get_blobColor(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.blobColor);
}

static int pyopencv_SimpleBlobDetector_Params_set_blobColor(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the blobColor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.blobColor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_collectContours(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.collectContours);
}

static int pyopencv_SimpleBlobDetector_Params_set_collectContours(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the collectContours attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.collectContours, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByArea(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.filterByArea);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByArea(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByArea attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.filterByArea, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByCircularity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.filterByCircularity);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByCircularity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByCircularity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.filterByCircularity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByColor(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.filterByColor);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByColor(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByColor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.filterByColor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByConvexity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.filterByConvexity);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByConvexity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByConvexity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.filterByConvexity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByInertia(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.filterByInertia);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByInertia(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByInertia attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.filterByInertia, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxArea(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.maxArea);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxArea(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxArea attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxArea, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxCircularity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.maxCircularity);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxCircularity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxCircularity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxCircularity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxConvexity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.maxConvexity);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxConvexity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxConvexity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxConvexity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.maxInertiaRatio);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxInertiaRatio attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxInertiaRatio, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxThreshold(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.maxThreshold);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxThreshold(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxThreshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxThreshold, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minArea(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minArea);
}

static int pyopencv_SimpleBlobDetector_Params_set_minArea(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minArea attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minArea, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minCircularity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minCircularity);
}

static int pyopencv_SimpleBlobDetector_Params_set_minCircularity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minCircularity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minCircularity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minConvexity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minConvexity);
}

static int pyopencv_SimpleBlobDetector_Params_set_minConvexity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minConvexity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minConvexity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minDistBetweenBlobs(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minDistBetweenBlobs);
}

static int pyopencv_SimpleBlobDetector_Params_set_minDistBetweenBlobs(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDistBetweenBlobs attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minDistBetweenBlobs, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minInertiaRatio);
}

static int pyopencv_SimpleBlobDetector_Params_set_minInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minInertiaRatio attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minInertiaRatio, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minRepeatability(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minRepeatability);
}

static int pyopencv_SimpleBlobDetector_Params_set_minRepeatability(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minRepeatability attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minRepeatability, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minThreshold(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.minThreshold);
}

static int pyopencv_SimpleBlobDetector_Params_set_minThreshold(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minThreshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minThreshold, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_thresholdStep(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.thresholdStep);
}

static int pyopencv_SimpleBlobDetector_Params_set_thresholdStep(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thresholdStep attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.thresholdStep, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (SimpleBlobDetector_Params)

static int pyopencv_cv_SimpleBlobDetector_Params_SimpleBlobDetector_Params(pyopencv_SimpleBlobDetector_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::SimpleBlobDetector::Params());
        return 0;
    }

    return -1;
}



// Tables (SimpleBlobDetector_Params)

static PyGetSetDef pyopencv_SimpleBlobDetector_Params_getseters[] =
{
    {(char*)"blobColor", (getter)pyopencv_SimpleBlobDetector_Params_get_blobColor, (setter)pyopencv_SimpleBlobDetector_Params_set_blobColor, (char*)"blobColor", NULL},
    {(char*)"collectContours", (getter)pyopencv_SimpleBlobDetector_Params_get_collectContours, (setter)pyopencv_SimpleBlobDetector_Params_set_collectContours, (char*)"collectContours", NULL},
    {(char*)"filterByArea", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByArea, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByArea, (char*)"filterByArea", NULL},
    {(char*)"filterByCircularity", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByCircularity, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByCircularity, (char*)"filterByCircularity", NULL},
    {(char*)"filterByColor", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByColor, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByColor, (char*)"filterByColor", NULL},
    {(char*)"filterByConvexity", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByConvexity, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByConvexity, (char*)"filterByConvexity", NULL},
    {(char*)"filterByInertia", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByInertia, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByInertia, (char*)"filterByInertia", NULL},
    {(char*)"maxArea", (getter)pyopencv_SimpleBlobDetector_Params_get_maxArea, (setter)pyopencv_SimpleBlobDetector_Params_set_maxArea, (char*)"maxArea", NULL},
    {(char*)"maxCircularity", (getter)pyopencv_SimpleBlobDetector_Params_get_maxCircularity, (setter)pyopencv_SimpleBlobDetector_Params_set_maxCircularity, (char*)"maxCircularity", NULL},
    {(char*)"maxConvexity", (getter)pyopencv_SimpleBlobDetector_Params_get_maxConvexity, (setter)pyopencv_SimpleBlobDetector_Params_set_maxConvexity, (char*)"maxConvexity", NULL},
    {(char*)"maxInertiaRatio", (getter)pyopencv_SimpleBlobDetector_Params_get_maxInertiaRatio, (setter)pyopencv_SimpleBlobDetector_Params_set_maxInertiaRatio, (char*)"maxInertiaRatio", NULL},
    {(char*)"maxThreshold", (getter)pyopencv_SimpleBlobDetector_Params_get_maxThreshold, (setter)pyopencv_SimpleBlobDetector_Params_set_maxThreshold, (char*)"maxThreshold", NULL},
    {(char*)"minArea", (getter)pyopencv_SimpleBlobDetector_Params_get_minArea, (setter)pyopencv_SimpleBlobDetector_Params_set_minArea, (char*)"minArea", NULL},
    {(char*)"minCircularity", (getter)pyopencv_SimpleBlobDetector_Params_get_minCircularity, (setter)pyopencv_SimpleBlobDetector_Params_set_minCircularity, (char*)"minCircularity", NULL},
    {(char*)"minConvexity", (getter)pyopencv_SimpleBlobDetector_Params_get_minConvexity, (setter)pyopencv_SimpleBlobDetector_Params_set_minConvexity, (char*)"minConvexity", NULL},
    {(char*)"minDistBetweenBlobs", (getter)pyopencv_SimpleBlobDetector_Params_get_minDistBetweenBlobs, (setter)pyopencv_SimpleBlobDetector_Params_set_minDistBetweenBlobs, (char*)"minDistBetweenBlobs", NULL},
    {(char*)"minInertiaRatio", (getter)pyopencv_SimpleBlobDetector_Params_get_minInertiaRatio, (setter)pyopencv_SimpleBlobDetector_Params_set_minInertiaRatio, (char*)"minInertiaRatio", NULL},
    {(char*)"minRepeatability", (getter)pyopencv_SimpleBlobDetector_Params_get_minRepeatability, (setter)pyopencv_SimpleBlobDetector_Params_set_minRepeatability, (char*)"minRepeatability", NULL},
    {(char*)"minThreshold", (getter)pyopencv_SimpleBlobDetector_Params_get_minThreshold, (setter)pyopencv_SimpleBlobDetector_Params_set_minThreshold, (char*)"minThreshold", NULL},
    {(char*)"thresholdStep", (getter)pyopencv_SimpleBlobDetector_Params_get_thresholdStep, (setter)pyopencv_SimpleBlobDetector_Params_set_thresholdStep, (char*)"thresholdStep", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_SimpleBlobDetector_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (SimpleBlobDetector_Params)

template<>
struct PyOpenCV_Converter< cv::SimpleBlobDetector::Params >
{
    static PyObject* from(const cv::SimpleBlobDetector::Params& r)
    {
        return pyopencv_SimpleBlobDetector_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::SimpleBlobDetector::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::SimpleBlobDetector::Params * dst_;
        if (pyopencv_SimpleBlobDetector_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::SimpleBlobDetector::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// SparseOpticalFlow (Generic)
//================================================================================

// GetSet (SparseOpticalFlow)



// Methods (SparseOpticalFlow)

static Napi::Value pyopencv_cv_SparseOpticalFlow_calc(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparseOpticalFlow> * self1 = 0;
    if (!pyopencv_SparseOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparseOpticalFlow' or its derivative)");
    Ptr<cv::SparseOpticalFlow> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_prevImg = NULL;
    Mat prevImg;
    Napi::Value* pyobj_nextImg = NULL;
    Mat nextImg;
    Napi::Value* pyobj_prevPts = NULL;
    Mat prevPts;
    Napi::Value* pyobj_nextPts = NULL;
    Mat nextPts;
    Napi::Value* pyobj_status = NULL;
    Mat status;
    Napi::Value* pyobj_err = NULL;
    Mat err;

    const char* keywords[] = { "prevImg", "nextImg", "prevPts", "nextPts", "status", "err", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OO:SparseOpticalFlow.calc", (char**)keywords, &pyobj_prevImg, &pyobj_nextImg, &pyobj_prevPts, &pyobj_nextPts, &pyobj_status, &pyobj_err) &&
        jsopencv_to_safe(info, pyobj_prevImg, prevImg, ArgInfo("prevImg", 0)) &&
        jsopencv_to_safe(info, pyobj_nextImg, nextImg, ArgInfo("nextImg", 0)) &&
        jsopencv_to_safe(info, pyobj_prevPts, prevPts, ArgInfo("prevPts", 0)) &&
        jsopencv_to_safe(info, pyobj_nextPts, nextPts, ArgInfo("nextPts", 1)) &&
        jsopencv_to_safe(info, pyobj_status, status, ArgInfo("status", 1)) &&
        jsopencv_to_safe(info, pyobj_err, err, ArgInfo("err", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->calc(prevImg, nextImg, prevPts, nextPts, status, err));
        return Py_BuildValue("(NNN)", jsopencv_from(nextPts), jsopencv_from(status), jsopencv_from(err));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_prevImg = NULL;
    UMat prevImg;
    Napi::Value* pyobj_nextImg = NULL;
    UMat nextImg;
    Napi::Value* pyobj_prevPts = NULL;
    UMat prevPts;
    Napi::Value* pyobj_nextPts = NULL;
    UMat nextPts;
    Napi::Value* pyobj_status = NULL;
    UMat status;
    Napi::Value* pyobj_err = NULL;
    UMat err;

    const char* keywords[] = { "prevImg", "nextImg", "prevPts", "nextPts", "status", "err", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OO:SparseOpticalFlow.calc", (char**)keywords, &pyobj_prevImg, &pyobj_nextImg, &pyobj_prevPts, &pyobj_nextPts, &pyobj_status, &pyobj_err) &&
        jsopencv_to_safe(info, pyobj_prevImg, prevImg, ArgInfo("prevImg", 0)) &&
        jsopencv_to_safe(info, pyobj_nextImg, nextImg, ArgInfo("nextImg", 0)) &&
        jsopencv_to_safe(info, pyobj_prevPts, prevPts, ArgInfo("prevPts", 0)) &&
        jsopencv_to_safe(info, pyobj_nextPts, nextPts, ArgInfo("nextPts", 1)) &&
        jsopencv_to_safe(info, pyobj_status, status, ArgInfo("status", 1)) &&
        jsopencv_to_safe(info, pyobj_err, err, ArgInfo("err", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->calc(prevImg, nextImg, prevPts, nextPts, status, err));
        return Py_BuildValue("(NNN)", jsopencv_from(nextPts), jsopencv_from(status), jsopencv_from(err));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("calc");

    return NULL;
}



// Tables (SparseOpticalFlow)

static PyGetSetDef pyopencv_SparseOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_SparseOpticalFlow_methods[] =
{
    {"calc", CV_JS_FN_WITH_KW_(pyopencv_cv_SparseOpticalFlow_calc, 0), "calc(prevImg, nextImg, prevPts, nextPts[, status[, err]]) -> nextPts, status, err\n.   @brief Calculates a sparse optical flow.\n.   \n.       @param prevImg First input image.\n.       @param nextImg Second input image of the same size and the same type as prevImg.\n.       @param prevPts Vector of 2D points for which the flow needs to be found.\n.       @param nextPts Output vector of 2D points containing the calculated new positions of input features in the second image.\n.       @param status Output status vector. Each element of the vector is set to 1 if the\n.                     flow for the corresponding features has been found. Otherwise, it is set to 0.\n.       @param err Optional output vector that contains error response for each point (inverse confidence)."},

    {NULL,          NULL}
};

// Converter (SparseOpticalFlow)

template<>
struct PyOpenCV_Converter< Ptr<cv::SparseOpticalFlow> >
{
    static PyObject* from(const Ptr<cv::SparseOpticalFlow>& r)
    {
        return pyopencv_SparseOpticalFlow_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::SparseOpticalFlow>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::SparseOpticalFlow> * dst_;
        if (pyopencv_SparseOpticalFlow_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::SparseOpticalFlow> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// SparsePyrLKOpticalFlow (Generic)
//================================================================================

// GetSet (SparsePyrLKOpticalFlow)



// Methods (SparsePyrLKOpticalFlow)

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_winSize = NULL;
    Size winSize=Size(21, 21);
    Napi::Value* pyobj_maxLevel = NULL;
    int maxLevel=3;
    Napi::Value* pyobj_crit = NULL;
    TermCriteria crit=TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01);
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    Napi::Value* pyobj_minEigThreshold = NULL;
    double minEigThreshold=1e-4;
    Ptr<SparsePyrLKOpticalFlow> retval;

    const char* keywords[] = { "winSize", "maxLevel", "crit", "flags", "minEigThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:SparsePyrLKOpticalFlow.create", (char**)keywords, &pyobj_winSize, &pyobj_maxLevel, &pyobj_crit, &pyobj_flags, &pyobj_minEigThreshold) &&
        jsopencv_to_safe(info, pyobj_winSize, winSize, ArgInfo("winSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxLevel, maxLevel, ArgInfo("maxLevel", 0)) &&
        jsopencv_to_safe(info, pyobj_crit, crit, ArgInfo("crit", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)) &&
        jsopencv_to_safe(info, pyobj_minEigThreshold, minEigThreshold, ArgInfo("minEigThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::SparsePyrLKOpticalFlow::create(winSize, maxLevel, crit, flags, minEigThreshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_getFlags(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFlags());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_getMaxLevel(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxLevel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_getMinEigThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinEigThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_getWinSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWinSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_setFlags(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_flags = NULL;
    int flags=0;

    const char* keywords[] = { "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SparsePyrLKOpticalFlow.setFlags", (char**)keywords, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFlags(flags));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_setMaxLevel(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_maxLevel = NULL;
    int maxLevel=0;

    const char* keywords[] = { "maxLevel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SparsePyrLKOpticalFlow.setMaxLevel", (char**)keywords, &pyobj_maxLevel) &&
        jsopencv_to_safe(info, pyobj_maxLevel, maxLevel, ArgInfo("maxLevel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxLevel(maxLevel));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_setMinEigThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_minEigThreshold = NULL;
    double minEigThreshold=0;

    const char* keywords[] = { "minEigThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SparsePyrLKOpticalFlow.setMinEigThreshold", (char**)keywords, &pyobj_minEigThreshold) &&
        jsopencv_to_safe(info, pyobj_minEigThreshold, minEigThreshold, ArgInfo("minEigThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinEigThreshold(minEigThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_crit = NULL;
    TermCriteria crit;

    const char* keywords[] = { "crit", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SparsePyrLKOpticalFlow.setTermCriteria", (char**)keywords, &pyobj_crit) &&
        jsopencv_to_safe(info, pyobj_crit, crit, ArgInfo("crit", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(crit));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_SparsePyrLKOpticalFlow_setWinSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::SparsePyrLKOpticalFlow> * self1 = 0;
    if (!pyopencv_SparsePyrLKOpticalFlow_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Ptr<cv::SparsePyrLKOpticalFlow> _self_ = *(self1);
    Napi::Value* pyobj_winSize = NULL;
    Size winSize;

    const char* keywords[] = { "winSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SparsePyrLKOpticalFlow.setWinSize", (char**)keywords, &pyobj_winSize) &&
        jsopencv_to_safe(info, pyobj_winSize, winSize, ArgInfo("winSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWinSize(winSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (SparsePyrLKOpticalFlow)

static PyGetSetDef pyopencv_SparsePyrLKOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_SparsePyrLKOpticalFlow_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_create_static, METH_STATIC), "create([, winSize[, maxLevel[, crit[, flags[, minEigThreshold]]]]]) -> retval\n."},
    {"getFlags", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getFlags, 0), "getFlags() -> retval\n."},
    {"getMaxLevel", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getMaxLevel, 0), "getMaxLevel() -> retval\n."},
    {"getMinEigThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getMinEigThreshold, 0), "getMinEigThreshold() -> retval\n."},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getTermCriteria, 0), "getTermCriteria() -> retval\n."},
    {"getWinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getWinSize, 0), "getWinSize() -> retval\n."},
    {"setFlags", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setFlags, 0), "setFlags(flags) -> None\n."},
    {"setMaxLevel", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setMaxLevel, 0), "setMaxLevel(maxLevel) -> None\n."},
    {"setMinEigThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setMinEigThreshold, 0), "setMinEigThreshold(minEigThreshold) -> None\n."},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setTermCriteria, 0), "setTermCriteria(crit) -> None\n."},
    {"setWinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setWinSize, 0), "setWinSize(winSize) -> None\n."},

    {NULL,          NULL}
};

// Converter (SparsePyrLKOpticalFlow)

template<>
struct PyOpenCV_Converter< Ptr<cv::SparsePyrLKOpticalFlow> >
{
    static PyObject* from(const Ptr<cv::SparsePyrLKOpticalFlow>& r)
    {
        return pyopencv_SparsePyrLKOpticalFlow_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::SparsePyrLKOpticalFlow>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::SparsePyrLKOpticalFlow> * dst_;
        if (pyopencv_SparsePyrLKOpticalFlow_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::SparsePyrLKOpticalFlow> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// StereoBM (Generic)
//================================================================================

// GetSet (StereoBM)



// Methods (StereoBM)

static Napi::Value pyopencv_cv_StereoBM_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_numDisparities = NULL;
    int numDisparities=0;
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=21;
    Ptr<StereoBM> retval;

    const char* keywords[] = { "numDisparities", "blockSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:StereoBM.create", (char**)keywords, &pyobj_numDisparities, &pyobj_blockSize) &&
        jsopencv_to_safe(info, pyobj_numDisparities, numDisparities, ArgInfo("numDisparities", 0)) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::StereoBM::create(numDisparities, blockSize));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getPreFilterCap(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPreFilterCap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getPreFilterSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPreFilterSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getPreFilterType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPreFilterType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getROI1(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Rect retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getROI1());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getROI2(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Rect retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getROI2());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getSmallerBlockSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSmallerBlockSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getTextureThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTextureThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_getUniquenessRatio(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUniquenessRatio());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setPreFilterCap(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_preFilterCap = NULL;
    int preFilterCap=0;

    const char* keywords[] = { "preFilterCap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setPreFilterCap", (char**)keywords, &pyobj_preFilterCap) &&
        jsopencv_to_safe(info, pyobj_preFilterCap, preFilterCap, ArgInfo("preFilterCap", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreFilterCap(preFilterCap));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setPreFilterSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_preFilterSize = NULL;
    int preFilterSize=0;

    const char* keywords[] = { "preFilterSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setPreFilterSize", (char**)keywords, &pyobj_preFilterSize) &&
        jsopencv_to_safe(info, pyobj_preFilterSize, preFilterSize, ArgInfo("preFilterSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreFilterSize(preFilterSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setPreFilterType(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_preFilterType = NULL;
    int preFilterType=0;

    const char* keywords[] = { "preFilterType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setPreFilterType", (char**)keywords, &pyobj_preFilterType) &&
        jsopencv_to_safe(info, pyobj_preFilterType, preFilterType, ArgInfo("preFilterType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreFilterType(preFilterType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setROI1(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_roi1 = NULL;
    Rect roi1;

    const char* keywords[] = { "roi1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setROI1", (char**)keywords, &pyobj_roi1) &&
        jsopencv_to_safe(info, pyobj_roi1, roi1, ArgInfo("roi1", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setROI1(roi1));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setROI2(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_roi2 = NULL;
    Rect roi2;

    const char* keywords[] = { "roi2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setROI2", (char**)keywords, &pyobj_roi2) &&
        jsopencv_to_safe(info, pyobj_roi2, roi2, ArgInfo("roi2", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setROI2(roi2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setSmallerBlockSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=0;

    const char* keywords[] = { "blockSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setSmallerBlockSize", (char**)keywords, &pyobj_blockSize) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSmallerBlockSize(blockSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setTextureThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_textureThreshold = NULL;
    int textureThreshold=0;

    const char* keywords[] = { "textureThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setTextureThreshold", (char**)keywords, &pyobj_textureThreshold) &&
        jsopencv_to_safe(info, pyobj_textureThreshold, textureThreshold, ArgInfo("textureThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTextureThreshold(textureThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoBM_setUniquenessRatio(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoBM> * self1 = 0;
    if (!pyopencv_StereoBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Ptr<cv::StereoBM> _self_ = *(self1);
    Napi::Value* pyobj_uniquenessRatio = NULL;
    int uniquenessRatio=0;

    const char* keywords[] = { "uniquenessRatio", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoBM.setUniquenessRatio", (char**)keywords, &pyobj_uniquenessRatio) &&
        jsopencv_to_safe(info, pyobj_uniquenessRatio, uniquenessRatio, ArgInfo("uniquenessRatio", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUniquenessRatio(uniquenessRatio));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (StereoBM)

static PyGetSetDef pyopencv_StereoBM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_StereoBM_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_create_static, METH_STATIC), "create([, numDisparities[, blockSize]]) -> retval\n.   @brief Creates StereoBM object\n.   \n.       @param numDisparities the disparity search range. For each pixel algorithm will find the best\n.       disparity from 0 (default minimum disparity) to numDisparities. The search range can then be\n.       shifted by changing the minimum disparity.\n.       @param blockSize the linear size of the blocks compared by the algorithm. The size should be odd\n.       (as the block is centered at the current pixel). Larger block size implies smoother, though less\n.       accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher\n.       chance for algorithm to find a wrong correspondence.\n.   \n.       The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for\n.       a specific stereo pair."},
    {"getPreFilterCap", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getPreFilterCap, 0), "getPreFilterCap() -> retval\n."},
    {"getPreFilterSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getPreFilterSize, 0), "getPreFilterSize() -> retval\n."},
    {"getPreFilterType", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getPreFilterType, 0), "getPreFilterType() -> retval\n."},
    {"getROI1", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getROI1, 0), "getROI1() -> retval\n."},
    {"getROI2", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getROI2, 0), "getROI2() -> retval\n."},
    {"getSmallerBlockSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getSmallerBlockSize, 0), "getSmallerBlockSize() -> retval\n."},
    {"getTextureThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getTextureThreshold, 0), "getTextureThreshold() -> retval\n."},
    {"getUniquenessRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_getUniquenessRatio, 0), "getUniquenessRatio() -> retval\n."},
    {"setPreFilterCap", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setPreFilterCap, 0), "setPreFilterCap(preFilterCap) -> None\n."},
    {"setPreFilterSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setPreFilterSize, 0), "setPreFilterSize(preFilterSize) -> None\n."},
    {"setPreFilterType", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setPreFilterType, 0), "setPreFilterType(preFilterType) -> None\n."},
    {"setROI1", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setROI1, 0), "setROI1(roi1) -> None\n."},
    {"setROI2", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setROI2, 0), "setROI2(roi2) -> None\n."},
    {"setSmallerBlockSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setSmallerBlockSize, 0), "setSmallerBlockSize(blockSize) -> None\n."},
    {"setTextureThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setTextureThreshold, 0), "setTextureThreshold(textureThreshold) -> None\n."},
    {"setUniquenessRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoBM_setUniquenessRatio, 0), "setUniquenessRatio(uniquenessRatio) -> None\n."},

    {NULL,          NULL}
};

// Converter (StereoBM)

template<>
struct PyOpenCV_Converter< Ptr<cv::StereoBM> >
{
    static PyObject* from(const Ptr<cv::StereoBM>& r)
    {
        return pyopencv_StereoBM_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::StereoBM>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::StereoBM> * dst_;
        if (pyopencv_StereoBM_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::StereoBM> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// StereoMatcher (Generic)
//================================================================================

// GetSet (StereoMatcher)



// Methods (StereoMatcher)

static Napi::Value pyopencv_cv_StereoMatcher_compute(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_left = NULL;
    Mat left;
    Napi::Value* pyobj_right = NULL;
    Mat right;
    Napi::Value* pyobj_disparity = NULL;
    Mat disparity;

    const char* keywords[] = { "left", "right", "disparity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:StereoMatcher.compute", (char**)keywords, &pyobj_left, &pyobj_right, &pyobj_disparity) &&
        jsopencv_to_safe(info, pyobj_left, left, ArgInfo("left", 0)) &&
        jsopencv_to_safe(info, pyobj_right, right, ArgInfo("right", 0)) &&
        jsopencv_to_safe(info, pyobj_disparity, disparity, ArgInfo("disparity", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(left, right, disparity));
        return jsopencv_from(disparity);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_left = NULL;
    UMat left;
    Napi::Value* pyobj_right = NULL;
    UMat right;
    Napi::Value* pyobj_disparity = NULL;
    UMat disparity;

    const char* keywords[] = { "left", "right", "disparity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:StereoMatcher.compute", (char**)keywords, &pyobj_left, &pyobj_right, &pyobj_disparity) &&
        jsopencv_to_safe(info, pyobj_left, left, ArgInfo("left", 0)) &&
        jsopencv_to_safe(info, pyobj_right, right, ArgInfo("right", 0)) &&
        jsopencv_to_safe(info, pyobj_disparity, disparity, ArgInfo("disparity", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(left, right, disparity));
        return jsopencv_from(disparity);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_getBlockSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBlockSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_getDisp12MaxDiff(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDisp12MaxDiff());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_getMinDisparity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinDisparity());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_getNumDisparities(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumDisparities());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_getSpeckleRange(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSpeckleRange());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_getSpeckleWindowSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSpeckleWindowSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_setBlockSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=0;

    const char* keywords[] = { "blockSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoMatcher.setBlockSize", (char**)keywords, &pyobj_blockSize) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBlockSize(blockSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_setDisp12MaxDiff(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    Napi::Value* pyobj_disp12MaxDiff = NULL;
    int disp12MaxDiff=0;

    const char* keywords[] = { "disp12MaxDiff", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoMatcher.setDisp12MaxDiff", (char**)keywords, &pyobj_disp12MaxDiff) &&
        jsopencv_to_safe(info, pyobj_disp12MaxDiff, disp12MaxDiff, ArgInfo("disp12MaxDiff", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDisp12MaxDiff(disp12MaxDiff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_setMinDisparity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    Napi::Value* pyobj_minDisparity = NULL;
    int minDisparity=0;

    const char* keywords[] = { "minDisparity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoMatcher.setMinDisparity", (char**)keywords, &pyobj_minDisparity) &&
        jsopencv_to_safe(info, pyobj_minDisparity, minDisparity, ArgInfo("minDisparity", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinDisparity(minDisparity));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_setNumDisparities(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    Napi::Value* pyobj_numDisparities = NULL;
    int numDisparities=0;

    const char* keywords[] = { "numDisparities", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoMatcher.setNumDisparities", (char**)keywords, &pyobj_numDisparities) &&
        jsopencv_to_safe(info, pyobj_numDisparities, numDisparities, ArgInfo("numDisparities", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumDisparities(numDisparities));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_setSpeckleRange(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    Napi::Value* pyobj_speckleRange = NULL;
    int speckleRange=0;

    const char* keywords[] = { "speckleRange", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoMatcher.setSpeckleRange", (char**)keywords, &pyobj_speckleRange) &&
        jsopencv_to_safe(info, pyobj_speckleRange, speckleRange, ArgInfo("speckleRange", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSpeckleRange(speckleRange));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoMatcher_setSpeckleWindowSize(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoMatcher> * self1 = 0;
    if (!pyopencv_StereoMatcher_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    Ptr<cv::StereoMatcher> _self_ = *(self1);
    Napi::Value* pyobj_speckleWindowSize = NULL;
    int speckleWindowSize=0;

    const char* keywords[] = { "speckleWindowSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoMatcher.setSpeckleWindowSize", (char**)keywords, &pyobj_speckleWindowSize) &&
        jsopencv_to_safe(info, pyobj_speckleWindowSize, speckleWindowSize, ArgInfo("speckleWindowSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSpeckleWindowSize(speckleWindowSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (StereoMatcher)

static PyGetSetDef pyopencv_StereoMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_StereoMatcher_methods[] =
{
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_compute, 0), "compute(left, right[, disparity]) -> disparity\n.   @brief Computes disparity map for the specified stereo pair\n.   \n.       @param left Left 8-bit single-channel image.\n.       @param right Right image of the same size and the same type as the left one.\n.       @param disparity Output disparity map. It has the same size as the input images. Some algorithms,\n.       like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value\n.       has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map."},
    {"getBlockSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getBlockSize, 0), "getBlockSize() -> retval\n."},
    {"getDisp12MaxDiff", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getDisp12MaxDiff, 0), "getDisp12MaxDiff() -> retval\n."},
    {"getMinDisparity", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getMinDisparity, 0), "getMinDisparity() -> retval\n."},
    {"getNumDisparities", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getNumDisparities, 0), "getNumDisparities() -> retval\n."},
    {"getSpeckleRange", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getSpeckleRange, 0), "getSpeckleRange() -> retval\n."},
    {"getSpeckleWindowSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getSpeckleWindowSize, 0), "getSpeckleWindowSize() -> retval\n."},
    {"setBlockSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setBlockSize, 0), "setBlockSize(blockSize) -> None\n."},
    {"setDisp12MaxDiff", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setDisp12MaxDiff, 0), "setDisp12MaxDiff(disp12MaxDiff) -> None\n."},
    {"setMinDisparity", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setMinDisparity, 0), "setMinDisparity(minDisparity) -> None\n."},
    {"setNumDisparities", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setNumDisparities, 0), "setNumDisparities(numDisparities) -> None\n."},
    {"setSpeckleRange", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setSpeckleRange, 0), "setSpeckleRange(speckleRange) -> None\n."},
    {"setSpeckleWindowSize", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setSpeckleWindowSize, 0), "setSpeckleWindowSize(speckleWindowSize) -> None\n."},

    {NULL,          NULL}
};

// Converter (StereoMatcher)

template<>
struct PyOpenCV_Converter< Ptr<cv::StereoMatcher> >
{
    static PyObject* from(const Ptr<cv::StereoMatcher>& r)
    {
        return pyopencv_StereoMatcher_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::StereoMatcher>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::StereoMatcher> * dst_;
        if (pyopencv_StereoMatcher_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::StereoMatcher> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// StereoSGBM (Generic)
//================================================================================

// GetSet (StereoSGBM)



// Methods (StereoSGBM)

static Napi::Value pyopencv_cv_StereoSGBM_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_minDisparity = NULL;
    int minDisparity=0;
    Napi::Value* pyobj_numDisparities = NULL;
    int numDisparities=16;
    Napi::Value* pyobj_blockSize = NULL;
    int blockSize=3;
    Napi::Value* pyobj_P1 = NULL;
    int P1=0;
    Napi::Value* pyobj_P2 = NULL;
    int P2=0;
    Napi::Value* pyobj_disp12MaxDiff = NULL;
    int disp12MaxDiff=0;
    Napi::Value* pyobj_preFilterCap = NULL;
    int preFilterCap=0;
    Napi::Value* pyobj_uniquenessRatio = NULL;
    int uniquenessRatio=0;
    Napi::Value* pyobj_speckleWindowSize = NULL;
    int speckleWindowSize=0;
    Napi::Value* pyobj_speckleRange = NULL;
    int speckleRange=0;
    Napi::Value* pyobj_mode = NULL;
    int mode=StereoSGBM::MODE_SGBM;
    Ptr<StereoSGBM> retval;

    const char* keywords[] = { "minDisparity", "numDisparities", "blockSize", "P1", "P2", "disp12MaxDiff", "preFilterCap", "uniquenessRatio", "speckleWindowSize", "speckleRange", "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOOOOO:StereoSGBM.create", (char**)keywords, &pyobj_minDisparity, &pyobj_numDisparities, &pyobj_blockSize, &pyobj_P1, &pyobj_P2, &pyobj_disp12MaxDiff, &pyobj_preFilterCap, &pyobj_uniquenessRatio, &pyobj_speckleWindowSize, &pyobj_speckleRange, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_minDisparity, minDisparity, ArgInfo("minDisparity", 0)) &&
        jsopencv_to_safe(info, pyobj_numDisparities, numDisparities, ArgInfo("numDisparities", 0)) &&
        jsopencv_to_safe(info, pyobj_blockSize, blockSize, ArgInfo("blockSize", 0)) &&
        jsopencv_to_safe(info, pyobj_P1, P1, ArgInfo("P1", 0)) &&
        jsopencv_to_safe(info, pyobj_P2, P2, ArgInfo("P2", 0)) &&
        jsopencv_to_safe(info, pyobj_disp12MaxDiff, disp12MaxDiff, ArgInfo("disp12MaxDiff", 0)) &&
        jsopencv_to_safe(info, pyobj_preFilterCap, preFilterCap, ArgInfo("preFilterCap", 0)) &&
        jsopencv_to_safe(info, pyobj_uniquenessRatio, uniquenessRatio, ArgInfo("uniquenessRatio", 0)) &&
        jsopencv_to_safe(info, pyobj_speckleWindowSize, speckleWindowSize, ArgInfo("speckleWindowSize", 0)) &&
        jsopencv_to_safe(info, pyobj_speckleRange, speckleRange, ArgInfo("speckleRange", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::StereoSGBM::create(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio, speckleWindowSize, speckleRange, mode));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_getMode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMode());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_getP1(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getP1());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_getP2(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getP2());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_getPreFilterCap(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPreFilterCap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_getUniquenessRatio(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUniquenessRatio());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_setMode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    Napi::Value* pyobj_mode = NULL;
    int mode=0;

    const char* keywords[] = { "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoSGBM.setMode", (char**)keywords, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMode(mode));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_setP1(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    Napi::Value* pyobj_P1 = NULL;
    int P1=0;

    const char* keywords[] = { "P1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoSGBM.setP1", (char**)keywords, &pyobj_P1) &&
        jsopencv_to_safe(info, pyobj_P1, P1, ArgInfo("P1", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setP1(P1));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_setP2(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    Napi::Value* pyobj_P2 = NULL;
    int P2=0;

    const char* keywords[] = { "P2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoSGBM.setP2", (char**)keywords, &pyobj_P2) &&
        jsopencv_to_safe(info, pyobj_P2, P2, ArgInfo("P2", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setP2(P2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_setPreFilterCap(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    Napi::Value* pyobj_preFilterCap = NULL;
    int preFilterCap=0;

    const char* keywords[] = { "preFilterCap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoSGBM.setPreFilterCap", (char**)keywords, &pyobj_preFilterCap) &&
        jsopencv_to_safe(info, pyobj_preFilterCap, preFilterCap, ArgInfo("preFilterCap", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreFilterCap(preFilterCap));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_StereoSGBM_setUniquenessRatio(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::StereoSGBM> * self1 = 0;
    if (!pyopencv_StereoSGBM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    Ptr<cv::StereoSGBM> _self_ = *(self1);
    Napi::Value* pyobj_uniquenessRatio = NULL;
    int uniquenessRatio=0;

    const char* keywords[] = { "uniquenessRatio", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:StereoSGBM.setUniquenessRatio", (char**)keywords, &pyobj_uniquenessRatio) &&
        jsopencv_to_safe(info, pyobj_uniquenessRatio, uniquenessRatio, ArgInfo("uniquenessRatio", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUniquenessRatio(uniquenessRatio));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (StereoSGBM)

static PyGetSetDef pyopencv_StereoSGBM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_StereoSGBM_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_create_static, METH_STATIC), "create([, minDisparity[, numDisparities[, blockSize[, P1[, P2[, disp12MaxDiff[, preFilterCap[, uniquenessRatio[, speckleWindowSize[, speckleRange[, mode]]]]]]]]]]]) -> retval\n.   @brief Creates StereoSGBM object\n.   \n.       @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes\n.       rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.\n.       @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than\n.       zero. In the current implementation, this parameter must be divisible by 16.\n.       @param blockSize Matched block size. It must be an odd number \\>=1 . Normally, it should be\n.       somewhere in the 3..11 range.\n.       @param P1 The first parameter controlling the disparity smoothness. See below.\n.       @param P2 The second parameter controlling the disparity smoothness. The larger the values are,\n.       the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1\n.       between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor\n.       pixels. The algorithm requires P2 \\> P1 . See stereo_match.cpp sample where some reasonably good\n.       P1 and P2 values are shown (like 8\\*number_of_image_channels\\*blockSize\\*blockSize and\n.       32\\*number_of_image_channels\\*blockSize\\*blockSize , respectively).\n.       @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right\n.       disparity check. Set it to a non-positive value to disable the check.\n.       @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first\n.       computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.\n.       The result values are passed to the Birchfield-Tomasi pixel cost function.\n.       @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function\n.       value should \"win\" the second best value to consider the found match correct. Normally, a value\n.       within the 5-15 range is good enough.\n.       @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles\n.       and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the\n.       50-200 range.\n.       @param speckleRange Maximum disparity variation within each connected component. If you do speckle\n.       filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.\n.       Normally, 1 or 2 is good enough.\n.       @param mode Set it to StereoSGBM::MODE_HH to run the full-scale two-pass dynamic programming\n.       algorithm. It will consume O(W\\*H\\*numDisparities) bytes, which is large for 640x480 stereo and\n.       huge for HD-size pictures. By default, it is set to false .\n.   \n.       The first constructor initializes StereoSGBM with all the default parameters. So, you only have to\n.       set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter\n.       to a custom value."},
    {"getMode", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getMode, 0), "getMode() -> retval\n."},
    {"getP1", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getP1, 0), "getP1() -> retval\n."},
    {"getP2", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getP2, 0), "getP2() -> retval\n."},
    {"getPreFilterCap", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getPreFilterCap, 0), "getPreFilterCap() -> retval\n."},
    {"getUniquenessRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getUniquenessRatio, 0), "getUniquenessRatio() -> retval\n."},
    {"setMode", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setMode, 0), "setMode(mode) -> None\n."},
    {"setP1", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setP1, 0), "setP1(P1) -> None\n."},
    {"setP2", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setP2, 0), "setP2(P2) -> None\n."},
    {"setPreFilterCap", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setPreFilterCap, 0), "setPreFilterCap(preFilterCap) -> None\n."},
    {"setUniquenessRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setUniquenessRatio, 0), "setUniquenessRatio(uniquenessRatio) -> None\n."},

    {NULL,          NULL}
};

// Converter (StereoSGBM)

template<>
struct PyOpenCV_Converter< Ptr<cv::StereoSGBM> >
{
    static PyObject* from(const Ptr<cv::StereoSGBM>& r)
    {
        return pyopencv_StereoSGBM_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::StereoSGBM>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::StereoSGBM> * dst_;
        if (pyopencv_StereoSGBM_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::StereoSGBM> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// Subdiv2D (Generic)
//================================================================================

// GetSet (Subdiv2D)



// Methods (Subdiv2D)

static int pyopencv_cv_Subdiv2D_Subdiv2D(pyopencv_Subdiv2D_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::Subdiv2D>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::Subdiv2D()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rect = NULL;
    Rect rect;

    const char* keywords[] = { "rect", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D", (char**)keywords, &pyobj_rect) &&
        jsopencv_to_safe(info, pyobj_rect, rect, ArgInfo("rect", 0)))
    {
        new (&(self->v)) Ptr<cv::Subdiv2D>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::Subdiv2D(rect)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Subdiv2D");

    return -1;
}

static Napi::Value pyopencv_cv_Subdiv2D_edgeDst(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_edge = NULL;
    int edge=0;
    Point2f dstpt;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.edgeDst", (char**)keywords, &pyobj_edge) &&
        jsopencv_to_safe(info, pyobj_edge, edge, ArgInfo("edge", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->edgeDst(edge, &dstpt));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(dstpt));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_edgeOrg(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_edge = NULL;
    int edge=0;
    Point2f orgpt;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.edgeOrg", (char**)keywords, &pyobj_edge) &&
        jsopencv_to_safe(info, pyobj_edge, edge, ArgInfo("edge", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->edgeOrg(edge, &orgpt));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(orgpt));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_findNearest(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_pt = NULL;
    Point2f pt;
    Point2f nearestPt;
    int retval;

    const char* keywords[] = { "pt", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.findNearest", (char**)keywords, &pyobj_pt) &&
        jsopencv_to_safe(info, pyobj_pt, pt, ArgInfo("pt", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->findNearest(pt, &nearestPt));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(nearestPt));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_getEdge(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_edge = NULL;
    int edge=0;
    Napi::Value* pyobj_nextEdgeType = NULL;
    int nextEdgeType=0;
    int retval;

    const char* keywords[] = { "edge", "nextEdgeType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:Subdiv2D.getEdge", (char**)keywords, &pyobj_edge, &pyobj_nextEdgeType) &&
        jsopencv_to_safe(info, pyobj_edge, edge, ArgInfo("edge", 0)) &&
        jsopencv_to_safe(info, pyobj_nextEdgeType, nextEdgeType, ArgInfo("nextEdgeType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEdge(edge, nextEdgeType));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_getEdgeList(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    vector_Vec4f edgeList;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->getEdgeList(edgeList));
        return jsopencv_from(edgeList);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_getLeadingEdgeList(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    vector_int leadingEdgeList;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->getLeadingEdgeList(leadingEdgeList));
        return jsopencv_from(leadingEdgeList);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_getTriangleList(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    vector_Vec6f triangleList;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->getTriangleList(triangleList));
        return jsopencv_from(triangleList);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_getVertex(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_vertex = NULL;
    int vertex=0;
    int firstEdge;
    Point2f retval;

    const char* keywords[] = { "vertex", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.getVertex", (char**)keywords, &pyobj_vertex) &&
        jsopencv_to_safe(info, pyobj_vertex, vertex, ArgInfo("vertex", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVertex(vertex, &firstEdge));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(firstEdge));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_getVoronoiFacetList(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_idx = NULL;
    vector_int idx;
    vector_vector_Point2f facetList;
    vector_Point2f facetCenters;

    const char* keywords[] = { "idx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.getVoronoiFacetList", (char**)keywords, &pyobj_idx) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getVoronoiFacetList(idx, facetList, facetCenters));
        return Py_BuildValue("(NN)", jsopencv_from(facetList), jsopencv_from(facetCenters));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_initDelaunay(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_rect = NULL;
    Rect rect;

    const char* keywords[] = { "rect", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.initDelaunay", (char**)keywords, &pyobj_rect) &&
        jsopencv_to_safe(info, pyobj_rect, rect, ArgInfo("rect", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->initDelaunay(rect));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_insert(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_pt = NULL;
    Point2f pt;
    int retval;

    const char* keywords[] = { "pt", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.insert", (char**)keywords, &pyobj_pt) &&
        jsopencv_to_safe(info, pyobj_pt, pt, ArgInfo("pt", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->insert(pt));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ptvec = NULL;
    vector_Point2f ptvec;

    const char* keywords[] = { "ptvec", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.insert", (char**)keywords, &pyobj_ptvec) &&
        jsopencv_to_safe(info, pyobj_ptvec, ptvec, ArgInfo("ptvec", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->insert(ptvec));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("insert");

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_locate(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_pt = NULL;
    Point2f pt;
    int edge;
    int vertex;
    int retval;

    const char* keywords[] = { "pt", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.locate", (char**)keywords, &pyobj_pt) &&
        jsopencv_to_safe(info, pyobj_pt, pt, ArgInfo("pt", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->locate(pt, edge, vertex));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(edge), jsopencv_from(vertex));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_nextEdge(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_edge = NULL;
    int edge=0;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.nextEdge", (char**)keywords, &pyobj_edge) &&
        jsopencv_to_safe(info, pyobj_edge, edge, ArgInfo("edge", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nextEdge(edge));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_rotateEdge(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_edge = NULL;
    int edge=0;
    Napi::Value* pyobj_rotate = NULL;
    int rotate=0;
    int retval;

    const char* keywords[] = { "edge", "rotate", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:Subdiv2D.rotateEdge", (char**)keywords, &pyobj_edge, &pyobj_rotate) &&
        jsopencv_to_safe(info, pyobj_edge, edge, ArgInfo("edge", 0)) &&
        jsopencv_to_safe(info, pyobj_rotate, rotate, ArgInfo("rotate", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->rotateEdge(edge, rotate));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Subdiv2D_symEdge(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Subdiv2D> * self1 = 0;
    if (!pyopencv_Subdiv2D_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    Ptr<cv::Subdiv2D> _self_ = *(self1);
    Napi::Value* pyobj_edge = NULL;
    int edge=0;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Subdiv2D.symEdge", (char**)keywords, &pyobj_edge) &&
        jsopencv_to_safe(info, pyobj_edge, edge, ArgInfo("edge", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->symEdge(edge));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (Subdiv2D)

static PyGetSetDef pyopencv_Subdiv2D_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_Subdiv2D_methods[] =
{
    {"edgeDst", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_edgeDst, 0), "edgeDst(edge) -> retval, dstpt\n.   @brief Returns the edge destination.\n.   \n.       @param edge Subdivision edge ID.\n.       @param dstpt Output vertex location.\n.   \n.       @returns vertex ID."},
    {"edgeOrg", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_edgeOrg, 0), "edgeOrg(edge) -> retval, orgpt\n.   @brief Returns the edge origin.\n.   \n.       @param edge Subdivision edge ID.\n.       @param orgpt Output vertex location.\n.   \n.       @returns vertex ID."},
    {"findNearest", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_findNearest, 0), "findNearest(pt) -> retval, nearestPt\n.   @brief Finds the subdivision vertex closest to the given point.\n.   \n.       @param pt Input point.\n.       @param nearestPt Output subdivision vertex point.\n.   \n.       The function is another function that locates the input point within the subdivision. It finds the\n.       subdivision vertex that is the closest to the input point. It is not necessarily one of vertices\n.       of the facet containing the input point, though the facet (located using locate() ) is used as a\n.       starting point.\n.   \n.       @returns vertex ID."},
    {"getEdge", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getEdge, 0), "getEdge(edge, nextEdgeType) -> retval\n.   @brief Returns one of the edges related to the given edge.\n.   \n.       @param edge Subdivision edge ID.\n.       @param nextEdgeType Parameter specifying which of the related edges to return.\n.       The following values are possible:\n.       -   NEXT_AROUND_ORG next around the edge origin ( eOnext on the picture below if e is the input edge)\n.       -   NEXT_AROUND_DST next around the edge vertex ( eDnext )\n.       -   PREV_AROUND_ORG previous around the edge origin (reversed eRnext )\n.       -   PREV_AROUND_DST previous around the edge destination (reversed eLnext )\n.       -   NEXT_AROUND_LEFT next around the left facet ( eLnext )\n.       -   NEXT_AROUND_RIGHT next around the right facet ( eRnext )\n.       -   PREV_AROUND_LEFT previous around the left facet (reversed eOnext )\n.       -   PREV_AROUND_RIGHT previous around the right facet (reversed eDnext )\n.   \n.       ![sample output](pics/quadedge.png)\n.   \n.       @returns edge ID related to the input edge."},
    {"getEdgeList", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getEdgeList, 0), "getEdgeList() -> edgeList\n.   @brief Returns a list of all edges.\n.   \n.       @param edgeList Output vector.\n.   \n.       The function gives each edge as a 4 numbers vector, where each two are one of the edge\n.       vertices. i.e. org_x = v[0], org_y = v[1], dst_x = v[2], dst_y = v[3]."},
    {"getLeadingEdgeList", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getLeadingEdgeList, 0), "getLeadingEdgeList() -> leadingEdgeList\n.   @brief Returns a list of the leading edge ID connected to each triangle.\n.   \n.       @param leadingEdgeList Output vector.\n.   \n.       The function gives one edge ID for each triangle."},
    {"getTriangleList", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getTriangleList, 0), "getTriangleList() -> triangleList\n.   @brief Returns a list of all triangles.\n.   \n.       @param triangleList Output vector.\n.   \n.       The function gives each triangle as a 6 numbers vector, where each two are one of the triangle\n.       vertices. i.e. p1_x = v[0], p1_y = v[1], p2_x = v[2], p2_y = v[3], p3_x = v[4], p3_y = v[5]."},
    {"getVertex", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getVertex, 0), "getVertex(vertex) -> retval, firstEdge\n.   @brief Returns vertex location from vertex ID.\n.   \n.       @param vertex vertex ID.\n.       @param firstEdge Optional. The first edge ID which is connected to the vertex.\n.       @returns vertex (x,y)"},
    {"getVoronoiFacetList", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getVoronoiFacetList, 0), "getVoronoiFacetList(idx) -> facetList, facetCenters\n.   @brief Returns a list of all Voronoi facets.\n.   \n.       @param idx Vector of vertices IDs to consider. For all vertices you can pass empty vector.\n.       @param facetList Output vector of the Voronoi facets.\n.       @param facetCenters Output vector of the Voronoi facets center points."},
    {"initDelaunay", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_initDelaunay, 0), "initDelaunay(rect) -> None\n.   @brief Creates a new empty Delaunay subdivision\n.   \n.       @param rect Rectangle that includes all of the 2D points that are to be added to the subdivision."},
    {"insert", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_insert, 0), "insert(pt) -> retval\n.   @brief Insert a single point into a Delaunay triangulation.\n.   \n.       @param pt Point to insert.\n.   \n.       The function inserts a single point into a subdivision and modifies the subdivision topology\n.       appropriately. If a point with the same coordinates exists already, no new point is added.\n.       @returns the ID of the point.\n.   \n.       @note If the point is outside of the triangulation specified rect a runtime error is raised.\n\n\n\ninsert(ptvec) -> None\n.   @brief Insert multiple points into a Delaunay triangulation.\n.   \n.       @param ptvec Points to insert.\n.   \n.       The function inserts a vector of points into a subdivision and modifies the subdivision topology\n.       appropriately."},
    {"locate", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_locate, 0), "locate(pt) -> retval, edge, vertex\n.   @brief Returns the location of a point within a Delaunay triangulation.\n.   \n.       @param pt Point to locate.\n.       @param edge Output edge that the point belongs to or is located to the right of it.\n.       @param vertex Optional output vertex the input point coincides with.\n.   \n.       The function locates the input point within the subdivision and gives one of the triangle edges\n.       or vertices.\n.   \n.       @returns an integer which specify one of the following five cases for point location:\n.       -  The point falls into some facet. The function returns #PTLOC_INSIDE and edge will contain one of\n.          edges of the facet.\n.       -  The point falls onto the edge. The function returns #PTLOC_ON_EDGE and edge will contain this edge.\n.       -  The point coincides with one of the subdivision vertices. The function returns #PTLOC_VERTEX and\n.          vertex will contain a pointer to the vertex.\n.       -  The point is outside the subdivision reference rectangle. The function returns #PTLOC_OUTSIDE_RECT\n.          and no pointers are filled.\n.       -  One of input arguments is invalid. A runtime error is raised or, if silent or \"parent\" error\n.          processing mode is selected, #PTLOC_ERROR is returned."},
    {"nextEdge", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_nextEdge, 0), "nextEdge(edge) -> retval\n.   @brief Returns next edge around the edge origin.\n.   \n.       @param edge Subdivision edge ID.\n.   \n.       @returns an integer which is next edge ID around the edge origin: eOnext on the\n.       picture above if e is the input edge)."},
    {"rotateEdge", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_rotateEdge, 0), "rotateEdge(edge, rotate) -> retval\n.   @brief Returns another edge of the same quad-edge.\n.   \n.       @param edge Subdivision edge ID.\n.       @param rotate Parameter specifying which of the edges of the same quad-edge as the input\n.       one to return. The following values are possible:\n.       -   0 - the input edge ( e on the picture below if e is the input edge)\n.       -   1 - the rotated edge ( eRot )\n.       -   2 - the reversed edge (reversed e (in green))\n.       -   3 - the reversed rotated edge (reversed eRot (in green))\n.   \n.       @returns one of the edges ID of the same quad-edge as the input edge."},
    {"symEdge", CV_JS_FN_WITH_KW_(pyopencv_cv_Subdiv2D_symEdge, 0), "symEdge(edge) -> retval\n."},

    {NULL,          NULL}
};

// Converter (Subdiv2D)

template<>
struct PyOpenCV_Converter< Ptr<cv::Subdiv2D> >
{
    static PyObject* from(const Ptr<cv::Subdiv2D>& r)
    {
        return pyopencv_Subdiv2D_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::Subdiv2D>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::Subdiv2D> * dst_;
        if (pyopencv_Subdiv2D_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::Subdiv2D> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TickMeter (Generic)
//================================================================================

// GetSet (TickMeter)



// Methods (TickMeter)

static int pyopencv_cv_TickMeter_TickMeter(pyopencv_TickMeter_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::TickMeter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::TickMeter()));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_TickMeter_getAvgTimeMilli(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAvgTimeMilli());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getAvgTimeSec(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAvgTimeSec());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getCounter(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    int64 retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCounter());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getFPS(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFPS());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getTimeMicro(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTimeMicro());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getTimeMilli(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTimeMilli());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getTimeSec(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTimeSec());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_getTimeTicks(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);
    int64 retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTimeTicks());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_reset(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->reset());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_start(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->start());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TickMeter_stop(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TickMeter> * self1 = 0;
    if (!pyopencv_TickMeter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    Ptr<cv::TickMeter> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->stop());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (TickMeter)

static PyGetSetDef pyopencv_TickMeter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TickMeter_methods[] =
{
    {"getAvgTimeMilli", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getAvgTimeMilli, 0), "getAvgTimeMilli() -> retval\n."},
    {"getAvgTimeSec", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getAvgTimeSec, 0), "getAvgTimeSec() -> retval\n."},
    {"getCounter", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getCounter, 0), "getCounter() -> retval\n."},
    {"getFPS", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getFPS, 0), "getFPS() -> retval\n."},
    {"getTimeMicro", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeMicro, 0), "getTimeMicro() -> retval\n."},
    {"getTimeMilli", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeMilli, 0), "getTimeMilli() -> retval\n."},
    {"getTimeSec", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeSec, 0), "getTimeSec() -> retval\n."},
    {"getTimeTicks", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeTicks, 0), "getTimeTicks() -> retval\n."},
    {"reset", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_reset, 0), "reset() -> None\n."},
    {"start", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_start, 0), "start() -> None\n."},
    {"stop", CV_JS_FN_WITH_KW_(pyopencv_cv_TickMeter_stop, 0), "stop() -> None\n."},

    {NULL,          NULL}
};

// Converter (TickMeter)

template<>
struct PyOpenCV_Converter< Ptr<cv::TickMeter> >
{
    static PyObject* from(const Ptr<cv::TickMeter>& r)
    {
        return pyopencv_TickMeter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TickMeter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TickMeter> * dst_;
        if (pyopencv_TickMeter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TickMeter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// Tonemap (Generic)
//================================================================================

// GetSet (Tonemap)



// Methods (Tonemap)

static Napi::Value pyopencv_cv_Tonemap_getGamma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Tonemap> * self1 = 0;
    if (!pyopencv_Tonemap_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Tonemap' or its derivative)");
    Ptr<cv::Tonemap> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGamma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_Tonemap_process(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Tonemap> * self1 = 0;
    if (!pyopencv_Tonemap_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Tonemap' or its derivative)");
    Ptr<cv::Tonemap> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Tonemap.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Tonemap.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->process(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}

static Napi::Value pyopencv_cv_Tonemap_setGamma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Tonemap> * self1 = 0;
    if (!pyopencv_Tonemap_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Tonemap' or its derivative)");
    Ptr<cv::Tonemap> _self_ = *(self1);
    Napi::Value* pyobj_gamma = NULL;
    float gamma=0.f;

    const char* keywords[] = { "gamma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Tonemap.setGamma", (char**)keywords, &pyobj_gamma) &&
        jsopencv_to_safe(info, pyobj_gamma, gamma, ArgInfo("gamma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGamma(gamma));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (Tonemap)

static PyGetSetDef pyopencv_Tonemap_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_Tonemap_methods[] =
{
    {"getGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_Tonemap_getGamma, 0), "getGamma() -> retval\n."},
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_Tonemap_process, 0), "process(src[, dst]) -> dst\n.   @brief Tonemaps image\n.   \n.       @param src source image - CV_32FC3 Mat (float 32 bits 3 channels)\n.       @param dst destination image - CV_32FC3 Mat with values in [0, 1] range"},
    {"setGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_Tonemap_setGamma, 0), "setGamma(gamma) -> None\n."},

    {NULL,          NULL}
};

// Converter (Tonemap)

template<>
struct PyOpenCV_Converter< Ptr<cv::Tonemap> >
{
    static PyObject* from(const Ptr<cv::Tonemap>& r)
    {
        return pyopencv_Tonemap_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::Tonemap>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::Tonemap> * dst_;
        if (pyopencv_Tonemap_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::Tonemap> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TonemapDrago (Generic)
//================================================================================

// GetSet (TonemapDrago)



// Methods (TonemapDrago)

static Napi::Value pyopencv_cv_TonemapDrago_getBias(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapDrago> * self1 = 0;
    if (!pyopencv_TonemapDrago_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    Ptr<cv::TonemapDrago> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBias());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapDrago_getSaturation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapDrago> * self1 = 0;
    if (!pyopencv_TonemapDrago_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    Ptr<cv::TonemapDrago> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSaturation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapDrago_setBias(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapDrago> * self1 = 0;
    if (!pyopencv_TonemapDrago_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    Ptr<cv::TonemapDrago> _self_ = *(self1);
    Napi::Value* pyobj_bias = NULL;
    float bias=0.f;

    const char* keywords[] = { "bias", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapDrago.setBias", (char**)keywords, &pyobj_bias) &&
        jsopencv_to_safe(info, pyobj_bias, bias, ArgInfo("bias", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBias(bias));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapDrago_setSaturation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapDrago> * self1 = 0;
    if (!pyopencv_TonemapDrago_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    Ptr<cv::TonemapDrago> _self_ = *(self1);
    Napi::Value* pyobj_saturation = NULL;
    float saturation=0.f;

    const char* keywords[] = { "saturation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapDrago.setSaturation", (char**)keywords, &pyobj_saturation) &&
        jsopencv_to_safe(info, pyobj_saturation, saturation, ArgInfo("saturation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSaturation(saturation));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (TonemapDrago)

static PyGetSetDef pyopencv_TonemapDrago_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TonemapDrago_methods[] =
{
    {"getBias", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapDrago_getBias, 0), "getBias() -> retval\n."},
    {"getSaturation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapDrago_getSaturation, 0), "getSaturation() -> retval\n."},
    {"setBias", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapDrago_setBias, 0), "setBias(bias) -> None\n."},
    {"setSaturation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapDrago_setSaturation, 0), "setSaturation(saturation) -> None\n."},

    {NULL,          NULL}
};

// Converter (TonemapDrago)

template<>
struct PyOpenCV_Converter< Ptr<cv::TonemapDrago> >
{
    static PyObject* from(const Ptr<cv::TonemapDrago>& r)
    {
        return pyopencv_TonemapDrago_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TonemapDrago>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TonemapDrago> * dst_;
        if (pyopencv_TonemapDrago_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TonemapDrago> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TonemapMantiuk (Generic)
//================================================================================

// GetSet (TonemapMantiuk)



// Methods (TonemapMantiuk)

static Napi::Value pyopencv_cv_TonemapMantiuk_getSaturation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapMantiuk> * self1 = 0;
    if (!pyopencv_TonemapMantiuk_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    Ptr<cv::TonemapMantiuk> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSaturation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapMantiuk_getScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapMantiuk> * self1 = 0;
    if (!pyopencv_TonemapMantiuk_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    Ptr<cv::TonemapMantiuk> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapMantiuk_setSaturation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapMantiuk> * self1 = 0;
    if (!pyopencv_TonemapMantiuk_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    Ptr<cv::TonemapMantiuk> _self_ = *(self1);
    Napi::Value* pyobj_saturation = NULL;
    float saturation=0.f;

    const char* keywords[] = { "saturation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapMantiuk.setSaturation", (char**)keywords, &pyobj_saturation) &&
        jsopencv_to_safe(info, pyobj_saturation, saturation, ArgInfo("saturation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSaturation(saturation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapMantiuk_setScale(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapMantiuk> * self1 = 0;
    if (!pyopencv_TonemapMantiuk_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    Ptr<cv::TonemapMantiuk> _self_ = *(self1);
    Napi::Value* pyobj_scale = NULL;
    float scale=0.f;

    const char* keywords[] = { "scale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapMantiuk.setScale", (char**)keywords, &pyobj_scale) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScale(scale));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (TonemapMantiuk)

static PyGetSetDef pyopencv_TonemapMantiuk_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TonemapMantiuk_methods[] =
{
    {"getSaturation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_getSaturation, 0), "getSaturation() -> retval\n."},
    {"getScale", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_getScale, 0), "getScale() -> retval\n."},
    {"setSaturation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_setSaturation, 0), "setSaturation(saturation) -> None\n."},
    {"setScale", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_setScale, 0), "setScale(scale) -> None\n."},

    {NULL,          NULL}
};

// Converter (TonemapMantiuk)

template<>
struct PyOpenCV_Converter< Ptr<cv::TonemapMantiuk> >
{
    static PyObject* from(const Ptr<cv::TonemapMantiuk>& r)
    {
        return pyopencv_TonemapMantiuk_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TonemapMantiuk>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TonemapMantiuk> * dst_;
        if (pyopencv_TonemapMantiuk_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TonemapMantiuk> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TonemapReinhard (Generic)
//================================================================================

// GetSet (TonemapReinhard)



// Methods (TonemapReinhard)

static Napi::Value pyopencv_cv_TonemapReinhard_getColorAdaptation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapReinhard> * self1 = 0;
    if (!pyopencv_TonemapReinhard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    Ptr<cv::TonemapReinhard> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getColorAdaptation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapReinhard_getIntensity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapReinhard> * self1 = 0;
    if (!pyopencv_TonemapReinhard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    Ptr<cv::TonemapReinhard> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIntensity());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapReinhard_getLightAdaptation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapReinhard> * self1 = 0;
    if (!pyopencv_TonemapReinhard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    Ptr<cv::TonemapReinhard> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLightAdaptation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapReinhard_setColorAdaptation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapReinhard> * self1 = 0;
    if (!pyopencv_TonemapReinhard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    Ptr<cv::TonemapReinhard> _self_ = *(self1);
    Napi::Value* pyobj_color_adapt = NULL;
    float color_adapt=0.f;

    const char* keywords[] = { "color_adapt", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapReinhard.setColorAdaptation", (char**)keywords, &pyobj_color_adapt) &&
        jsopencv_to_safe(info, pyobj_color_adapt, color_adapt, ArgInfo("color_adapt", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setColorAdaptation(color_adapt));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapReinhard_setIntensity(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapReinhard> * self1 = 0;
    if (!pyopencv_TonemapReinhard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    Ptr<cv::TonemapReinhard> _self_ = *(self1);
    Napi::Value* pyobj_intensity = NULL;
    float intensity=0.f;

    const char* keywords[] = { "intensity", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapReinhard.setIntensity", (char**)keywords, &pyobj_intensity) &&
        jsopencv_to_safe(info, pyobj_intensity, intensity, ArgInfo("intensity", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setIntensity(intensity));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TonemapReinhard_setLightAdaptation(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TonemapReinhard> * self1 = 0;
    if (!pyopencv_TonemapReinhard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    Ptr<cv::TonemapReinhard> _self_ = *(self1);
    Napi::Value* pyobj_light_adapt = NULL;
    float light_adapt=0.f;

    const char* keywords[] = { "light_adapt", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TonemapReinhard.setLightAdaptation", (char**)keywords, &pyobj_light_adapt) &&
        jsopencv_to_safe(info, pyobj_light_adapt, light_adapt, ArgInfo("light_adapt", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLightAdaptation(light_adapt));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (TonemapReinhard)

static PyGetSetDef pyopencv_TonemapReinhard_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TonemapReinhard_methods[] =
{
    {"getColorAdaptation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_getColorAdaptation, 0), "getColorAdaptation() -> retval\n."},
    {"getIntensity", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_getIntensity, 0), "getIntensity() -> retval\n."},
    {"getLightAdaptation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_getLightAdaptation, 0), "getLightAdaptation() -> retval\n."},
    {"setColorAdaptation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_setColorAdaptation, 0), "setColorAdaptation(color_adapt) -> None\n."},
    {"setIntensity", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_setIntensity, 0), "setIntensity(intensity) -> None\n."},
    {"setLightAdaptation", CV_JS_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_setLightAdaptation, 0), "setLightAdaptation(light_adapt) -> None\n."},

    {NULL,          NULL}
};

// Converter (TonemapReinhard)

template<>
struct PyOpenCV_Converter< Ptr<cv::TonemapReinhard> >
{
    static PyObject* from(const Ptr<cv::TonemapReinhard>& r)
    {
        return pyopencv_TonemapReinhard_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TonemapReinhard>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TonemapReinhard> * dst_;
        if (pyopencv_TonemapReinhard_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TonemapReinhard> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// Tracker (Generic)
//================================================================================

// GetSet (Tracker)



// Methods (Tracker)

static Napi::Value pyopencv_cv_Tracker_init(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Tracker> * self1 = 0;
    if (!pyopencv_Tracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Tracker' or its derivative)");
    Ptr<cv::Tracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_boundingBox = NULL;
    Rect boundingBox;

    const char* keywords[] = { "image", "boundingBox", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:Tracker.init", (char**)keywords, &pyobj_image, &pyobj_boundingBox) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->init(image, boundingBox));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_boundingBox = NULL;
    Rect boundingBox;

    const char* keywords[] = { "image", "boundingBox", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:Tracker.init", (char**)keywords, &pyobj_image, &pyobj_boundingBox) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->init(image, boundingBox));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("init");

    return NULL;
}

static Napi::Value pyopencv_cv_Tracker_update(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::Tracker> * self1 = 0;
    if (!pyopencv_Tracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'Tracker' or its derivative)");
    Ptr<cv::Tracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Rect boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Tracker.update", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(boundingBox));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Rect boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Tracker.update", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(boundingBox));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("update");

    return NULL;
}



// Tables (Tracker)

static PyGetSetDef pyopencv_Tracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_Tracker_methods[] =
{
    {"init", CV_JS_FN_WITH_KW_(pyopencv_cv_Tracker_init, 0), "init(image, boundingBox) -> None\n.   @brief Initialize the tracker with a known bounding box that surrounded the target\n.       @param image The initial frame\n.       @param boundingBox The initial bounding box"},
    {"update", CV_JS_FN_WITH_KW_(pyopencv_cv_Tracker_update, 0), "update(image) -> retval, boundingBox\n.   @brief Update the tracker, find the new most likely bounding box for the target\n.       @param image The current frame\n.       @param boundingBox The bounding box that represent the new target location, if true was returned, not\n.       modified otherwise\n.   \n.       @return True means that target was located and false means that tracker cannot locate target in\n.       current frame. Note, that latter *does not* imply that tracker has failed, maybe target is indeed\n.       missing from the frame (say, out of sight)"},

    {NULL,          NULL}
};

// Converter (Tracker)

template<>
struct PyOpenCV_Converter< Ptr<cv::Tracker> >
{
    static PyObject* from(const Ptr<cv::Tracker>& r)
    {
        return pyopencv_Tracker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::Tracker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::Tracker> * dst_;
        if (pyopencv_Tracker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::Tracker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerCSRT (Generic)
//================================================================================

// GetSet (TrackerCSRT)



// Methods (TrackerCSRT)

static Napi::Value pyopencv_cv_TrackerCSRT_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::TrackerCSRT::Params parameters=TrackerCSRT::Params();
    Ptr<TrackerCSRT> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:TrackerCSRT.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::TrackerCSRT::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TrackerCSRT_setInitialMask(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TrackerCSRT> * self1 = 0;
    if (!pyopencv_TrackerCSRT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TrackerCSRT' or its derivative)");
    Ptr<cv::TrackerCSRT> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TrackerCSRT.setInitialMask", (char**)keywords, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitialMask(mask));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TrackerCSRT.setInitialMask", (char**)keywords, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitialMask(mask));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setInitialMask");

    return NULL;
}



// Tables (TrackerCSRT)

static PyGetSetDef pyopencv_TrackerCSRT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerCSRT_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerCSRT_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Create CSRT tracker instance\n.       @param parameters CSRT parameters TrackerCSRT::Params"},
    {"setInitialMask", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerCSRT_setInitialMask, 0), "setInitialMask(mask) -> None\n."},

    {NULL,          NULL}
};

// Converter (TrackerCSRT)

template<>
struct PyOpenCV_Converter< Ptr<cv::TrackerCSRT> >
{
    static PyObject* from(const Ptr<cv::TrackerCSRT>& r)
    {
        return pyopencv_TrackerCSRT_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TrackerCSRT>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TrackerCSRT> * dst_;
        if (pyopencv_TrackerCSRT_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TrackerCSRT> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerCSRT_Params (Generic)
//================================================================================

// GetSet (TrackerCSRT_Params)


static PyObject* pyopencv_TrackerCSRT_Params_get_admm_iterations(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.admm_iterations);
}

static int pyopencv_TrackerCSRT_Params_set_admm_iterations(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the admm_iterations attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.admm_iterations, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_background_ratio(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.background_ratio);
}

static int pyopencv_TrackerCSRT_Params_set_background_ratio(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the background_ratio attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.background_ratio, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_cheb_attenuation(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.cheb_attenuation);
}

static int pyopencv_TrackerCSRT_Params_set_cheb_attenuation(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cheb_attenuation attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cheb_attenuation, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_filter_lr(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.filter_lr);
}

static int pyopencv_TrackerCSRT_Params_set_filter_lr(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filter_lr attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.filter_lr, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_gsl_sigma(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.gsl_sigma);
}

static int pyopencv_TrackerCSRT_Params_set_gsl_sigma(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the gsl_sigma attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.gsl_sigma, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_histogram_bins(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.histogram_bins);
}

static int pyopencv_TrackerCSRT_Params_set_histogram_bins(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the histogram_bins attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.histogram_bins, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_histogram_lr(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.histogram_lr);
}

static int pyopencv_TrackerCSRT_Params_set_histogram_lr(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the histogram_lr attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.histogram_lr, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_hog_clip(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.hog_clip);
}

static int pyopencv_TrackerCSRT_Params_set_hog_clip(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the hog_clip attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.hog_clip, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_hog_orientations(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.hog_orientations);
}

static int pyopencv_TrackerCSRT_Params_set_hog_orientations(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the hog_orientations attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.hog_orientations, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_kaiser_alpha(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.kaiser_alpha);
}

static int pyopencv_TrackerCSRT_Params_set_kaiser_alpha(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the kaiser_alpha attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.kaiser_alpha, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_num_hog_channels_used(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.num_hog_channels_used);
}

static int pyopencv_TrackerCSRT_Params_set_num_hog_channels_used(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the num_hog_channels_used attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.num_hog_channels_used, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_number_of_scales(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.number_of_scales);
}

static int pyopencv_TrackerCSRT_Params_set_number_of_scales(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the number_of_scales attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.number_of_scales, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_padding(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.padding);
}

static int pyopencv_TrackerCSRT_Params_set_padding(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the padding attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.padding, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_psr_threshold(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.psr_threshold);
}

static int pyopencv_TrackerCSRT_Params_set_psr_threshold(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the psr_threshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.psr_threshold, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_scale_lr(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.scale_lr);
}

static int pyopencv_TrackerCSRT_Params_set_scale_lr(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the scale_lr attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.scale_lr, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_scale_model_max_area(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.scale_model_max_area);
}

static int pyopencv_TrackerCSRT_Params_set_scale_model_max_area(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the scale_model_max_area attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.scale_model_max_area, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_scale_sigma_factor(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.scale_sigma_factor);
}

static int pyopencv_TrackerCSRT_Params_set_scale_sigma_factor(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the scale_sigma_factor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.scale_sigma_factor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_scale_step(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.scale_step);
}

static int pyopencv_TrackerCSRT_Params_set_scale_step(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the scale_step attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.scale_step, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_template_size(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.template_size);
}

static int pyopencv_TrackerCSRT_Params_set_template_size(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the template_size attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.template_size, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_use_channel_weights(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.use_channel_weights);
}

static int pyopencv_TrackerCSRT_Params_set_use_channel_weights(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the use_channel_weights attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.use_channel_weights, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_use_color_names(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.use_color_names);
}

static int pyopencv_TrackerCSRT_Params_set_use_color_names(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the use_color_names attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.use_color_names, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_use_gray(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.use_gray);
}

static int pyopencv_TrackerCSRT_Params_set_use_gray(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the use_gray attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.use_gray, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_use_hog(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.use_hog);
}

static int pyopencv_TrackerCSRT_Params_set_use_hog(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the use_hog attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.use_hog, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_use_rgb(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.use_rgb);
}

static int pyopencv_TrackerCSRT_Params_set_use_rgb(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the use_rgb attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.use_rgb, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_use_segmentation(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.use_segmentation);
}

static int pyopencv_TrackerCSRT_Params_set_use_segmentation(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the use_segmentation attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.use_segmentation, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_weights_lr(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.weights_lr);
}

static int pyopencv_TrackerCSRT_Params_set_weights_lr(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the weights_lr attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.weights_lr, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerCSRT_Params_get_window_function(pyopencv_TrackerCSRT_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.window_function);
}

static int pyopencv_TrackerCSRT_Params_set_window_function(pyopencv_TrackerCSRT_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the window_function attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.window_function, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (TrackerCSRT_Params)

static int pyopencv_cv_TrackerCSRT_Params_TrackerCSRT_Params(pyopencv_TrackerCSRT_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::TrackerCSRT::Params());
        return 0;
    }

    return -1;
}



// Tables (TrackerCSRT_Params)

static PyGetSetDef pyopencv_TrackerCSRT_Params_getseters[] =
{
    {(char*)"admm_iterations", (getter)pyopencv_TrackerCSRT_Params_get_admm_iterations, (setter)pyopencv_TrackerCSRT_Params_set_admm_iterations, (char*)"admm_iterations", NULL},
    {(char*)"background_ratio", (getter)pyopencv_TrackerCSRT_Params_get_background_ratio, (setter)pyopencv_TrackerCSRT_Params_set_background_ratio, (char*)"background_ratio", NULL},
    {(char*)"cheb_attenuation", (getter)pyopencv_TrackerCSRT_Params_get_cheb_attenuation, (setter)pyopencv_TrackerCSRT_Params_set_cheb_attenuation, (char*)"cheb_attenuation", NULL},
    {(char*)"filter_lr", (getter)pyopencv_TrackerCSRT_Params_get_filter_lr, (setter)pyopencv_TrackerCSRT_Params_set_filter_lr, (char*)"filter_lr", NULL},
    {(char*)"gsl_sigma", (getter)pyopencv_TrackerCSRT_Params_get_gsl_sigma, (setter)pyopencv_TrackerCSRT_Params_set_gsl_sigma, (char*)"gsl_sigma", NULL},
    {(char*)"histogram_bins", (getter)pyopencv_TrackerCSRT_Params_get_histogram_bins, (setter)pyopencv_TrackerCSRT_Params_set_histogram_bins, (char*)"histogram_bins", NULL},
    {(char*)"histogram_lr", (getter)pyopencv_TrackerCSRT_Params_get_histogram_lr, (setter)pyopencv_TrackerCSRT_Params_set_histogram_lr, (char*)"histogram_lr", NULL},
    {(char*)"hog_clip", (getter)pyopencv_TrackerCSRT_Params_get_hog_clip, (setter)pyopencv_TrackerCSRT_Params_set_hog_clip, (char*)"hog_clip", NULL},
    {(char*)"hog_orientations", (getter)pyopencv_TrackerCSRT_Params_get_hog_orientations, (setter)pyopencv_TrackerCSRT_Params_set_hog_orientations, (char*)"hog_orientations", NULL},
    {(char*)"kaiser_alpha", (getter)pyopencv_TrackerCSRT_Params_get_kaiser_alpha, (setter)pyopencv_TrackerCSRT_Params_set_kaiser_alpha, (char*)"kaiser_alpha", NULL},
    {(char*)"num_hog_channels_used", (getter)pyopencv_TrackerCSRT_Params_get_num_hog_channels_used, (setter)pyopencv_TrackerCSRT_Params_set_num_hog_channels_used, (char*)"num_hog_channels_used", NULL},
    {(char*)"number_of_scales", (getter)pyopencv_TrackerCSRT_Params_get_number_of_scales, (setter)pyopencv_TrackerCSRT_Params_set_number_of_scales, (char*)"number_of_scales", NULL},
    {(char*)"padding", (getter)pyopencv_TrackerCSRT_Params_get_padding, (setter)pyopencv_TrackerCSRT_Params_set_padding, (char*)"padding", NULL},
    {(char*)"psr_threshold", (getter)pyopencv_TrackerCSRT_Params_get_psr_threshold, (setter)pyopencv_TrackerCSRT_Params_set_psr_threshold, (char*)"psr_threshold", NULL},
    {(char*)"scale_lr", (getter)pyopencv_TrackerCSRT_Params_get_scale_lr, (setter)pyopencv_TrackerCSRT_Params_set_scale_lr, (char*)"scale_lr", NULL},
    {(char*)"scale_model_max_area", (getter)pyopencv_TrackerCSRT_Params_get_scale_model_max_area, (setter)pyopencv_TrackerCSRT_Params_set_scale_model_max_area, (char*)"scale_model_max_area", NULL},
    {(char*)"scale_sigma_factor", (getter)pyopencv_TrackerCSRT_Params_get_scale_sigma_factor, (setter)pyopencv_TrackerCSRT_Params_set_scale_sigma_factor, (char*)"scale_sigma_factor", NULL},
    {(char*)"scale_step", (getter)pyopencv_TrackerCSRT_Params_get_scale_step, (setter)pyopencv_TrackerCSRT_Params_set_scale_step, (char*)"scale_step", NULL},
    {(char*)"template_size", (getter)pyopencv_TrackerCSRT_Params_get_template_size, (setter)pyopencv_TrackerCSRT_Params_set_template_size, (char*)"template_size", NULL},
    {(char*)"use_channel_weights", (getter)pyopencv_TrackerCSRT_Params_get_use_channel_weights, (setter)pyopencv_TrackerCSRT_Params_set_use_channel_weights, (char*)"use_channel_weights", NULL},
    {(char*)"use_color_names", (getter)pyopencv_TrackerCSRT_Params_get_use_color_names, (setter)pyopencv_TrackerCSRT_Params_set_use_color_names, (char*)"use_color_names", NULL},
    {(char*)"use_gray", (getter)pyopencv_TrackerCSRT_Params_get_use_gray, (setter)pyopencv_TrackerCSRT_Params_set_use_gray, (char*)"use_gray", NULL},
    {(char*)"use_hog", (getter)pyopencv_TrackerCSRT_Params_get_use_hog, (setter)pyopencv_TrackerCSRT_Params_set_use_hog, (char*)"use_hog", NULL},
    {(char*)"use_rgb", (getter)pyopencv_TrackerCSRT_Params_get_use_rgb, (setter)pyopencv_TrackerCSRT_Params_set_use_rgb, (char*)"use_rgb", NULL},
    {(char*)"use_segmentation", (getter)pyopencv_TrackerCSRT_Params_get_use_segmentation, (setter)pyopencv_TrackerCSRT_Params_set_use_segmentation, (char*)"use_segmentation", NULL},
    {(char*)"weights_lr", (getter)pyopencv_TrackerCSRT_Params_get_weights_lr, (setter)pyopencv_TrackerCSRT_Params_set_weights_lr, (char*)"weights_lr", NULL},
    {(char*)"window_function", (getter)pyopencv_TrackerCSRT_Params_get_window_function, (setter)pyopencv_TrackerCSRT_Params_set_window_function, (char*)"window_function", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerCSRT_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (TrackerCSRT_Params)

template<>
struct PyOpenCV_Converter< cv::TrackerCSRT::Params >
{
    static PyObject* from(const cv::TrackerCSRT::Params& r)
    {
        return pyopencv_TrackerCSRT_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::TrackerCSRT::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::TrackerCSRT::Params * dst_;
        if (pyopencv_TrackerCSRT_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::TrackerCSRT::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerDaSiamRPN (Generic)
//================================================================================

// GetSet (TrackerDaSiamRPN)



// Methods (TrackerDaSiamRPN)

static Napi::Value pyopencv_cv_TrackerDaSiamRPN_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::TrackerDaSiamRPN::Params parameters=TrackerDaSiamRPN::Params();
    Ptr<TrackerDaSiamRPN> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:TrackerDaSiamRPN.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::TrackerDaSiamRPN::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TrackerDaSiamRPN_getTrackingScore(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TrackerDaSiamRPN> * self1 = 0;
    if (!pyopencv_TrackerDaSiamRPN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TrackerDaSiamRPN' or its derivative)");
    Ptr<cv::TrackerDaSiamRPN> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrackingScore());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (TrackerDaSiamRPN)

static PyGetSetDef pyopencv_TrackerDaSiamRPN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerDaSiamRPN_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerDaSiamRPN_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Constructor\n.       @param parameters DaSiamRPN parameters TrackerDaSiamRPN::Params"},
    {"getTrackingScore", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerDaSiamRPN_getTrackingScore, 0), "getTrackingScore() -> retval\n.   @brief Return tracking score"},

    {NULL,          NULL}
};

// Converter (TrackerDaSiamRPN)

template<>
struct PyOpenCV_Converter< Ptr<cv::TrackerDaSiamRPN> >
{
    static PyObject* from(const Ptr<cv::TrackerDaSiamRPN>& r)
    {
        return pyopencv_TrackerDaSiamRPN_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TrackerDaSiamRPN>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TrackerDaSiamRPN> * dst_;
        if (pyopencv_TrackerDaSiamRPN_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TrackerDaSiamRPN> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerDaSiamRPN_Params (Generic)
//================================================================================

// GetSet (TrackerDaSiamRPN_Params)


static PyObject* pyopencv_TrackerDaSiamRPN_Params_get_backend(pyopencv_TrackerDaSiamRPN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.backend);
}

static int pyopencv_TrackerDaSiamRPN_Params_set_backend(pyopencv_TrackerDaSiamRPN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the backend attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.backend, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerDaSiamRPN_Params_get_kernel_cls1(pyopencv_TrackerDaSiamRPN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.kernel_cls1);
}

static int pyopencv_TrackerDaSiamRPN_Params_set_kernel_cls1(pyopencv_TrackerDaSiamRPN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the kernel_cls1 attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.kernel_cls1, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerDaSiamRPN_Params_get_kernel_r1(pyopencv_TrackerDaSiamRPN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.kernel_r1);
}

static int pyopencv_TrackerDaSiamRPN_Params_set_kernel_r1(pyopencv_TrackerDaSiamRPN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the kernel_r1 attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.kernel_r1, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerDaSiamRPN_Params_get_model(pyopencv_TrackerDaSiamRPN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.model);
}

static int pyopencv_TrackerDaSiamRPN_Params_set_model(pyopencv_TrackerDaSiamRPN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the model attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.model, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerDaSiamRPN_Params_get_target(pyopencv_TrackerDaSiamRPN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.target);
}

static int pyopencv_TrackerDaSiamRPN_Params_set_target(pyopencv_TrackerDaSiamRPN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the target attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.target, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (TrackerDaSiamRPN_Params)

static int pyopencv_cv_TrackerDaSiamRPN_Params_TrackerDaSiamRPN_Params(pyopencv_TrackerDaSiamRPN_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::TrackerDaSiamRPN::Params());
        return 0;
    }

    return -1;
}



// Tables (TrackerDaSiamRPN_Params)

static PyGetSetDef pyopencv_TrackerDaSiamRPN_Params_getseters[] =
{
    {(char*)"backend", (getter)pyopencv_TrackerDaSiamRPN_Params_get_backend, (setter)pyopencv_TrackerDaSiamRPN_Params_set_backend, (char*)"backend", NULL},
    {(char*)"kernel_cls1", (getter)pyopencv_TrackerDaSiamRPN_Params_get_kernel_cls1, (setter)pyopencv_TrackerDaSiamRPN_Params_set_kernel_cls1, (char*)"kernel_cls1", NULL},
    {(char*)"kernel_r1", (getter)pyopencv_TrackerDaSiamRPN_Params_get_kernel_r1, (setter)pyopencv_TrackerDaSiamRPN_Params_set_kernel_r1, (char*)"kernel_r1", NULL},
    {(char*)"model", (getter)pyopencv_TrackerDaSiamRPN_Params_get_model, (setter)pyopencv_TrackerDaSiamRPN_Params_set_model, (char*)"model", NULL},
    {(char*)"target", (getter)pyopencv_TrackerDaSiamRPN_Params_get_target, (setter)pyopencv_TrackerDaSiamRPN_Params_set_target, (char*)"target", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerDaSiamRPN_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (TrackerDaSiamRPN_Params)

template<>
struct PyOpenCV_Converter< cv::TrackerDaSiamRPN::Params >
{
    static PyObject* from(const cv::TrackerDaSiamRPN::Params& r)
    {
        return pyopencv_TrackerDaSiamRPN_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::TrackerDaSiamRPN::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::TrackerDaSiamRPN::Params * dst_;
        if (pyopencv_TrackerDaSiamRPN_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::TrackerDaSiamRPN::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerGOTURN (Generic)
//================================================================================

// GetSet (TrackerGOTURN)



// Methods (TrackerGOTURN)

static Napi::Value pyopencv_cv_TrackerGOTURN_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::TrackerGOTURN::Params parameters=TrackerGOTURN::Params();
    Ptr<TrackerGOTURN> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:TrackerGOTURN.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::TrackerGOTURN::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (TrackerGOTURN)

static PyGetSetDef pyopencv_TrackerGOTURN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerGOTURN_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerGOTURN_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Constructor\n.       @param parameters GOTURN parameters TrackerGOTURN::Params"},

    {NULL,          NULL}
};

// Converter (TrackerGOTURN)

template<>
struct PyOpenCV_Converter< Ptr<cv::TrackerGOTURN> >
{
    static PyObject* from(const Ptr<cv::TrackerGOTURN>& r)
    {
        return pyopencv_TrackerGOTURN_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TrackerGOTURN>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TrackerGOTURN> * dst_;
        if (pyopencv_TrackerGOTURN_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TrackerGOTURN> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerGOTURN_Params (Generic)
//================================================================================

// GetSet (TrackerGOTURN_Params)


static PyObject* pyopencv_TrackerGOTURN_Params_get_modelBin(pyopencv_TrackerGOTURN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.modelBin);
}

static int pyopencv_TrackerGOTURN_Params_set_modelBin(pyopencv_TrackerGOTURN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the modelBin attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.modelBin, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerGOTURN_Params_get_modelTxt(pyopencv_TrackerGOTURN_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.modelTxt);
}

static int pyopencv_TrackerGOTURN_Params_set_modelTxt(pyopencv_TrackerGOTURN_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the modelTxt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.modelTxt, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (TrackerGOTURN_Params)

static int pyopencv_cv_TrackerGOTURN_Params_TrackerGOTURN_Params(pyopencv_TrackerGOTURN_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::TrackerGOTURN::Params());
        return 0;
    }

    return -1;
}



// Tables (TrackerGOTURN_Params)

static PyGetSetDef pyopencv_TrackerGOTURN_Params_getseters[] =
{
    {(char*)"modelBin", (getter)pyopencv_TrackerGOTURN_Params_get_modelBin, (setter)pyopencv_TrackerGOTURN_Params_set_modelBin, (char*)"modelBin", NULL},
    {(char*)"modelTxt", (getter)pyopencv_TrackerGOTURN_Params_get_modelTxt, (setter)pyopencv_TrackerGOTURN_Params_set_modelTxt, (char*)"modelTxt", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerGOTURN_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (TrackerGOTURN_Params)

template<>
struct PyOpenCV_Converter< cv::TrackerGOTURN::Params >
{
    static PyObject* from(const cv::TrackerGOTURN::Params& r)
    {
        return pyopencv_TrackerGOTURN_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::TrackerGOTURN::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::TrackerGOTURN::Params * dst_;
        if (pyopencv_TrackerGOTURN_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::TrackerGOTURN::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerKCF (Generic)
//================================================================================

// GetSet (TrackerKCF)



// Methods (TrackerKCF)

static Napi::Value pyopencv_cv_TrackerKCF_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::TrackerKCF::Params parameters=TrackerKCF::Params();
    Ptr<TrackerKCF> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:TrackerKCF.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::TrackerKCF::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (TrackerKCF)

static PyGetSetDef pyopencv_TrackerKCF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerKCF_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerKCF_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Create KCF tracker instance\n.       @param parameters KCF parameters TrackerKCF::Params"},

    {NULL,          NULL}
};

// Converter (TrackerKCF)

template<>
struct PyOpenCV_Converter< Ptr<cv::TrackerKCF> >
{
    static PyObject* from(const Ptr<cv::TrackerKCF>& r)
    {
        return pyopencv_TrackerKCF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TrackerKCF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TrackerKCF> * dst_;
        if (pyopencv_TrackerKCF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TrackerKCF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerKCF_Params (Generic)
//================================================================================

// GetSet (TrackerKCF_Params)


static PyObject* pyopencv_TrackerKCF_Params_get_compress_feature(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.compress_feature);
}

static int pyopencv_TrackerKCF_Params_set_compress_feature(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the compress_feature attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.compress_feature, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_compressed_size(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.compressed_size);
}

static int pyopencv_TrackerKCF_Params_set_compressed_size(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the compressed_size attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.compressed_size, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_desc_npca(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.desc_npca);
}

static int pyopencv_TrackerKCF_Params_set_desc_npca(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the desc_npca attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.desc_npca, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_desc_pca(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.desc_pca);
}

static int pyopencv_TrackerKCF_Params_set_desc_pca(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the desc_pca attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.desc_pca, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_detect_thresh(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.detect_thresh);
}

static int pyopencv_TrackerKCF_Params_set_detect_thresh(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the detect_thresh attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.detect_thresh, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_interp_factor(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.interp_factor);
}

static int pyopencv_TrackerKCF_Params_set_interp_factor(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the interp_factor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.interp_factor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_lambda(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.lambda);
}

static int pyopencv_TrackerKCF_Params_set_lambda(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lambda attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lambda, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_max_patch_size(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.max_patch_size);
}

static int pyopencv_TrackerKCF_Params_set_max_patch_size(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the max_patch_size attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.max_patch_size, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_output_sigma_factor(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.output_sigma_factor);
}

static int pyopencv_TrackerKCF_Params_set_output_sigma_factor(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the output_sigma_factor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.output_sigma_factor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_pca_learning_rate(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.pca_learning_rate);
}

static int pyopencv_TrackerKCF_Params_set_pca_learning_rate(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the pca_learning_rate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.pca_learning_rate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_resize(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.resize);
}

static int pyopencv_TrackerKCF_Params_set_resize(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the resize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.resize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_sigma(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.sigma);
}

static int pyopencv_TrackerKCF_Params_set_sigma(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the sigma attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.sigma, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_split_coeff(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.split_coeff);
}

static int pyopencv_TrackerKCF_Params_set_split_coeff(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the split_coeff attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.split_coeff, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerKCF_Params_get_wrap_kernel(pyopencv_TrackerKCF_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.wrap_kernel);
}

static int pyopencv_TrackerKCF_Params_set_wrap_kernel(pyopencv_TrackerKCF_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the wrap_kernel attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.wrap_kernel, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (TrackerKCF_Params)

static int pyopencv_cv_TrackerKCF_Params_TrackerKCF_Params(pyopencv_TrackerKCF_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::TrackerKCF::Params());
        return 0;
    }

    return -1;
}



// Tables (TrackerKCF_Params)

static PyGetSetDef pyopencv_TrackerKCF_Params_getseters[] =
{
    {(char*)"compress_feature", (getter)pyopencv_TrackerKCF_Params_get_compress_feature, (setter)pyopencv_TrackerKCF_Params_set_compress_feature, (char*)"compress_feature", NULL},
    {(char*)"compressed_size", (getter)pyopencv_TrackerKCF_Params_get_compressed_size, (setter)pyopencv_TrackerKCF_Params_set_compressed_size, (char*)"compressed_size", NULL},
    {(char*)"desc_npca", (getter)pyopencv_TrackerKCF_Params_get_desc_npca, (setter)pyopencv_TrackerKCF_Params_set_desc_npca, (char*)"desc_npca", NULL},
    {(char*)"desc_pca", (getter)pyopencv_TrackerKCF_Params_get_desc_pca, (setter)pyopencv_TrackerKCF_Params_set_desc_pca, (char*)"desc_pca", NULL},
    {(char*)"detect_thresh", (getter)pyopencv_TrackerKCF_Params_get_detect_thresh, (setter)pyopencv_TrackerKCF_Params_set_detect_thresh, (char*)"detect_thresh", NULL},
    {(char*)"interp_factor", (getter)pyopencv_TrackerKCF_Params_get_interp_factor, (setter)pyopencv_TrackerKCF_Params_set_interp_factor, (char*)"interp_factor", NULL},
    {(char*)"lambda_", (getter)pyopencv_TrackerKCF_Params_get_lambda, (setter)pyopencv_TrackerKCF_Params_set_lambda, (char*)"lambda_", NULL},
    {(char*)"max_patch_size", (getter)pyopencv_TrackerKCF_Params_get_max_patch_size, (setter)pyopencv_TrackerKCF_Params_set_max_patch_size, (char*)"max_patch_size", NULL},
    {(char*)"output_sigma_factor", (getter)pyopencv_TrackerKCF_Params_get_output_sigma_factor, (setter)pyopencv_TrackerKCF_Params_set_output_sigma_factor, (char*)"output_sigma_factor", NULL},
    {(char*)"pca_learning_rate", (getter)pyopencv_TrackerKCF_Params_get_pca_learning_rate, (setter)pyopencv_TrackerKCF_Params_set_pca_learning_rate, (char*)"pca_learning_rate", NULL},
    {(char*)"resize", (getter)pyopencv_TrackerKCF_Params_get_resize, (setter)pyopencv_TrackerKCF_Params_set_resize, (char*)"resize", NULL},
    {(char*)"sigma", (getter)pyopencv_TrackerKCF_Params_get_sigma, (setter)pyopencv_TrackerKCF_Params_set_sigma, (char*)"sigma", NULL},
    {(char*)"split_coeff", (getter)pyopencv_TrackerKCF_Params_get_split_coeff, (setter)pyopencv_TrackerKCF_Params_set_split_coeff, (char*)"split_coeff", NULL},
    {(char*)"wrap_kernel", (getter)pyopencv_TrackerKCF_Params_get_wrap_kernel, (setter)pyopencv_TrackerKCF_Params_set_wrap_kernel, (char*)"wrap_kernel", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerKCF_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (TrackerKCF_Params)

template<>
struct PyOpenCV_Converter< cv::TrackerKCF::Params >
{
    static PyObject* from(const cv::TrackerKCF::Params& r)
    {
        return pyopencv_TrackerKCF_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::TrackerKCF::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::TrackerKCF::Params * dst_;
        if (pyopencv_TrackerKCF_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::TrackerKCF::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerMIL (Generic)
//================================================================================

// GetSet (TrackerMIL)



// Methods (TrackerMIL)

static Napi::Value pyopencv_cv_TrackerMIL_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::TrackerMIL::Params parameters=TrackerMIL::Params();
    Ptr<TrackerMIL> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:TrackerMIL.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::TrackerMIL::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (TrackerMIL)

static PyGetSetDef pyopencv_TrackerMIL_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerMIL_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerMIL_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Create MIL tracker instance\n.        *  @param parameters MIL parameters TrackerMIL::Params"},

    {NULL,          NULL}
};

// Converter (TrackerMIL)

template<>
struct PyOpenCV_Converter< Ptr<cv::TrackerMIL> >
{
    static PyObject* from(const Ptr<cv::TrackerMIL>& r)
    {
        return pyopencv_TrackerMIL_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TrackerMIL>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TrackerMIL> * dst_;
        if (pyopencv_TrackerMIL_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TrackerMIL> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerMIL_Params (Generic)
//================================================================================

// GetSet (TrackerMIL_Params)


static PyObject* pyopencv_TrackerMIL_Params_get_featureSetNumFeatures(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.featureSetNumFeatures);
}

static int pyopencv_TrackerMIL_Params_set_featureSetNumFeatures(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the featureSetNumFeatures attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.featureSetNumFeatures, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerMIL_Params_get_samplerInitInRadius(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.samplerInitInRadius);
}

static int pyopencv_TrackerMIL_Params_set_samplerInitInRadius(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the samplerInitInRadius attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.samplerInitInRadius, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerMIL_Params_get_samplerInitMaxNegNum(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.samplerInitMaxNegNum);
}

static int pyopencv_TrackerMIL_Params_set_samplerInitMaxNegNum(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the samplerInitMaxNegNum attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.samplerInitMaxNegNum, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerMIL_Params_get_samplerSearchWinSize(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.samplerSearchWinSize);
}

static int pyopencv_TrackerMIL_Params_set_samplerSearchWinSize(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the samplerSearchWinSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.samplerSearchWinSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerMIL_Params_get_samplerTrackInRadius(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.samplerTrackInRadius);
}

static int pyopencv_TrackerMIL_Params_set_samplerTrackInRadius(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the samplerTrackInRadius attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.samplerTrackInRadius, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerMIL_Params_get_samplerTrackMaxNegNum(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.samplerTrackMaxNegNum);
}

static int pyopencv_TrackerMIL_Params_set_samplerTrackMaxNegNum(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the samplerTrackMaxNegNum attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.samplerTrackMaxNegNum, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerMIL_Params_get_samplerTrackMaxPosNum(pyopencv_TrackerMIL_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.samplerTrackMaxPosNum);
}

static int pyopencv_TrackerMIL_Params_set_samplerTrackMaxPosNum(pyopencv_TrackerMIL_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the samplerTrackMaxPosNum attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.samplerTrackMaxPosNum, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (TrackerMIL_Params)

static int pyopencv_cv_TrackerMIL_Params_TrackerMIL_Params(pyopencv_TrackerMIL_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::TrackerMIL::Params());
        return 0;
    }

    return -1;
}



// Tables (TrackerMIL_Params)

static PyGetSetDef pyopencv_TrackerMIL_Params_getseters[] =
{
    {(char*)"featureSetNumFeatures", (getter)pyopencv_TrackerMIL_Params_get_featureSetNumFeatures, (setter)pyopencv_TrackerMIL_Params_set_featureSetNumFeatures, (char*)"featureSetNumFeatures", NULL},
    {(char*)"samplerInitInRadius", (getter)pyopencv_TrackerMIL_Params_get_samplerInitInRadius, (setter)pyopencv_TrackerMIL_Params_set_samplerInitInRadius, (char*)"samplerInitInRadius", NULL},
    {(char*)"samplerInitMaxNegNum", (getter)pyopencv_TrackerMIL_Params_get_samplerInitMaxNegNum, (setter)pyopencv_TrackerMIL_Params_set_samplerInitMaxNegNum, (char*)"samplerInitMaxNegNum", NULL},
    {(char*)"samplerSearchWinSize", (getter)pyopencv_TrackerMIL_Params_get_samplerSearchWinSize, (setter)pyopencv_TrackerMIL_Params_set_samplerSearchWinSize, (char*)"samplerSearchWinSize", NULL},
    {(char*)"samplerTrackInRadius", (getter)pyopencv_TrackerMIL_Params_get_samplerTrackInRadius, (setter)pyopencv_TrackerMIL_Params_set_samplerTrackInRadius, (char*)"samplerTrackInRadius", NULL},
    {(char*)"samplerTrackMaxNegNum", (getter)pyopencv_TrackerMIL_Params_get_samplerTrackMaxNegNum, (setter)pyopencv_TrackerMIL_Params_set_samplerTrackMaxNegNum, (char*)"samplerTrackMaxNegNum", NULL},
    {(char*)"samplerTrackMaxPosNum", (getter)pyopencv_TrackerMIL_Params_get_samplerTrackMaxPosNum, (setter)pyopencv_TrackerMIL_Params_set_samplerTrackMaxPosNum, (char*)"samplerTrackMaxPosNum", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerMIL_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (TrackerMIL_Params)

template<>
struct PyOpenCV_Converter< cv::TrackerMIL::Params >
{
    static PyObject* from(const cv::TrackerMIL::Params& r)
    {
        return pyopencv_TrackerMIL_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::TrackerMIL::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::TrackerMIL::Params * dst_;
        if (pyopencv_TrackerMIL_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::TrackerMIL::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerNano (Generic)
//================================================================================

// GetSet (TrackerNano)



// Methods (TrackerNano)

static Napi::Value pyopencv_cv_TrackerNano_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_parameters = NULL;
    cv::TrackerNano::Params parameters=TrackerNano::Params();
    Ptr<TrackerNano> retval;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:TrackerNano.create", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::TrackerNano::create(parameters));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_TrackerNano_getTrackingScore(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::TrackerNano> * self1 = 0;
    if (!pyopencv_TrackerNano_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'TrackerNano' or its derivative)");
    Ptr<cv::TrackerNano> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrackingScore());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (TrackerNano)

static PyGetSetDef pyopencv_TrackerNano_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerNano_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerNano_create_static, METH_STATIC), "create([, parameters]) -> retval\n.   @brief Constructor\n.       @param parameters NanoTrack parameters TrackerNano::Params"},
    {"getTrackingScore", CV_JS_FN_WITH_KW_(pyopencv_cv_TrackerNano_getTrackingScore, 0), "getTrackingScore() -> retval\n.   @brief Return tracking score"},

    {NULL,          NULL}
};

// Converter (TrackerNano)

template<>
struct PyOpenCV_Converter< Ptr<cv::TrackerNano> >
{
    static PyObject* from(const Ptr<cv::TrackerNano>& r)
    {
        return pyopencv_TrackerNano_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::TrackerNano>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::TrackerNano> * dst_;
        if (pyopencv_TrackerNano_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::TrackerNano> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// TrackerNano_Params (Generic)
//================================================================================

// GetSet (TrackerNano_Params)


static PyObject* pyopencv_TrackerNano_Params_get_backbone(pyopencv_TrackerNano_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.backbone);
}

static int pyopencv_TrackerNano_Params_set_backbone(pyopencv_TrackerNano_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the backbone attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.backbone, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerNano_Params_get_backend(pyopencv_TrackerNano_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.backend);
}

static int pyopencv_TrackerNano_Params_set_backend(pyopencv_TrackerNano_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the backend attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.backend, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerNano_Params_get_neckhead(pyopencv_TrackerNano_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.neckhead);
}

static int pyopencv_TrackerNano_Params_set_neckhead(pyopencv_TrackerNano_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the neckhead attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.neckhead, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_TrackerNano_Params_get_target(pyopencv_TrackerNano_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.target);
}

static int pyopencv_TrackerNano_Params_set_target(pyopencv_TrackerNano_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the target attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.target, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (TrackerNano_Params)

static int pyopencv_cv_TrackerNano_Params_TrackerNano_Params(pyopencv_TrackerNano_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::TrackerNano::Params());
        return 0;
    }

    return -1;
}



// Tables (TrackerNano_Params)

static PyGetSetDef pyopencv_TrackerNano_Params_getseters[] =
{
    {(char*)"backbone", (getter)pyopencv_TrackerNano_Params_get_backbone, (setter)pyopencv_TrackerNano_Params_set_backbone, (char*)"backbone", NULL},
    {(char*)"backend", (getter)pyopencv_TrackerNano_Params_get_backend, (setter)pyopencv_TrackerNano_Params_set_backend, (char*)"backend", NULL},
    {(char*)"neckhead", (getter)pyopencv_TrackerNano_Params_get_neckhead, (setter)pyopencv_TrackerNano_Params_set_neckhead, (char*)"neckhead", NULL},
    {(char*)"target", (getter)pyopencv_TrackerNano_Params_get_target, (setter)pyopencv_TrackerNano_Params_set_target, (char*)"target", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_TrackerNano_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (TrackerNano_Params)

template<>
struct PyOpenCV_Converter< cv::TrackerNano::Params >
{
    static PyObject* from(const cv::TrackerNano::Params& r)
    {
        return pyopencv_TrackerNano_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::TrackerNano::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::TrackerNano::Params * dst_;
        if (pyopencv_TrackerNano_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::TrackerNano::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// UMat (Generic)
//================================================================================

// GetSet (UMat)


static PyObject* pyopencv_UMat_get_offset(pyopencv_UMat_t* p, void *closure)
{
    return jsopencv_from(p->v->offset);
}

static int pyopencv_UMat_set_offset(pyopencv_UMat_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the offset attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->offset, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (UMat)

static int pyopencv_cv_UMat_UMat(pyopencv_UMat_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(9);

    {
    Napi::Value* pyobj_usageFlags = NULL;
    UMatUsageFlags usageFlags=USAGE_DEFAULT;

    const char* keywords[] = { "usageFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:UMat", (char**)keywords, &pyobj_usageFlags) &&
        jsopencv_to_safe(info, pyobj_usageFlags, usageFlags, ArgInfo("usageFlags", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(usageFlags)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_usageFlags = NULL;
    UMatUsageFlags usageFlags=USAGE_DEFAULT;

    const char* keywords[] = { "rows", "cols", "type", "usageFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:UMat", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type, &pyobj_usageFlags) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_usageFlags, usageFlags, ArgInfo("usageFlags", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(rows, cols, type, usageFlags)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_usageFlags = NULL;
    UMatUsageFlags usageFlags=USAGE_DEFAULT;

    const char* keywords[] = { "size", "type", "usageFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:UMat", (char**)keywords, &pyobj_size, &pyobj_type, &pyobj_usageFlags) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_usageFlags, usageFlags, ArgInfo("usageFlags", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(size, type, usageFlags)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_usageFlags = NULL;
    UMatUsageFlags usageFlags=USAGE_DEFAULT;

    const char* keywords[] = { "rows", "cols", "type", "s", "usageFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:UMat", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type, &pyobj_s, &pyobj_usageFlags) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_usageFlags, usageFlags, ArgInfo("usageFlags", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(rows, cols, type, s, usageFlags)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_usageFlags = NULL;
    UMatUsageFlags usageFlags=USAGE_DEFAULT;

    const char* keywords[] = { "size", "type", "s", "usageFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:UMat", (char**)keywords, &pyobj_size, &pyobj_type, &pyobj_s, &pyobj_usageFlags) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_usageFlags, usageFlags, ArgInfo("usageFlags", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(size, type, s, usageFlags)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    UMat m;

    const char* keywords[] = { "m", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:UMat", (char**)keywords, &pyobj_m) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(m)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    UMat m;
    Napi::Value* pyobj_rowRange = NULL;
    Range rowRange;
    Napi::Value* pyobj_colRange = NULL;
    Range colRange=Range::all();

    const char* keywords[] = { "m", "rowRange", "colRange", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:UMat", (char**)keywords, &pyobj_m, &pyobj_rowRange, &pyobj_colRange) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_rowRange, rowRange, ArgInfo("rowRange", 0)) &&
        jsopencv_to_safe(info, pyobj_colRange, colRange, ArgInfo("colRange", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(m, rowRange, colRange)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    UMat m;
    Napi::Value* pyobj_roi = NULL;
    Rect roi;

    const char* keywords[] = { "m", "roi", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:UMat", (char**)keywords, &pyobj_m, &pyobj_roi) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_roi, roi, ArgInfo("roi", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(m, roi)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    UMat m;
    Napi::Value* pyobj_ranges = NULL;
    vector_Range ranges;

    const char* keywords[] = { "m", "ranges", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:UMat", (char**)keywords, &pyobj_m, &pyobj_ranges) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_ranges, ranges, ArgInfo("ranges", 0)))
    {
        new (&(self->v)) Ptr<cv::UMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::UMat(m, ranges)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("UMat");

    return -1;
}

static Napi::Value pyopencv_cv_UMat_context_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    void* retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv_UMat_context());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_UMat_get(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::UMat> * self1 = 0;
    if (!pyopencv_UMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'UMat' or its derivative)");
    Ptr<cv::UMat> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv_UMat_get(_self_));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_UMat_handle(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::UMat> * self1 = 0;
    if (!pyopencv_UMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'UMat' or its derivative)");
    Ptr<cv::UMat> _self_ = *(self1);
    Napi::Value* pyobj_accessFlags = NULL;
    AccessFlag accessFlags=static_cast<AccessFlag>(0);
    void* retval;

    const char* keywords[] = { "accessFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:UMat.handle", (char**)keywords, &pyobj_accessFlags) &&
        jsopencv_to_safe(info, pyobj_accessFlags, accessFlags, ArgInfo("accessFlags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->handle(accessFlags));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_UMat_isContinuous(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::UMat> * self1 = 0;
    if (!pyopencv_UMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'UMat' or its derivative)");
    Ptr<cv::UMat> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isContinuous());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_UMat_isSubmatrix(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::UMat> * self1 = 0;
    if (!pyopencv_UMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'UMat' or its derivative)");
    Ptr<cv::UMat> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isSubmatrix());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_UMat_queue_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    void* retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv_UMat_queue());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (UMat)

static PyGetSetDef pyopencv_UMat_getseters[] =
{
    {(char*)"offset", (getter)pyopencv_UMat_get_offset, (setter)pyopencv_UMat_set_offset, (char*)"offset", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_UMat_methods[] =
{
    {"context", CV_JS_FN_WITH_KW_(pyopencv_cv_UMat_context_static, METH_STATIC), "context() -> retval\n."},
    {"get", CV_JS_FN_WITH_KW_(pyopencv_cv_UMat_get, 0), "get() -> retval\n."},
    {"handle", CV_JS_FN_WITH_KW_(pyopencv_cv_UMat_handle, 0), "handle(accessFlags) -> retval\n."},
    {"isContinuous", CV_JS_FN_WITH_KW_(pyopencv_cv_UMat_isContinuous, 0), "isContinuous() -> retval\n."},
    {"isSubmatrix", CV_JS_FN_WITH_KW_(pyopencv_cv_UMat_isSubmatrix, 0), "isSubmatrix() -> retval\n."},
    {"queue", CV_JS_FN_WITH_KW_(pyopencv_cv_UMat_queue_static, METH_STATIC), "queue() -> retval\n."},

    {NULL,          NULL}
};

// Converter (UMat)

template<>
struct PyOpenCV_Converter< Ptr<cv::UMat> >
{
    static PyObject* from(const Ptr<cv::UMat>& r)
    {
        return pyopencv_UMat_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::UMat>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::UMat> * dst_;
        if (pyopencv_UMat_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
    {
        Ptr<Mat> _src;
        if (pyopencv_to_safe(src, _src, info))
        {
            return cv_mappable_to(_src, dst);
        }
    }

        failmsg("Expected Ptr<cv::UMat> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// UsacParams (Generic)
//================================================================================

// GetSet (UsacParams)


static PyObject* pyopencv_UsacParams_get_confidence(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.confidence);
}

static int pyopencv_UsacParams_set_confidence(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the confidence attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.confidence, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_isParallel(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.isParallel);
}

static int pyopencv_UsacParams_set_isParallel(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the isParallel attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.isParallel, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_loIterations(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.loIterations);
}

static int pyopencv_UsacParams_set_loIterations(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the loIterations attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.loIterations, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_loMethod(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.loMethod);
}

static int pyopencv_UsacParams_set_loMethod(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the loMethod attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.loMethod, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_loSampleSize(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.loSampleSize);
}

static int pyopencv_UsacParams_set_loSampleSize(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the loSampleSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.loSampleSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_maxIterations(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.maxIterations);
}

static int pyopencv_UsacParams_set_maxIterations(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxIterations attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxIterations, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_neighborsSearch(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.neighborsSearch);
}

static int pyopencv_UsacParams_set_neighborsSearch(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the neighborsSearch attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.neighborsSearch, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_randomGeneratorState(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.randomGeneratorState);
}

static int pyopencv_UsacParams_set_randomGeneratorState(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the randomGeneratorState attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.randomGeneratorState, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_sampler(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.sampler);
}

static int pyopencv_UsacParams_set_sampler(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the sampler attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.sampler, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_score(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.score);
}

static int pyopencv_UsacParams_set_score(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the score attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.score, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_UsacParams_get_threshold(pyopencv_UsacParams_t* p, void *closure)
{
    return jsopencv_from(p->v.threshold);
}

static int pyopencv_UsacParams_set_threshold(pyopencv_UsacParams_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the threshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.threshold, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (UsacParams)

static int pyopencv_cv_UsacParams_UsacParams(pyopencv_UsacParams_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::UsacParams());
        return 0;
    }

    return -1;
}



// Tables (UsacParams)

static PyGetSetDef pyopencv_UsacParams_getseters[] =
{
    {(char*)"confidence", (getter)pyopencv_UsacParams_get_confidence, (setter)pyopencv_UsacParams_set_confidence, (char*)"confidence", NULL},
    {(char*)"isParallel", (getter)pyopencv_UsacParams_get_isParallel, (setter)pyopencv_UsacParams_set_isParallel, (char*)"isParallel", NULL},
    {(char*)"loIterations", (getter)pyopencv_UsacParams_get_loIterations, (setter)pyopencv_UsacParams_set_loIterations, (char*)"loIterations", NULL},
    {(char*)"loMethod", (getter)pyopencv_UsacParams_get_loMethod, (setter)pyopencv_UsacParams_set_loMethod, (char*)"loMethod", NULL},
    {(char*)"loSampleSize", (getter)pyopencv_UsacParams_get_loSampleSize, (setter)pyopencv_UsacParams_set_loSampleSize, (char*)"loSampleSize", NULL},
    {(char*)"maxIterations", (getter)pyopencv_UsacParams_get_maxIterations, (setter)pyopencv_UsacParams_set_maxIterations, (char*)"maxIterations", NULL},
    {(char*)"neighborsSearch", (getter)pyopencv_UsacParams_get_neighborsSearch, (setter)pyopencv_UsacParams_set_neighborsSearch, (char*)"neighborsSearch", NULL},
    {(char*)"randomGeneratorState", (getter)pyopencv_UsacParams_get_randomGeneratorState, (setter)pyopencv_UsacParams_set_randomGeneratorState, (char*)"randomGeneratorState", NULL},
    {(char*)"sampler", (getter)pyopencv_UsacParams_get_sampler, (setter)pyopencv_UsacParams_set_sampler, (char*)"sampler", NULL},
    {(char*)"score", (getter)pyopencv_UsacParams_get_score, (setter)pyopencv_UsacParams_set_score, (char*)"score", NULL},
    {(char*)"threshold", (getter)pyopencv_UsacParams_get_threshold, (setter)pyopencv_UsacParams_set_threshold, (char*)"threshold", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_UsacParams_methods[] =
{

    {NULL,          NULL}
};

// Converter (UsacParams)

template<>
struct PyOpenCV_Converter< cv::UsacParams >
{
    static PyObject* from(const cv::UsacParams& r)
    {
        return pyopencv_UsacParams_Instance(r);
    }
    static bool to(PyObject* src, cv::UsacParams& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::UsacParams * dst_;
        if (pyopencv_UsacParams_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::UsacParams for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// VariationalRefinement (Generic)
//================================================================================

// GetSet (VariationalRefinement)



// Methods (VariationalRefinement)

static Napi::Value pyopencv_cv_VariationalRefinement_calcUV(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_I0 = NULL;
    Mat I0;
    Napi::Value* pyobj_I1 = NULL;
    Mat I1;
    Napi::Value* pyobj_flow_u = NULL;
    Mat flow_u;
    Napi::Value* pyobj_flow_v = NULL;
    Mat flow_v;

    const char* keywords[] = { "I0", "I1", "flow_u", "flow_v", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:VariationalRefinement.calcUV", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow_u, &pyobj_flow_v) &&
        jsopencv_to_safe(info, pyobj_I0, I0, ArgInfo("I0", 0)) &&
        jsopencv_to_safe(info, pyobj_I1, I1, ArgInfo("I1", 0)) &&
        jsopencv_to_safe(info, pyobj_flow_u, flow_u, ArgInfo("flow_u", 1)) &&
        jsopencv_to_safe(info, pyobj_flow_v, flow_v, ArgInfo("flow_v", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->calcUV(I0, I1, flow_u, flow_v));
        return Py_BuildValue("(NN)", jsopencv_from(flow_u), jsopencv_from(flow_v));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_I0 = NULL;
    UMat I0;
    Napi::Value* pyobj_I1 = NULL;
    UMat I1;
    Napi::Value* pyobj_flow_u = NULL;
    UMat flow_u;
    Napi::Value* pyobj_flow_v = NULL;
    UMat flow_v;

    const char* keywords[] = { "I0", "I1", "flow_u", "flow_v", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:VariationalRefinement.calcUV", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow_u, &pyobj_flow_v) &&
        jsopencv_to_safe(info, pyobj_I0, I0, ArgInfo("I0", 0)) &&
        jsopencv_to_safe(info, pyobj_I1, I1, ArgInfo("I1", 0)) &&
        jsopencv_to_safe(info, pyobj_flow_u, flow_u, ArgInfo("flow_u", 1)) &&
        jsopencv_to_safe(info, pyobj_flow_v, flow_v, ArgInfo("flow_v", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->calcUV(I0, I1, flow_u, flow_v));
        return Py_BuildValue("(NN)", jsopencv_from(flow_u), jsopencv_from(flow_v));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("calcUV");

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Ptr<VariationalRefinement> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::VariationalRefinement::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_getAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlpha());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_getDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDelta());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_getFixedPointIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFixedPointIterations());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_getGamma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGamma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_getOmega(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getOmega());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_getSorIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSorIterations());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_setAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VariationalRefinement.setAlpha", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAlpha(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_setDelta(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VariationalRefinement.setDelta", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDelta(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_setFixedPointIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VariationalRefinement.setFixedPointIterations", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFixedPointIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_setGamma(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VariationalRefinement.setGamma", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_setOmega(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VariationalRefinement.setOmega", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setOmega(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VariationalRefinement_setSorIterations(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VariationalRefinement> * self1 = 0;
    if (!pyopencv_VariationalRefinement_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VariationalRefinement' or its derivative)");
    Ptr<cv::VariationalRefinement> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VariationalRefinement.setSorIterations", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSorIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (VariationalRefinement)

static PyGetSetDef pyopencv_VariationalRefinement_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_VariationalRefinement_methods[] =
{
    {"calcUV", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_calcUV, 0), "calcUV(I0, I1, flow_u, flow_v) -> flow_u, flow_v\n.   @brief @ref calc function overload to handle separate horizontal (u) and vertical (v) flow components\n.   (to avoid extra splits/merges)"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_create_static, METH_STATIC), "create() -> retval\n.   @brief Creates an instance of VariationalRefinement"},
    {"getAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_getAlpha, 0), "getAlpha() -> retval\n.   @brief Weight of the smoothness term\n.   @see setAlpha"},
    {"getDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_getDelta, 0), "getDelta() -> retval\n.   @brief Weight of the color constancy term\n.   @see setDelta"},
    {"getFixedPointIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_getFixedPointIterations, 0), "getFixedPointIterations() -> retval\n.   @brief Number of outer (fixed-point) iterations in the minimization procedure.\n.   @see setFixedPointIterations"},
    {"getGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_getGamma, 0), "getGamma() -> retval\n.   @brief Weight of the gradient constancy term\n.   @see setGamma"},
    {"getOmega", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_getOmega, 0), "getOmega() -> retval\n.   @brief Relaxation factor in SOR\n.   @see setOmega"},
    {"getSorIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_getSorIterations, 0), "getSorIterations() -> retval\n.   @brief Number of inner successive over-relaxation (SOR) iterations\n.           in the minimization procedure to solve the respective linear system.\n.   @see setSorIterations"},
    {"setAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_setAlpha, 0), "setAlpha(val) -> None\n.   @copybrief getAlpha @see getAlpha"},
    {"setDelta", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_setDelta, 0), "setDelta(val) -> None\n.   @copybrief getDelta @see getDelta"},
    {"setFixedPointIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_setFixedPointIterations, 0), "setFixedPointIterations(val) -> None\n.   @copybrief getFixedPointIterations @see getFixedPointIterations"},
    {"setGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_setGamma, 0), "setGamma(val) -> None\n.   @copybrief getGamma @see getGamma"},
    {"setOmega", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_setOmega, 0), "setOmega(val) -> None\n.   @copybrief getOmega @see getOmega"},
    {"setSorIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_VariationalRefinement_setSorIterations, 0), "setSorIterations(val) -> None\n.   @copybrief getSorIterations @see getSorIterations"},

    {NULL,          NULL}
};

// Converter (VariationalRefinement)

template<>
struct PyOpenCV_Converter< Ptr<cv::VariationalRefinement> >
{
    static PyObject* from(const Ptr<cv::VariationalRefinement>& r)
    {
        return pyopencv_VariationalRefinement_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::VariationalRefinement>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::VariationalRefinement> * dst_;
        if (pyopencv_VariationalRefinement_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::VariationalRefinement> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// VideoCapture (Generic)
//================================================================================

// GetSet (VideoCapture)



// Methods (VideoCapture)

static int pyopencv_cv_VideoCapture_VideoCapture(pyopencv_VideoCapture_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(5);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoCapture()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=CAP_ANY;

    const char* keywords[] = { "filename", "apiPreference", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:VideoCapture", (char**)keywords, &pyobj_filename, &pyobj_apiPreference) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoCapture(filename, apiPreference)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_params = NULL;
    vector_int params;

    const char* keywords[] = { "filename", "apiPreference", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:VideoCapture", (char**)keywords, &pyobj_filename, &pyobj_apiPreference, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoCapture(filename, apiPreference, params)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_index = NULL;
    int index=0;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=CAP_ANY;

    const char* keywords[] = { "index", "apiPreference", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:VideoCapture", (char**)keywords, &pyobj_index, &pyobj_apiPreference) &&
        jsopencv_to_safe(info, pyobj_index, index, ArgInfo("index", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoCapture(index, apiPreference)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_index = NULL;
    int index=0;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_params = NULL;
    vector_int params;

    const char* keywords[] = { "index", "apiPreference", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:VideoCapture", (char**)keywords, &pyobj_index, &pyobj_apiPreference, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_index, index, ArgInfo("index", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoCapture(index, apiPreference, params)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("VideoCapture");

    return -1;
}

static Napi::Value pyopencv_cv_VideoCapture_get(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    Napi::Value* pyobj_propId = NULL;
    int propId=0;
    double retval;

    const char* keywords[] = { "propId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VideoCapture.get", (char**)keywords, &pyobj_propId) &&
        jsopencv_to_safe(info, pyobj_propId, propId, ArgInfo("propId", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get(propId));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_getBackendName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBackendName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_getExceptionMode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getExceptionMode());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_grab(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->grab());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_isOpened(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isOpened());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_open(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=CAP_ANY;
    bool retval;

    const char* keywords[] = { "filename", "apiPreference", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:VideoCapture.open", (char**)keywords, &pyobj_filename, &pyobj_apiPreference) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, apiPreference));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_params = NULL;
    vector_int params;
    bool retval;

    const char* keywords[] = { "filename", "apiPreference", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:VideoCapture.open", (char**)keywords, &pyobj_filename, &pyobj_apiPreference, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, apiPreference, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_index = NULL;
    int index=0;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=CAP_ANY;
    bool retval;

    const char* keywords[] = { "index", "apiPreference", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:VideoCapture.open", (char**)keywords, &pyobj_index, &pyobj_apiPreference) &&
        jsopencv_to_safe(info, pyobj_index, index, ArgInfo("index", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(index, apiPreference));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_index = NULL;
    int index=0;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_params = NULL;
    vector_int params;
    bool retval;

    const char* keywords[] = { "index", "apiPreference", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:VideoCapture.open", (char**)keywords, &pyobj_index, &pyobj_apiPreference, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_index, index, ArgInfo("index", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(index, apiPreference, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("open");

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_read(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:VideoCapture.read", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->read(image));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(image));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:VideoCapture.read", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->read(image));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(image));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("read");

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_release(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_retrieve(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_flag = NULL;
    int flag=0;
    bool retval;

    const char* keywords[] = { "image", "flag", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:VideoCapture.retrieve", (char**)keywords, &pyobj_image, &pyobj_flag) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_flag, flag, ArgInfo("flag", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->retrieve(image, flag));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(image));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_flag = NULL;
    int flag=0;
    bool retval;

    const char* keywords[] = { "image", "flag", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:VideoCapture.retrieve", (char**)keywords, &pyobj_image, &pyobj_flag) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_flag, flag, ArgInfo("flag", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->retrieve(image, flag));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(image));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("retrieve");

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_set(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    Napi::Value* pyobj_propId = NULL;
    int propId=0;
    Napi::Value* pyobj_value = NULL;
    double value=0;
    bool retval;

    const char* keywords[] = { "propId", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:VideoCapture.set", (char**)keywords, &pyobj_propId, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_propId, propId, ArgInfo("propId", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->set(propId, value));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_setExceptionMode(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoCapture> * self1 = 0;
    if (!pyopencv_VideoCapture_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    Ptr<cv::VideoCapture> _self_ = *(self1);
    Napi::Value* pyobj_enable = NULL;
    bool enable=0;

    const char* keywords[] = { "enable", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VideoCapture.setExceptionMode", (char**)keywords, &pyobj_enable) &&
        jsopencv_to_safe(info, pyobj_enable, enable, ArgInfo("enable", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setExceptionMode(enable));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoCapture_waitAny_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_streams = NULL;
    vector_VideoCapture streams;
    vector_int readyIndex;
    Napi::Value* pyobj_timeoutNs = NULL;
    int64 timeoutNs=0;
    bool retval;

    const char* keywords[] = { "streams", "timeoutNs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:VideoCapture.waitAny", (char**)keywords, &pyobj_streams, &pyobj_timeoutNs) &&
        jsopencv_to_safe(info, pyobj_streams, streams, ArgInfo("streams", 0)) &&
        jsopencv_to_safe(info, pyobj_timeoutNs, timeoutNs, ArgInfo("timeoutNs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::VideoCapture::waitAny(streams, readyIndex, timeoutNs));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(readyIndex));
    }

    return NULL;
}



// Tables (VideoCapture)

static PyGetSetDef pyopencv_VideoCapture_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_VideoCapture_methods[] =
{
    {"get", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_get, 0), "get(propId) -> retval\n.   @brief Returns the specified VideoCapture property\n.   \n.       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n.       or one from @ref videoio_flags_others\n.       @return Value for the specified property. Value 0 is returned when querying a property that is\n.       not supported by the backend used by the VideoCapture instance.\n.   \n.       @note Reading / writing properties involves many layers. Some unexpected result might happens\n.       along this chain.\n.       @code{.txt}\n.       VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware\n.       @endcode\n.       The returned value might be different from what really used by the device or it could be encoded\n.       using device dependent rules (eg. steps or percentage). Effective behaviour depends from device\n.       driver and API Backend"},
    {"getBackendName", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_getBackendName, 0), "getBackendName() -> retval\n.   @brief Returns used backend API name\n.   \n.        @note Stream should be opened."},
    {"getExceptionMode", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_getExceptionMode, 0), "getExceptionMode() -> retval\n."},
    {"grab", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_grab, 0), "grab() -> retval\n.   @brief Grabs the next frame from video file or capturing device.\n.   \n.       @return `true` (non-zero) in the case of success.\n.   \n.       The method/function grabs the next frame from video file or camera and returns true (non-zero) in\n.       the case of success.\n.   \n.       The primary use of the function is in multi-camera environments, especially when the cameras do not\n.       have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that\n.       call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way\n.       the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames\n.       from different cameras will be closer in time.\n.   \n.       Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the\n.       correct way of retrieving data from it is to call VideoCapture::grab() first and then call\n.       VideoCapture::retrieve() one or more times with different values of the channel parameter.\n.   \n.       @ref tutorial_kinect_openni"},
    {"isOpened", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_isOpened, 0), "isOpened() -> retval\n.   @brief Returns true if video capturing has been initialized already.\n.   \n.       If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns\n.       true."},
    {"open", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_open, 0), "open(filename[, apiPreference]) -> retval\n.   @brief  Opens a video file or a capturing device or an IP video stream for video capturing.\n.   \n.       @overload\n.   \n.       Parameters are same as the constructor VideoCapture(const String& filename, int apiPreference = CAP_ANY)\n.       @return `true` if the file has been successfully opened\n.   \n.       The method first calls VideoCapture::release to close the already opened file or camera.\n\n\n\nopen(filename, apiPreference, params) -> retval\n.   @brief  Opens a video file or a capturing device or an IP video stream for video capturing with API Preference and parameters\n.   \n.       @overload\n.   \n.       The `params` parameter allows to specify extra parameters encoded as pairs `(paramId_1, paramValue_1, paramId_2, paramValue_2, ...)`.\n.       See cv::VideoCaptureProperties\n.   \n.       @return `true` if the file has been successfully opened\n.   \n.       The method first calls VideoCapture::release to close the already opened file or camera.\n\n\n\nopen(index[, apiPreference]) -> retval\n.   @brief  Opens a camera for video capturing\n.   \n.       @overload\n.   \n.       Parameters are same as the constructor VideoCapture(int index, int apiPreference = CAP_ANY)\n.       @return `true` if the camera has been successfully opened.\n.   \n.       The method first calls VideoCapture::release to close the already opened file or camera.\n\n\n\nopen(index, apiPreference, params) -> retval\n.   @brief  Opens a camera for video capturing with API Preference and parameters\n.   \n.       @overload\n.   \n.       The `params` parameter allows to specify extra parameters encoded as pairs `(paramId_1, paramValue_1, paramId_2, paramValue_2, ...)`.\n.       See cv::VideoCaptureProperties\n.   \n.       @return `true` if the camera has been successfully opened.\n.   \n.       The method first calls VideoCapture::release to close the already opened file or camera."},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_read, 0), "read([, image]) -> retval, image\n.   @brief Grabs, decodes and returns the next video frame.\n.   \n.       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n.       @return `false` if no frames has been grabbed\n.   \n.       The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the\n.       most convenient method for reading video files or capturing data from decode and returns the just\n.       grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more\n.       frames in video file), the method returns false and the function returns empty image (with %cv::Mat, test it with Mat::empty()).\n.   \n.       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n.       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n.       cvCloneImage and then do whatever you want with the copy."},
    {"release", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_release, 0), "release() -> None\n.   @brief Closes video file or capturing device.\n.   \n.       The method is automatically called by subsequent VideoCapture::open and by VideoCapture\n.       destructor.\n.   \n.       The C function also deallocates memory and clears \\*capture pointer."},
    {"retrieve", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_retrieve, 0), "retrieve([, image[, flag]]) -> retval, image\n.   @brief Decodes and returns the grabbed video frame.\n.   \n.       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n.       @param flag it could be a frame index or a driver specific flag\n.       @return `false` if no frames has been grabbed\n.   \n.       The method decodes and returns the just grabbed frame. If no frames has been grabbed\n.       (camera has been disconnected, or there are no more frames in video file), the method returns false\n.       and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).\n.   \n.       @sa read()\n.   \n.       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n.       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n.       cvCloneImage and then do whatever you want with the copy."},
    {"set", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_set, 0), "set(propId, value) -> retval\n.   @brief Sets a property in the VideoCapture.\n.   \n.       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n.       or one from @ref videoio_flags_others\n.       @param value Value of the property.\n.       @return `true` if the property is supported by backend used by the VideoCapture instance.\n.       @note Even if it returns `true` this doesn't ensure that the property\n.       value has been accepted by the capture device. See note in VideoCapture::get()"},
    {"setExceptionMode", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_setExceptionMode, 0), "setExceptionMode(enable) -> None\n.   Switches exceptions mode\n.        *\n.        * methods raise exceptions if not successful instead of returning an error code"},
    {"waitAny", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoCapture_waitAny_static, METH_STATIC), "waitAny(streams[, timeoutNs]) -> retval, readyIndex\n.   @brief Wait for ready frames from VideoCapture.\n.   \n.       @param streams input video streams\n.       @param readyIndex stream indexes with grabbed frames (ready to use .retrieve() to fetch actual frame)\n.       @param timeoutNs number of nanoseconds (0 - infinite)\n.       @return `true` if streamReady is not empty\n.   \n.       @throws Exception %Exception on stream errors (check .isOpened() to filter out malformed streams) or VideoCapture type is not supported\n.   \n.       The primary use of the function is in multi-camera environments.\n.       The method fills the ready state vector, grabs video frame, if camera is ready.\n.   \n.       After this call use VideoCapture::retrieve() to decode and fetch frame data."},

    {NULL,          NULL}
};

// Converter (VideoCapture)

template<>
struct PyOpenCV_Converter< Ptr<cv::VideoCapture> >
{
    static PyObject* from(const Ptr<cv::VideoCapture>& r)
    {
        return pyopencv_VideoCapture_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::VideoCapture>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::VideoCapture> * dst_;
        if (pyopencv_VideoCapture_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::VideoCapture> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// VideoWriter (Generic)
//================================================================================

// GetSet (VideoWriter)



// Methods (VideoWriter)

static int pyopencv_cv_VideoWriter_VideoWriter(pyopencv_VideoWriter_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv;

    pyPrepareArgumentConversionErrorsStorage(5);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoWriter()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_isColor = NULL;
    bool isColor=true;

    const char* keywords[] = { "filename", "fourcc", "fps", "frameSize", "isColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:VideoWriter", (char**)keywords, &pyobj_filename, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_isColor) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_isColor, isColor, ArgInfo("isColor", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoWriter(filename, fourcc, fps, frameSize, isColor)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_isColor = NULL;
    bool isColor=true;

    const char* keywords[] = { "filename", "apiPreference", "fourcc", "fps", "frameSize", "isColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|O:VideoWriter", (char**)keywords, &pyobj_filename, &pyobj_apiPreference, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_isColor) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_isColor, isColor, ArgInfo("isColor", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoWriter(filename, apiPreference, fourcc, fps, frameSize, isColor)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_params = NULL;
    vector_int params;

    const char* keywords[] = { "filename", "fourcc", "fps", "frameSize", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO:VideoWriter", (char**)keywords, &pyobj_filename, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoWriter(filename, fourcc, fps, frameSize, params)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_params = NULL;
    vector_int params;

    const char* keywords[] = { "filename", "apiPreference", "fourcc", "fps", "frameSize", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOOO:VideoWriter", (char**)keywords, &pyobj_filename, &pyobj_apiPreference, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::VideoWriter(filename, apiPreference, fourcc, fps, frameSize, params)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("VideoWriter");

    return -1;
}

static Napi::Value pyopencv_cv_VideoWriter_fourcc_static(const Napi::CallbackInfo &info)
{
    using namespace cv;

    Napi::Value* pyobj_c1 = NULL;
    char c1;
    Napi::Value* pyobj_c2 = NULL;
    char c2;
    Napi::Value* pyobj_c3 = NULL;
    char c3;
    Napi::Value* pyobj_c4 = NULL;
    char c4;
    int retval;

    const char* keywords[] = { "c1", "c2", "c3", "c4", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:VideoWriter.fourcc", (char**)keywords, &pyobj_c1, &pyobj_c2, &pyobj_c3, &pyobj_c4) &&
        convert_to_char(pyobj_c1, &c1, ArgInfo("c1", 0)) &&
        convert_to_char(pyobj_c2, &c2, ArgInfo("c2", 0)) &&
        convert_to_char(pyobj_c3, &c3, ArgInfo("c3", 0)) &&
        convert_to_char(pyobj_c4, &c4, ArgInfo("c4", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::VideoWriter::fourcc(c1, c2, c3, c4));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_get(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);
    Napi::Value* pyobj_propId = NULL;
    int propId=0;
    double retval;

    const char* keywords[] = { "propId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VideoWriter.get", (char**)keywords, &pyobj_propId) &&
        jsopencv_to_safe(info, pyobj_propId, propId, ArgInfo("propId", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get(propId));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_getBackendName(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBackendName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_isOpened(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isOpened());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_open(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_isColor = NULL;
    bool isColor=true;
    bool retval;

    const char* keywords[] = { "filename", "fourcc", "fps", "frameSize", "isColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:VideoWriter.open", (char**)keywords, &pyobj_filename, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_isColor) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_isColor, isColor, ArgInfo("isColor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, fourcc, fps, frameSize, isColor));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_isColor = NULL;
    bool isColor=true;
    bool retval;

    const char* keywords[] = { "filename", "apiPreference", "fourcc", "fps", "frameSize", "isColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|O:VideoWriter.open", (char**)keywords, &pyobj_filename, &pyobj_apiPreference, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_isColor) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_isColor, isColor, ArgInfo("isColor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, apiPreference, fourcc, fps, frameSize, isColor));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_params = NULL;
    vector_int params;
    bool retval;

    const char* keywords[] = { "filename", "fourcc", "fps", "frameSize", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO:VideoWriter.open", (char**)keywords, &pyobj_filename, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, fourcc, fps, frameSize, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_apiPreference = NULL;
    int apiPreference=0;
    Napi::Value* pyobj_fourcc = NULL;
    int fourcc=0;
    Napi::Value* pyobj_fps = NULL;
    double fps=0;
    Napi::Value* pyobj_frameSize = NULL;
    Size frameSize;
    Napi::Value* pyobj_params = NULL;
    vector_int params;
    bool retval;

    const char* keywords[] = { "filename", "apiPreference", "fourcc", "fps", "frameSize", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOOO:VideoWriter.open", (char**)keywords, &pyobj_filename, &pyobj_apiPreference, &pyobj_fourcc, &pyobj_fps, &pyobj_frameSize, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_apiPreference, apiPreference, ArgInfo("apiPreference", 0)) &&
        jsopencv_to_safe(info, pyobj_fourcc, fourcc, ArgInfo("fourcc", 0)) &&
        jsopencv_to_safe(info, pyobj_fps, fps, ArgInfo("fps", 0)) &&
        jsopencv_to_safe(info, pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->open(filename, apiPreference, fourcc, fps, frameSize, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("open");

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_release(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_set(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);
    Napi::Value* pyobj_propId = NULL;
    int propId=0;
    Napi::Value* pyobj_value = NULL;
    double value=0;
    bool retval;

    const char* keywords[] = { "propId", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:VideoWriter.set", (char**)keywords, &pyobj_propId, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_propId, propId, ArgInfo("propId", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->set(propId, value));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_VideoWriter_write(const Napi::CallbackInfo &info)
{
    using namespace cv;


    Ptr<cv::VideoWriter> * self1 = 0;
    if (!pyopencv_VideoWriter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    Ptr<cv::VideoWriter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VideoWriter.write", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(image));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:VideoWriter.write", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(image));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("write");

    return NULL;
}



// Tables (VideoWriter)

static PyGetSetDef pyopencv_VideoWriter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_VideoWriter_methods[] =
{
    {"fourcc", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_fourcc_static, METH_STATIC), "fourcc(c1, c2, c3, c4) -> retval\n.   @brief Concatenates 4 chars to a fourcc code\n.   \n.       @return a fourcc code\n.   \n.       This static method constructs the fourcc code of the codec to be used in the constructor\n.       VideoWriter::VideoWriter or VideoWriter::open."},
    {"get", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_get, 0), "get(propId) -> retval\n.   @brief Returns the specified VideoWriter property\n.   \n.        @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)\n.        or one of @ref videoio_flags_others\n.   \n.        @return Value for the specified property. Value 0 is returned when querying a property that is\n.        not supported by the backend used by the VideoWriter instance."},
    {"getBackendName", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_getBackendName, 0), "getBackendName() -> retval\n.   @brief Returns used backend API name\n.   \n.        @note Stream should be opened."},
    {"isOpened", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_isOpened, 0), "isOpened() -> retval\n.   @brief Returns true if video writer has been successfully initialized."},
    {"open", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_open, 0), "open(filename, fourcc, fps, frameSize[, isColor]) -> retval\n.   @brief Initializes or reinitializes video writer.\n.   \n.       The method opens video writer. Parameters are the same as in the constructor\n.       VideoWriter::VideoWriter.\n.       @return `true` if video writer has been successfully initialized\n.   \n.       The method first calls VideoWriter::release to close the already opened file.\n\n\n\nopen(filename, apiPreference, fourcc, fps, frameSize[, isColor]) -> retval\n.   @overload\n\n\n\nopen(filename, fourcc, fps, frameSize, params) -> retval\n.   @overload\n\n\n\nopen(filename, apiPreference, fourcc, fps, frameSize, params) -> retval\n.   @overload"},
    {"release", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_release, 0), "release() -> None\n.   @brief Closes the video writer.\n.   \n.       The method is automatically called by subsequent VideoWriter::open and by the VideoWriter\n.       destructor."},
    {"set", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_set, 0), "set(propId, value) -> retval\n.   @brief Sets a property in the VideoWriter.\n.   \n.        @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)\n.        or one of @ref videoio_flags_others\n.   \n.        @param value Value of the property.\n.        @return  `true` if the property is supported by the backend used by the VideoWriter instance."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_VideoWriter_write, 0), "write(image) -> None\n.   @brief Writes the next video frame\n.   \n.       @param image The written frame. In general, color images are expected in BGR format.\n.   \n.       The function/method writes the specified image to video file. It must have the same size as has\n.       been specified when opening the video writer."},

    {NULL,          NULL}
};

// Converter (VideoWriter)

template<>
struct PyOpenCV_Converter< Ptr<cv::VideoWriter> >
{
    static PyObject* from(const Ptr<cv::VideoWriter>& r)
    {
        return pyopencv_VideoWriter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::VideoWriter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::VideoWriter> * dst_;
        if (pyopencv_VideoWriter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::VideoWriter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_ArucoDetector (Generic)
//================================================================================

// GetSet (aruco_ArucoDetector)



// Methods (aruco_ArucoDetector)

static int pyopencv_cv_aruco_aruco_ArucoDetector_ArucoDetector(pyopencv_aruco_ArucoDetector_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary=getPredefinedDictionary(cv::aruco::DICT_4X4_50);
    Napi::Value* pyobj_detectorParams = NULL;
    DetectorParameters detectorParams;
    Napi::Value* pyobj_refineParams = NULL;
    RefineParameters refineParams;

    const char* keywords[] = { "dictionary", "detectorParams", "refineParams", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:ArucoDetector", (char**)keywords, &pyobj_dictionary, &pyobj_detectorParams, &pyobj_refineParams) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_detectorParams, detectorParams, ArgInfo("detectorParams", 0)) &&
        jsopencv_to_safe(info, pyobj_refineParams, refineParams, ArgInfo("refineParams", 0)))
    {
        new (&(self->v)) Ptr<cv::aruco::ArucoDetector>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::aruco::ArucoDetector(dictionary, detectorParams, refineParams)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_detectMarkers(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_corners = NULL;
    vector_Mat corners;
    Napi::Value* pyobj_ids = NULL;
    Mat ids;
    Napi::Value* pyobj_rejectedImgPoints = NULL;
    vector_Mat rejectedImgPoints;

    const char* keywords[] = { "image", "corners", "ids", "rejectedImgPoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:aruco_ArucoDetector.detectMarkers", (char**)keywords, &pyobj_image, &pyobj_corners, &pyobj_ids, &pyobj_rejectedImgPoints) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_corners, corners, ArgInfo("corners", 1)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 1)) &&
        jsopencv_to_safe(info, pyobj_rejectedImgPoints, rejectedImgPoints, ArgInfo("rejectedImgPoints", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMarkers(image, corners, ids, rejectedImgPoints));
        return Py_BuildValue("(NNN)", jsopencv_from(corners), jsopencv_from(ids), jsopencv_from(rejectedImgPoints));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_corners = NULL;
    vector_UMat corners;
    Napi::Value* pyobj_ids = NULL;
    UMat ids;
    Napi::Value* pyobj_rejectedImgPoints = NULL;
    vector_UMat rejectedImgPoints;

    const char* keywords[] = { "image", "corners", "ids", "rejectedImgPoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:aruco_ArucoDetector.detectMarkers", (char**)keywords, &pyobj_image, &pyobj_corners, &pyobj_ids, &pyobj_rejectedImgPoints) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_corners, corners, ArgInfo("corners", 1)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 1)) &&
        jsopencv_to_safe(info, pyobj_rejectedImgPoints, rejectedImgPoints, ArgInfo("rejectedImgPoints", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectMarkers(image, corners, ids, rejectedImgPoints));
        return Py_BuildValue("(NNN)", jsopencv_from(corners), jsopencv_from(ids), jsopencv_from(rejectedImgPoints));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectMarkers");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_getDetectorParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    DetectorParameters retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDetectorParameters());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_getDictionary(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    Dictionary retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDictionary());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_getRefineParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    RefineParameters retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRefineParameters());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_read(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_fn = NULL;
    cv::FileNode fn;

    const char* keywords[] = { "fn", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_ArucoDetector.read", (char**)keywords, &pyobj_fn) &&
        jsopencv_to_safe(info, pyobj_fn, fn, ArgInfo("fn", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(fn));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_refineDetectedMarkers(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_board = NULL;
    Board board;
    Napi::Value* pyobj_detectedCorners = NULL;
    vector_Mat detectedCorners;
    Napi::Value* pyobj_detectedIds = NULL;
    Mat detectedIds;
    Napi::Value* pyobj_rejectedCorners = NULL;
    vector_Mat rejectedCorners;
    Napi::Value* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    Napi::Value* pyobj_distCoeffs = NULL;
    Mat distCoeffs;
    Napi::Value* pyobj_recoveredIdxs = NULL;
    Mat recoveredIdxs;

    const char* keywords[] = { "image", "board", "detectedCorners", "detectedIds", "rejectedCorners", "cameraMatrix", "distCoeffs", "recoveredIdxs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|OOO:aruco_ArucoDetector.refineDetectedMarkers", (char**)keywords, &pyobj_image, &pyobj_board, &pyobj_detectedCorners, &pyobj_detectedIds, &pyobj_rejectedCorners, &pyobj_cameraMatrix, &pyobj_distCoeffs, &pyobj_recoveredIdxs) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_board, board, ArgInfo("board", 0)) &&
        jsopencv_to_safe(info, pyobj_detectedCorners, detectedCorners, ArgInfo("detectedCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_detectedIds, detectedIds, ArgInfo("detectedIds", 1)) &&
        jsopencv_to_safe(info, pyobj_rejectedCorners, rejectedCorners, ArgInfo("rejectedCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        jsopencv_to_safe(info, pyobj_distCoeffs, distCoeffs, ArgInfo("distCoeffs", 0)) &&
        jsopencv_to_safe(info, pyobj_recoveredIdxs, recoveredIdxs, ArgInfo("recoveredIdxs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->refineDetectedMarkers(image, board, detectedCorners, detectedIds, rejectedCorners, cameraMatrix, distCoeffs, recoveredIdxs));
        return Py_BuildValue("(NNNN)", jsopencv_from(detectedCorners), jsopencv_from(detectedIds), jsopencv_from(rejectedCorners), jsopencv_from(recoveredIdxs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_board = NULL;
    Board board;
    Napi::Value* pyobj_detectedCorners = NULL;
    vector_UMat detectedCorners;
    Napi::Value* pyobj_detectedIds = NULL;
    UMat detectedIds;
    Napi::Value* pyobj_rejectedCorners = NULL;
    vector_UMat rejectedCorners;
    Napi::Value* pyobj_cameraMatrix = NULL;
    UMat cameraMatrix;
    Napi::Value* pyobj_distCoeffs = NULL;
    UMat distCoeffs;
    Napi::Value* pyobj_recoveredIdxs = NULL;
    UMat recoveredIdxs;

    const char* keywords[] = { "image", "board", "detectedCorners", "detectedIds", "rejectedCorners", "cameraMatrix", "distCoeffs", "recoveredIdxs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|OOO:aruco_ArucoDetector.refineDetectedMarkers", (char**)keywords, &pyobj_image, &pyobj_board, &pyobj_detectedCorners, &pyobj_detectedIds, &pyobj_rejectedCorners, &pyobj_cameraMatrix, &pyobj_distCoeffs, &pyobj_recoveredIdxs) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_board, board, ArgInfo("board", 0)) &&
        jsopencv_to_safe(info, pyobj_detectedCorners, detectedCorners, ArgInfo("detectedCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_detectedIds, detectedIds, ArgInfo("detectedIds", 1)) &&
        jsopencv_to_safe(info, pyobj_rejectedCorners, rejectedCorners, ArgInfo("rejectedCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        jsopencv_to_safe(info, pyobj_distCoeffs, distCoeffs, ArgInfo("distCoeffs", 0)) &&
        jsopencv_to_safe(info, pyobj_recoveredIdxs, recoveredIdxs, ArgInfo("recoveredIdxs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->refineDetectedMarkers(image, board, detectedCorners, detectedIds, rejectedCorners, cameraMatrix, distCoeffs, recoveredIdxs));
        return Py_BuildValue("(NNNN)", jsopencv_from(detectedCorners), jsopencv_from(detectedIds), jsopencv_from(rejectedCorners), jsopencv_from(recoveredIdxs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("refineDetectedMarkers");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_setDetectorParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_detectorParameters = NULL;
    DetectorParameters detectorParameters;

    const char* keywords[] = { "detectorParameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_ArucoDetector.setDetectorParameters", (char**)keywords, &pyobj_detectorParameters) &&
        jsopencv_to_safe(info, pyobj_detectorParameters, detectorParameters, ArgInfo("detectorParameters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDetectorParameters(detectorParameters));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_setDictionary(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;

    const char* keywords[] = { "dictionary", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_ArucoDetector.setDictionary", (char**)keywords, &pyobj_dictionary) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDictionary(dictionary));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_setRefineParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_refineParameters = NULL;
    RefineParameters refineParameters;

    const char* keywords[] = { "refineParameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_ArucoDetector.setRefineParameters", (char**)keywords, &pyobj_refineParameters) &&
        jsopencv_to_safe(info, pyobj_refineParameters, refineParameters, ArgInfo("refineParameters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRefineParameters(refineParameters));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_ArucoDetector_write(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::ArucoDetector> * self1 = 0;
    if (!pyopencv_aruco_ArucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_ArucoDetector' or its derivative)");
    Ptr<cv::aruco::ArucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:aruco_ArucoDetector.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(*fs, name));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (aruco_ArucoDetector)

static PyGetSetDef pyopencv_aruco_ArucoDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_ArucoDetector_methods[] =
{
    {"detectMarkers", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_detectMarkers, 0), "detectMarkers(image[, corners[, ids[, rejectedImgPoints]]]) -> corners, ids, rejectedImgPoints\n.   @brief Basic marker detection\n.        *\n.        * @param image input image\n.        * @param corners vector of detected marker corners. For each marker, its four corners\n.        * are provided, (e.g std::vector<std::vector<cv::Point2f> > ). For N detected markers,\n.        * the dimensions of this array is Nx4. The order of the corners is clockwise.\n.        * @param ids vector of identifiers of the detected markers. The identifier is of type int\n.        * (e.g. std::vector<int>). For N detected markers, the size of ids is also N.\n.        * The identifiers have the same order than the markers in the imgPoints array.\n.        * @param rejectedImgPoints contains the imgPoints of those squares whose inner code has not a\n.        * correct codification. Useful for debugging purposes.\n.        *\n.        * Performs marker detection in the input image. Only markers included in the specific dictionary\n.        * are searched. For each detected marker, it returns the 2D position of its corner in the image\n.        * and its corresponding identifier.\n.        * Note that this function does not perform pose estimation.\n.        * @note The function does not correct lens distortion or takes it into account. It's recommended to undistort\n.        * input image with corresponging camera model, if camera parameters are known\n.        * @sa undistort, estimatePoseSingleMarkers,  estimatePoseBoard"},
    {"getDetectorParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_getDetectorParameters, 0), "getDetectorParameters() -> retval\n."},
    {"getDictionary", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_getDictionary, 0), "getDictionary() -> retval\n."},
    {"getRefineParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_getRefineParameters, 0), "getRefineParameters() -> retval\n."},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_read, 0), "read(fn) -> None\n.   @brief Reads algorithm parameters from a file storage"},
    {"refineDetectedMarkers", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_refineDetectedMarkers, 0), "refineDetectedMarkers(image, board, detectedCorners, detectedIds, rejectedCorners[, cameraMatrix[, distCoeffs[, recoveredIdxs]]]) -> detectedCorners, detectedIds, rejectedCorners, recoveredIdxs\n.   @brief Refind not detected markers based on the already detected and the board layout\n.        *\n.        * @param image input image\n.        * @param board layout of markers in the board.\n.        * @param detectedCorners vector of already detected marker corners.\n.        * @param detectedIds vector of already detected marker identifiers.\n.        * @param rejectedCorners vector of rejected candidates during the marker detection process.\n.        * @param cameraMatrix optional input 3x3 floating-point camera matrix\n.        * \\f$A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$\n.        * @param distCoeffs optional vector of distortion coefficients\n.        * \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6],[s_1, s_2, s_3, s_4]])\\f$ of 4, 5, 8 or 12 elements\n.        * @param recoveredIdxs Optional array to returns the indexes of the recovered candidates in the\n.        * original rejectedCorners array.\n.        *\n.        * This function tries to find markers that were not detected in the basic detecMarkers function.\n.        * First, based on the current detected marker and the board layout, the function interpolates\n.        * the position of the missing markers. Then it tries to find correspondence between the reprojected\n.        * markers and the rejected candidates based on the minRepDistance and errorCorrectionRate parameters.\n.        * If camera parameters and distortion coefficients are provided, missing markers are reprojected\n.        * using projectPoint function. If not, missing marker projections are interpolated using global\n.        * homography, and all the marker corners in the board must have the same Z coordinate."},
    {"setDetectorParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_setDetectorParameters, 0), "setDetectorParameters(detectorParameters) -> None\n."},
    {"setDictionary", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_setDictionary, 0), "setDictionary(dictionary) -> None\n."},
    {"setRefineParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_setRefineParameters, 0), "setRefineParameters(refineParameters) -> None\n."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_ArucoDetector_write, 0), "write(fs, name) -> None\n.   @brief simplified API for language bindings"},

    {NULL,          NULL}
};

// Converter (aruco_ArucoDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::aruco::ArucoDetector> >
{
    static PyObject* from(const Ptr<cv::aruco::ArucoDetector>& r)
    {
        return pyopencv_aruco_ArucoDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::aruco::ArucoDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::aruco::ArucoDetector> * dst_;
        if (pyopencv_aruco_ArucoDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::aruco::ArucoDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_Board (Generic)
//================================================================================

// GetSet (aruco_Board)



// Methods (aruco_Board)

static int pyopencv_cv_aruco_aruco_Board_Board(pyopencv_aruco_Board_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_objPoints = NULL;
    vector_Mat objPoints;
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;
    Napi::Value* pyobj_ids = NULL;
    Mat ids;

    const char* keywords[] = { "objPoints", "dictionary", "ids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:Board", (char**)keywords, &pyobj_objPoints, &pyobj_dictionary, &pyobj_ids) &&
        jsopencv_to_safe(info, pyobj_objPoints, objPoints, ArgInfo("objPoints", 0)) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::Board(objPoints, dictionary, ids));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_objPoints = NULL;
    vector_UMat objPoints;
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;
    Napi::Value* pyobj_ids = NULL;
    UMat ids;

    const char* keywords[] = { "objPoints", "dictionary", "ids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:Board", (char**)keywords, &pyobj_objPoints, &pyobj_dictionary, &pyobj_ids) &&
        jsopencv_to_safe(info, pyobj_objPoints, objPoints, ArgInfo("objPoints", 0)) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::Board(objPoints, dictionary, ids));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Board");

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_Board_generateImage(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Board * self1 = 0;
    if (!pyopencv_aruco_Board_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Board' or its derivative)");
    cv::aruco::Board* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_outSize = NULL;
    Size outSize;
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_marginSize = NULL;
    int marginSize=0;
    Napi::Value* pyobj_borderBits = NULL;
    int borderBits=1;

    const char* keywords[] = { "outSize", "img", "marginSize", "borderBits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:aruco_Board.generateImage", (char**)keywords, &pyobj_outSize, &pyobj_img, &pyobj_marginSize, &pyobj_borderBits) &&
        jsopencv_to_safe(info, pyobj_outSize, outSize, ArgInfo("outSize", 0)) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 1)) &&
        jsopencv_to_safe(info, pyobj_marginSize, marginSize, ArgInfo("marginSize", 0)) &&
        jsopencv_to_safe(info, pyobj_borderBits, borderBits, ArgInfo("borderBits", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->generateImage(outSize, img, marginSize, borderBits));
        return jsopencv_from(img);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_outSize = NULL;
    Size outSize;
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_marginSize = NULL;
    int marginSize=0;
    Napi::Value* pyobj_borderBits = NULL;
    int borderBits=1;

    const char* keywords[] = { "outSize", "img", "marginSize", "borderBits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:aruco_Board.generateImage", (char**)keywords, &pyobj_outSize, &pyobj_img, &pyobj_marginSize, &pyobj_borderBits) &&
        jsopencv_to_safe(info, pyobj_outSize, outSize, ArgInfo("outSize", 0)) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 1)) &&
        jsopencv_to_safe(info, pyobj_marginSize, marginSize, ArgInfo("marginSize", 0)) &&
        jsopencv_to_safe(info, pyobj_borderBits, borderBits, ArgInfo("borderBits", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->generateImage(outSize, img, marginSize, borderBits));
        return jsopencv_from(img);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("generateImage");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Board_getDictionary(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Board * self1 = 0;
    if (!pyopencv_aruco_Board_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Board' or its derivative)");
    cv::aruco::Board* _self_ = (self1);
    Dictionary retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDictionary());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Board_getIds(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Board * self1 = 0;
    if (!pyopencv_aruco_Board_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Board' or its derivative)");
    cv::aruco::Board* _self_ = (self1);
    std::vector<int> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIds());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Board_getObjPoints(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Board * self1 = 0;
    if (!pyopencv_aruco_Board_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Board' or its derivative)");
    cv::aruco::Board* _self_ = (self1);
    std::vector<std::vector<Point3f> > retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getObjPoints());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Board_getRightBottomCorner(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Board * self1 = 0;
    if (!pyopencv_aruco_Board_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Board' or its derivative)");
    cv::aruco::Board* _self_ = (self1);
    Point3f retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRightBottomCorner());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Board_matchImagePoints(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Board * self1 = 0;
    if (!pyopencv_aruco_Board_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Board' or its derivative)");
    cv::aruco::Board* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_detectedCorners = NULL;
    vector_Mat detectedCorners;
    Napi::Value* pyobj_detectedIds = NULL;
    Mat detectedIds;
    Napi::Value* pyobj_objPoints = NULL;
    Mat objPoints;
    Napi::Value* pyobj_imgPoints = NULL;
    Mat imgPoints;

    const char* keywords[] = { "detectedCorners", "detectedIds", "objPoints", "imgPoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:aruco_Board.matchImagePoints", (char**)keywords, &pyobj_detectedCorners, &pyobj_detectedIds, &pyobj_objPoints, &pyobj_imgPoints) &&
        jsopencv_to_safe(info, pyobj_detectedCorners, detectedCorners, ArgInfo("detectedCorners", 0)) &&
        jsopencv_to_safe(info, pyobj_detectedIds, detectedIds, ArgInfo("detectedIds", 0)) &&
        jsopencv_to_safe(info, pyobj_objPoints, objPoints, ArgInfo("objPoints", 1)) &&
        jsopencv_to_safe(info, pyobj_imgPoints, imgPoints, ArgInfo("imgPoints", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->matchImagePoints(detectedCorners, detectedIds, objPoints, imgPoints));
        return Py_BuildValue("(NN)", jsopencv_from(objPoints), jsopencv_from(imgPoints));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_detectedCorners = NULL;
    vector_UMat detectedCorners;
    Napi::Value* pyobj_detectedIds = NULL;
    UMat detectedIds;
    Napi::Value* pyobj_objPoints = NULL;
    UMat objPoints;
    Napi::Value* pyobj_imgPoints = NULL;
    UMat imgPoints;

    const char* keywords[] = { "detectedCorners", "detectedIds", "objPoints", "imgPoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:aruco_Board.matchImagePoints", (char**)keywords, &pyobj_detectedCorners, &pyobj_detectedIds, &pyobj_objPoints, &pyobj_imgPoints) &&
        jsopencv_to_safe(info, pyobj_detectedCorners, detectedCorners, ArgInfo("detectedCorners", 0)) &&
        jsopencv_to_safe(info, pyobj_detectedIds, detectedIds, ArgInfo("detectedIds", 0)) &&
        jsopencv_to_safe(info, pyobj_objPoints, objPoints, ArgInfo("objPoints", 1)) &&
        jsopencv_to_safe(info, pyobj_imgPoints, imgPoints, ArgInfo("imgPoints", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->matchImagePoints(detectedCorners, detectedIds, objPoints, imgPoints));
        return Py_BuildValue("(NN)", jsopencv_from(objPoints), jsopencv_from(imgPoints));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("matchImagePoints");

    return NULL;
}



// Tables (aruco_Board)

static PyGetSetDef pyopencv_aruco_Board_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_Board_methods[] =
{
    {"generateImage", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_generateImage, 0), "generateImage(outSize[, img[, marginSize[, borderBits]]]) -> img\n.   @brief Draw a planar board\n.        *\n.        * @param outSize size of the output image in pixels.\n.        * @param img output image with the board. The size of this image will be outSize\n.        * and the board will be on the center, keeping the board proportions.\n.        * @param marginSize minimum margins (in pixels) of the board in the output image\n.        * @param borderBits width of the marker borders.\n.        *\n.        * This function return the image of the board, ready to be printed."},
    {"getDictionary", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_getDictionary, 0), "getDictionary() -> retval\n.   @brief return the Dictionary of markers employed for this board"},
    {"getIds", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_getIds, 0), "getIds() -> retval\n.   @brief vector of the identifiers of the markers in the board (should be the same size as objPoints)\n.        * @return vector of the identifiers of the markers"},
    {"getObjPoints", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_getObjPoints, 0), "getObjPoints() -> retval\n.   @brief return array of object points of all the marker corners in the board.\n.        *\n.        * Each marker include its 4 corners in this order:\n.        * -   objPoints[i][0] - left-top point of i-th marker\n.        * -   objPoints[i][1] - right-top point of i-th marker\n.        * -   objPoints[i][2] - right-bottom point of i-th marker\n.        * -   objPoints[i][3] - left-bottom point of i-th marker\n.        *\n.        * Markers are placed in a certain order - row by row, left to right in every row. For M markers, the size is Mx4."},
    {"getRightBottomCorner", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_getRightBottomCorner, 0), "getRightBottomCorner() -> retval\n.   @brief get coordinate of the bottom right corner of the board, is set when calling the function create()"},
    {"matchImagePoints", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_matchImagePoints, 0), "matchImagePoints(detectedCorners, detectedIds[, objPoints[, imgPoints]]) -> objPoints, imgPoints\n.   @brief Given a board configuration and a set of detected markers, returns the corresponding\n.        * image points and object points to call solvePnP()\n.        *\n.        * @param detectedCorners List of detected marker corners of the board.\n.        * For CharucoBoard class you can set list of charuco corners.\n.        * @param detectedIds List of identifiers for each marker or list of charuco identifiers for each corner.\n.        * For CharucoBoard class you can set list of charuco identifiers for each corner.\n.        * @param objPoints Vector of vectors of board marker points in the board coordinate space.\n.        * @param imgPoints Vector of vectors of the projections of board marker corner points."},

    {NULL,          NULL}
};

// Converter (aruco_Board)

template<>
struct PyOpenCV_Converter< cv::aruco::Board >
{
    static PyObject* from(const cv::aruco::Board& r)
    {
        return pyopencv_aruco_Board_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::Board& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::Board * dst_;
        if (pyopencv_aruco_Board_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::Board for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_CharucoBoard (Generic)
//================================================================================

// GetSet (aruco_CharucoBoard)



// Methods (aruco_CharucoBoard)

static int pyopencv_cv_aruco_aruco_CharucoBoard_CharucoBoard(pyopencv_aruco_CharucoBoard_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_squareLength = NULL;
    float squareLength=0.f;
    Napi::Value* pyobj_markerLength = NULL;
    float markerLength=0.f;
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;
    Napi::Value* pyobj_ids = NULL;
    Mat ids;

    const char* keywords[] = { "size", "squareLength", "markerLength", "dictionary", "ids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:CharucoBoard", (char**)keywords, &pyobj_size, &pyobj_squareLength, &pyobj_markerLength, &pyobj_dictionary, &pyobj_ids) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_squareLength, squareLength, ArgInfo("squareLength", 0)) &&
        jsopencv_to_safe(info, pyobj_markerLength, markerLength, ArgInfo("markerLength", 0)) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::CharucoBoard(size, squareLength, markerLength, dictionary, ids));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_squareLength = NULL;
    float squareLength=0.f;
    Napi::Value* pyobj_markerLength = NULL;
    float markerLength=0.f;
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;
    Napi::Value* pyobj_ids = NULL;
    UMat ids;

    const char* keywords[] = { "size", "squareLength", "markerLength", "dictionary", "ids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:CharucoBoard", (char**)keywords, &pyobj_size, &pyobj_squareLength, &pyobj_markerLength, &pyobj_dictionary, &pyobj_ids) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_squareLength, squareLength, ArgInfo("squareLength", 0)) &&
        jsopencv_to_safe(info, pyobj_markerLength, markerLength, ArgInfo("markerLength", 0)) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::CharucoBoard(size, squareLength, markerLength, dictionary, ids));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("CharucoBoard");

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoBoard_checkCharucoCornersCollinear(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::CharucoBoard * self1 = 0;
    if (!pyopencv_aruco_CharucoBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    cv::aruco::CharucoBoard* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_charucoIds = NULL;
    Mat charucoIds;
    bool retval;

    const char* keywords[] = { "charucoIds", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_CharucoBoard.checkCharucoCornersCollinear", (char**)keywords, &pyobj_charucoIds) &&
        jsopencv_to_safe(info, pyobj_charucoIds, charucoIds, ArgInfo("charucoIds", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->checkCharucoCornersCollinear(charucoIds));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_charucoIds = NULL;
    UMat charucoIds;
    bool retval;

    const char* keywords[] = { "charucoIds", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_CharucoBoard.checkCharucoCornersCollinear", (char**)keywords, &pyobj_charucoIds) &&
        jsopencv_to_safe(info, pyobj_charucoIds, charucoIds, ArgInfo("charucoIds", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->checkCharucoCornersCollinear(charucoIds));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("checkCharucoCornersCollinear");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoBoard_getChessboardCorners(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::CharucoBoard * self1 = 0;
    if (!pyopencv_aruco_CharucoBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    cv::aruco::CharucoBoard* _self_ = (self1);
    std::vector<Point3f> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getChessboardCorners());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoBoard_getChessboardSize(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::CharucoBoard * self1 = 0;
    if (!pyopencv_aruco_CharucoBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    cv::aruco::CharucoBoard* _self_ = (self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getChessboardSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoBoard_getMarkerLength(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::CharucoBoard * self1 = 0;
    if (!pyopencv_aruco_CharucoBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    cv::aruco::CharucoBoard* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMarkerLength());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoBoard_getSquareLength(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::CharucoBoard * self1 = 0;
    if (!pyopencv_aruco_CharucoBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    cv::aruco::CharucoBoard* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSquareLength());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (aruco_CharucoBoard)

static PyGetSetDef pyopencv_aruco_CharucoBoard_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_CharucoBoard_methods[] =
{
    {"checkCharucoCornersCollinear", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_checkCharucoCornersCollinear, 0), "checkCharucoCornersCollinear(charucoIds) -> retval\n.   @brief check whether the ChArUco markers are collinear\n.        *\n.        * @param charucoIds list of identifiers for each corner in charucoCorners per frame.\n.        * @return bool value, 1 (true) if detected corners form a line, 0 (false) if they do not.\n.        * solvePnP, calibration functions will fail if the corners are collinear (true).\n.        *\n.        * The number of ids in charucoIDs should be <= the number of chessboard corners in the board.\n.        * This functions checks whether the charuco corners are on a straight line (returns true, if so), or not (false).\n.        * Axis parallel, as well as diagonal and other straight lines detected.  Degenerate cases:\n.        * for number of charucoIDs <= 2,the function returns true."},
    {"getChessboardCorners", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getChessboardCorners, 0), "getChessboardCorners() -> retval\n.   @brief get CharucoBoard::chessboardCorners"},
    {"getChessboardSize", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getChessboardSize, 0), "getChessboardSize() -> retval\n."},
    {"getMarkerLength", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getMarkerLength, 0), "getMarkerLength() -> retval\n."},
    {"getSquareLength", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getSquareLength, 0), "getSquareLength() -> retval\n."},

    {NULL,          NULL}
};

// Converter (aruco_CharucoBoard)

template<>
struct PyOpenCV_Converter< cv::aruco::CharucoBoard >
{
    static PyObject* from(const cv::aruco::CharucoBoard& r)
    {
        return pyopencv_aruco_CharucoBoard_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::CharucoBoard& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::CharucoBoard * dst_;
        if (pyopencv_aruco_CharucoBoard_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::CharucoBoard for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_CharucoDetector (Generic)
//================================================================================

// GetSet (aruco_CharucoDetector)



// Methods (aruco_CharucoDetector)

static int pyopencv_cv_aruco_aruco_CharucoDetector_CharucoDetector(pyopencv_aruco_CharucoDetector_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    Napi::Value* pyobj_board = NULL;
    CharucoBoard board;
    Napi::Value* pyobj_charucoParams = NULL;
    CharucoParameters charucoParams;
    Napi::Value* pyobj_detectorParams = NULL;
    DetectorParameters detectorParams;
    Napi::Value* pyobj_refineParams = NULL;
    RefineParameters refineParams;

    const char* keywords[] = { "board", "charucoParams", "detectorParams", "refineParams", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:CharucoDetector", (char**)keywords, &pyobj_board, &pyobj_charucoParams, &pyobj_detectorParams, &pyobj_refineParams) &&
        jsopencv_to_safe(info, pyobj_board, board, ArgInfo("board", 0)) &&
        jsopencv_to_safe(info, pyobj_charucoParams, charucoParams, ArgInfo("charucoParams", 0)) &&
        jsopencv_to_safe(info, pyobj_detectorParams, detectorParams, ArgInfo("detectorParams", 0)) &&
        jsopencv_to_safe(info, pyobj_refineParams, refineParams, ArgInfo("refineParams", 0)))
    {
        new (&(self->v)) Ptr<cv::aruco::CharucoDetector>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::aruco::CharucoDetector(board, charucoParams, detectorParams, refineParams)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_detectBoard(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_charucoCorners = NULL;
    Mat charucoCorners;
    Napi::Value* pyobj_charucoIds = NULL;
    Mat charucoIds;
    Napi::Value* pyobj_markerCorners = NULL;
    vector_Mat markerCorners;
    Napi::Value* pyobj_markerIds = NULL;
    Mat markerIds;

    const char* keywords[] = { "image", "charucoCorners", "charucoIds", "markerCorners", "markerIds", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:aruco_CharucoDetector.detectBoard", (char**)keywords, &pyobj_image, &pyobj_charucoCorners, &pyobj_charucoIds, &pyobj_markerCorners, &pyobj_markerIds) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_charucoCorners, charucoCorners, ArgInfo("charucoCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_charucoIds, charucoIds, ArgInfo("charucoIds", 1)) &&
        jsopencv_to_safe(info, pyobj_markerCorners, markerCorners, ArgInfo("markerCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_markerIds, markerIds, ArgInfo("markerIds", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectBoard(image, charucoCorners, charucoIds, markerCorners, markerIds));
        return Py_BuildValue("(NNNN)", jsopencv_from(charucoCorners), jsopencv_from(charucoIds), jsopencv_from(markerCorners), jsopencv_from(markerIds));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_charucoCorners = NULL;
    UMat charucoCorners;
    Napi::Value* pyobj_charucoIds = NULL;
    UMat charucoIds;
    Napi::Value* pyobj_markerCorners = NULL;
    vector_UMat markerCorners;
    Napi::Value* pyobj_markerIds = NULL;
    UMat markerIds;

    const char* keywords[] = { "image", "charucoCorners", "charucoIds", "markerCorners", "markerIds", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:aruco_CharucoDetector.detectBoard", (char**)keywords, &pyobj_image, &pyobj_charucoCorners, &pyobj_charucoIds, &pyobj_markerCorners, &pyobj_markerIds) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_charucoCorners, charucoCorners, ArgInfo("charucoCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_charucoIds, charucoIds, ArgInfo("charucoIds", 1)) &&
        jsopencv_to_safe(info, pyobj_markerCorners, markerCorners, ArgInfo("markerCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_markerIds, markerIds, ArgInfo("markerIds", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectBoard(image, charucoCorners, charucoIds, markerCorners, markerIds));
        return Py_BuildValue("(NNNN)", jsopencv_from(charucoCorners), jsopencv_from(charucoIds), jsopencv_from(markerCorners), jsopencv_from(markerIds));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectBoard");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_detectDiamonds(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_diamondCorners = NULL;
    vector_Mat diamondCorners;
    Napi::Value* pyobj_diamondIds = NULL;
    Mat diamondIds;
    Napi::Value* pyobj_markerCorners = NULL;
    vector_Mat markerCorners;
    Napi::Value* pyobj_markerIds = NULL;
    vector_Mat markerIds;

    const char* keywords[] = { "image", "diamondCorners", "diamondIds", "markerCorners", "markerIds", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:aruco_CharucoDetector.detectDiamonds", (char**)keywords, &pyobj_image, &pyobj_diamondCorners, &pyobj_diamondIds, &pyobj_markerCorners, &pyobj_markerIds) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_diamondCorners, diamondCorners, ArgInfo("diamondCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_diamondIds, diamondIds, ArgInfo("diamondIds", 1)) &&
        jsopencv_to_safe(info, pyobj_markerCorners, markerCorners, ArgInfo("markerCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_markerIds, markerIds, ArgInfo("markerIds", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectDiamonds(image, diamondCorners, diamondIds, markerCorners, markerIds));
        return Py_BuildValue("(NNNN)", jsopencv_from(diamondCorners), jsopencv_from(diamondIds), jsopencv_from(markerCorners), jsopencv_from(markerIds));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_diamondCorners = NULL;
    vector_UMat diamondCorners;
    Napi::Value* pyobj_diamondIds = NULL;
    UMat diamondIds;
    Napi::Value* pyobj_markerCorners = NULL;
    vector_UMat markerCorners;
    Napi::Value* pyobj_markerIds = NULL;
    vector_UMat markerIds;

    const char* keywords[] = { "image", "diamondCorners", "diamondIds", "markerCorners", "markerIds", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOO:aruco_CharucoDetector.detectDiamonds", (char**)keywords, &pyobj_image, &pyobj_diamondCorners, &pyobj_diamondIds, &pyobj_markerCorners, &pyobj_markerIds) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_diamondCorners, diamondCorners, ArgInfo("diamondCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_diamondIds, diamondIds, ArgInfo("diamondIds", 1)) &&
        jsopencv_to_safe(info, pyobj_markerCorners, markerCorners, ArgInfo("markerCorners", 1)) &&
        jsopencv_to_safe(info, pyobj_markerIds, markerIds, ArgInfo("markerIds", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectDiamonds(image, diamondCorners, diamondIds, markerCorners, markerIds));
        return Py_BuildValue("(NNNN)", jsopencv_from(diamondCorners), jsopencv_from(diamondIds), jsopencv_from(markerCorners), jsopencv_from(markerIds));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectDiamonds");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_getBoard(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    CharucoBoard retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBoard());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_getCharucoParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    CharucoParameters retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCharucoParameters());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_getDetectorParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    DetectorParameters retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDetectorParameters());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_getRefineParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    RefineParameters retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRefineParameters());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_setBoard(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_board = NULL;
    CharucoBoard board;

    const char* keywords[] = { "board", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_CharucoDetector.setBoard", (char**)keywords, &pyobj_board) &&
        jsopencv_to_safe(info, pyobj_board, board, ArgInfo("board", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBoard(board));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_setCharucoParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_charucoParameters = NULL;
    CharucoParameters charucoParameters;

    const char* keywords[] = { "charucoParameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_CharucoDetector.setCharucoParameters", (char**)keywords, &pyobj_charucoParameters) &&
        jsopencv_to_safe(info, pyobj_charucoParameters, charucoParameters, ArgInfo("charucoParameters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCharucoParameters(charucoParameters));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_setDetectorParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_detectorParameters = NULL;
    DetectorParameters detectorParameters;

    const char* keywords[] = { "detectorParameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_CharucoDetector.setDetectorParameters", (char**)keywords, &pyobj_detectorParameters) &&
        jsopencv_to_safe(info, pyobj_detectorParameters, detectorParameters, ArgInfo("detectorParameters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDetectorParameters(detectorParameters));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_CharucoDetector_setRefineParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    Ptr<cv::aruco::CharucoDetector> * self1 = 0;
    if (!pyopencv_aruco_CharucoDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoDetector' or its derivative)");
    Ptr<cv::aruco::CharucoDetector> _self_ = *(self1);
    Napi::Value* pyobj_refineParameters = NULL;
    RefineParameters refineParameters;

    const char* keywords[] = { "refineParameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_CharucoDetector.setRefineParameters", (char**)keywords, &pyobj_refineParameters) &&
        jsopencv_to_safe(info, pyobj_refineParameters, refineParameters, ArgInfo("refineParameters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRefineParameters(refineParameters));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (aruco_CharucoDetector)

static PyGetSetDef pyopencv_aruco_CharucoDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_CharucoDetector_methods[] =
{
    {"detectBoard", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_detectBoard, 0), "detectBoard(image[, charucoCorners[, charucoIds[, markerCorners[, markerIds]]]]) -> charucoCorners, charucoIds, markerCorners, markerIds\n.   * @brief detect aruco markers and interpolate position of ChArUco board corners\n.        * @param image input image necesary for corner refinement. Note that markers are not detected and\n.        * should be sent in corners and ids parameters.\n.        * @param charucoCorners interpolated chessboard corners.\n.        * @param charucoIds interpolated chessboard corners identifiers.\n.        * @param markerCorners vector of already detected markers corners. For each marker, its four\n.        * corners are provided, (e.g std::vector<std::vector<cv::Point2f> > ). For N detected markers, the\n.        * dimensions of this array should be Nx4. The order of the corners should be clockwise.\n.        * If markerCorners and markerCorners are empty, the function detect aruco markers and ids.\n.        * @param markerIds list of identifiers for each marker in corners.\n.        *  If markerCorners and markerCorners are empty, the function detect aruco markers and ids.\n.        *\n.        * This function receives the detected markers and returns the 2D position of the chessboard corners\n.        * from a ChArUco board using the detected Aruco markers.\n.        *\n.        * If markerCorners and markerCorners are empty, the detectMarkers() will run and detect aruco markers and ids.\n.        *\n.        * If camera parameters are provided, the process is based in an approximated pose estimation, else it is based on local homography.\n.        * Only visible corners are returned. For each corner, its corresponding identifier is also returned in charucoIds.\n.        * @sa findChessboardCorners"},
    {"detectDiamonds", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_detectDiamonds, 0), "detectDiamonds(image[, diamondCorners[, diamondIds[, markerCorners[, markerIds]]]]) -> diamondCorners, diamondIds, markerCorners, markerIds\n.   * @brief Detect ChArUco Diamond markers\n.        *\n.        * @param image input image necessary for corner subpixel.\n.        * @param diamondCorners output list of detected diamond corners (4 corners per diamond). The order\n.        * is the same than in marker corners: top left, top right, bottom right and bottom left. Similar\n.        * format than the corners returned by detectMarkers (e.g std::vector<std::vector<cv::Point2f> > ).\n.        * @param diamondIds ids of the diamonds in diamondCorners. The id of each diamond is in fact of\n.        * type Vec4i, so each diamond has 4 ids, which are the ids of the aruco markers composing the\n.        * diamond.\n.        * @param markerCorners list of detected marker corners from detectMarkers function.\n.        * If markerCorners and markerCorners are empty, the function detect aruco markers and ids.\n.        * @param markerIds list of marker ids in markerCorners.\n.        * If markerCorners and markerCorners are empty, the function detect aruco markers and ids.\n.        *\n.        * This function detects Diamond markers from the previous detected ArUco markers. The diamonds\n.        * are returned in the diamondCorners and diamondIds parameters. If camera calibration parameters\n.        * are provided, the diamond search is based on reprojection. If not, diamond search is based on\n.        * homography. Homography is faster than reprojection, but less accurate."},
    {"getBoard", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_getBoard, 0), "getBoard() -> retval\n."},
    {"getCharucoParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_getCharucoParameters, 0), "getCharucoParameters() -> retval\n."},
    {"getDetectorParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_getDetectorParameters, 0), "getDetectorParameters() -> retval\n."},
    {"getRefineParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_getRefineParameters, 0), "getRefineParameters() -> retval\n."},
    {"setBoard", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_setBoard, 0), "setBoard(board) -> None\n."},
    {"setCharucoParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_setCharucoParameters, 0), "setCharucoParameters(charucoParameters) -> None\n."},
    {"setDetectorParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_setDetectorParameters, 0), "setDetectorParameters(detectorParameters) -> None\n."},
    {"setRefineParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoDetector_setRefineParameters, 0), "setRefineParameters(refineParameters) -> None\n."},

    {NULL,          NULL}
};

// Converter (aruco_CharucoDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::aruco::CharucoDetector> >
{
    static PyObject* from(const Ptr<cv::aruco::CharucoDetector>& r)
    {
        return pyopencv_aruco_CharucoDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::aruco::CharucoDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::aruco::CharucoDetector> * dst_;
        if (pyopencv_aruco_CharucoDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::aruco::CharucoDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_CharucoParameters (Generic)
//================================================================================

// GetSet (aruco_CharucoParameters)


static PyObject* pyopencv_aruco_CharucoParameters_get_cameraMatrix(pyopencv_aruco_CharucoParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.cameraMatrix);
}

static int pyopencv_aruco_CharucoParameters_set_cameraMatrix(pyopencv_aruco_CharucoParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cameraMatrix attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cameraMatrix, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_CharucoParameters_get_distCoeffs(pyopencv_aruco_CharucoParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.distCoeffs);
}

static int pyopencv_aruco_CharucoParameters_set_distCoeffs(pyopencv_aruco_CharucoParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the distCoeffs attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.distCoeffs, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_CharucoParameters_get_minMarkers(pyopencv_aruco_CharucoParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minMarkers);
}

static int pyopencv_aruco_CharucoParameters_set_minMarkers(pyopencv_aruco_CharucoParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minMarkers attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minMarkers, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_CharucoParameters_get_tryRefineMarkers(pyopencv_aruco_CharucoParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.tryRefineMarkers);
}

static int pyopencv_aruco_CharucoParameters_set_tryRefineMarkers(pyopencv_aruco_CharucoParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the tryRefineMarkers attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.tryRefineMarkers, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (aruco_CharucoParameters)



// Tables (aruco_CharucoParameters)

static PyGetSetDef pyopencv_aruco_CharucoParameters_getseters[] =
{
    {(char*)"cameraMatrix", (getter)pyopencv_aruco_CharucoParameters_get_cameraMatrix, (setter)pyopencv_aruco_CharucoParameters_set_cameraMatrix, (char*)"cameraMatrix", NULL},
    {(char*)"distCoeffs", (getter)pyopencv_aruco_CharucoParameters_get_distCoeffs, (setter)pyopencv_aruco_CharucoParameters_set_distCoeffs, (char*)"distCoeffs", NULL},
    {(char*)"minMarkers", (getter)pyopencv_aruco_CharucoParameters_get_minMarkers, (setter)pyopencv_aruco_CharucoParameters_set_minMarkers, (char*)"minMarkers", NULL},
    {(char*)"tryRefineMarkers", (getter)pyopencv_aruco_CharucoParameters_get_tryRefineMarkers, (setter)pyopencv_aruco_CharucoParameters_set_tryRefineMarkers, (char*)"tryRefineMarkers", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_CharucoParameters_methods[] =
{

    {NULL,          NULL}
};

// Converter (aruco_CharucoParameters)

template<>
struct PyOpenCV_Converter< cv::aruco::CharucoParameters >
{
    static PyObject* from(const cv::aruco::CharucoParameters& r)
    {
        return pyopencv_aruco_CharucoParameters_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::CharucoParameters& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::CharucoParameters * dst_;
        if (pyopencv_aruco_CharucoParameters_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::CharucoParameters for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_DetectorParameters (Generic)
//================================================================================

// GetSet (aruco_DetectorParameters)


static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshConstant(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.adaptiveThreshConstant);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshConstant(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshConstant attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.adaptiveThreshConstant, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMax(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.adaptiveThreshWinSizeMax);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMax(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeMax attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.adaptiveThreshWinSizeMax, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMin(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.adaptiveThreshWinSizeMin);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMin(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeMin attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.adaptiveThreshWinSizeMin, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeStep(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.adaptiveThreshWinSizeStep);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeStep(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeStep attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.adaptiveThreshWinSizeStep, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagCriticalRad(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagCriticalRad);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagCriticalRad(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagCriticalRad attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagCriticalRad, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagDeglitch(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagDeglitch);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagDeglitch(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagDeglitch attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagDeglitch, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMaxLineFitMse(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagMaxLineFitMse);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMaxLineFitMse(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMaxLineFitMse attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagMaxLineFitMse, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMaxNmaxima(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagMaxNmaxima);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMaxNmaxima(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMaxNmaxima attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagMaxNmaxima, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMinClusterPixels(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagMinClusterPixels);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMinClusterPixels(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMinClusterPixels attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagMinClusterPixels, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMinWhiteBlackDiff(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagMinWhiteBlackDiff);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMinWhiteBlackDiff(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMinWhiteBlackDiff attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagMinWhiteBlackDiff, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagQuadDecimate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagQuadDecimate);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagQuadDecimate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagQuadDecimate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagQuadDecimate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagQuadSigma(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.aprilTagQuadSigma);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagQuadSigma(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagQuadSigma attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.aprilTagQuadSigma, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementMaxIterations(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.cornerRefinementMaxIterations);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementMaxIterations(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementMaxIterations attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cornerRefinementMaxIterations, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementMethod(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.cornerRefinementMethod);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementMethod(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementMethod attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cornerRefinementMethod, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementMinAccuracy(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.cornerRefinementMinAccuracy);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementMinAccuracy(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementMinAccuracy attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cornerRefinementMinAccuracy, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementWinSize(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.cornerRefinementWinSize);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementWinSize(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementWinSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cornerRefinementWinSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_detectInvertedMarker(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.detectInvertedMarker);
}

static int pyopencv_aruco_DetectorParameters_set_detectInvertedMarker(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the detectInvertedMarker attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.detectInvertedMarker, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_errorCorrectionRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.errorCorrectionRate);
}

static int pyopencv_aruco_DetectorParameters_set_errorCorrectionRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCorrectionRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.errorCorrectionRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_markerBorderBits(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.markerBorderBits);
}

static int pyopencv_aruco_DetectorParameters_set_markerBorderBits(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the markerBorderBits attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.markerBorderBits, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_maxErroneousBitsInBorderRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.maxErroneousBitsInBorderRate);
}

static int pyopencv_aruco_DetectorParameters_set_maxErroneousBitsInBorderRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxErroneousBitsInBorderRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxErroneousBitsInBorderRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_maxMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.maxMarkerPerimeterRate);
}

static int pyopencv_aruco_DetectorParameters_set_maxMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxMarkerPerimeterRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxMarkerPerimeterRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minCornerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minCornerDistanceRate);
}

static int pyopencv_aruco_DetectorParameters_set_minCornerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minCornerDistanceRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minCornerDistanceRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minDistanceToBorder(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minDistanceToBorder);
}

static int pyopencv_aruco_DetectorParameters_set_minDistanceToBorder(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDistanceToBorder attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minDistanceToBorder, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minMarkerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minMarkerDistanceRate);
}

static int pyopencv_aruco_DetectorParameters_set_minMarkerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minMarkerDistanceRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minMarkerDistanceRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minMarkerLengthRatioOriginalImg(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minMarkerLengthRatioOriginalImg);
}

static int pyopencv_aruco_DetectorParameters_set_minMarkerLengthRatioOriginalImg(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minMarkerLengthRatioOriginalImg attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minMarkerLengthRatioOriginalImg, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minMarkerPerimeterRate);
}

static int pyopencv_aruco_DetectorParameters_set_minMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minMarkerPerimeterRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minMarkerPerimeterRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minOtsuStdDev(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minOtsuStdDev);
}

static int pyopencv_aruco_DetectorParameters_set_minOtsuStdDev(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minOtsuStdDev attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minOtsuStdDev, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minSideLengthCanonicalImg(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minSideLengthCanonicalImg);
}

static int pyopencv_aruco_DetectorParameters_set_minSideLengthCanonicalImg(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minSideLengthCanonicalImg attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minSideLengthCanonicalImg, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_perspectiveRemoveIgnoredMarginPerCell(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.perspectiveRemoveIgnoredMarginPerCell);
}

static int pyopencv_aruco_DetectorParameters_set_perspectiveRemoveIgnoredMarginPerCell(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the perspectiveRemoveIgnoredMarginPerCell attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.perspectiveRemoveIgnoredMarginPerCell, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_perspectiveRemovePixelPerCell(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.perspectiveRemovePixelPerCell);
}

static int pyopencv_aruco_DetectorParameters_set_perspectiveRemovePixelPerCell(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the perspectiveRemovePixelPerCell attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.perspectiveRemovePixelPerCell, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_polygonalApproxAccuracyRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.polygonalApproxAccuracyRate);
}

static int pyopencv_aruco_DetectorParameters_set_polygonalApproxAccuracyRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the polygonalApproxAccuracyRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.polygonalApproxAccuracyRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_useAruco3Detection(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.useAruco3Detection);
}

static int pyopencv_aruco_DetectorParameters_set_useAruco3Detection(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the useAruco3Detection attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.useAruco3Detection, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (aruco_DetectorParameters)

static int pyopencv_cv_aruco_aruco_DetectorParameters_DetectorParameters(pyopencv_aruco_DetectorParameters_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::DetectorParameters());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_DetectorParameters_readDetectorParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::DetectorParameters * self1 = 0;
    if (!pyopencv_aruco_DetectorParameters_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_DetectorParameters' or its derivative)");
    cv::aruco::DetectorParameters* _self_ = (self1);
    Napi::Value* pyobj_fn = NULL;
    cv::FileNode fn;
    bool retval;

    const char* keywords[] = { "fn", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_DetectorParameters.readDetectorParameters", (char**)keywords, &pyobj_fn) &&
        jsopencv_to_safe(info, pyobj_fn, fn, ArgInfo("fn", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->readDetectorParameters(fn));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_DetectorParameters_writeDetectorParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::DetectorParameters * self1 = 0;
    if (!pyopencv_aruco_DetectorParameters_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_DetectorParameters' or its derivative)");
    cv::aruco::DetectorParameters* _self_ = (self1);
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;
    bool retval;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:aruco_DetectorParameters.writeDetectorParameters", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->writeDetectorParameters(*fs, name));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (aruco_DetectorParameters)

static PyGetSetDef pyopencv_aruco_DetectorParameters_getseters[] =
{
    {(char*)"adaptiveThreshConstant", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshConstant, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshConstant, (char*)"adaptiveThreshConstant", NULL},
    {(char*)"adaptiveThreshWinSizeMax", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMax, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMax, (char*)"adaptiveThreshWinSizeMax", NULL},
    {(char*)"adaptiveThreshWinSizeMin", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMin, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMin, (char*)"adaptiveThreshWinSizeMin", NULL},
    {(char*)"adaptiveThreshWinSizeStep", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeStep, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeStep, (char*)"adaptiveThreshWinSizeStep", NULL},
    {(char*)"aprilTagCriticalRad", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagCriticalRad, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagCriticalRad, (char*)"aprilTagCriticalRad", NULL},
    {(char*)"aprilTagDeglitch", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagDeglitch, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagDeglitch, (char*)"aprilTagDeglitch", NULL},
    {(char*)"aprilTagMaxLineFitMse", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMaxLineFitMse, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMaxLineFitMse, (char*)"aprilTagMaxLineFitMse", NULL},
    {(char*)"aprilTagMaxNmaxima", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMaxNmaxima, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMaxNmaxima, (char*)"aprilTagMaxNmaxima", NULL},
    {(char*)"aprilTagMinClusterPixels", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMinClusterPixels, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMinClusterPixels, (char*)"aprilTagMinClusterPixels", NULL},
    {(char*)"aprilTagMinWhiteBlackDiff", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMinWhiteBlackDiff, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMinWhiteBlackDiff, (char*)"aprilTagMinWhiteBlackDiff", NULL},
    {(char*)"aprilTagQuadDecimate", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagQuadDecimate, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagQuadDecimate, (char*)"aprilTagQuadDecimate", NULL},
    {(char*)"aprilTagQuadSigma", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagQuadSigma, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagQuadSigma, (char*)"aprilTagQuadSigma", NULL},
    {(char*)"cornerRefinementMaxIterations", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementMaxIterations, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementMaxIterations, (char*)"cornerRefinementMaxIterations", NULL},
    {(char*)"cornerRefinementMethod", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementMethod, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementMethod, (char*)"cornerRefinementMethod", NULL},
    {(char*)"cornerRefinementMinAccuracy", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementMinAccuracy, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementMinAccuracy, (char*)"cornerRefinementMinAccuracy", NULL},
    {(char*)"cornerRefinementWinSize", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementWinSize, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementWinSize, (char*)"cornerRefinementWinSize", NULL},
    {(char*)"detectInvertedMarker", (getter)pyopencv_aruco_DetectorParameters_get_detectInvertedMarker, (setter)pyopencv_aruco_DetectorParameters_set_detectInvertedMarker, (char*)"detectInvertedMarker", NULL},
    {(char*)"errorCorrectionRate", (getter)pyopencv_aruco_DetectorParameters_get_errorCorrectionRate, (setter)pyopencv_aruco_DetectorParameters_set_errorCorrectionRate, (char*)"errorCorrectionRate", NULL},
    {(char*)"markerBorderBits", (getter)pyopencv_aruco_DetectorParameters_get_markerBorderBits, (setter)pyopencv_aruco_DetectorParameters_set_markerBorderBits, (char*)"markerBorderBits", NULL},
    {(char*)"maxErroneousBitsInBorderRate", (getter)pyopencv_aruco_DetectorParameters_get_maxErroneousBitsInBorderRate, (setter)pyopencv_aruco_DetectorParameters_set_maxErroneousBitsInBorderRate, (char*)"maxErroneousBitsInBorderRate", NULL},
    {(char*)"maxMarkerPerimeterRate", (getter)pyopencv_aruco_DetectorParameters_get_maxMarkerPerimeterRate, (setter)pyopencv_aruco_DetectorParameters_set_maxMarkerPerimeterRate, (char*)"maxMarkerPerimeterRate", NULL},
    {(char*)"minCornerDistanceRate", (getter)pyopencv_aruco_DetectorParameters_get_minCornerDistanceRate, (setter)pyopencv_aruco_DetectorParameters_set_minCornerDistanceRate, (char*)"minCornerDistanceRate", NULL},
    {(char*)"minDistanceToBorder", (getter)pyopencv_aruco_DetectorParameters_get_minDistanceToBorder, (setter)pyopencv_aruco_DetectorParameters_set_minDistanceToBorder, (char*)"minDistanceToBorder", NULL},
    {(char*)"minMarkerDistanceRate", (getter)pyopencv_aruco_DetectorParameters_get_minMarkerDistanceRate, (setter)pyopencv_aruco_DetectorParameters_set_minMarkerDistanceRate, (char*)"minMarkerDistanceRate", NULL},
    {(char*)"minMarkerLengthRatioOriginalImg", (getter)pyopencv_aruco_DetectorParameters_get_minMarkerLengthRatioOriginalImg, (setter)pyopencv_aruco_DetectorParameters_set_minMarkerLengthRatioOriginalImg, (char*)"minMarkerLengthRatioOriginalImg", NULL},
    {(char*)"minMarkerPerimeterRate", (getter)pyopencv_aruco_DetectorParameters_get_minMarkerPerimeterRate, (setter)pyopencv_aruco_DetectorParameters_set_minMarkerPerimeterRate, (char*)"minMarkerPerimeterRate", NULL},
    {(char*)"minOtsuStdDev", (getter)pyopencv_aruco_DetectorParameters_get_minOtsuStdDev, (setter)pyopencv_aruco_DetectorParameters_set_minOtsuStdDev, (char*)"minOtsuStdDev", NULL},
    {(char*)"minSideLengthCanonicalImg", (getter)pyopencv_aruco_DetectorParameters_get_minSideLengthCanonicalImg, (setter)pyopencv_aruco_DetectorParameters_set_minSideLengthCanonicalImg, (char*)"minSideLengthCanonicalImg", NULL},
    {(char*)"perspectiveRemoveIgnoredMarginPerCell", (getter)pyopencv_aruco_DetectorParameters_get_perspectiveRemoveIgnoredMarginPerCell, (setter)pyopencv_aruco_DetectorParameters_set_perspectiveRemoveIgnoredMarginPerCell, (char*)"perspectiveRemoveIgnoredMarginPerCell", NULL},
    {(char*)"perspectiveRemovePixelPerCell", (getter)pyopencv_aruco_DetectorParameters_get_perspectiveRemovePixelPerCell, (setter)pyopencv_aruco_DetectorParameters_set_perspectiveRemovePixelPerCell, (char*)"perspectiveRemovePixelPerCell", NULL},
    {(char*)"polygonalApproxAccuracyRate", (getter)pyopencv_aruco_DetectorParameters_get_polygonalApproxAccuracyRate, (setter)pyopencv_aruco_DetectorParameters_set_polygonalApproxAccuracyRate, (char*)"polygonalApproxAccuracyRate", NULL},
    {(char*)"useAruco3Detection", (getter)pyopencv_aruco_DetectorParameters_get_useAruco3Detection, (setter)pyopencv_aruco_DetectorParameters_set_useAruco3Detection, (char*)"useAruco3Detection", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_DetectorParameters_methods[] =
{
    {"readDetectorParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_DetectorParameters_readDetectorParameters, 0), "readDetectorParameters(fn) -> retval\n.   @brief Read a new set of DetectorParameters from FileNode (use FileStorage.root())."},
    {"writeDetectorParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_DetectorParameters_writeDetectorParameters, 0), "writeDetectorParameters(fs[, name]) -> retval\n.   @brief Write a set of DetectorParameters to FileStorage"},

    {NULL,          NULL}
};

// Converter (aruco_DetectorParameters)

template<>
struct PyOpenCV_Converter< cv::aruco::DetectorParameters >
{
    static PyObject* from(const cv::aruco::DetectorParameters& r)
    {
        return pyopencv_aruco_DetectorParameters_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::DetectorParameters& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::DetectorParameters * dst_;
        if (pyopencv_aruco_DetectorParameters_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::DetectorParameters for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_Dictionary (Generic)
//================================================================================

// GetSet (aruco_Dictionary)


static PyObject* pyopencv_aruco_Dictionary_get_bytesList(pyopencv_aruco_Dictionary_t* p, void *closure)
{
    return jsopencv_from(p->v.bytesList);
}

static int pyopencv_aruco_Dictionary_set_bytesList(pyopencv_aruco_Dictionary_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the bytesList attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.bytesList, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_Dictionary_get_markerSize(pyopencv_aruco_Dictionary_t* p, void *closure)
{
    return jsopencv_from(p->v.markerSize);
}

static int pyopencv_aruco_Dictionary_set_markerSize(pyopencv_aruco_Dictionary_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the markerSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.markerSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_Dictionary_get_maxCorrectionBits(pyopencv_aruco_Dictionary_t* p, void *closure)
{
    return jsopencv_from(p->v.maxCorrectionBits);
}

static int pyopencv_aruco_Dictionary_set_maxCorrectionBits(pyopencv_aruco_Dictionary_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxCorrectionBits attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.maxCorrectionBits, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (aruco_Dictionary)

static int pyopencv_cv_aruco_aruco_Dictionary_Dictionary(pyopencv_aruco_Dictionary_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::Dictionary());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_bytesList = NULL;
    Mat bytesList;
    Napi::Value* pyobj__markerSize = NULL;
    int _markerSize=0;
    Napi::Value* pyobj_maxcorr = NULL;
    int maxcorr=0;

    const char* keywords[] = { "bytesList", "_markerSize", "maxcorr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Dictionary", (char**)keywords, &pyobj_bytesList, &pyobj__markerSize, &pyobj_maxcorr) &&
        jsopencv_to_safe(info, pyobj_bytesList, bytesList, ArgInfo("bytesList", 0)) &&
        jsopencv_to_safe(info, pyobj__markerSize, _markerSize, ArgInfo("_markerSize", 0)) &&
        jsopencv_to_safe(info, pyobj_maxcorr, maxcorr, ArgInfo("maxcorr", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::Dictionary(bytesList, _markerSize, maxcorr));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Dictionary");

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_generateImageMarker(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Dictionary * self1 = 0;
    if (!pyopencv_aruco_Dictionary_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Dictionary' or its derivative)");
    cv::aruco::Dictionary* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_id = NULL;
    int id=0;
    Napi::Value* pyobj_sidePixels = NULL;
    int sidePixels=0;
    Napi::Value* pyobj__img = NULL;
    Mat _img;
    Napi::Value* pyobj_borderBits = NULL;
    int borderBits=1;

    const char* keywords[] = { "id", "sidePixels", "_img", "borderBits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:aruco_Dictionary.generateImageMarker", (char**)keywords, &pyobj_id, &pyobj_sidePixels, &pyobj__img, &pyobj_borderBits) &&
        jsopencv_to_safe(info, pyobj_id, id, ArgInfo("id", 0)) &&
        jsopencv_to_safe(info, pyobj_sidePixels, sidePixels, ArgInfo("sidePixels", 0)) &&
        jsopencv_to_safe(info, pyobj__img, _img, ArgInfo("_img", 1)) &&
        jsopencv_to_safe(info, pyobj_borderBits, borderBits, ArgInfo("borderBits", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->generateImageMarker(id, sidePixels, _img, borderBits));
        return jsopencv_from(_img);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_id = NULL;
    int id=0;
    Napi::Value* pyobj_sidePixels = NULL;
    int sidePixels=0;
    Napi::Value* pyobj__img = NULL;
    UMat _img;
    Napi::Value* pyobj_borderBits = NULL;
    int borderBits=1;

    const char* keywords[] = { "id", "sidePixels", "_img", "borderBits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:aruco_Dictionary.generateImageMarker", (char**)keywords, &pyobj_id, &pyobj_sidePixels, &pyobj__img, &pyobj_borderBits) &&
        jsopencv_to_safe(info, pyobj_id, id, ArgInfo("id", 0)) &&
        jsopencv_to_safe(info, pyobj_sidePixels, sidePixels, ArgInfo("sidePixels", 0)) &&
        jsopencv_to_safe(info, pyobj__img, _img, ArgInfo("_img", 1)) &&
        jsopencv_to_safe(info, pyobj_borderBits, borderBits, ArgInfo("borderBits", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->generateImageMarker(id, sidePixels, _img, borderBits));
        return jsopencv_from(_img);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("generateImageMarker");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_getBitsFromByteList_static(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;

    Napi::Value* pyobj_byteList = NULL;
    Mat byteList;
    Napi::Value* pyobj_markerSize = NULL;
    int markerSize=0;
    Mat retval;

    const char* keywords[] = { "byteList", "markerSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:aruco_Dictionary.getBitsFromByteList", (char**)keywords, &pyobj_byteList, &pyobj_markerSize) &&
        jsopencv_to_safe(info, pyobj_byteList, byteList, ArgInfo("byteList", 0)) &&
        jsopencv_to_safe(info, pyobj_markerSize, markerSize, ArgInfo("markerSize", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::aruco::Dictionary::getBitsFromByteList(byteList, markerSize));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_getByteListFromBits_static(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;

    Napi::Value* pyobj_bits = NULL;
    Mat bits;
    Mat retval;

    const char* keywords[] = { "bits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_Dictionary.getByteListFromBits", (char**)keywords, &pyobj_bits) &&
        jsopencv_to_safe(info, pyobj_bits, bits, ArgInfo("bits", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::aruco::Dictionary::getByteListFromBits(bits));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_getDistanceToId(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Dictionary * self1 = 0;
    if (!pyopencv_aruco_Dictionary_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Dictionary' or its derivative)");
    cv::aruco::Dictionary* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_bits = NULL;
    Mat bits;
    Napi::Value* pyobj_id = NULL;
    int id=0;
    Napi::Value* pyobj_allRotations = NULL;
    bool allRotations=true;
    int retval;

    const char* keywords[] = { "bits", "id", "allRotations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:aruco_Dictionary.getDistanceToId", (char**)keywords, &pyobj_bits, &pyobj_id, &pyobj_allRotations) &&
        jsopencv_to_safe(info, pyobj_bits, bits, ArgInfo("bits", 0)) &&
        jsopencv_to_safe(info, pyobj_id, id, ArgInfo("id", 0)) &&
        jsopencv_to_safe(info, pyobj_allRotations, allRotations, ArgInfo("allRotations", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDistanceToId(bits, id, allRotations));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_bits = NULL;
    UMat bits;
    Napi::Value* pyobj_id = NULL;
    int id=0;
    Napi::Value* pyobj_allRotations = NULL;
    bool allRotations=true;
    int retval;

    const char* keywords[] = { "bits", "id", "allRotations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:aruco_Dictionary.getDistanceToId", (char**)keywords, &pyobj_bits, &pyobj_id, &pyobj_allRotations) &&
        jsopencv_to_safe(info, pyobj_bits, bits, ArgInfo("bits", 0)) &&
        jsopencv_to_safe(info, pyobj_id, id, ArgInfo("id", 0)) &&
        jsopencv_to_safe(info, pyobj_allRotations, allRotations, ArgInfo("allRotations", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDistanceToId(bits, id, allRotations));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getDistanceToId");

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_identify(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Dictionary * self1 = 0;
    if (!pyopencv_aruco_Dictionary_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Dictionary' or its derivative)");
    cv::aruco::Dictionary* _self_ = (self1);
    Napi::Value* pyobj_onlyBits = NULL;
    Mat onlyBits;
    int idx;
    int rotation;
    Napi::Value* pyobj_maxCorrectionRate = NULL;
    double maxCorrectionRate=0;
    bool retval;

    const char* keywords[] = { "onlyBits", "maxCorrectionRate", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:aruco_Dictionary.identify", (char**)keywords, &pyobj_onlyBits, &pyobj_maxCorrectionRate) &&
        jsopencv_to_safe(info, pyobj_onlyBits, onlyBits, ArgInfo("onlyBits", 0)) &&
        jsopencv_to_safe(info, pyobj_maxCorrectionRate, maxCorrectionRate, ArgInfo("maxCorrectionRate", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->identify(onlyBits, idx, rotation, maxCorrectionRate));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(idx), jsopencv_from(rotation));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_readDictionary(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Dictionary * self1 = 0;
    if (!pyopencv_aruco_Dictionary_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Dictionary' or its derivative)");
    cv::aruco::Dictionary* _self_ = (self1);
    Napi::Value* pyobj_fn = NULL;
    cv::FileNode fn;
    bool retval;

    const char* keywords[] = { "fn", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_Dictionary.readDictionary", (char**)keywords, &pyobj_fn) &&
        jsopencv_to_safe(info, pyobj_fn, fn, ArgInfo("fn", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->readDictionary(fn));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_Dictionary_writeDictionary(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::Dictionary * self1 = 0;
    if (!pyopencv_aruco_Dictionary_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_Dictionary' or its derivative)");
    cv::aruco::Dictionary* _self_ = (self1);
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:aruco_Dictionary.writeDictionary", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->writeDictionary(*fs, name));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (aruco_Dictionary)

static PyGetSetDef pyopencv_aruco_Dictionary_getseters[] =
{
    {(char*)"bytesList", (getter)pyopencv_aruco_Dictionary_get_bytesList, (setter)pyopencv_aruco_Dictionary_set_bytesList, (char*)"bytesList", NULL},
    {(char*)"markerSize", (getter)pyopencv_aruco_Dictionary_get_markerSize, (setter)pyopencv_aruco_Dictionary_set_markerSize, (char*)"markerSize", NULL},
    {(char*)"maxCorrectionBits", (getter)pyopencv_aruco_Dictionary_get_maxCorrectionBits, (setter)pyopencv_aruco_Dictionary_set_maxCorrectionBits, (char*)"maxCorrectionBits", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_Dictionary_methods[] =
{
    {"generateImageMarker", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_generateImageMarker, 0), "generateImageMarker(id, sidePixels[, _img[, borderBits]]) -> _img\n.   @brief Generate a canonical marker image"},
    {"getBitsFromByteList", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_getBitsFromByteList_static, METH_STATIC), "getBitsFromByteList(byteList, markerSize) -> retval\n.   @brief Transform list of bytes to matrix of bits"},
    {"getByteListFromBits", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_getByteListFromBits_static, METH_STATIC), "getByteListFromBits(bits) -> retval\n.   @brief Transform matrix of bits to list of bytes in the 4 rotations"},
    {"getDistanceToId", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_getDistanceToId, 0), "getDistanceToId(bits, id[, allRotations]) -> retval\n.   @brief Returns the distance of the input bits to the specific id.\n.        *\n.        * If allRotations is true, the four posible bits rotation are considered"},
    {"identify", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_identify, 0), "identify(onlyBits, maxCorrectionRate) -> retval, idx, rotation\n.   @brief Given a matrix of bits. Returns whether if marker is identified or not.\n.        *\n.        * It returns by reference the correct id (if any) and the correct rotation"},
    {"readDictionary", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_readDictionary, 0), "readDictionary(fn) -> retval\n.   @brief Read a new dictionary from FileNode.\n.        *\n.        * Dictionary format:\\n\n.        * nmarkers: 35\\n\n.        * markersize: 6\\n\n.        * maxCorrectionBits: 5\\n\n.        * marker_0: \"101011111011111001001001101100000000\"\\n\n.        * ...\\n\n.        * marker_34: \"011111010000111011111110110101100101\""},
    {"writeDictionary", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_writeDictionary, 0), "writeDictionary(fs[, name]) -> None\n.   @brief Write a dictionary to FileStorage, format is the same as in readDictionary()."},

    {NULL,          NULL}
};

// Converter (aruco_Dictionary)

template<>
struct PyOpenCV_Converter< cv::aruco::Dictionary >
{
    static PyObject* from(const cv::aruco::Dictionary& r)
    {
        return pyopencv_aruco_Dictionary_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::Dictionary& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::Dictionary * dst_;
        if (pyopencv_aruco_Dictionary_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::Dictionary for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_GridBoard (Generic)
//================================================================================

// GetSet (aruco_GridBoard)



// Methods (aruco_GridBoard)

static int pyopencv_cv_aruco_aruco_GridBoard_GridBoard(pyopencv_aruco_GridBoard_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_markerLength = NULL;
    float markerLength=0.f;
    Napi::Value* pyobj_markerSeparation = NULL;
    float markerSeparation=0.f;
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;
    Napi::Value* pyobj_ids = NULL;
    Mat ids;

    const char* keywords[] = { "size", "markerLength", "markerSeparation", "dictionary", "ids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:GridBoard", (char**)keywords, &pyobj_size, &pyobj_markerLength, &pyobj_markerSeparation, &pyobj_dictionary, &pyobj_ids) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_markerLength, markerLength, ArgInfo("markerLength", 0)) &&
        jsopencv_to_safe(info, pyobj_markerSeparation, markerSeparation, ArgInfo("markerSeparation", 0)) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::GridBoard(size, markerLength, markerSeparation, dictionary, ids));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_markerLength = NULL;
    float markerLength=0.f;
    Napi::Value* pyobj_markerSeparation = NULL;
    float markerSeparation=0.f;
    Napi::Value* pyobj_dictionary = NULL;
    Dictionary dictionary;
    Napi::Value* pyobj_ids = NULL;
    UMat ids;

    const char* keywords[] = { "size", "markerLength", "markerSeparation", "dictionary", "ids", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:GridBoard", (char**)keywords, &pyobj_size, &pyobj_markerLength, &pyobj_markerSeparation, &pyobj_dictionary, &pyobj_ids) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_markerLength, markerLength, ArgInfo("markerLength", 0)) &&
        jsopencv_to_safe(info, pyobj_markerSeparation, markerSeparation, ArgInfo("markerSeparation", 0)) &&
        jsopencv_to_safe(info, pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        jsopencv_to_safe(info, pyobj_ids, ids, ArgInfo("ids", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::GridBoard(size, markerLength, markerSeparation, dictionary, ids));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GridBoard");

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_GridBoard_getGridSize(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::GridBoard * self1 = 0;
    if (!pyopencv_aruco_GridBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    cv::aruco::GridBoard* _self_ = (self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGridSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_GridBoard_getMarkerLength(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::GridBoard * self1 = 0;
    if (!pyopencv_aruco_GridBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    cv::aruco::GridBoard* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMarkerLength());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_GridBoard_getMarkerSeparation(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::GridBoard * self1 = 0;
    if (!pyopencv_aruco_GridBoard_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    cv::aruco::GridBoard* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMarkerSeparation());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (aruco_GridBoard)

static PyGetSetDef pyopencv_aruco_GridBoard_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_GridBoard_methods[] =
{
    {"getGridSize", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_getGridSize, 0), "getGridSize() -> retval\n."},
    {"getMarkerLength", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_getMarkerLength, 0), "getMarkerLength() -> retval\n."},
    {"getMarkerSeparation", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_getMarkerSeparation, 0), "getMarkerSeparation() -> retval\n."},

    {NULL,          NULL}
};

// Converter (aruco_GridBoard)

template<>
struct PyOpenCV_Converter< cv::aruco::GridBoard >
{
    static PyObject* from(const cv::aruco::GridBoard& r)
    {
        return pyopencv_aruco_GridBoard_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::GridBoard& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::GridBoard * dst_;
        if (pyopencv_aruco_GridBoard_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::GridBoard for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// aruco_RefineParameters (Generic)
//================================================================================

// GetSet (aruco_RefineParameters)


static PyObject* pyopencv_aruco_RefineParameters_get_checkAllOrders(pyopencv_aruco_RefineParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.checkAllOrders);
}

static int pyopencv_aruco_RefineParameters_set_checkAllOrders(pyopencv_aruco_RefineParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the checkAllOrders attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.checkAllOrders, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_RefineParameters_get_errorCorrectionRate(pyopencv_aruco_RefineParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.errorCorrectionRate);
}

static int pyopencv_aruco_RefineParameters_set_errorCorrectionRate(pyopencv_aruco_RefineParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCorrectionRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.errorCorrectionRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_aruco_RefineParameters_get_minRepDistance(pyopencv_aruco_RefineParameters_t* p, void *closure)
{
    return jsopencv_from(p->v.minRepDistance);
}

static int pyopencv_aruco_RefineParameters_set_minRepDistance(pyopencv_aruco_RefineParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minRepDistance attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.minRepDistance, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (aruco_RefineParameters)

static int pyopencv_cv_aruco_aruco_RefineParameters_RefineParameters(pyopencv_aruco_RefineParameters_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::aruco;

    Napi::Value* pyobj_minRepDistance = NULL;
    float minRepDistance=10.f;
    Napi::Value* pyobj_errorCorrectionRate = NULL;
    float errorCorrectionRate=3.f;
    Napi::Value* pyobj_checkAllOrders = NULL;
    bool checkAllOrders=true;

    const char* keywords[] = { "minRepDistance", "errorCorrectionRate", "checkAllOrders", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:RefineParameters", (char**)keywords, &pyobj_minRepDistance, &pyobj_errorCorrectionRate, &pyobj_checkAllOrders) &&
        jsopencv_to_safe(info, pyobj_minRepDistance, minRepDistance, ArgInfo("minRepDistance", 0)) &&
        jsopencv_to_safe(info, pyobj_errorCorrectionRate, errorCorrectionRate, ArgInfo("errorCorrectionRate", 0)) &&
        jsopencv_to_safe(info, pyobj_checkAllOrders, checkAllOrders, ArgInfo("checkAllOrders", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::aruco::RefineParameters(minRepDistance, errorCorrectionRate, checkAllOrders));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_aruco_aruco_RefineParameters_readRefineParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::RefineParameters * self1 = 0;
    if (!pyopencv_aruco_RefineParameters_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_RefineParameters' or its derivative)");
    cv::aruco::RefineParameters* _self_ = (self1);
    Napi::Value* pyobj_fn = NULL;
    cv::FileNode fn;
    bool retval;

    const char* keywords[] = { "fn", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:aruco_RefineParameters.readRefineParameters", (char**)keywords, &pyobj_fn) &&
        jsopencv_to_safe(info, pyobj_fn, fn, ArgInfo("fn", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->readRefineParameters(fn));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_aruco_aruco_RefineParameters_writeRefineParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::aruco;


    cv::aruco::RefineParameters * self1 = 0;
    if (!pyopencv_aruco_RefineParameters_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'aruco_RefineParameters' or its derivative)");
    cv::aruco::RefineParameters* _self_ = (self1);
    Napi::Value* pyobj_fs = NULL;
    Ptr<cv::FileStorage> fs;
    Napi::Value* pyobj_name = NULL;
    String name;
    bool retval;

    const char* keywords[] = { "fs", "name", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:aruco_RefineParameters.writeRefineParameters", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        jsopencv_to_safe(info, pyobj_fs, fs, ArgInfo("fs", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->writeRefineParameters(*fs, name));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (aruco_RefineParameters)

static PyGetSetDef pyopencv_aruco_RefineParameters_getseters[] =
{
    {(char*)"checkAllOrders", (getter)pyopencv_aruco_RefineParameters_get_checkAllOrders, (setter)pyopencv_aruco_RefineParameters_set_checkAllOrders, (char*)"checkAllOrders", NULL},
    {(char*)"errorCorrectionRate", (getter)pyopencv_aruco_RefineParameters_get_errorCorrectionRate, (setter)pyopencv_aruco_RefineParameters_set_errorCorrectionRate, (char*)"errorCorrectionRate", NULL},
    {(char*)"minRepDistance", (getter)pyopencv_aruco_RefineParameters_get_minRepDistance, (setter)pyopencv_aruco_RefineParameters_set_minRepDistance, (char*)"minRepDistance", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_aruco_RefineParameters_methods[] =
{
    {"readRefineParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_RefineParameters_readRefineParameters, 0), "readRefineParameters(fn) -> retval\n.   @brief Read a new set of RefineParameters from FileNode (use FileStorage.root())."},
    {"writeRefineParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_aruco_aruco_RefineParameters_writeRefineParameters, 0), "writeRefineParameters(fs[, name]) -> retval\n.   @brief Write a set of RefineParameters to FileStorage"},

    {NULL,          NULL}
};

// Converter (aruco_RefineParameters)

template<>
struct PyOpenCV_Converter< cv::aruco::RefineParameters >
{
    static PyObject* from(const cv::aruco::RefineParameters& r)
    {
        return pyopencv_aruco_RefineParameters_Instance(r);
    }
    static bool to(PyObject* src, cv::aruco::RefineParameters& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::aruco::RefineParameters * dst_;
        if (pyopencv_aruco_RefineParameters_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::aruco::RefineParameters for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// barcode_BarcodeDetector (Generic)
//================================================================================

// GetSet (barcode_BarcodeDetector)



// Methods (barcode_BarcodeDetector)

static int pyopencv_cv_barcode_barcode_BarcodeDetector_BarcodeDetector(pyopencv_barcode_BarcodeDetector_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::barcode;

    Napi::Value* pyobj_prototxt_path = NULL;
    std::string prototxt_path="";
    Napi::Value* pyobj_model_path = NULL;
    std::string model_path="";

    const char* keywords[] = { "prototxt_path", "model_path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:BarcodeDetector", (char**)keywords, &pyobj_prototxt_path, &pyobj_model_path) &&
        jsopencv_to_safe(info, pyobj_prototxt_path, prototxt_path, ArgInfo("prototxt_path", 0)) &&
        jsopencv_to_safe(info, pyobj_model_path, model_path, ArgInfo("model_path", 0)))
    {
        new (&(self->v)) Ptr<cv::barcode::BarcodeDetector>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::barcode::BarcodeDetector(prototxt_path, model_path)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_barcode_barcode_BarcodeDetector_decode(const Napi::CallbackInfo &info)
{
    using namespace cv::barcode;


    Ptr<cv::barcode::BarcodeDetector> * self1 = 0;
    if (!pyopencv_barcode_BarcodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'barcode_BarcodeDetector' or its derivative)");
    Ptr<cv::barcode::BarcodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    vector_string decoded_info;
    vector_BarcodeType decoded_type;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:barcode_BarcodeDetector.decode", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decode(img, points, decoded_info, decoded_type));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(decoded_type));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    vector_string decoded_info;
    vector_BarcodeType decoded_type;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:barcode_BarcodeDetector.decode", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->decode(img, points, decoded_info, decoded_type));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(decoded_type));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("decode");

    return NULL;
}

static Napi::Value pyopencv_cv_barcode_barcode_BarcodeDetector_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::barcode;


    Ptr<cv::barcode::BarcodeDetector> * self1 = 0;
    if (!pyopencv_barcode_BarcodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'barcode_BarcodeDetector' or its derivative)");
    Ptr<cv::barcode::BarcodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:barcode_BarcodeDetector.detect", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detect(img, points));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:barcode_BarcodeDetector.detect", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detect(img, points));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_barcode_barcode_BarcodeDetector_detectAndDecode(const Napi::CallbackInfo &info)
{
    using namespace cv::barcode;


    Ptr<cv::barcode::BarcodeDetector> * self1 = 0;
    if (!pyopencv_barcode_BarcodeDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'barcode_BarcodeDetector' or its derivative)");
    Ptr<cv::barcode::BarcodeDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    vector_string decoded_info;
    vector_BarcodeType decoded_type;
    Napi::Value* pyobj_points = NULL;
    Mat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:barcode_BarcodeDetector.detectAndDecode", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecode(img, decoded_info, decoded_type, points));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(decoded_type), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    vector_string decoded_info;
    vector_BarcodeType decoded_type;
    Napi::Value* pyobj_points = NULL;
    UMat points;
    bool retval;

    const char* keywords[] = { "img", "points", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:barcode_BarcodeDetector.detectAndDecode", (char**)keywords, &pyobj_img, &pyobj_points) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_points, points, ArgInfo("points", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->detectAndDecode(img, decoded_info, decoded_type, points));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(decoded_info), jsopencv_from(decoded_type), jsopencv_from(points));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectAndDecode");

    return NULL;
}



// Tables (barcode_BarcodeDetector)

static PyGetSetDef pyopencv_barcode_BarcodeDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_barcode_BarcodeDetector_methods[] =
{
    {"decode", CV_JS_FN_WITH_KW_(pyopencv_cv_barcode_barcode_BarcodeDetector_decode, 0), "decode(img, points) -> retval, decoded_info, decoded_type\n.   @brief Decodes barcode in image once it's found by the detect() method.\n.        *\n.        * @param img grayscale or color (BGR) image containing bar code.\n.        * @param points vector of rotated rectangle vertices found by detect() method (or some other algorithm).\n.        * For N detected barcodes, the dimensions of this array should be [N][4].\n.        * Order of four points in vector<Point2f> is bottomLeft, topLeft, topRight, bottomRight.\n.        * @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.\n.        * @param decoded_type vector of BarcodeType, specifies the type of these barcodes"},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_barcode_barcode_BarcodeDetector_detect, 0), "detect(img[, points]) -> retval, points\n.   @brief Detects Barcode in image and returns the rectangle(s) containing the code.\n.        *\n.        * @param img grayscale or color (BGR) image containing (or not) Barcode.\n.        * @param points Output vector of vector of vertices of the minimum-area rotated rectangle containing the codes.\n.        * For N detected barcodes, the dimensions of this array should be [N][4].\n.        * Order of four points in vector< Point2f> is bottomLeft, topLeft, topRight, bottomRight."},
    {"detectAndDecode", CV_JS_FN_WITH_KW_(pyopencv_cv_barcode_barcode_BarcodeDetector_detectAndDecode, 0), "detectAndDecode(img[, points]) -> retval, decoded_info, decoded_type, points\n.   @brief Both detects and decodes barcode\n.   \n.        * @param img grayscale or color (BGR) image containing barcode.\n.        * @param decoded_info UTF8-encoded output vector of string(s) or empty vector of string if the codes cannot be decoded.\n.        * @param decoded_type vector of BarcodeType, specifies the type of these barcodes\n.        * @param points optional output vector of vertices of the found  barcode rectangle. Will be empty if not found."},

    {NULL,          NULL}
};

// Converter (barcode_BarcodeDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::barcode::BarcodeDetector> >
{
    static PyObject* from(const Ptr<cv::barcode::BarcodeDetector>& r)
    {
        return pyopencv_barcode_BarcodeDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::barcode::BarcodeDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::barcode::BarcodeDetector> * dst_;
        if (pyopencv_barcode_BarcodeDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::barcode::BarcodeDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ccm_ColorCorrectionModel (Generic)
//================================================================================

// GetSet (ccm_ColorCorrectionModel)



// Methods (ccm_ColorCorrectionModel)

static int pyopencv_cv_ccm_ccm_ColorCorrectionModel_ColorCorrectionModel(pyopencv_ccm_ColorCorrectionModel_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::ccm;

    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_constcolor = NULL;
    CONST_COLOR constcolor=static_cast<CONST_COLOR>(0);

    const char* keywords[] = { "src", "constcolor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:ColorCorrectionModel", (char**)keywords, &pyobj_src, &pyobj_constcolor) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_constcolor, constcolor, ArgInfo("constcolor", 0)))
    {
        new (&(self->v)) Ptr<cv::ccm::ColorCorrectionModel>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::ccm::ColorCorrectionModel(src, constcolor)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_colors = NULL;
    Mat colors;
    Napi::Value* pyobj_ref_cs = NULL;
    COLOR_SPACE ref_cs=static_cast<COLOR_SPACE>(0);

    const char* keywords[] = { "src", "colors", "ref_cs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ColorCorrectionModel", (char**)keywords, &pyobj_src, &pyobj_colors, &pyobj_ref_cs) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_colors, colors, ArgInfo("colors", 0)) &&
        jsopencv_to_safe(info, pyobj_ref_cs, ref_cs, ArgInfo("ref_cs", 0)))
    {
        new (&(self->v)) Ptr<cv::ccm::ColorCorrectionModel>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::ccm::ColorCorrectionModel(src, colors, ref_cs)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_colors = NULL;
    Mat colors;
    Napi::Value* pyobj_ref_cs = NULL;
    COLOR_SPACE ref_cs=static_cast<COLOR_SPACE>(0);
    Napi::Value* pyobj_colored = NULL;
    Mat colored;

    const char* keywords[] = { "src", "colors", "ref_cs", "colored", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:ColorCorrectionModel", (char**)keywords, &pyobj_src, &pyobj_colors, &pyobj_ref_cs, &pyobj_colored) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_colors, colors, ArgInfo("colors", 0)) &&
        jsopencv_to_safe(info, pyobj_ref_cs, ref_cs, ArgInfo("ref_cs", 0)) &&
        jsopencv_to_safe(info, pyobj_colored, colored, ArgInfo("colored", 0)))
    {
        new (&(self->v)) Ptr<cv::ccm::ColorCorrectionModel>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::ccm::ColorCorrectionModel(src, colors, ref_cs, colored)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("ColorCorrectionModel");

    return -1;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_getCCM(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCCM());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_getLoss(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLoss());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_getMask(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMask());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_getWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_get_dst_rgbl(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get_dst_rgbl());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_get_src_rgbl(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get_src_rgbl());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_infer(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_islinear = NULL;
    bool islinear=false;
    Mat retval;

    const char* keywords[] = { "img", "islinear", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ccm_ColorCorrectionModel.infer", (char**)keywords, &pyobj_img, &pyobj_islinear) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_islinear, islinear, ArgInfo("islinear", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->infer(img, islinear));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_run(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->run());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setCCM_TYPE(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_ccm_type = NULL;
    CCM_TYPE ccm_type=static_cast<CCM_TYPE>(0);

    const char* keywords[] = { "ccm_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setCCM_TYPE", (char**)keywords, &pyobj_ccm_type) &&
        jsopencv_to_safe(info, pyobj_ccm_type, ccm_type, ArgInfo("ccm_type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCCM_TYPE(ccm_type));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setColorSpace(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_cs = NULL;
    COLOR_SPACE cs=static_cast<COLOR_SPACE>(0);

    const char* keywords[] = { "cs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setColorSpace", (char**)keywords, &pyobj_cs) &&
        jsopencv_to_safe(info, pyobj_cs, cs, ArgInfo("cs", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setColorSpace(cs));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setDistance(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_distance = NULL;
    DISTANCE_TYPE distance=static_cast<DISTANCE_TYPE>(0);

    const char* keywords[] = { "distance", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setDistance", (char**)keywords, &pyobj_distance) &&
        jsopencv_to_safe(info, pyobj_distance, distance, ArgInfo("distance", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDistance(distance));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setEpsilon(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_epsilon = NULL;
    double epsilon=0;

    const char* keywords[] = { "epsilon", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setEpsilon", (char**)keywords, &pyobj_epsilon) &&
        jsopencv_to_safe(info, pyobj_epsilon, epsilon, ArgInfo("epsilon", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEpsilon(epsilon));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setInitialMethod(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_initial_method_type = NULL;
    INITIAL_METHOD_TYPE initial_method_type=static_cast<INITIAL_METHOD_TYPE>(0);

    const char* keywords[] = { "initial_method_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setInitialMethod", (char**)keywords, &pyobj_initial_method_type) &&
        jsopencv_to_safe(info, pyobj_initial_method_type, initial_method_type, ArgInfo("initial_method_type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitialMethod(initial_method_type));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setLinear(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_linear_type = NULL;
    LINEAR_TYPE linear_type=static_cast<LINEAR_TYPE>(0);

    const char* keywords[] = { "linear_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setLinear", (char**)keywords, &pyobj_linear_type) &&
        jsopencv_to_safe(info, pyobj_linear_type, linear_type, ArgInfo("linear_type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLinear(linear_type));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setLinearDegree(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_deg = NULL;
    int deg=0;

    const char* keywords[] = { "deg", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setLinearDegree", (char**)keywords, &pyobj_deg) &&
        jsopencv_to_safe(info, pyobj_deg, deg, ArgInfo("deg", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLinearDegree(deg));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setLinearGamma(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_gamma = NULL;
    double gamma=0;

    const char* keywords[] = { "gamma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setLinearGamma", (char**)keywords, &pyobj_gamma) &&
        jsopencv_to_safe(info, pyobj_gamma, gamma, ArgInfo("gamma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLinearGamma(gamma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setMaxCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_max_count = NULL;
    int max_count=0;

    const char* keywords[] = { "max_count", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setMaxCount", (char**)keywords, &pyobj_max_count) &&
        jsopencv_to_safe(info, pyobj_max_count, max_count, ArgInfo("max_count", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxCount(max_count));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setSaturatedThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_lower = NULL;
    double lower=0;
    Napi::Value* pyobj_upper = NULL;
    double upper=0;

    const char* keywords[] = { "lower", "upper", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:ccm_ColorCorrectionModel.setSaturatedThreshold", (char**)keywords, &pyobj_lower, &pyobj_upper) &&
        jsopencv_to_safe(info, pyobj_lower, lower, ArgInfo("lower", 0)) &&
        jsopencv_to_safe(info, pyobj_upper, upper, ArgInfo("upper", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSaturatedThreshold(lower, upper));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setWeightCoeff(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_weights_coeff = NULL;
    double weights_coeff=0;

    const char* keywords[] = { "weights_coeff", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setWeightCoeff", (char**)keywords, &pyobj_weights_coeff) &&
        jsopencv_to_safe(info, pyobj_weights_coeff, weights_coeff, ArgInfo("weights_coeff", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightCoeff(weights_coeff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ccm_ccm_ColorCorrectionModel_setWeightsList(const Napi::CallbackInfo &info)
{
    using namespace cv::ccm;


    Ptr<cv::ccm::ColorCorrectionModel> * self1 = 0;
    if (!pyopencv_ccm_ColorCorrectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ccm_ColorCorrectionModel' or its derivative)");
    Ptr<cv::ccm::ColorCorrectionModel> _self_ = *(self1);
    Napi::Value* pyobj_weights_list = NULL;
    Mat weights_list;

    const char* keywords[] = { "weights_list", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ccm_ColorCorrectionModel.setWeightsList", (char**)keywords, &pyobj_weights_list) &&
        jsopencv_to_safe(info, pyobj_weights_list, weights_list, ArgInfo("weights_list", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightsList(weights_list));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ccm_ColorCorrectionModel)

static PyGetSetDef pyopencv_ccm_ColorCorrectionModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ccm_ColorCorrectionModel_methods[] =
{
    {"getCCM", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_getCCM, 0), "getCCM() -> retval\n."},
    {"getLoss", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_getLoss, 0), "getLoss() -> retval\n."},
    {"getMask", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_getMask, 0), "getMask() -> retval\n."},
    {"getWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_getWeights, 0), "getWeights() -> retval\n."},
    {"get_dst_rgbl", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_get_dst_rgbl, 0), "get_dst_rgbl() -> retval\n."},
    {"get_src_rgbl", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_get_src_rgbl, 0), "get_src_rgbl() -> retval\n."},
    {"infer", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_infer, 0), "infer(img[, islinear]) -> retval\n.   @brief Infer using fitting ccm.\n.       @param img the input image.\n.       @param islinear default false.\n.       @return the output array."},
    {"run", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_run, 0), "run() -> None\n.   @brief make color correction"},
    {"setCCM_TYPE", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setCCM_TYPE, 0), "setCCM_TYPE(ccm_type) -> None\n.   @brief set ccm_type\n.       @param ccm_type the shape of color correction matrix(CCM);\\n\n.                       default: @ref CCM_3x3"},
    {"setColorSpace", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setColorSpace, 0), "setColorSpace(cs) -> None\n.   @brief set ColorSpace\n.       @note It should be some RGB color space;\n.       Supported list of color cards:\n.       - @ref COLOR_SPACE_sRGB\n.       - @ref COLOR_SPACE_AdobeRGB\n.       - @ref COLOR_SPACE_WideGamutRGB\n.       - @ref COLOR_SPACE_ProPhotoRGB\n.       - @ref COLOR_SPACE_DCI_P3_RGB\n.       - @ref COLOR_SPACE_AppleRGB\n.       - @ref COLOR_SPACE_REC_709_RGB\n.       - @ref COLOR_SPACE_REC_2020_RGB\n.        @param cs the absolute color space that detected colors convert to;\\n\n.                 default: @ref COLOR_SPACE_sRGB"},
    {"setDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setDistance, 0), "setDistance(distance) -> None\n.   @brief set Distance\n.       @param distance the type of color distance;\\n\n.                       default: @ref DISTANCE_CIE2000"},
    {"setEpsilon", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setEpsilon, 0), "setEpsilon(epsilon) -> None\n.   @brief set Epsilon\n.       @param epsilon used in MinProblemSolver-DownhillSolver;\\n\n.           Terminal criteria to the algorithm;\\n\n.                      default: 1e-4;"},
    {"setInitialMethod", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setInitialMethod, 0), "setInitialMethod(initial_method_type) -> None\n.   @brief set InitialMethod\n.       @param initial_method_type the method of calculating CCM initial value;\\n\n.               default: INITIAL_METHOD_LEAST_SQUARE"},
    {"setLinear", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setLinear, 0), "setLinear(linear_type) -> None\n.   @brief set Linear\n.       @param linear_type the method of linearization;\\n\n.                          default: @ref LINEARIZATION_GAMMA"},
    {"setLinearDegree", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setLinearDegree, 0), "setLinearDegree(deg) -> None\n.   @brief set degree\n.           @note only valid when linear is set to\n.          - @ref LINEARIZATION_COLORPOLYFIT\n.          - @ref LINEARIZATION_GRAYPOLYFIT\n.          - @ref LINEARIZATION_COLORLOGPOLYFIT\n.          - @ref LINEARIZATION_GRAYLOGPOLYFIT\n.   \n.       @param deg the degree of linearization polynomial;\\n\n.               default: 3"},
    {"setLinearGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setLinearGamma, 0), "setLinearGamma(gamma) -> None\n.   @brief set Gamma\n.   \n.       @note only valid when linear is set to \"gamma\";\\n\n.   \n.       @param gamma the gamma value of gamma correction;\\n\n.                    default: 2.2;"},
    {"setMaxCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setMaxCount, 0), "setMaxCount(max_count) -> None\n.   @brief set MaxCount\n.       @param max_count used in MinProblemSolver-DownhillSolver;\\n\n.           Terminal criteria to the algorithm;\\n\n.                        default: 5000;"},
    {"setSaturatedThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setSaturatedThreshold, 0), "setSaturatedThreshold(lower, upper) -> None\n.   @brief set SaturatedThreshold.\n.                   The colors in the closed interval [lower, upper] are reserved to participate\n.                   in the calculation of the loss function and initialization parameters\n.       @param lower the lower threshold to determine saturation;\\n\n.               default: 0;\n.       @param upper the upper threshold to determine saturation;\\n\n.                    default: 0"},
    {"setWeightCoeff", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setWeightCoeff, 0), "setWeightCoeff(weights_coeff) -> None\n.   @brief set WeightCoeff\n.       @param weights_coeff the exponent number of L* component of the reference color in CIE Lab color space;\\n\n.                            default: 0"},
    {"setWeightsList", CV_JS_FN_WITH_KW_(pyopencv_cv_ccm_ccm_ColorCorrectionModel_setWeightsList, 0), "setWeightsList(weights_list) -> None\n.   @brief set WeightsList\n.       @param weights_list the list of weight of each color;\\n\n.                           default: empty array"},

    {NULL,          NULL}
};

// Converter (ccm_ColorCorrectionModel)

template<>
struct PyOpenCV_Converter< Ptr<cv::ccm::ColorCorrectionModel> >
{
    static PyObject* from(const Ptr<cv::ccm::ColorCorrectionModel>& r)
    {
        return pyopencv_ccm_ColorCorrectionModel_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ccm::ColorCorrectionModel>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ccm::ColorCorrectionModel> * dst_;
        if (pyopencv_ccm_ColorCorrectionModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ccm::ColorCorrectionModel> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_BufferPool (Generic)
//================================================================================

// GetSet (cuda_BufferPool)



// Methods (cuda_BufferPool)

static int pyopencv_cv_cuda_cuda_BufferPool_BufferPool(pyopencv_cuda_BufferPool_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:BufferPool", (char**)keywords, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::BufferPool>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::BufferPool(stream)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_cuda_cuda_BufferPool_getAllocator(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::BufferPool> * self1 = 0;
    if (!pyopencv_cuda_BufferPool_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_BufferPool' or its derivative)");
    Ptr<cv::cuda::BufferPool> _self_ = *(self1);
    Ptr<GpuMat::Allocator> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAllocator());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_BufferPool_getBuffer(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::BufferPool> * self1 = 0;
    if (!pyopencv_cuda_BufferPool_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_BufferPool' or its derivative)");
    Ptr<cv::cuda::BufferPool> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    GpuMat retval;

    const char* keywords[] = { "rows", "cols", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:cuda_BufferPool.getBuffer", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBuffer(rows, cols, type));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    GpuMat retval;

    const char* keywords[] = { "size", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_BufferPool.getBuffer", (char**)keywords, &pyobj_size, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBuffer(size, type));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getBuffer");

    return NULL;
}



// Tables (cuda_BufferPool)

static PyGetSetDef pyopencv_cuda_BufferPool_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_BufferPool_methods[] =
{
    {"getAllocator", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_BufferPool_getAllocator, 0), "getAllocator() -> retval\n."},
    {"getBuffer", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_BufferPool_getBuffer, 0), "getBuffer(rows, cols, type) -> retval\n.   \n\n\n\ngetBuffer(size, type) -> retval\n."},

    {NULL,          NULL}
};

// Converter (cuda_BufferPool)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::BufferPool> >
{
    static PyObject* from(const Ptr<cv::cuda::BufferPool>& r)
    {
        return pyopencv_cuda_BufferPool_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::BufferPool>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::BufferPool> * dst_;
        if (pyopencv_cuda_BufferPool_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::BufferPool> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_DeviceInfo (Generic)
//================================================================================

// GetSet (cuda_DeviceInfo)



// Methods (cuda_DeviceInfo)

static int pyopencv_cv_cuda_cuda_DeviceInfo_DeviceInfo(pyopencv_cuda_DeviceInfo_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::cuda;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::cuda::DeviceInfo>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::DeviceInfo()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_device_id = NULL;
    int device_id=0;

    const char* keywords[] = { "device_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DeviceInfo", (char**)keywords, &pyobj_device_id) &&
        jsopencv_to_safe(info, pyobj_device_id, device_id, ArgInfo("device_id", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::DeviceInfo>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::DeviceInfo(device_id)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("DeviceInfo");

    return -1;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_ECCEnabled(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->ECCEnabled());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_asyncEngineCount(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->asyncEngineCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_canMapHostMemory(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->canMapHostMemory());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_clockRate(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->clockRate());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_computeMode(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    DeviceInfo::ComputeMode retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->computeMode());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_concurrentKernels(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->concurrentKernels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_deviceID(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->deviceID());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_freeMemory(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->freeMemory());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_integrated(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->integrated());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_isCompatible(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isCompatible());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_kernelExecTimeoutEnabled(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->kernelExecTimeoutEnabled());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_l2CacheSize(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->l2CacheSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_majorVersion(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->majorVersion());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxGridSize(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxGridSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface1D(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurface1D());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface1DLayered(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurface1DLayered());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface2D(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurface2D());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface2DLayered(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurface2DLayered());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface3D(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurface3D());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurfaceCubemap(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurfaceCubemap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxSurfaceCubemapLayered(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSurfaceCubemapLayered());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1D(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture1D());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1DLayered(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture1DLayered());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1DLinear(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture1DLinear());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1DMipmap(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture1DMipmap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2D(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture2D());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DGather(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture2DGather());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DLayered(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture2DLayered());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DLinear(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture2DLinear());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DMipmap(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture2DMipmap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture3D(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTexture3D());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTextureCubemap(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTextureCubemap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxTextureCubemapLayered(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec2i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxTextureCubemapLayered());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxThreadsDim(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Vec3i retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxThreadsDim());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxThreadsPerBlock(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxThreadsPerBlock());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_maxThreadsPerMultiProcessor(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxThreadsPerMultiProcessor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_memPitch(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->memPitch());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_memoryBusWidth(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->memoryBusWidth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_memoryClockRate(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->memoryClockRate());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_minorVersion(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->minorVersion());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_multiProcessorCount(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->multiProcessorCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_pciBusID(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->pciBusID());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_pciDeviceID(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->pciDeviceID());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_pciDomainID(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->pciDomainID());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_queryMemory(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    Napi::Value* pyobj_totalMemory = NULL;
    size_t totalMemory=0;
    Napi::Value* pyobj_freeMemory = NULL;
    size_t freeMemory=0;

    const char* keywords[] = { "totalMemory", "freeMemory", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_DeviceInfo.queryMemory", (char**)keywords, &pyobj_totalMemory, &pyobj_freeMemory) &&
        jsopencv_to_safe(info, pyobj_totalMemory, totalMemory, ArgInfo("totalMemory", 0)) &&
        jsopencv_to_safe(info, pyobj_freeMemory, freeMemory, ArgInfo("freeMemory", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->queryMemory(totalMemory, freeMemory));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_regsPerBlock(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->regsPerBlock());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_sharedMemPerBlock(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->sharedMemPerBlock());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_surfaceAlignment(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->surfaceAlignment());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_tccDriver(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->tccDriver());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_textureAlignment(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->textureAlignment());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_texturePitchAlignment(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->texturePitchAlignment());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_totalConstMem(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->totalConstMem());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_totalGlobalMem(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->totalGlobalMem());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_totalMemory(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->totalMemory());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_unifiedAddressing(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->unifiedAddressing());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_DeviceInfo_warpSize(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::DeviceInfo> * self1 = 0;
    if (!pyopencv_cuda_DeviceInfo_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_DeviceInfo' or its derivative)");
    Ptr<cv::cuda::DeviceInfo> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->warpSize());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (cuda_DeviceInfo)

static PyGetSetDef pyopencv_cuda_DeviceInfo_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_DeviceInfo_methods[] =
{
    {"ECCEnabled", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_ECCEnabled, 0), "ECCEnabled() -> retval\n."},
    {"asyncEngineCount", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_asyncEngineCount, 0), "asyncEngineCount() -> retval\n."},
    {"canMapHostMemory", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_canMapHostMemory, 0), "canMapHostMemory() -> retval\n."},
    {"clockRate", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_clockRate, 0), "clockRate() -> retval\n."},
    {"computeMode", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_computeMode, 0), "computeMode() -> retval\n."},
    {"concurrentKernels", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_concurrentKernels, 0), "concurrentKernels() -> retval\n."},
    {"deviceID", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_deviceID, 0), "deviceID() -> retval\n.   @brief Returns system index of the CUDA device starting with 0."},
    {"freeMemory", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_freeMemory, 0), "freeMemory() -> retval\n."},
    {"integrated", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_integrated, 0), "integrated() -> retval\n."},
    {"isCompatible", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_isCompatible, 0), "isCompatible() -> retval\n.   @brief Checks the CUDA module and device compatibility.\n.   \n.       This function returns true if the CUDA module can be run on the specified device. Otherwise, it\n.       returns false ."},
    {"kernelExecTimeoutEnabled", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_kernelExecTimeoutEnabled, 0), "kernelExecTimeoutEnabled() -> retval\n."},
    {"l2CacheSize", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_l2CacheSize, 0), "l2CacheSize() -> retval\n."},
    {"majorVersion", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_majorVersion, 0), "majorVersion() -> retval\n."},
    {"maxGridSize", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxGridSize, 0), "maxGridSize() -> retval\n."},
    {"maxSurface1D", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface1D, 0), "maxSurface1D() -> retval\n."},
    {"maxSurface1DLayered", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface1DLayered, 0), "maxSurface1DLayered() -> retval\n."},
    {"maxSurface2D", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface2D, 0), "maxSurface2D() -> retval\n."},
    {"maxSurface2DLayered", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface2DLayered, 0), "maxSurface2DLayered() -> retval\n."},
    {"maxSurface3D", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurface3D, 0), "maxSurface3D() -> retval\n."},
    {"maxSurfaceCubemap", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurfaceCubemap, 0), "maxSurfaceCubemap() -> retval\n."},
    {"maxSurfaceCubemapLayered", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxSurfaceCubemapLayered, 0), "maxSurfaceCubemapLayered() -> retval\n."},
    {"maxTexture1D", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1D, 0), "maxTexture1D() -> retval\n."},
    {"maxTexture1DLayered", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1DLayered, 0), "maxTexture1DLayered() -> retval\n."},
    {"maxTexture1DLinear", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1DLinear, 0), "maxTexture1DLinear() -> retval\n."},
    {"maxTexture1DMipmap", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture1DMipmap, 0), "maxTexture1DMipmap() -> retval\n."},
    {"maxTexture2D", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2D, 0), "maxTexture2D() -> retval\n."},
    {"maxTexture2DGather", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DGather, 0), "maxTexture2DGather() -> retval\n."},
    {"maxTexture2DLayered", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DLayered, 0), "maxTexture2DLayered() -> retval\n."},
    {"maxTexture2DLinear", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DLinear, 0), "maxTexture2DLinear() -> retval\n."},
    {"maxTexture2DMipmap", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture2DMipmap, 0), "maxTexture2DMipmap() -> retval\n."},
    {"maxTexture3D", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTexture3D, 0), "maxTexture3D() -> retval\n."},
    {"maxTextureCubemap", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTextureCubemap, 0), "maxTextureCubemap() -> retval\n."},
    {"maxTextureCubemapLayered", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxTextureCubemapLayered, 0), "maxTextureCubemapLayered() -> retval\n."},
    {"maxThreadsDim", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxThreadsDim, 0), "maxThreadsDim() -> retval\n."},
    {"maxThreadsPerBlock", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxThreadsPerBlock, 0), "maxThreadsPerBlock() -> retval\n."},
    {"maxThreadsPerMultiProcessor", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_maxThreadsPerMultiProcessor, 0), "maxThreadsPerMultiProcessor() -> retval\n."},
    {"memPitch", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_memPitch, 0), "memPitch() -> retval\n."},
    {"memoryBusWidth", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_memoryBusWidth, 0), "memoryBusWidth() -> retval\n."},
    {"memoryClockRate", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_memoryClockRate, 0), "memoryClockRate() -> retval\n."},
    {"minorVersion", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_minorVersion, 0), "minorVersion() -> retval\n."},
    {"multiProcessorCount", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_multiProcessorCount, 0), "multiProcessorCount() -> retval\n."},
    {"pciBusID", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_pciBusID, 0), "pciBusID() -> retval\n."},
    {"pciDeviceID", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_pciDeviceID, 0), "pciDeviceID() -> retval\n."},
    {"pciDomainID", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_pciDomainID, 0), "pciDomainID() -> retval\n."},
    {"queryMemory", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_queryMemory, 0), "queryMemory(totalMemory, freeMemory) -> None\n."},
    {"regsPerBlock", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_regsPerBlock, 0), "regsPerBlock() -> retval\n."},
    {"sharedMemPerBlock", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_sharedMemPerBlock, 0), "sharedMemPerBlock() -> retval\n."},
    {"surfaceAlignment", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_surfaceAlignment, 0), "surfaceAlignment() -> retval\n."},
    {"tccDriver", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_tccDriver, 0), "tccDriver() -> retval\n."},
    {"textureAlignment", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_textureAlignment, 0), "textureAlignment() -> retval\n."},
    {"texturePitchAlignment", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_texturePitchAlignment, 0), "texturePitchAlignment() -> retval\n."},
    {"totalConstMem", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_totalConstMem, 0), "totalConstMem() -> retval\n."},
    {"totalGlobalMem", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_totalGlobalMem, 0), "totalGlobalMem() -> retval\n."},
    {"totalMemory", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_totalMemory, 0), "totalMemory() -> retval\n."},
    {"unifiedAddressing", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_unifiedAddressing, 0), "unifiedAddressing() -> retval\n."},
    {"warpSize", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_DeviceInfo_warpSize, 0), "warpSize() -> retval\n."},

    {NULL,          NULL}
};

// Converter (cuda_DeviceInfo)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::DeviceInfo> >
{
    static PyObject* from(const Ptr<cv::cuda::DeviceInfo>& r)
    {
        return pyopencv_cuda_DeviceInfo_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::DeviceInfo>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::DeviceInfo> * dst_;
        if (pyopencv_cuda_DeviceInfo_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::DeviceInfo> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_Event (Generic)
//================================================================================

// GetSet (cuda_Event)



// Methods (cuda_Event)

static int pyopencv_cv_cuda_cuda_Event_Event(pyopencv_cuda_Event_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_flags = NULL;
    Event_CreateFlags flags=Event::CreateFlags::DEFAULT;

    const char* keywords[] = { "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:Event", (char**)keywords, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::Event>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::Event(flags)));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_cuda_cuda_Event_elapsedTime_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_start = NULL;
    Event start;
    Napi::Value* pyobj_end = NULL;
    Event end;
    float retval;

    const char* keywords[] = { "start", "end", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_Event.elapsedTime", (char**)keywords, &pyobj_start, &pyobj_end) &&
        jsopencv_to_safe(info, pyobj_start, start, ArgInfo("start", 0)) &&
        jsopencv_to_safe(info, pyobj_end, end, ArgInfo("end", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::Event::elapsedTime(start, end));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Event_queryIfComplete(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Event> * self1 = 0;
    if (!pyopencv_cuda_Event_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Event' or its derivative)");
    Ptr<cv::cuda::Event> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->queryIfComplete());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Event_record(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Event> * self1 = 0;
    if (!pyopencv_cuda_Event_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Event' or its derivative)");
    Ptr<cv::cuda::Event> _self_ = *(self1);
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_Event.record", (char**)keywords, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->record(stream));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Event_waitForCompletion(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Event> * self1 = 0;
    if (!pyopencv_cuda_Event_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Event' or its derivative)");
    Ptr<cv::cuda::Event> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->waitForCompletion());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (cuda_Event)

static PyGetSetDef pyopencv_cuda_Event_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_Event_methods[] =
{
    {"elapsedTime", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Event_elapsedTime_static, METH_STATIC), "elapsedTime(start, end) -> retval\n."},
    {"queryIfComplete", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Event_queryIfComplete, 0), "queryIfComplete() -> retval\n."},
    {"record", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Event_record, 0), "record([, stream]) -> None\n."},
    {"waitForCompletion", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Event_waitForCompletion, 0), "waitForCompletion() -> None\n."},

    {NULL,          NULL}
};

// Converter (cuda_Event)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::Event> >
{
    static PyObject* from(const Ptr<cv::cuda::Event>& r)
    {
        return pyopencv_cuda_Event_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::Event>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::Event> * dst_;
        if (pyopencv_cuda_Event_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::Event> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_GpuData (Generic)
//================================================================================

// GetSet (cuda_GpuData)



// Methods (cuda_GpuData)



// Tables (cuda_GpuData)

static PyGetSetDef pyopencv_cuda_GpuData_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_GpuData_methods[] =
{

    {NULL,          NULL}
};

// Converter (cuda_GpuData)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::GpuData> >
{
    static PyObject* from(const Ptr<cv::cuda::GpuData>& r)
    {
        return pyopencv_cuda_GpuData_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::GpuData>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::GpuData> * dst_;
        if (pyopencv_cuda_GpuData_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::GpuData> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_GpuMat (Generic)
//================================================================================

// GetSet (cuda_GpuMat)


static PyObject* pyopencv_cuda_GpuMat_get_step(pyopencv_cuda_GpuMat_t* p, void *closure)
{
    return jsopencv_from(p->v->step);
}


// Methods (cuda_GpuMat)

static int pyopencv_cv_cuda_cuda_GpuMat_GpuMat(pyopencv_cuda_GpuMat_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::cuda;

    pyPrepareArgumentConversionErrorsStorage(11);

    {
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:GpuMat", (char**)keywords, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "rows", "cols", "type", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:GpuMat", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(rows, cols, type, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "size", "type", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:GpuMat", (char**)keywords, &pyobj_size, &pyobj_type, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(size, type, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "rows", "cols", "type", "s", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:GpuMat", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type, &pyobj_s, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(rows, cols, type, s, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "size", "type", "s", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:GpuMat", (char**)keywords, &pyobj_size, &pyobj_type, &pyobj_s, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(size, type, s, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    GpuMat m;

    const char* keywords[] = { "m", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GpuMat", (char**)keywords, &pyobj_m) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(m)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    GpuMat m;
    Napi::Value* pyobj_rowRange = NULL;
    Range rowRange;
    Napi::Value* pyobj_colRange = NULL;
    Range colRange;

    const char* keywords[] = { "m", "rowRange", "colRange", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:GpuMat", (char**)keywords, &pyobj_m, &pyobj_rowRange, &pyobj_colRange) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_rowRange, rowRange, ArgInfo("rowRange", 0)) &&
        jsopencv_to_safe(info, pyobj_colRange, colRange, ArgInfo("colRange", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(m, rowRange, colRange)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_m = NULL;
    GpuMat m;
    Napi::Value* pyobj_roi = NULL;
    Rect roi;

    const char* keywords[] = { "m", "roi", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:GpuMat", (char**)keywords, &pyobj_m, &pyobj_roi) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_roi, roi, ArgInfo("roi", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(m, roi)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    Mat arr;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "arr", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GpuMat", (char**)keywords, &pyobj_arr, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(arr, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    cuda::GpuMat arr;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "arr", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GpuMat", (char**)keywords, &pyobj_arr, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(arr, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    UMat arr;
    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator=GpuMat::defaultAllocator();

    const char* keywords[] = { "arr", "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:GpuMat", (char**)keywords, &pyobj_arr, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::GpuMat>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::GpuMat(arr, allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GpuMat");

    return -1;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_adjustROI(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_dtop = NULL;
    int dtop=0;
    Napi::Value* pyobj_dbottom = NULL;
    int dbottom=0;
    Napi::Value* pyobj_dleft = NULL;
    int dleft=0;
    Napi::Value* pyobj_dright = NULL;
    int dright=0;
    GpuMat retval;

    const char* keywords[] = { "dtop", "dbottom", "dleft", "dright", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:cuda_GpuMat.adjustROI", (char**)keywords, &pyobj_dtop, &pyobj_dbottom, &pyobj_dleft, &pyobj_dright) &&
        jsopencv_to_safe(info, pyobj_dtop, dtop, ArgInfo("dtop", 0)) &&
        jsopencv_to_safe(info, pyobj_dbottom, dbottom, ArgInfo("dbottom", 0)) &&
        jsopencv_to_safe(info, pyobj_dleft, dleft, ArgInfo("dleft", 0)) &&
        jsopencv_to_safe(info, pyobj_dright, dright, ArgInfo("dright", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->adjustROI(dtop, dbottom, dleft, dright));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_assignTo(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_m = NULL;
    GpuMat m;
    Napi::Value* pyobj_type = NULL;
    int type=-1;

    const char* keywords[] = { "m", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.assignTo", (char**)keywords, &pyobj_m, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->assignTo(m, type));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_channels(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->channels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_clone(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    GpuMat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->clone());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_col(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_x = NULL;
    int x=0;
    GpuMat retval;

    const char* keywords[] = { "x", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.col", (char**)keywords, &pyobj_x) &&
        jsopencv_to_safe(info, pyobj_x, x, ArgInfo("x", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->col(x));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_colRange(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_startcol = NULL;
    int startcol=0;
    Napi::Value* pyobj_endcol = NULL;
    int endcol=0;
    GpuMat retval;

    const char* keywords[] = { "startcol", "endcol", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.colRange", (char**)keywords, &pyobj_startcol, &pyobj_endcol) &&
        jsopencv_to_safe(info, pyobj_startcol, startcol, ArgInfo("startcol", 0)) &&
        jsopencv_to_safe(info, pyobj_endcol, endcol, ArgInfo("endcol", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->colRange(startcol, endcol));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_r = NULL;
    Range r;
    GpuMat retval;

    const char* keywords[] = { "r", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.colRange", (char**)keywords, &pyobj_r) &&
        jsopencv_to_safe(info, pyobj_r, r, ArgInfo("r", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->colRange(r));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("colRange");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_convertTo(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(15);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;

    const char* keywords[] = { "rtype", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;

    const char* keywords[] = { "rtype", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;

    const char* keywords[] = { "rtype", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_beta = NULL;
    double beta=0.0;

    const char* keywords[] = { "rtype", "alpha", "dst", "beta", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_dst, &pyobj_beta) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_beta, beta, ArgInfo("beta", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, beta));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_beta = NULL;
    double beta=0.0;

    const char* keywords[] = { "rtype", "alpha", "dst", "beta", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_dst, &pyobj_beta) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_beta, beta, ArgInfo("beta", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, beta));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_beta = NULL;
    double beta=0.0;

    const char* keywords[] = { "rtype", "alpha", "dst", "beta", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_dst, &pyobj_beta) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_beta, beta, ArgInfo("beta", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, beta));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "alpha", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "alpha", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "alpha", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_beta = NULL;
    double beta=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "alpha", "beta", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_beta, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_beta, beta, ArgInfo("beta", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, beta, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_beta = NULL;
    double beta=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "alpha", "beta", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_beta, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_beta, beta, ArgInfo("beta", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, beta, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_rtype = NULL;
    int rtype=0;
    Napi::Value* pyobj_alpha = NULL;
    double alpha=0;
    Napi::Value* pyobj_beta = NULL;
    double beta=0;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "rtype", "alpha", "beta", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:cuda_GpuMat.convertTo", (char**)keywords, &pyobj_rtype, &pyobj_alpha, &pyobj_beta, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_rtype, rtype, ArgInfo("rtype", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_beta, beta, ArgInfo("beta", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->convertTo(dst, rtype, alpha, beta, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("convertTo");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_copyTo(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(12);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "mask", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_mask, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, mask));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_mask = NULL;
    cuda::GpuMat mask;

    const char* keywords[] = { "mask", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_mask, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, mask));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "mask", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_mask, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, mask));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "mask", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_mask, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, mask, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_mask = NULL;
    cuda::GpuMat mask;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "mask", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_mask, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, mask, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "mask", "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_GpuMat.copyTo", (char**)keywords, &pyobj_mask, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->copyTo(dst, mask, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("copyTo");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_create(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;

    const char* keywords[] = { "rows", "cols", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:cuda_GpuMat.create", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->create(rows, cols, type));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;

    const char* keywords[] = { "size", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.create", (char**)keywords, &pyobj_size, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->create(size, type));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_cudaPtr(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    void* retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cudaPtr());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_defaultAllocator_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    GpuMat::Allocator* retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::GpuMat::defaultAllocator());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_depth(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->depth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_download(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(6);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_GpuMat.download", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->download(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_GpuMat.download", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->download(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:cuda_GpuMat.download", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->download(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.download", (char**)keywords, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->download(dst, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    cuda::GpuMat dst;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.download", (char**)keywords, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->download(dst, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "stream", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.download", (char**)keywords, &pyobj_stream, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->download(dst, stream));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("download");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_elemSize(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->elemSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_elemSize1(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->elemSize1());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_isContinuous(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isContinuous());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_locateROI(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_wholeSize = NULL;
    Size wholeSize;
    Napi::Value* pyobj_ofs = NULL;
    Point ofs;

    const char* keywords[] = { "wholeSize", "ofs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.locateROI", (char**)keywords, &pyobj_wholeSize, &pyobj_ofs) &&
        jsopencv_to_safe(info, pyobj_wholeSize, wholeSize, ArgInfo("wholeSize", 0)) &&
        jsopencv_to_safe(info, pyobj_ofs, ofs, ArgInfo("ofs", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->locateROI(wholeSize, ofs));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_release(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_reshape(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_cn = NULL;
    int cn=0;
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    GpuMat retval;

    const char* keywords[] = { "cn", "rows", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_GpuMat.reshape", (char**)keywords, &pyobj_cn, &pyobj_rows) &&
        jsopencv_to_safe(info, pyobj_cn, cn, ArgInfo("cn", 0)) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->reshape(cn, rows));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_row(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_y = NULL;
    int y=0;
    GpuMat retval;

    const char* keywords[] = { "y", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.row", (char**)keywords, &pyobj_y) &&
        jsopencv_to_safe(info, pyobj_y, y, ArgInfo("y", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->row(y));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_rowRange(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_startrow = NULL;
    int startrow=0;
    Napi::Value* pyobj_endrow = NULL;
    int endrow=0;
    GpuMat retval;

    const char* keywords[] = { "startrow", "endrow", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.rowRange", (char**)keywords, &pyobj_startrow, &pyobj_endrow) &&
        jsopencv_to_safe(info, pyobj_startrow, startrow, ArgInfo("startrow", 0)) &&
        jsopencv_to_safe(info, pyobj_endrow, endrow, ArgInfo("endrow", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->rowRange(startrow, endrow));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_r = NULL;
    Range r;
    GpuMat retval;

    const char* keywords[] = { "r", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.rowRange", (char**)keywords, &pyobj_r) &&
        jsopencv_to_safe(info, pyobj_r, r, ArgInfo("r", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->rowRange(r));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("rowRange");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_setDefaultAllocator_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_allocator = NULL;
    GpuMat_Allocator* allocator;

    const char* keywords[] = { "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.setDefaultAllocator", (char**)keywords, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        ERRWRAP2_NAPI(info, cv::cuda::GpuMat::setDefaultAllocator(allocator));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_setTo(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(8);

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    GpuMat retval;

    const char* keywords[] = { "s", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();
    GpuMat retval;

    const char* keywords[] = { "s", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, stream));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    GpuMat retval;

    const char* keywords[] = { "s", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, mask));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_mask = NULL;
    cuda::GpuMat mask;
    GpuMat retval;

    const char* keywords[] = { "s", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, mask));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    GpuMat retval;

    const char* keywords[] = { "s", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, mask));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();
    GpuMat retval;

    const char* keywords[] = { "s", "mask", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_mask, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, mask, stream));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_mask = NULL;
    cuda::GpuMat mask;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();
    GpuMat retval;

    const char* keywords[] = { "s", "mask", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_mask, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, mask, stream));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    Scalar s;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();
    GpuMat retval;

    const char* keywords[] = { "s", "mask", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:cuda_GpuMat.setTo", (char**)keywords, &pyobj_s, &pyobj_mask, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setTo(s, mask, stream));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setTo");

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_size(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->size());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_step1(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->step1());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_swap(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    Napi::Value* pyobj_mat = NULL;
    GpuMat mat;

    const char* keywords[] = { "mat", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.swap", (char**)keywords, &pyobj_mat) &&
        jsopencv_to_safe(info, pyobj_mat, mat, ArgInfo("mat", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->swap(mat));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_type(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->type());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_updateContinuityFlag(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->updateContinuityFlag());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_GpuMat_upload(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::GpuMat> * self1 = 0;
    if (!pyopencv_cuda_GpuMat_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_GpuMat' or its derivative)");
    Ptr<cv::cuda::GpuMat> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(6);

    {
    Napi::Value* pyobj_arr = NULL;
    Mat arr;

    const char* keywords[] = { "arr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.upload", (char**)keywords, &pyobj_arr) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upload(arr));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    cuda::GpuMat arr;

    const char* keywords[] = { "arr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.upload", (char**)keywords, &pyobj_arr) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upload(arr));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    UMat arr;

    const char* keywords[] = { "arr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_GpuMat.upload", (char**)keywords, &pyobj_arr) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upload(arr));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    Mat arr;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "arr", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.upload", (char**)keywords, &pyobj_arr, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upload(arr, stream));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    cuda::GpuMat arr;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "arr", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.upload", (char**)keywords, &pyobj_arr, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upload(arr, stream));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    UMat arr;
    Napi::Value* pyobj_stream = NULL;
    Stream stream=Stream::Null();

    const char* keywords[] = { "arr", "stream", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_GpuMat.upload", (char**)keywords, &pyobj_arr, &pyobj_stream) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_stream, stream, ArgInfo("stream", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upload(arr, stream));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("upload");

    return NULL;
}



// Tables (cuda_GpuMat)

static PyGetSetDef pyopencv_cuda_GpuMat_getseters[] =
{
    {(char*)"step", (getter)pyopencv_cuda_GpuMat_get_step, NULL, (char*)"step", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_GpuMat_methods[] =
{
    {"adjustROI", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_adjustROI, 0), "adjustROI(dtop, dbottom, dleft, dright) -> retval\n."},
    {"assignTo", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_assignTo, 0), "assignTo(m[, type]) -> None\n."},
    {"channels", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_channels, 0), "channels() -> retval\n."},
    {"clone", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_clone, 0), "clone() -> retval\n."},
    {"col", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_col, 0), "col(x) -> retval\n."},
    {"colRange", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_colRange, 0), "colRange(startcol, endcol) -> retval\n.   \n\n\n\ncolRange(r) -> retval\n."},
    {"convertTo", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_convertTo, 0), "convertTo(rtype[, dst]) -> dst\n.   \n\n\n\nconvertTo(rtype, stream[, dst]) -> dst\n.   \n\n\n\nconvertTo(rtype, alpha[, dst[, beta]]) -> dst\n.   \n\n\n\nconvertTo(rtype, alpha, stream[, dst]) -> dst\n.   \n\n\n\nconvertTo(rtype, alpha, beta, stream[, dst]) -> dst\n."},
    {"copyTo", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_copyTo, 0), "copyTo([, dst]) -> dst\n.   \n\n\n\ncopyTo(stream[, dst]) -> dst\n.   \n\n\n\ncopyTo(mask[, dst]) -> dst\n.   \n\n\n\ncopyTo(mask, stream[, dst]) -> dst\n."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_create, 0), "create(rows, cols, type) -> None\n.   \n\n\n\ncreate(size, type) -> None\n."},
    {"cudaPtr", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_cudaPtr, 0), "cudaPtr() -> retval\n."},
    {"defaultAllocator", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_defaultAllocator_static, METH_STATIC), "defaultAllocator() -> retval\n."},
    {"depth", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_depth, 0), "depth() -> retval\n."},
    {"download", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_download, 0), "download([, dst]) -> dst\n.   @brief Performs data download from GpuMat (Blocking call)\n.   \n.       This function copies data from device memory to host memory. As being a blocking call, it is\n.       guaranteed that the copy operation is finished when this function returns.\n\n\n\ndownload(stream[, dst]) -> dst\n.   @brief Performs data download from GpuMat (Non-Blocking call)\n.   \n.       This function copies data from device memory to host memory. As being a non-blocking call, this\n.       function may return even if the copy operation is not finished.\n.   \n.       The copy operation may be overlapped with operations in other non-default streams if \\p stream is\n.       not the default stream and \\p dst is HostMem allocated with HostMem::PAGE_LOCKED option."},
    {"elemSize", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_elemSize, 0), "elemSize() -> retval\n."},
    {"elemSize1", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_elemSize1, 0), "elemSize1() -> retval\n."},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_empty, 0), "empty() -> retval\n."},
    {"isContinuous", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_isContinuous, 0), "isContinuous() -> retval\n."},
    {"locateROI", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_locateROI, 0), "locateROI(wholeSize, ofs) -> None\n."},
    {"release", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_release, 0), "release() -> None\n."},
    {"reshape", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_reshape, 0), "reshape(cn[, rows]) -> retval\n."},
    {"row", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_row, 0), "row(y) -> retval\n."},
    {"rowRange", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_rowRange, 0), "rowRange(startrow, endrow) -> retval\n.   \n\n\n\nrowRange(r) -> retval\n."},
    {"setDefaultAllocator", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_setDefaultAllocator_static, METH_STATIC), "setDefaultAllocator(allocator) -> None\n."},
    {"setTo", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_setTo, 0), "setTo(s) -> retval\n.   \n\n\n\nsetTo(s, stream) -> retval\n.   \n\n\n\nsetTo(s, mask) -> retval\n.   \n\n\n\nsetTo(s, mask, stream) -> retval\n."},
    {"size", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_size, 0), "size() -> retval\n."},
    {"step1", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_step1, 0), "step1() -> retval\n."},
    {"swap", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_swap, 0), "swap(mat) -> None\n."},
    {"type", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_type, 0), "type() -> retval\n."},
    {"updateContinuityFlag", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_updateContinuityFlag, 0), "updateContinuityFlag() -> None\n."},
    {"upload", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_GpuMat_upload, 0), "upload(arr) -> None\n.   @brief Performs data upload to GpuMat (Blocking call)\n.   \n.       This function copies data from host memory to device memory. As being a blocking call, it is\n.       guaranteed that the copy operation is finished when this function returns.\n\n\n\nupload(arr, stream) -> None\n.   @brief Performs data upload to GpuMat (Non-Blocking call)\n.   \n.       This function copies data from host memory to device memory. As being a non-blocking call, this\n.       function may return even if the copy operation is not finished.\n.   \n.       The copy operation may be overlapped with operations in other non-default streams if \\p stream is\n.       not the default stream and \\p dst is HostMem allocated with HostMem::PAGE_LOCKED option."},

    {NULL,          NULL}
};

// Converter (cuda_GpuMat)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::GpuMat> >
{
    static PyObject* from(const Ptr<cv::cuda::GpuMat>& r)
    {
        return pyopencv_cuda_GpuMat_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::GpuMat>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::GpuMat> * dst_;
        if (pyopencv_cuda_GpuMat_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::GpuMat> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_GpuMatND (Generic)
//================================================================================

// GetSet (cuda_GpuMatND)



// Methods (cuda_GpuMatND)



// Tables (cuda_GpuMatND)

static PyGetSetDef pyopencv_cuda_GpuMatND_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_GpuMatND_methods[] =
{

    {NULL,          NULL}
};

// Converter (cuda_GpuMatND)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::GpuMatND> >
{
    static PyObject* from(const Ptr<cv::cuda::GpuMatND>& r)
    {
        return pyopencv_cuda_GpuMatND_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::GpuMatND>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::GpuMatND> * dst_;
        if (pyopencv_cuda_GpuMatND_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::GpuMatND> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_GpuMat_Allocator (Generic)
//================================================================================

// GetSet (cuda_GpuMat_Allocator)



// Methods (cuda_GpuMat_Allocator)



// Tables (cuda_GpuMat_Allocator)

static PyGetSetDef pyopencv_cuda_GpuMat_Allocator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_GpuMat_Allocator_methods[] =
{

    {NULL,          NULL}
};

// Converter (cuda_GpuMat_Allocator)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::GpuMat::Allocator> >
{
    static PyObject* from(const Ptr<cv::cuda::GpuMat::Allocator>& r)
    {
        return pyopencv_cuda_GpuMat_Allocator_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::GpuMat::Allocator>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::GpuMat::Allocator> * dst_;
        if (pyopencv_cuda_GpuMat_Allocator_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::GpuMat::Allocator> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_HostMem (Generic)
//================================================================================

// GetSet (cuda_HostMem)


static PyObject* pyopencv_cuda_HostMem_get_step(pyopencv_cuda_HostMem_t* p, void *closure)
{
    return jsopencv_from(p->v->step);
}


// Methods (cuda_HostMem)

static int pyopencv_cv_cuda_cuda_HostMem_HostMem(pyopencv_cuda_HostMem_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::cuda;

    pyPrepareArgumentConversionErrorsStorage(6);

    {
    Napi::Value* pyobj_alloc_type = NULL;
    HostMem_AllocType alloc_type=HostMem::AllocType::PAGE_LOCKED;

    const char* keywords[] = { "alloc_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:HostMem", (char**)keywords, &pyobj_alloc_type) &&
        jsopencv_to_safe(info, pyobj_alloc_type, alloc_type, ArgInfo("alloc_type", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::HostMem>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::HostMem(alloc_type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_alloc_type = NULL;
    HostMem_AllocType alloc_type=HostMem::AllocType::PAGE_LOCKED;

    const char* keywords[] = { "rows", "cols", "type", "alloc_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:HostMem", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type, &pyobj_alloc_type) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_alloc_type, alloc_type, ArgInfo("alloc_type", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::HostMem>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::HostMem(rows, cols, type, alloc_type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_alloc_type = NULL;
    HostMem_AllocType alloc_type=HostMem::AllocType::PAGE_LOCKED;

    const char* keywords[] = { "size", "type", "alloc_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:HostMem", (char**)keywords, &pyobj_size, &pyobj_type, &pyobj_alloc_type) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_alloc_type, alloc_type, ArgInfo("alloc_type", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::HostMem>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::HostMem(size, type, alloc_type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    Mat arr;
    Napi::Value* pyobj_alloc_type = NULL;
    HostMem_AllocType alloc_type=HostMem::AllocType::PAGE_LOCKED;

    const char* keywords[] = { "arr", "alloc_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:HostMem", (char**)keywords, &pyobj_arr, &pyobj_alloc_type) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_alloc_type, alloc_type, ArgInfo("alloc_type", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::HostMem>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::HostMem(arr, alloc_type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    cuda::GpuMat arr;
    Napi::Value* pyobj_alloc_type = NULL;
    HostMem_AllocType alloc_type=HostMem::AllocType::PAGE_LOCKED;

    const char* keywords[] = { "arr", "alloc_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:HostMem", (char**)keywords, &pyobj_arr, &pyobj_alloc_type) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_alloc_type, alloc_type, ArgInfo("alloc_type", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::HostMem>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::HostMem(arr, alloc_type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_arr = NULL;
    UMat arr;
    Napi::Value* pyobj_alloc_type = NULL;
    HostMem_AllocType alloc_type=HostMem::AllocType::PAGE_LOCKED;

    const char* keywords[] = { "arr", "alloc_type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:HostMem", (char**)keywords, &pyobj_arr, &pyobj_alloc_type) &&
        jsopencv_to_safe(info, pyobj_arr, arr, ArgInfo("arr", 0)) &&
        jsopencv_to_safe(info, pyobj_alloc_type, alloc_type, ArgInfo("alloc_type", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::HostMem>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::HostMem(arr, alloc_type)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("HostMem");

    return -1;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_channels(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->channels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_clone(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    HostMem retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->clone());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_create(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    Napi::Value* pyobj_cols = NULL;
    int cols=0;
    Napi::Value* pyobj_type = NULL;
    int type=0;

    const char* keywords[] = { "rows", "cols", "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:cuda_HostMem.create", (char**)keywords, &pyobj_rows, &pyobj_cols, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)) &&
        jsopencv_to_safe(info, pyobj_cols, cols, ArgInfo("cols", 0)) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->create(rows, cols, type));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_createMatHeader(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->createMatHeader());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_depth(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->depth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_elemSize(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->elemSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_elemSize1(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->elemSize1());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_isContinuous(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isContinuous());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_reshape(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    Napi::Value* pyobj_cn = NULL;
    int cn=0;
    Napi::Value* pyobj_rows = NULL;
    int rows=0;
    HostMem retval;

    const char* keywords[] = { "cn", "rows", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:cuda_HostMem.reshape", (char**)keywords, &pyobj_cn, &pyobj_rows) &&
        jsopencv_to_safe(info, pyobj_cn, cn, ArgInfo("cn", 0)) &&
        jsopencv_to_safe(info, pyobj_rows, rows, ArgInfo("rows", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->reshape(cn, rows));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_size(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    Size retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->size());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_step1(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->step1());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_swap(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    Napi::Value* pyobj_b = NULL;
    HostMem b;

    const char* keywords[] = { "b", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_HostMem.swap", (char**)keywords, &pyobj_b) &&
        jsopencv_to_safe(info, pyobj_b, b, ArgInfo("b", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->swap(b));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_HostMem_type(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::HostMem> * self1 = 0;
    if (!pyopencv_cuda_HostMem_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_HostMem' or its derivative)");
    Ptr<cv::cuda::HostMem> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->type());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (cuda_HostMem)

static PyGetSetDef pyopencv_cuda_HostMem_getseters[] =
{
    {(char*)"step", (getter)pyopencv_cuda_HostMem_get_step, NULL, (char*)"step", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_HostMem_methods[] =
{
    {"channels", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_channels, 0), "channels() -> retval\n."},
    {"clone", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_clone, 0), "clone() -> retval\n."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_create, 0), "create(rows, cols, type) -> None\n."},
    {"createMatHeader", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_createMatHeader, 0), "createMatHeader() -> retval\n."},
    {"depth", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_depth, 0), "depth() -> retval\n."},
    {"elemSize", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_elemSize, 0), "elemSize() -> retval\n."},
    {"elemSize1", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_elemSize1, 0), "elemSize1() -> retval\n."},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_empty, 0), "empty() -> retval\n."},
    {"isContinuous", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_isContinuous, 0), "isContinuous() -> retval\n.   @brief Maps CPU memory to GPU address space and creates the cuda::GpuMat header without reference counting\n.       for it.\n.   \n.       This can be done only if memory was allocated with the SHARED flag and if it is supported by the\n.       hardware. Laptops often share video and CPU memory, so address spaces can be mapped, which\n.       eliminates an extra copy."},
    {"reshape", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_reshape, 0), "reshape(cn[, rows]) -> retval\n."},
    {"size", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_size, 0), "size() -> retval\n."},
    {"step1", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_step1, 0), "step1() -> retval\n."},
    {"swap", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_swap, 0), "swap(b) -> None\n."},
    {"type", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_HostMem_type, 0), "type() -> retval\n."},

    {NULL,          NULL}
};

// Converter (cuda_HostMem)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::HostMem> >
{
    static PyObject* from(const Ptr<cv::cuda::HostMem>& r)
    {
        return pyopencv_cuda_HostMem_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::HostMem>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::HostMem> * dst_;
        if (pyopencv_cuda_HostMem_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::HostMem> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_SURF_CUDA (Generic)
//================================================================================

// GetSet (cuda_SURF_CUDA)


static PyObject* pyopencv_cuda_SURF_CUDA_get_extended(pyopencv_cuda_SURF_CUDA_t* p, void *closure)
{
    return jsopencv_from(p->v->extended);
}

static PyObject* pyopencv_cuda_SURF_CUDA_get_hessianThreshold(pyopencv_cuda_SURF_CUDA_t* p, void *closure)
{
    return jsopencv_from(p->v->hessianThreshold);
}

static PyObject* pyopencv_cuda_SURF_CUDA_get_keypointsRatio(pyopencv_cuda_SURF_CUDA_t* p, void *closure)
{
    return jsopencv_from(p->v->keypointsRatio);
}

static PyObject* pyopencv_cuda_SURF_CUDA_get_nOctaveLayers(pyopencv_cuda_SURF_CUDA_t* p, void *closure)
{
    return jsopencv_from(p->v->nOctaveLayers);
}

static PyObject* pyopencv_cuda_SURF_CUDA_get_nOctaves(pyopencv_cuda_SURF_CUDA_t* p, void *closure)
{
    return jsopencv_from(p->v->nOctaves);
}

static PyObject* pyopencv_cuda_SURF_CUDA_get_upright(pyopencv_cuda_SURF_CUDA_t* p, void *closure)
{
    return jsopencv_from(p->v->upright);
}


// Methods (cuda_SURF_CUDA)

static Napi::Value pyopencv_cv_cuda_cuda_SURF_CUDA_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj__hessianThreshold = NULL;
    double _hessianThreshold=0;
    Napi::Value* pyobj__nOctaves = NULL;
    int _nOctaves=4;
    Napi::Value* pyobj__nOctaveLayers = NULL;
    int _nOctaveLayers=2;
    Napi::Value* pyobj__extended = NULL;
    bool _extended=false;
    Napi::Value* pyobj__keypointsRatio = NULL;
    float _keypointsRatio=0.01f;
    Napi::Value* pyobj__upright = NULL;
    bool _upright=false;
    Ptr<SURF_CUDA> retval;

    const char* keywords[] = { "_hessianThreshold", "_nOctaves", "_nOctaveLayers", "_extended", "_keypointsRatio", "_upright", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOOOO:cuda_SURF_CUDA.create", (char**)keywords, &pyobj__hessianThreshold, &pyobj__nOctaves, &pyobj__nOctaveLayers, &pyobj__extended, &pyobj__keypointsRatio, &pyobj__upright) &&
        jsopencv_to_safe(info, pyobj__hessianThreshold, _hessianThreshold, ArgInfo("_hessianThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj__nOctaves, _nOctaves, ArgInfo("_nOctaves", 0)) &&
        jsopencv_to_safe(info, pyobj__nOctaveLayers, _nOctaveLayers, ArgInfo("_nOctaveLayers", 0)) &&
        jsopencv_to_safe(info, pyobj__extended, _extended, ArgInfo("_extended", 0)) &&
        jsopencv_to_safe(info, pyobj__keypointsRatio, _keypointsRatio, ArgInfo("_keypointsRatio", 0)) &&
        jsopencv_to_safe(info, pyobj__upright, _upright, ArgInfo("_upright", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::SURF_CUDA::create(_hessianThreshold, _nOctaves, _nOctaveLayers, _extended, _keypointsRatio, _upright));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_SURF_CUDA_defaultNorm(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::SURF_CUDA> * self1 = 0;
    if (!pyopencv_cuda_SURF_CUDA_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_SURF_CUDA' or its derivative)");
    Ptr<cv::cuda::SURF_CUDA> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->defaultNorm());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_SURF_CUDA_descriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::SURF_CUDA> * self1 = 0;
    if (!pyopencv_cuda_SURF_CUDA_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_SURF_CUDA' or its derivative)");
    Ptr<cv::cuda::SURF_CUDA> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->descriptorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_SURF_CUDA_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::SURF_CUDA> * self1 = 0;
    if (!pyopencv_cuda_SURF_CUDA_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_SURF_CUDA' or its derivative)");
    Ptr<cv::cuda::SURF_CUDA> _self_ = *(self1);
    Napi::Value* pyobj_img = NULL;
    GpuMat img;
    Napi::Value* pyobj_mask = NULL;
    GpuMat mask;
    Napi::Value* pyobj_keypoints = NULL;
    GpuMat keypoints;

    const char* keywords[] = { "img", "mask", "keypoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:cuda_SURF_CUDA.detect", (char**)keywords, &pyobj_img, &pyobj_mask, &pyobj_keypoints) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(img, mask, keypoints));
        return jsopencv_from(keypoints);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_SURF_CUDA_detectWithDescriptors(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::SURF_CUDA> * self1 = 0;
    if (!pyopencv_cuda_SURF_CUDA_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_SURF_CUDA' or its derivative)");
    Ptr<cv::cuda::SURF_CUDA> _self_ = *(self1);
    Napi::Value* pyobj_img = NULL;
    GpuMat img;
    Napi::Value* pyobj_mask = NULL;
    GpuMat mask;
    Napi::Value* pyobj_keypoints = NULL;
    GpuMat keypoints;
    Napi::Value* pyobj_descriptors = NULL;
    GpuMat descriptors;
    Napi::Value* pyobj_useProvidedKeypoints = NULL;
    bool useProvidedKeypoints=false;

    const char* keywords[] = { "img", "mask", "keypoints", "descriptors", "useProvidedKeypoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:cuda_SURF_CUDA.detectWithDescriptors", (char**)keywords, &pyobj_img, &pyobj_mask, &pyobj_keypoints, &pyobj_descriptors, &pyobj_useProvidedKeypoints) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        jsopencv_to_safe(info, pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) &&
        jsopencv_to_safe(info, pyobj_useProvidedKeypoints, useProvidedKeypoints, ArgInfo("useProvidedKeypoints", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectWithDescriptors(img, mask, keypoints, descriptors, useProvidedKeypoints));
        return Py_BuildValue("(NN)", jsopencv_from(keypoints), jsopencv_from(descriptors));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_SURF_CUDA_downloadKeypoints(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::SURF_CUDA> * self1 = 0;
    if (!pyopencv_cuda_SURF_CUDA_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_SURF_CUDA' or its derivative)");
    Ptr<cv::cuda::SURF_CUDA> _self_ = *(self1);
    Napi::Value* pyobj_keypointsGPU = NULL;
    GpuMat keypointsGPU;
    vector_KeyPoint keypoints;

    const char* keywords[] = { "keypointsGPU", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_SURF_CUDA.downloadKeypoints", (char**)keywords, &pyobj_keypointsGPU) &&
        jsopencv_to_safe(info, pyobj_keypointsGPU, keypointsGPU, ArgInfo("keypointsGPU", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->downloadKeypoints(keypointsGPU, keypoints));
        return jsopencv_from(keypoints);
    }

    return NULL;
}



// Tables (cuda_SURF_CUDA)

static PyGetSetDef pyopencv_cuda_SURF_CUDA_getseters[] =
{
    {(char*)"extended", (getter)pyopencv_cuda_SURF_CUDA_get_extended, NULL, (char*)"extended", NULL},
    {(char*)"hessianThreshold", (getter)pyopencv_cuda_SURF_CUDA_get_hessianThreshold, NULL, (char*)"hessianThreshold", NULL},
    {(char*)"keypointsRatio", (getter)pyopencv_cuda_SURF_CUDA_get_keypointsRatio, NULL, (char*)"keypointsRatio", NULL},
    {(char*)"nOctaveLayers", (getter)pyopencv_cuda_SURF_CUDA_get_nOctaveLayers, NULL, (char*)"nOctaveLayers", NULL},
    {(char*)"nOctaves", (getter)pyopencv_cuda_SURF_CUDA_get_nOctaves, NULL, (char*)"nOctaves", NULL},
    {(char*)"upright", (getter)pyopencv_cuda_SURF_CUDA_get_upright, NULL, (char*)"upright", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_SURF_CUDA_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_SURF_CUDA_create_static, METH_STATIC), "create(_hessianThreshold[, _nOctaves[, _nOctaveLayers[, _extended[, _keypointsRatio[, _upright]]]]]) -> retval\n.   @param _hessianThreshold Threshold for hessian keypoint detector used in SURF.\n.       @param _nOctaves Number of pyramid octaves the keypoint detector will use.\n.       @param _nOctaveLayers Number of octave layers within each octave.\n.       @param _extended Extended descriptor flag (true - use extended 128-element descriptors; false - use\n.       64-element descriptors).\n.       @param _keypointsRatio\n.       @param _upright Up-right or rotated features flag (true - do not compute orientation of features;\n.       false - compute orientation)."},
    {"defaultNorm", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_SURF_CUDA_defaultNorm, 0), "defaultNorm() -> retval\n."},
    {"descriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_SURF_CUDA_descriptorSize, 0), "descriptorSize() -> retval\n."},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_SURF_CUDA_detect, 0), "detect(img, mask[, keypoints]) -> keypoints\n.   @brief Finds the keypoints using fast hessian detector used in SURF\n.   \n.       @param img Source image, currently supports only CV_8UC1 images.\n.       @param mask A mask image same size as src and of type CV_8UC1.\n.       @param keypoints Detected keypoints."},
    {"detectWithDescriptors", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_SURF_CUDA_detectWithDescriptors, 0), "detectWithDescriptors(img, mask[, keypoints[, descriptors[, useProvidedKeypoints]]]) -> keypoints, descriptors\n.   @brief Finds the keypoints and computes their descriptors using fast hessian detector used in SURF\n.   \n.       @param img Source image, currently supports only CV_8UC1 images.\n.       @param mask A mask image same size as src and of type CV_8UC1.\n.       @param keypoints Detected keypoints.\n.       @param descriptors Keypoint descriptors.\n.       @param useProvidedKeypoints Compute descriptors for the user-provided keypoints and recompute keypoints direction."},
    {"downloadKeypoints", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_SURF_CUDA_downloadKeypoints, 0), "downloadKeypoints(keypointsGPU) -> keypoints\n."},

    {NULL,          NULL}
};

// Converter (cuda_SURF_CUDA)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::SURF_CUDA> >
{
    static PyObject* from(const Ptr<cv::cuda::SURF_CUDA>& r)
    {
        return pyopencv_cuda_SURF_CUDA_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::SURF_CUDA>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::SURF_CUDA> * dst_;
        if (pyopencv_cuda_SURF_CUDA_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::SURF_CUDA> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_Stream (Generic)
//================================================================================

// GetSet (cuda_Stream)



// Methods (cuda_Stream)

static int pyopencv_cv_cuda_cuda_Stream_Stream(pyopencv_cuda_Stream_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::cuda;

    pyPrepareArgumentConversionErrorsStorage(3);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::cuda::Stream>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::Stream()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_allocator = NULL;
    Ptr<GpuMat::Allocator> allocator;

    const char* keywords[] = { "allocator", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Stream", (char**)keywords, &pyobj_allocator) &&
        jsopencv_to_safe(info, pyobj_allocator, allocator, ArgInfo("allocator", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::Stream>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::Stream(allocator)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_cudaFlags = NULL;
    size_t cudaFlags=0;

    const char* keywords[] = { "cudaFlags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Stream", (char**)keywords, &pyobj_cudaFlags) &&
        jsopencv_to_safe(info, pyobj_cudaFlags, cudaFlags, ArgInfo("cudaFlags", 0)))
    {
        new (&(self->v)) Ptr<cv::cuda::Stream>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::cuda::Stream(cudaFlags)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Stream");

    return -1;
}

static Napi::Value pyopencv_cv_cuda_cuda_Stream_Null_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Stream retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::Stream::Null());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Stream_cudaPtr(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Stream> * self1 = 0;
    if (!pyopencv_cuda_Stream_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Stream' or its derivative)");
    Ptr<cv::cuda::Stream> _self_ = *(self1);
    void* retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cudaPtr());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Stream_queryIfComplete(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Stream> * self1 = 0;
    if (!pyopencv_cuda_Stream_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Stream' or its derivative)");
    Ptr<cv::cuda::Stream> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->queryIfComplete());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Stream_waitEvent(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Stream> * self1 = 0;
    if (!pyopencv_cuda_Stream_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Stream' or its derivative)");
    Ptr<cv::cuda::Stream> _self_ = *(self1);
    Napi::Value* pyobj_event = NULL;
    Event event;

    const char* keywords[] = { "event", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:cuda_Stream.waitEvent", (char**)keywords, &pyobj_event) &&
        jsopencv_to_safe(info, pyobj_event, event, ArgInfo("event", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->waitEvent(event));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_Stream_waitForCompletion(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;


    Ptr<cv::cuda::Stream> * self1 = 0;
    if (!pyopencv_cuda_Stream_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'cuda_Stream' or its derivative)");
    Ptr<cv::cuda::Stream> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->waitForCompletion());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (cuda_Stream)

static PyGetSetDef pyopencv_cuda_Stream_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_Stream_methods[] =
{
    {"Null", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Stream_Null_static, METH_STATIC), "Null() -> retval\n.   @brief Adds a callback to be called on the host after all currently enqueued items in the stream have\n.       completed.\n.   \n.       @note Callbacks must not make any CUDA API calls. Callbacks must not perform any synchronization\n.       that may depend on outstanding device work or other callbacks that are not mandated to run earlier.\n.       Callbacks without a mandated order (in independent streams) execute in undefined order and may be\n.       serialized."},
    {"cudaPtr", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Stream_cudaPtr, 0), "cudaPtr() -> retval\n."},
    {"queryIfComplete", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Stream_queryIfComplete, 0), "queryIfComplete() -> retval\n.   @brief Returns true if the current stream queue is finished. Otherwise, it returns false."},
    {"waitEvent", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Stream_waitEvent, 0), "waitEvent(event) -> None\n.   @brief Makes a compute stream wait on an event."},
    {"waitForCompletion", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_Stream_waitForCompletion, 0), "waitForCompletion() -> None\n.   @brief Blocks the current CPU thread until all operations in the stream are complete."},

    {NULL,          NULL}
};

// Converter (cuda_Stream)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::Stream> >
{
    static PyObject* from(const Ptr<cv::cuda::Stream>& r)
    {
        return pyopencv_cuda_Stream_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::Stream>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::Stream> * dst_;
        if (pyopencv_cuda_Stream_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::Stream> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// cuda_TargetArchs (Generic)
//================================================================================

// GetSet (cuda_TargetArchs)



// Methods (cuda_TargetArchs)

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_has_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.has", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::has(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_hasBin_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.hasBin", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::hasBin(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrGreater_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.hasEqualOrGreater", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::hasEqualOrGreater(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrGreaterBin_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.hasEqualOrGreaterBin", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::hasEqualOrGreaterBin(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrGreaterPtx_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.hasEqualOrGreaterPtx", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::hasEqualOrGreaterPtx(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrLessPtx_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.hasEqualOrLessPtx", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::hasEqualOrLessPtx(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_cuda_cuda_TargetArchs_hasPtx_static(const Napi::CallbackInfo &info)
{
    using namespace cv::cuda;

    Napi::Value* pyobj_major = NULL;
    int major=0;
    Napi::Value* pyobj_minor = NULL;
    int minor=0;
    bool retval;

    const char* keywords[] = { "major", "minor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:cuda_TargetArchs.hasPtx", (char**)keywords, &pyobj_major, &pyobj_minor) &&
        jsopencv_to_safe(info, pyobj_major, major, ArgInfo("major", 0)) &&
        jsopencv_to_safe(info, pyobj_minor, minor, ArgInfo("minor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::cuda::TargetArchs::hasPtx(major, minor));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (cuda_TargetArchs)

static PyGetSetDef pyopencv_cuda_TargetArchs_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_cuda_TargetArchs_methods[] =
{
    {"has", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_has_static, METH_STATIC), "has(major, minor) -> retval\n.   @brief There is a set of methods to check whether the module contains intermediate (PTX) or binary CUDA\n.       code for the given architecture(s):\n.   \n.       @param major Major compute capability version.\n.       @param minor Minor compute capability version."},
    {"hasBin", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_hasBin_static, METH_STATIC), "hasBin(major, minor) -> retval\n."},
    {"hasEqualOrGreater", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrGreater_static, METH_STATIC), "hasEqualOrGreater(major, minor) -> retval\n."},
    {"hasEqualOrGreaterBin", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrGreaterBin_static, METH_STATIC), "hasEqualOrGreaterBin(major, minor) -> retval\n."},
    {"hasEqualOrGreaterPtx", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrGreaterPtx_static, METH_STATIC), "hasEqualOrGreaterPtx(major, minor) -> retval\n."},
    {"hasEqualOrLessPtx", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_hasEqualOrLessPtx_static, METH_STATIC), "hasEqualOrLessPtx(major, minor) -> retval\n."},
    {"hasPtx", CV_JS_FN_WITH_KW_(pyopencv_cv_cuda_cuda_TargetArchs_hasPtx_static, METH_STATIC), "hasPtx(major, minor) -> retval\n."},

    {NULL,          NULL}
};

// Converter (cuda_TargetArchs)

template<>
struct PyOpenCV_Converter< Ptr<cv::cuda::TargetArchs> >
{
    static PyObject* from(const Ptr<cv::cuda::TargetArchs>& r)
    {
        return pyopencv_cuda_TargetArchs_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::cuda::TargetArchs>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::cuda::TargetArchs> * dst_;
        if (pyopencv_cuda_TargetArchs_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::cuda::TargetArchs> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_ClassificationModel (Generic)
//================================================================================

// GetSet (dnn_ClassificationModel)



// Methods (dnn_ClassificationModel)

static int pyopencv_cv_dnn_dnn_ClassificationModel_ClassificationModel(pyopencv_dnn_ClassificationModel_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ClassificationModel", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::ClassificationModel(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ClassificationModel", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::ClassificationModel(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("ClassificationModel");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_ClassificationModel_classify(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::ClassificationModel * self1 = 0;
    if (!pyopencv_dnn_ClassificationModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_ClassificationModel' or its derivative)");
    cv::dnn::ClassificationModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    int classId;
    float conf;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_ClassificationModel.classify", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->classify(frame, classId, conf));
        return Py_BuildValue("(NN)", jsopencv_from(classId), jsopencv_from(conf));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    int classId;
    float conf;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_ClassificationModel.classify", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->classify(frame, classId, conf));
        return Py_BuildValue("(NN)", jsopencv_from(classId), jsopencv_from(conf));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("classify");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_ClassificationModel_getEnableSoftmaxPostProcessing(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::ClassificationModel * self1 = 0;
    if (!pyopencv_dnn_ClassificationModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_ClassificationModel' or its derivative)");
    cv::dnn::ClassificationModel* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEnableSoftmaxPostProcessing());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_ClassificationModel_setEnableSoftmaxPostProcessing(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::ClassificationModel * self1 = 0;
    if (!pyopencv_dnn_ClassificationModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_ClassificationModel' or its derivative)");
    cv::dnn::ClassificationModel* _self_ = (self1);
    Napi::Value* pyobj_enable = NULL;
    bool enable=0;
    ClassificationModel retval;

    const char* keywords[] = { "enable", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_ClassificationModel.setEnableSoftmaxPostProcessing", (char**)keywords, &pyobj_enable) &&
        jsopencv_to_safe(info, pyobj_enable, enable, ArgInfo("enable", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setEnableSoftmaxPostProcessing(enable));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_ClassificationModel)

static PyGetSetDef pyopencv_dnn_ClassificationModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_ClassificationModel_methods[] =
{
    {"classify", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_ClassificationModel_classify, 0), "classify(frame) -> classId, conf\n.   @overload"},
    {"getEnableSoftmaxPostProcessing", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_ClassificationModel_getEnableSoftmaxPostProcessing, 0), "getEnableSoftmaxPostProcessing() -> retval\n.   * @brief Get enable/disable softmax post processing option.\n.             *\n.             * This option defaults to false, softmax post processing is not applied within the classify() function."},
    {"setEnableSoftmaxPostProcessing", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_ClassificationModel_setEnableSoftmaxPostProcessing, 0), "setEnableSoftmaxPostProcessing(enable) -> retval\n.   * @brief Set enable/disable softmax post processing option.\n.             *\n.             * If this option is true, softmax is applied after forward inference within the classify() function\n.             * to convert the confidences range to [0.0-1.0].\n.             * This function allows you to toggle this behavior.\n.             * Please turn true when not contain softmax layer in model.\n.             * @param[in] enable Set enable softmax post processing within the classify() function."},

    {NULL,          NULL}
};

// Converter (dnn_ClassificationModel)

template<>
struct PyOpenCV_Converter< cv::dnn::ClassificationModel >
{
    static PyObject* from(const cv::dnn::ClassificationModel& r)
    {
        return pyopencv_dnn_ClassificationModel_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::ClassificationModel& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::ClassificationModel * dst_;
        if (pyopencv_dnn_ClassificationModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::ClassificationModel for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_DetectionModel (Generic)
//================================================================================

// GetSet (dnn_DetectionModel)



// Methods (dnn_DetectionModel)

static int pyopencv_cv_dnn_dnn_DetectionModel_DetectionModel(pyopencv_dnn_DetectionModel_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:DetectionModel", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::DetectionModel(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DetectionModel", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::DetectionModel(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("DetectionModel");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_DetectionModel_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::DetectionModel * self1 = 0;
    if (!pyopencv_dnn_DetectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DetectionModel' or its derivative)");
    cv::dnn::DetectionModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    vector_int classIds;
    vector_float confidences;
    vector_Rect boxes;
    Napi::Value* pyobj_confThreshold = NULL;
    float confThreshold=0.5f;
    Napi::Value* pyobj_nmsThreshold = NULL;
    float nmsThreshold=0.0f;

    const char* keywords[] = { "frame", "confThreshold", "nmsThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:dnn_DetectionModel.detect", (char**)keywords, &pyobj_frame, &pyobj_confThreshold, &pyobj_nmsThreshold) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_confThreshold, confThreshold, ArgInfo("confThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nmsThreshold, nmsThreshold, ArgInfo("nmsThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(frame, classIds, confidences, boxes, confThreshold, nmsThreshold));
        return Py_BuildValue("(NNN)", jsopencv_from(classIds), jsopencv_from(confidences), jsopencv_from(boxes));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    vector_int classIds;
    vector_float confidences;
    vector_Rect boxes;
    Napi::Value* pyobj_confThreshold = NULL;
    float confThreshold=0.5f;
    Napi::Value* pyobj_nmsThreshold = NULL;
    float nmsThreshold=0.0f;

    const char* keywords[] = { "frame", "confThreshold", "nmsThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:dnn_DetectionModel.detect", (char**)keywords, &pyobj_frame, &pyobj_confThreshold, &pyobj_nmsThreshold) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_confThreshold, confThreshold, ArgInfo("confThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nmsThreshold, nmsThreshold, ArgInfo("nmsThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(frame, classIds, confidences, boxes, confThreshold, nmsThreshold));
        return Py_BuildValue("(NNN)", jsopencv_from(classIds), jsopencv_from(confidences), jsopencv_from(boxes));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DetectionModel_getNmsAcrossClasses(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::DetectionModel * self1 = 0;
    if (!pyopencv_dnn_DetectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DetectionModel' or its derivative)");
    cv::dnn::DetectionModel* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNmsAcrossClasses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DetectionModel_setNmsAcrossClasses(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::DetectionModel * self1 = 0;
    if (!pyopencv_dnn_DetectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DetectionModel' or its derivative)");
    cv::dnn::DetectionModel* _self_ = (self1);
    Napi::Value* pyobj_value = NULL;
    bool value=0;
    DetectionModel retval;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_DetectionModel.setNmsAcrossClasses", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setNmsAcrossClasses(value));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_DetectionModel)

static PyGetSetDef pyopencv_dnn_DetectionModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_DetectionModel_methods[] =
{
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DetectionModel_detect, 0), "detect(frame[, confThreshold[, nmsThreshold]]) -> classIds, confidences, boxes\n.   @brief Given the @p input frame, create input blob, run net and return result detections.\n.             *  @param[in]  frame  The input image.\n.             *  @param[out] classIds Class indexes in result detection.\n.             *  @param[out] confidences A set of corresponding confidences.\n.             *  @param[out] boxes A set of bounding boxes.\n.             *  @param[in] confThreshold A threshold used to filter boxes by confidences.\n.             *  @param[in] nmsThreshold A threshold used in non maximum suppression."},
    {"getNmsAcrossClasses", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DetectionModel_getNmsAcrossClasses, 0), "getNmsAcrossClasses() -> retval\n.   * @brief Getter for nmsAcrossClasses. This variable defaults to false,\n.             * such that when non max suppression is used during the detect() function, it will do so only per-class"},
    {"setNmsAcrossClasses", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DetectionModel_setNmsAcrossClasses, 0), "setNmsAcrossClasses(value) -> retval\n.   * @brief nmsAcrossClasses defaults to false,\n.             * such that when non max suppression is used during the detect() function, it will do so per-class.\n.             * This function allows you to toggle this behaviour.\n.             * @param[in] value The new value for nmsAcrossClasses"},

    {NULL,          NULL}
};

// Converter (dnn_DetectionModel)

template<>
struct PyOpenCV_Converter< cv::dnn::DetectionModel >
{
    static PyObject* from(const cv::dnn::DetectionModel& r)
    {
        return pyopencv_dnn_DetectionModel_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::DetectionModel& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::DetectionModel * dst_;
        if (pyopencv_dnn_DetectionModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::DetectionModel for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_DictValue (Generic)
//================================================================================

// GetSet (dnn_DictValue)



// Methods (dnn_DictValue)

static int pyopencv_cv_dnn_dnn_DictValue_DictValue(pyopencv_dnn_DictValue_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_i = NULL;
    int i=0;

    const char* keywords[] = { "i", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DictValue", (char**)keywords, &pyobj_i) &&
        jsopencv_to_safe(info, pyobj_i, i, ArgInfo("i", 0)))
    {
        new (&(self->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::dnn::DictValue(i)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_p = NULL;
    double p=0;

    const char* keywords[] = { "p", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DictValue", (char**)keywords, &pyobj_p) &&
        jsopencv_to_safe(info, pyobj_p, p, ArgInfo("p", 0)))
    {
        new (&(self->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::dnn::DictValue(p)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_s = NULL;
    String s;

    const char* keywords[] = { "s", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:DictValue", (char**)keywords, &pyobj_s) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)))
    {
        new (&(self->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::dnn::DictValue(s)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("DictValue");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_DictValue_getIntValue(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::DictValue> * self1 = 0;
    if (!pyopencv_dnn_DictValue_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    Ptr<cv::dnn::DictValue> _self_ = *(self1);
    Napi::Value* pyobj_idx = NULL;
    int idx=-1;
    int retval;

    const char* keywords[] = { "idx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:dnn_DictValue.getIntValue", (char**)keywords, &pyobj_idx) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIntValue(idx));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DictValue_getRealValue(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::DictValue> * self1 = 0;
    if (!pyopencv_dnn_DictValue_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    Ptr<cv::dnn::DictValue> _self_ = *(self1);
    Napi::Value* pyobj_idx = NULL;
    int idx=-1;
    double retval;

    const char* keywords[] = { "idx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:dnn_DictValue.getRealValue", (char**)keywords, &pyobj_idx) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRealValue(idx));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DictValue_getStringValue(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::DictValue> * self1 = 0;
    if (!pyopencv_dnn_DictValue_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    Ptr<cv::dnn::DictValue> _self_ = *(self1);
    Napi::Value* pyobj_idx = NULL;
    int idx=-1;
    String retval;

    const char* keywords[] = { "idx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:dnn_DictValue.getStringValue", (char**)keywords, &pyobj_idx) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getStringValue(idx));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DictValue_isInt(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::DictValue> * self1 = 0;
    if (!pyopencv_dnn_DictValue_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    Ptr<cv::dnn::DictValue> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isInt());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DictValue_isReal(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::DictValue> * self1 = 0;
    if (!pyopencv_dnn_DictValue_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    Ptr<cv::dnn::DictValue> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isReal());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_DictValue_isString(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::DictValue> * self1 = 0;
    if (!pyopencv_dnn_DictValue_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    Ptr<cv::dnn::DictValue> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isString());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_DictValue)

static PyGetSetDef pyopencv_dnn_DictValue_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_DictValue_methods[] =
{
    {"getIntValue", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_getIntValue, 0), "getIntValue([, idx]) -> retval\n."},
    {"getRealValue", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_getRealValue, 0), "getRealValue([, idx]) -> retval\n."},
    {"getStringValue", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_getStringValue, 0), "getStringValue([, idx]) -> retval\n."},
    {"isInt", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_isInt, 0), "isInt() -> retval\n."},
    {"isReal", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_isReal, 0), "isReal() -> retval\n."},
    {"isString", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_isString, 0), "isString() -> retval\n."},

    {NULL,          NULL}
};

// Converter (dnn_DictValue)

template<>
struct PyOpenCV_Converter< Ptr<cv::dnn::DictValue> >
{
    static PyObject* from(const Ptr<cv::dnn::DictValue>& r)
    {
        return pyopencv_dnn_DictValue_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::dnn::DictValue>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::dnn::DictValue> * dst_;
        if (pyopencv_dnn_DictValue_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::dnn::DictValue> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_KeypointsModel (Generic)
//================================================================================

// GetSet (dnn_KeypointsModel)



// Methods (dnn_KeypointsModel)

static int pyopencv_cv_dnn_dnn_KeypointsModel_KeypointsModel(pyopencv_dnn_KeypointsModel_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:KeypointsModel", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::KeypointsModel(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:KeypointsModel", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::KeypointsModel(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("KeypointsModel");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_KeypointsModel_estimate(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::KeypointsModel * self1 = 0;
    if (!pyopencv_dnn_KeypointsModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_KeypointsModel' or its derivative)");
    cv::dnn::KeypointsModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    Napi::Value* pyobj_thresh = NULL;
    float thresh=0.5;
    std::vector<Point2f> retval;

    const char* keywords[] = { "frame", "thresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_KeypointsModel.estimate", (char**)keywords, &pyobj_frame, &pyobj_thresh) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_thresh, thresh, ArgInfo("thresh", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->estimate(frame, thresh));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    Napi::Value* pyobj_thresh = NULL;
    float thresh=0.5;
    std::vector<Point2f> retval;

    const char* keywords[] = { "frame", "thresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_KeypointsModel.estimate", (char**)keywords, &pyobj_frame, &pyobj_thresh) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_thresh, thresh, ArgInfo("thresh", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->estimate(frame, thresh));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("estimate");

    return NULL;
}



// Tables (dnn_KeypointsModel)

static PyGetSetDef pyopencv_dnn_KeypointsModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_KeypointsModel_methods[] =
{
    {"estimate", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_KeypointsModel_estimate, 0), "estimate(frame[, thresh]) -> retval\n.   @brief Given the @p input frame, create input blob, run net\n.             *  @param[in]  frame  The input image.\n.             *  @param thresh minimum confidence threshold to select a keypoint\n.             *  @returns a vector holding the x and y coordinates of each detected keypoint\n.             *"},

    {NULL,          NULL}
};

// Converter (dnn_KeypointsModel)

template<>
struct PyOpenCV_Converter< cv::dnn::KeypointsModel >
{
    static PyObject* from(const cv::dnn::KeypointsModel& r)
    {
        return pyopencv_dnn_KeypointsModel_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::KeypointsModel& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::KeypointsModel * dst_;
        if (pyopencv_dnn_KeypointsModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::KeypointsModel for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_Layer (Generic)
//================================================================================

// GetSet (dnn_Layer)


static PyObject* pyopencv_dnn_Layer_get_blobs(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (!_self_)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return jsopencv_from(_self_->blobs);
}

static int pyopencv_dnn_Layer_set_blobs(pyopencv_dnn_Layer_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the blobs attribute");
        return -1;
    }
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (!_self_)
    {
        failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
        return -1;
    }
    return pyopencv_to_safe(value, _self_->blobs, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_dnn_Layer_get_name(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (!_self_)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return jsopencv_from(_self_->name);
}

static PyObject* pyopencv_dnn_Layer_get_preferableTarget(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (!_self_)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return jsopencv_from(_self_->preferableTarget);
}

static PyObject* pyopencv_dnn_Layer_get_type(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (!_self_)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return jsopencv_from(_self_->type);
}


// Methods (dnn_Layer)

static Napi::Value pyopencv_cv_dnn_dnn_Layer_finalize(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::Layer> * self1 = 0;
    if (!pyopencv_dnn_Layer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Layer' or its derivative)");
    Ptr<cv::dnn::Layer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_inputs = NULL;
    vector_Mat inputs;
    Napi::Value* pyobj_outputs = NULL;
    vector_Mat outputs;

    const char* keywords[] = { "inputs", "outputs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Layer.finalize", (char**)keywords, &pyobj_inputs, &pyobj_outputs) &&
        jsopencv_to_safe(info, pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        jsopencv_to_safe(info, pyobj_outputs, outputs, ArgInfo("outputs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->finalize(inputs, outputs));
        return jsopencv_from(outputs);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_inputs = NULL;
    vector_UMat inputs;
    Napi::Value* pyobj_outputs = NULL;
    vector_UMat outputs;

    const char* keywords[] = { "inputs", "outputs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Layer.finalize", (char**)keywords, &pyobj_inputs, &pyobj_outputs) &&
        jsopencv_to_safe(info, pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        jsopencv_to_safe(info, pyobj_outputs, outputs, ArgInfo("outputs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->finalize(inputs, outputs));
        return jsopencv_from(outputs);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("finalize");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Layer_outputNameToIndex(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::Layer> * self1 = 0;
    if (!pyopencv_dnn_Layer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Layer' or its derivative)");
    Ptr<cv::dnn::Layer> _self_ = *(self1);
    Napi::Value* pyobj_outputName = NULL;
    String outputName;
    int retval;

    const char* keywords[] = { "outputName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Layer.outputNameToIndex", (char**)keywords, &pyobj_outputName) &&
        jsopencv_to_safe(info, pyobj_outputName, outputName, ArgInfo("outputName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->outputNameToIndex(outputName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Layer_run(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    Ptr<cv::dnn::Layer> * self1 = 0;
    if (!pyopencv_dnn_Layer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Layer' or its derivative)");
    Ptr<cv::dnn::Layer> _self_ = *(self1);
    Napi::Value* pyobj_inputs = NULL;
    vector_Mat inputs;
    Napi::Value* pyobj_outputs = NULL;
    vector_Mat outputs;
    Napi::Value* pyobj_internals = NULL;
    vector_Mat internals;

    const char* keywords[] = { "inputs", "internals", "outputs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:dnn_Layer.run", (char**)keywords, &pyobj_inputs, &pyobj_internals, &pyobj_outputs) &&
        jsopencv_to_safe(info, pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        jsopencv_to_safe(info, pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        jsopencv_to_safe(info, pyobj_internals, internals, ArgInfo("internals", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->run(inputs, outputs, internals));
        return Py_BuildValue("(NN)", jsopencv_from(outputs), jsopencv_from(internals));
    }

    return NULL;
}



// Tables (dnn_Layer)

static PyGetSetDef pyopencv_dnn_Layer_getseters[] =
{
    {(char*)"blobs", (getter)pyopencv_dnn_Layer_get_blobs, (setter)pyopencv_dnn_Layer_set_blobs, (char*)"blobs", NULL},
    {(char*)"name", (getter)pyopencv_dnn_Layer_get_name, NULL, (char*)"name", NULL},
    {(char*)"preferableTarget", (getter)pyopencv_dnn_Layer_get_preferableTarget, NULL, (char*)"preferableTarget", NULL},
    {(char*)"type", (getter)pyopencv_dnn_Layer_get_type, NULL, (char*)"type", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_Layer_methods[] =
{
    {"finalize", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Layer_finalize, 0), "finalize(inputs[, outputs]) -> outputs\n.   @brief Computes and sets internal parameters according to inputs, outputs and blobs.\n.            *  @param[in]  inputs  vector of already allocated input blobs\n.            *  @param[out] outputs vector of already allocated output blobs\n.            *\n.            * If this method is called after network has allocated all memory for input and output blobs\n.            * and before inferencing."},
    {"outputNameToIndex", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Layer_outputNameToIndex, 0), "outputNameToIndex(outputName) -> retval\n.   @brief Returns index of output blob in output array.\n.            *  @see inputNameToIndex()"},
    {"run", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Layer_run, 0), "run(inputs, internals[, outputs]) -> outputs, internals\n.   @brief Allocates layer and computes output.\n.            *  @deprecated This method will be removed in the future release."},

    {NULL,          NULL}
};

// Converter (dnn_Layer)

template<>
struct PyOpenCV_Converter< Ptr<cv::dnn::Layer> >
{
    static PyObject* from(const Ptr<cv::dnn::Layer>& r)
    {
        return pyopencv_dnn_Layer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::dnn::Layer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::dnn::Layer> * dst_;
        if (pyopencv_dnn_Layer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::dnn::Layer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_Model (Generic)
//================================================================================

// GetSet (dnn_Model)



// Methods (dnn_Model)

static int pyopencv_cv_dnn_dnn_Model_Model(pyopencv_dnn_Model_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:Model", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::Model(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:Model", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::Model(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Model");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_predict(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    Napi::Value* pyobj_outs = NULL;
    vector_Mat outs;

    const char* keywords[] = { "frame", "outs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Model.predict", (char**)keywords, &pyobj_frame, &pyobj_outs) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_outs, outs, ArgInfo("outs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->predict(frame, outs));
        return jsopencv_from(outs);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    Napi::Value* pyobj_outs = NULL;
    vector_UMat outs;

    const char* keywords[] = { "frame", "outs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Model.predict", (char**)keywords, &pyobj_frame, &pyobj_outs) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_outs, outs, ArgInfo("outs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->predict(frame, outs));
        return jsopencv_from(outs);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setInputCrop(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_crop = NULL;
    bool crop=0;
    Model retval;

    const char* keywords[] = { "crop", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setInputCrop", (char**)keywords, &pyobj_crop) &&
        jsopencv_to_safe(info, pyobj_crop, crop, ArgInfo("crop", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInputCrop(crop));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setInputMean(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_mean = NULL;
    Scalar mean;
    Model retval;

    const char* keywords[] = { "mean", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setInputMean", (char**)keywords, &pyobj_mean) &&
        jsopencv_to_safe(info, pyobj_mean, mean, ArgInfo("mean", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInputMean(mean));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setInputParams(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_scale = NULL;
    double scale=1.0;
    Napi::Value* pyobj_size = NULL;
    Size size;
    Napi::Value* pyobj_mean = NULL;
    Scalar mean;
    Napi::Value* pyobj_swapRB = NULL;
    bool swapRB=false;
    Napi::Value* pyobj_crop = NULL;
    bool crop=false;

    const char* keywords[] = { "scale", "size", "mean", "swapRB", "crop", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:dnn_Model.setInputParams", (char**)keywords, &pyobj_scale, &pyobj_size, &pyobj_mean, &pyobj_swapRB, &pyobj_crop) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)) &&
        jsopencv_to_safe(info, pyobj_mean, mean, ArgInfo("mean", 0)) &&
        jsopencv_to_safe(info, pyobj_swapRB, swapRB, ArgInfo("swapRB", 0)) &&
        jsopencv_to_safe(info, pyobj_crop, crop, ArgInfo("crop", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInputParams(scale, size, mean, swapRB, crop));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setInputScale(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_scale = NULL;
    double scale=0;
    Model retval;

    const char* keywords[] = { "scale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setInputScale", (char**)keywords, &pyobj_scale) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInputScale(scale));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setInputSize(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_size = NULL;
    Size size;
    Model retval;

    const char* keywords[] = { "size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setInputSize", (char**)keywords, &pyobj_size) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInputSize(size));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_width = NULL;
    int width=0;
    Napi::Value* pyobj_height = NULL;
    int height=0;
    Model retval;

    const char* keywords[] = { "width", "height", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Model.setInputSize", (char**)keywords, &pyobj_width, &pyobj_height) &&
        jsopencv_to_safe(info, pyobj_width, width, ArgInfo("width", 0)) &&
        jsopencv_to_safe(info, pyobj_height, height, ArgInfo("height", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInputSize(width, height));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setInputSize");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setInputSwapRB(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_swapRB = NULL;
    bool swapRB=0;
    Model retval;

    const char* keywords[] = { "swapRB", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setInputSwapRB", (char**)keywords, &pyobj_swapRB) &&
        jsopencv_to_safe(info, pyobj_swapRB, swapRB, ArgInfo("swapRB", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setInputSwapRB(swapRB));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setPreferableBackend(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_backendId = NULL;
    dnn_Backend backendId=static_cast<dnn_Backend>(0);
    Model retval;

    const char* keywords[] = { "backendId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setPreferableBackend", (char**)keywords, &pyobj_backendId) &&
        jsopencv_to_safe(info, pyobj_backendId, backendId, ArgInfo("backendId", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setPreferableBackend(backendId));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Model_setPreferableTarget(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Model * self1 = 0;
    if (!pyopencv_dnn_Model_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Model' or its derivative)");
    cv::dnn::Model* _self_ = (self1);
    Napi::Value* pyobj_targetId = NULL;
    dnn_Target targetId=static_cast<dnn_Target>(0);
    Model retval;

    const char* keywords[] = { "targetId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Model.setPreferableTarget", (char**)keywords, &pyobj_targetId) &&
        jsopencv_to_safe(info, pyobj_targetId, targetId, ArgInfo("targetId", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setPreferableTarget(targetId));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_Model)

static PyGetSetDef pyopencv_dnn_Model_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_Model_methods[] =
{
    {"predict", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_predict, 0), "predict(frame[, outs]) -> outs\n.   @brief Given the @p input frame, create input blob, run net and return the output @p blobs.\n.             *  @param[in]  frame  The input image.\n.             *  @param[out] outs Allocated output blobs, which will store results of the computation."},
    {"setInputCrop", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setInputCrop, 0), "setInputCrop(crop) -> retval\n.   @brief Set flag crop for frame.\n.             *  @param[in] crop Flag which indicates whether image will be cropped after resize or not."},
    {"setInputMean", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setInputMean, 0), "setInputMean(mean) -> retval\n.   @brief Set mean value for frame.\n.             *  @param[in] mean Scalar with mean values which are subtracted from channels."},
    {"setInputParams", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setInputParams, 0), "setInputParams([, scale[, size[, mean[, swapRB[, crop]]]]]) -> None\n.   @brief Set preprocessing parameters for frame.\n.            *  @param[in] size New input size.\n.            *  @param[in] mean Scalar with mean values which are subtracted from channels.\n.            *  @param[in] scale Multiplier for frame values.\n.            *  @param[in] swapRB Flag which indicates that swap first and last channels.\n.            *  @param[in] crop Flag which indicates whether image will be cropped after resize or not.\n.            *  blob(n, c, y, x) = scale * resize( frame(y, x, c) ) - mean(c) )"},
    {"setInputScale", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setInputScale, 0), "setInputScale(scale) -> retval\n.   @brief Set scalefactor value for frame.\n.             *  @param[in] scale Multiplier for frame values."},
    {"setInputSize", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setInputSize, 0), "setInputSize(size) -> retval\n.   @brief Set input size for frame.\n.             *  @param[in] size New input size.\n.             *  @note If shape of the new blob less than 0, then frame size not change.\n\n\n\nsetInputSize(width, height) -> retval\n.   @overload\n.            *  @param[in] width New input width.\n.            *  @param[in] height New input height."},
    {"setInputSwapRB", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setInputSwapRB, 0), "setInputSwapRB(swapRB) -> retval\n.   @brief Set flag swapRB for frame.\n.             *  @param[in] swapRB Flag which indicates that swap first and last channels."},
    {"setPreferableBackend", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setPreferableBackend, 0), "setPreferableBackend(backendId) -> retval\n."},
    {"setPreferableTarget", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Model_setPreferableTarget, 0), "setPreferableTarget(targetId) -> retval\n."},

    {NULL,          NULL}
};

// Converter (dnn_Model)

template<>
struct PyOpenCV_Converter< cv::dnn::Model >
{
    static PyObject* from(const cv::dnn::Model& r)
    {
        return pyopencv_dnn_Model_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::Model& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::Model * dst_;
        if (pyopencv_dnn_Model_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::Model for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_Net (Generic)
//================================================================================

// GetSet (dnn_Net)



// Methods (dnn_Net)

static int pyopencv_cv_dnn_dnn_Net_Net(pyopencv_dnn_Net_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::Net());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_connect(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_outPin = NULL;
    String outPin;
    Napi::Value* pyobj_inpPin = NULL;
    String inpPin;

    const char* keywords[] = { "outPin", "inpPin", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.connect", (char**)keywords, &pyobj_outPin, &pyobj_inpPin) &&
        jsopencv_to_safe(info, pyobj_outPin, outPin, ArgInfo("outPin", 0)) &&
        jsopencv_to_safe(info, pyobj_inpPin, inpPin, ArgInfo("inpPin", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->connect(outPin, inpPin));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_dump(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->dump());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_dumpToFile(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_path = NULL;
    String path;

    const char* keywords[] = { "path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.dumpToFile", (char**)keywords, &pyobj_path) &&
        jsopencv_to_safe(info, pyobj_path, path, ArgInfo("path", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->dumpToFile(path));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_enableFusion(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_fusion = NULL;
    bool fusion=0;

    const char* keywords[] = { "fusion", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.enableFusion", (char**)keywords, &pyobj_fusion) &&
        jsopencv_to_safe(info, pyobj_fusion, fusion, ArgInfo("fusion", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->enableFusion(fusion));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_enableWinograd(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_useWinograd = NULL;
    bool useWinograd=0;

    const char* keywords[] = { "useWinograd", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.enableWinograd", (char**)keywords, &pyobj_useWinograd) &&
        jsopencv_to_safe(info, pyobj_useWinograd, useWinograd, ArgInfo("useWinograd", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->enableWinograd(useWinograd));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_forward(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(5);

    {
    Napi::Value* pyobj_outputName = NULL;
    String outputName;
    Mat retval;

    const char* keywords[] = { "outputName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:dnn_Net.forward", (char**)keywords, &pyobj_outputName) &&
        jsopencv_to_safe(info, pyobj_outputName, outputName, ArgInfo("outputName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->forward(outputName));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_outputBlobs = NULL;
    vector_Mat outputBlobs;
    Napi::Value* pyobj_outputName = NULL;
    String outputName;

    const char* keywords[] = { "outputBlobs", "outputName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:dnn_Net.forward", (char**)keywords, &pyobj_outputBlobs, &pyobj_outputName) &&
        jsopencv_to_safe(info, pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        jsopencv_to_safe(info, pyobj_outputName, outputName, ArgInfo("outputName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->forward(outputBlobs, outputName));
        return jsopencv_from(outputBlobs);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_outputBlobs = NULL;
    vector_UMat outputBlobs;
    Napi::Value* pyobj_outputName = NULL;
    String outputName;

    const char* keywords[] = { "outputBlobs", "outputName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:dnn_Net.forward", (char**)keywords, &pyobj_outputBlobs, &pyobj_outputName) &&
        jsopencv_to_safe(info, pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        jsopencv_to_safe(info, pyobj_outputName, outputName, ArgInfo("outputName", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->forward(outputBlobs, outputName));
        return jsopencv_from(outputBlobs);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_outputBlobs = NULL;
    vector_Mat outputBlobs;
    Napi::Value* pyobj_outBlobNames = NULL;
    vector_String outBlobNames;

    const char* keywords[] = { "outBlobNames", "outputBlobs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Net.forward", (char**)keywords, &pyobj_outBlobNames, &pyobj_outputBlobs) &&
        jsopencv_to_safe(info, pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        jsopencv_to_safe(info, pyobj_outBlobNames, outBlobNames, ArgInfo("outBlobNames", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->forward(outputBlobs, outBlobNames));
        return jsopencv_from(outputBlobs);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_outputBlobs = NULL;
    vector_UMat outputBlobs;
    Napi::Value* pyobj_outBlobNames = NULL;
    vector_String outBlobNames;

    const char* keywords[] = { "outBlobNames", "outputBlobs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Net.forward", (char**)keywords, &pyobj_outBlobNames, &pyobj_outputBlobs) &&
        jsopencv_to_safe(info, pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        jsopencv_to_safe(info, pyobj_outBlobNames, outBlobNames, ArgInfo("outBlobNames", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->forward(outputBlobs, outBlobNames));
        return jsopencv_from(outputBlobs);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("forward");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_forwardAndRetrieve(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    vector_vector_Mat outputBlobs;
    Napi::Value* pyobj_outBlobNames = NULL;
    vector_String outBlobNames;

    const char* keywords[] = { "outBlobNames", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.forwardAndRetrieve", (char**)keywords, &pyobj_outBlobNames) &&
        jsopencv_to_safe(info, pyobj_outBlobNames, outBlobNames, ArgInfo("outBlobNames", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->forward(outputBlobs, outBlobNames));
        return jsopencv_from(outputBlobs);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_forwardAsync(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_outputName = NULL;
    String outputName;
    AsyncArray retval;

    const char* keywords[] = { "outputName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:dnn_Net.forwardAsync", (char**)keywords, &pyobj_outputName) &&
        jsopencv_to_safe(info, pyobj_outputName, outputName, ArgInfo("outputName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->forwardAsync(outputName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getFLOPS(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    int64 retval;

    const char* keywords[] = { "netInputShapes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getFLOPS", (char**)keywords, &pyobj_netInputShapes) &&
        jsopencv_to_safe(info, pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFLOPS(netInputShapes));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    int64 retval;

    const char* keywords[] = { "netInputShape", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getFLOPS", (char**)keywords, &pyobj_netInputShape) &&
        jsopencv_to_safe(info, pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFLOPS(netInputShape));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerId = NULL;
    int layerId=0;
    Napi::Value* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    int64 retval;

    const char* keywords[] = { "layerId", "netInputShapes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.getFLOPS", (char**)keywords, &pyobj_layerId, &pyobj_netInputShapes) &&
        jsopencv_to_safe(info, pyobj_layerId, layerId, ArgInfo("layerId", 0)) &&
        jsopencv_to_safe(info, pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFLOPS(layerId, netInputShapes));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerId = NULL;
    int layerId=0;
    Napi::Value* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    int64 retval;

    const char* keywords[] = { "layerId", "netInputShape", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.getFLOPS", (char**)keywords, &pyobj_layerId, &pyobj_netInputShape) &&
        jsopencv_to_safe(info, pyobj_layerId, layerId, ArgInfo("layerId", 0)) &&
        jsopencv_to_safe(info, pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFLOPS(layerId, netInputShape));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getFLOPS");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getInputDetails(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    vector_float scales;
    vector_int zeropoints;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->getInputDetails(scales, zeropoints));
        return Py_BuildValue("(NN)", jsopencv_from(scales), jsopencv_from(zeropoints));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getLayer(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_layerId = NULL;
    int layerId=0;
    Ptr<Layer> retval;

    const char* keywords[] = { "layerId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayer", (char**)keywords, &pyobj_layerId) &&
        jsopencv_to_safe(info, pyobj_layerId, layerId, ArgInfo("layerId", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayer(layerId));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerName = NULL;
    String layerName;
    Ptr<Layer> retval;

    const char* keywords[] = { "layerName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayer", (char**)keywords, &pyobj_layerName) &&
        jsopencv_to_safe(info, pyobj_layerName, layerName, ArgInfo("layerName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayer(layerName));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerId = NULL;
    LayerId layerId;
    Ptr<Layer> retval;

    const char* keywords[] = { "layerId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayer", (char**)keywords, &pyobj_layerId) &&
        jsopencv_to_safe(info, pyobj_layerId, layerId, ArgInfo("layerId", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayer(layerId));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLayer");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getLayerId(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_layer = NULL;
    String layer;
    int retval;

    const char* keywords[] = { "layer", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayerId", (char**)keywords, &pyobj_layer) &&
        jsopencv_to_safe(info, pyobj_layer, layer, ArgInfo("layer", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayerId(layer));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getLayerNames(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    std::vector<String> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayerNames());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getLayerTypes(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    vector_String layersTypes;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->getLayerTypes(layersTypes));
        return jsopencv_from(layersTypes);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getLayersCount(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_layerType = NULL;
    String layerType;
    int retval;

    const char* keywords[] = { "layerType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayersCount", (char**)keywords, &pyobj_layerType) &&
        jsopencv_to_safe(info, pyobj_layerType, layerType, ArgInfo("layerType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayersCount(layerType));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getLayersShapes(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    vector_int layersIds;
    vector_vector_MatShape inLayersShapes;
    vector_vector_MatShape outLayersShapes;

    const char* keywords[] = { "netInputShapes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayersShapes", (char**)keywords, &pyobj_netInputShapes) &&
        jsopencv_to_safe(info, pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLayersShapes(netInputShapes, layersIds, inLayersShapes, outLayersShapes));
        return Py_BuildValue("(NNN)", jsopencv_from(layersIds), jsopencv_from(inLayersShapes), jsopencv_from(outLayersShapes));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    vector_int layersIds;
    vector_vector_MatShape inLayersShapes;
    vector_vector_MatShape outLayersShapes;

    const char* keywords[] = { "netInputShape", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getLayersShapes", (char**)keywords, &pyobj_netInputShape) &&
        jsopencv_to_safe(info, pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLayersShapes(netInputShape, layersIds, inLayersShapes, outLayersShapes));
        return Py_BuildValue("(NNN)", jsopencv_from(layersIds), jsopencv_from(inLayersShapes), jsopencv_from(outLayersShapes));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLayersShapes");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getMemoryConsumption(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    size_t weights;
    size_t blobs;

    const char* keywords[] = { "netInputShape", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.getMemoryConsumption", (char**)keywords, &pyobj_netInputShape) &&
        jsopencv_to_safe(info, pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getMemoryConsumption(netInputShape, weights, blobs));
        return Py_BuildValue("(NN)", jsopencv_from(weights), jsopencv_from(blobs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerId = NULL;
    int layerId=0;
    Napi::Value* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    size_t weights;
    size_t blobs;

    const char* keywords[] = { "layerId", "netInputShapes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.getMemoryConsumption", (char**)keywords, &pyobj_layerId, &pyobj_netInputShapes) &&
        jsopencv_to_safe(info, pyobj_layerId, layerId, ArgInfo("layerId", 0)) &&
        jsopencv_to_safe(info, pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getMemoryConsumption(layerId, netInputShapes, weights, blobs));
        return Py_BuildValue("(NN)", jsopencv_from(weights), jsopencv_from(blobs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerId = NULL;
    int layerId=0;
    Napi::Value* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    size_t weights;
    size_t blobs;

    const char* keywords[] = { "layerId", "netInputShape", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.getMemoryConsumption", (char**)keywords, &pyobj_layerId, &pyobj_netInputShape) &&
        jsopencv_to_safe(info, pyobj_layerId, layerId, ArgInfo("layerId", 0)) &&
        jsopencv_to_safe(info, pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getMemoryConsumption(layerId, netInputShape, weights, blobs));
        return Py_BuildValue("(NN)", jsopencv_from(weights), jsopencv_from(blobs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getMemoryConsumption");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getOutputDetails(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    vector_float scales;
    vector_int zeropoints;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->getOutputDetails(scales, zeropoints));
        return Py_BuildValue("(NN)", jsopencv_from(scales), jsopencv_from(zeropoints));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getParam(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_layer = NULL;
    int layer=0;
    Napi::Value* pyobj_numParam = NULL;
    int numParam=0;
    Mat retval;

    const char* keywords[] = { "layer", "numParam", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Net.getParam", (char**)keywords, &pyobj_layer, &pyobj_numParam) &&
        jsopencv_to_safe(info, pyobj_layer, layer, ArgInfo("layer", 0)) &&
        jsopencv_to_safe(info, pyobj_numParam, numParam, ArgInfo("numParam", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getParam(layer, numParam));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerName = NULL;
    String layerName;
    Napi::Value* pyobj_numParam = NULL;
    int numParam=0;
    Mat retval;

    const char* keywords[] = { "layerName", "numParam", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_Net.getParam", (char**)keywords, &pyobj_layerName, &pyobj_numParam) &&
        jsopencv_to_safe(info, pyobj_layerName, layerName, ArgInfo("layerName", 0)) &&
        jsopencv_to_safe(info, pyobj_numParam, numParam, ArgInfo("numParam", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getParam(layerName, numParam));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getParam");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getPerfProfile(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    vector_double timings;
    int64 retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPerfProfile(timings));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(timings));
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getUnconnectedOutLayers(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    std::vector<int> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUnconnectedOutLayers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_getUnconnectedOutLayersNames(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    std::vector<String> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUnconnectedOutLayersNames());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_quantize(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_calibData = NULL;
    vector_Mat calibData;
    Napi::Value* pyobj_inputsDtype = NULL;
    int inputsDtype=0;
    Napi::Value* pyobj_outputsDtype = NULL;
    int outputsDtype=0;
    Napi::Value* pyobj_perChannel = NULL;
    bool perChannel=true;
    Net retval;

    const char* keywords[] = { "calibData", "inputsDtype", "outputsDtype", "perChannel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:dnn_Net.quantize", (char**)keywords, &pyobj_calibData, &pyobj_inputsDtype, &pyobj_outputsDtype, &pyobj_perChannel) &&
        jsopencv_to_safe(info, pyobj_calibData, calibData, ArgInfo("calibData", 0)) &&
        jsopencv_to_safe(info, pyobj_inputsDtype, inputsDtype, ArgInfo("inputsDtype", 0)) &&
        jsopencv_to_safe(info, pyobj_outputsDtype, outputsDtype, ArgInfo("outputsDtype", 0)) &&
        jsopencv_to_safe(info, pyobj_perChannel, perChannel, ArgInfo("perChannel", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->quantize(calibData, inputsDtype, outputsDtype, perChannel));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_calibData = NULL;
    vector_UMat calibData;
    Napi::Value* pyobj_inputsDtype = NULL;
    int inputsDtype=0;
    Napi::Value* pyobj_outputsDtype = NULL;
    int outputsDtype=0;
    Napi::Value* pyobj_perChannel = NULL;
    bool perChannel=true;
    Net retval;

    const char* keywords[] = { "calibData", "inputsDtype", "outputsDtype", "perChannel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:dnn_Net.quantize", (char**)keywords, &pyobj_calibData, &pyobj_inputsDtype, &pyobj_outputsDtype, &pyobj_perChannel) &&
        jsopencv_to_safe(info, pyobj_calibData, calibData, ArgInfo("calibData", 0)) &&
        jsopencv_to_safe(info, pyobj_inputsDtype, inputsDtype, ArgInfo("inputsDtype", 0)) &&
        jsopencv_to_safe(info, pyobj_outputsDtype, outputsDtype, ArgInfo("outputsDtype", 0)) &&
        jsopencv_to_safe(info, pyobj_perChannel, perChannel, ArgInfo("perChannel", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->quantize(calibData, inputsDtype, outputsDtype, perChannel));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("quantize");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_readFromModelOptimizer_static(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_xml = NULL;
    String xml;
    Napi::Value* pyobj_bin = NULL;
    String bin;
    Net retval;

    const char* keywords[] = { "xml", "bin", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.readFromModelOptimizer", (char**)keywords, &pyobj_xml, &pyobj_bin) &&
        jsopencv_to_safe(info, pyobj_xml, xml, ArgInfo("xml", 0)) &&
        jsopencv_to_safe(info, pyobj_bin, bin, ArgInfo("bin", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::dnn::Net::readFromModelOptimizer(xml, bin));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_bufferModelConfig = NULL;
    vector_uchar bufferModelConfig;
    Napi::Value* pyobj_bufferWeights = NULL;
    vector_uchar bufferWeights;
    Net retval;

    const char* keywords[] = { "bufferModelConfig", "bufferWeights", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.readFromModelOptimizer", (char**)keywords, &pyobj_bufferModelConfig, &pyobj_bufferWeights) &&
        jsopencv_to_safe(info, pyobj_bufferModelConfig, bufferModelConfig, ArgInfo("bufferModelConfig", 0)) &&
        jsopencv_to_safe(info, pyobj_bufferWeights, bufferWeights, ArgInfo("bufferWeights", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::dnn::Net::readFromModelOptimizer(bufferModelConfig, bufferWeights));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("readFromModelOptimizer");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setHalideScheduler(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_scheduler = NULL;
    String scheduler;

    const char* keywords[] = { "scheduler", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.setHalideScheduler", (char**)keywords, &pyobj_scheduler) &&
        jsopencv_to_safe(info, pyobj_scheduler, scheduler, ArgInfo("scheduler", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setHalideScheduler(scheduler));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setInput(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_blob = NULL;
    Mat blob;
    Napi::Value* pyobj_name = NULL;
    String name="";
    Napi::Value* pyobj_scalefactor = NULL;
    double scalefactor=1.0;
    Napi::Value* pyobj_mean = NULL;
    Scalar mean;

    const char* keywords[] = { "blob", "name", "scalefactor", "mean", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:dnn_Net.setInput", (char**)keywords, &pyobj_blob, &pyobj_name, &pyobj_scalefactor, &pyobj_mean) &&
        jsopencv_to_safe(info, pyobj_blob, blob, ArgInfo("blob", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_scalefactor, scalefactor, ArgInfo("scalefactor", 0)) &&
        jsopencv_to_safe(info, pyobj_mean, mean, ArgInfo("mean", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInput(blob, name, scalefactor, mean));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_blob = NULL;
    UMat blob;
    Napi::Value* pyobj_name = NULL;
    String name="";
    Napi::Value* pyobj_scalefactor = NULL;
    double scalefactor=1.0;
    Napi::Value* pyobj_mean = NULL;
    Scalar mean;

    const char* keywords[] = { "blob", "name", "scalefactor", "mean", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:dnn_Net.setInput", (char**)keywords, &pyobj_blob, &pyobj_name, &pyobj_scalefactor, &pyobj_mean) &&
        jsopencv_to_safe(info, pyobj_blob, blob, ArgInfo("blob", 0)) &&
        jsopencv_to_safe(info, pyobj_name, name, ArgInfo("name", 0)) &&
        jsopencv_to_safe(info, pyobj_scalefactor, scalefactor, ArgInfo("scalefactor", 0)) &&
        jsopencv_to_safe(info, pyobj_mean, mean, ArgInfo("mean", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInput(blob, name, scalefactor, mean));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setInput");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setInputShape(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_inputName = NULL;
    String inputName;
    Napi::Value* pyobj_shape = NULL;
    MatShape shape;

    const char* keywords[] = { "inputName", "shape", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_Net.setInputShape", (char**)keywords, &pyobj_inputName, &pyobj_shape) &&
        jsopencv_to_safe(info, pyobj_inputName, inputName, ArgInfo("inputName", 0)) &&
        jsopencv_to_safe(info, pyobj_shape, shape, ArgInfo("shape", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInputShape(inputName, shape));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setInputsNames(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_inputBlobNames = NULL;
    vector_String inputBlobNames;

    const char* keywords[] = { "inputBlobNames", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.setInputsNames", (char**)keywords, &pyobj_inputBlobNames) &&
        jsopencv_to_safe(info, pyobj_inputBlobNames, inputBlobNames, ArgInfo("inputBlobNames", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInputsNames(inputBlobNames));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setParam(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_layer = NULL;
    int layer=0;
    Napi::Value* pyobj_numParam = NULL;
    int numParam=0;
    Napi::Value* pyobj_blob = NULL;
    Mat blob;

    const char* keywords[] = { "layer", "numParam", "blob", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:dnn_Net.setParam", (char**)keywords, &pyobj_layer, &pyobj_numParam, &pyobj_blob) &&
        jsopencv_to_safe(info, pyobj_layer, layer, ArgInfo("layer", 0)) &&
        jsopencv_to_safe(info, pyobj_numParam, numParam, ArgInfo("numParam", 0)) &&
        jsopencv_to_safe(info, pyobj_blob, blob, ArgInfo("blob", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setParam(layer, numParam, blob));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_layerName = NULL;
    String layerName;
    Napi::Value* pyobj_numParam = NULL;
    int numParam=0;
    Napi::Value* pyobj_blob = NULL;
    Mat blob;

    const char* keywords[] = { "layerName", "numParam", "blob", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:dnn_Net.setParam", (char**)keywords, &pyobj_layerName, &pyobj_numParam, &pyobj_blob) &&
        jsopencv_to_safe(info, pyobj_layerName, layerName, ArgInfo("layerName", 0)) &&
        jsopencv_to_safe(info, pyobj_numParam, numParam, ArgInfo("numParam", 0)) &&
        jsopencv_to_safe(info, pyobj_blob, blob, ArgInfo("blob", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setParam(layerName, numParam, blob));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setParam");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setPreferableBackend(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_backendId = NULL;
    int backendId=0;

    const char* keywords[] = { "backendId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.setPreferableBackend", (char**)keywords, &pyobj_backendId) &&
        jsopencv_to_safe(info, pyobj_backendId, backendId, ArgInfo("backendId", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreferableBackend(backendId));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_Net_setPreferableTarget(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::Net * self1 = 0;
    if (!pyopencv_dnn_Net_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    cv::dnn::Net* _self_ = (self1);
    Napi::Value* pyobj_targetId = NULL;
    int targetId=0;

    const char* keywords[] = { "targetId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_Net.setPreferableTarget", (char**)keywords, &pyobj_targetId) &&
        jsopencv_to_safe(info, pyobj_targetId, targetId, ArgInfo("targetId", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreferableTarget(targetId));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (dnn_Net)

static PyGetSetDef pyopencv_dnn_Net_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_Net_methods[] =
{
    {"connect", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_connect, 0), "connect(outPin, inpPin) -> None\n.   @brief Connects output of the first layer to input of the second layer.\n.            *  @param outPin descriptor of the first layer output.\n.            *  @param inpPin descriptor of the second layer input.\n.            *\n.            * Descriptors have the following template <DFN>&lt;layer_name&gt;[.input_number]</DFN>:\n.            * - the first part of the template <DFN>layer_name</DFN> is string name of the added layer.\n.            *   If this part is empty then the network input pseudo layer will be used;\n.            * - the second optional part of the template <DFN>input_number</DFN>\n.            *   is either number of the layer input, either label one.\n.            *   If this part is omitted then the first layer input will be used.\n.            *\n.            *  @see setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()"},
    {"dump", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_dump, 0), "dump() -> retval\n.   @brief Dump net to String\n.            *  @returns String with structure, hyperparameters, backend, target and fusion\n.            *  Call method after setInput(). To see correct backend, target and fusion run after forward()."},
    {"dumpToFile", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_dumpToFile, 0), "dumpToFile(path) -> None\n.   @brief Dump net structure, hyperparameters, backend, target and fusion to dot file\n.            *  @param path   path to output file with .dot extension\n.            *  @see dump()"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_empty, 0), "empty() -> retval\n.   Returns true if there are no layers in the network."},
    {"enableFusion", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_enableFusion, 0), "enableFusion(fusion) -> None\n.   @brief Enables or disables layer fusion in the network.\n.            * @param fusion true to enable the fusion, false to disable. The fusion is enabled by default."},
    {"enableWinograd", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_enableWinograd, 0), "enableWinograd(useWinograd) -> None\n.   @brief Enables or disables the Winograd compute branch. The Winograd compute branch can speed up\n.            * 3x3 Convolution at a small loss of accuracy.\n.           * @param useWinograd true to enable the Winograd compute branch. The default is true."},
    {"forward", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_forward, 0), "forward([, outputName]) -> retval\n.   @brief Runs forward pass to compute output of layer with name @p outputName.\n.            *  @param outputName name for layer which output is needed to get\n.            *  @return blob for first output of specified layer.\n.            *  @details By default runs forward pass for the whole network.\n\n\n\nforward([, outputBlobs[, outputName]]) -> outputBlobs\n.   @brief Runs forward pass to compute output of layer with name @p outputName.\n.            *  @param outputBlobs contains all output blobs for specified layer.\n.            *  @param outputName name for layer which output is needed to get\n.            *  @details If @p outputName is empty, runs forward pass for the whole network.\n\n\n\nforward(outBlobNames[, outputBlobs]) -> outputBlobs\n.   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n.            *  @param outputBlobs contains blobs for first outputs of specified layers.\n.            *  @param outBlobNames names for layers which outputs are needed to get"},
    {"forwardAndRetrieve", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_forwardAndRetrieve, 0), "forwardAndRetrieve(outBlobNames) -> outputBlobs\n.   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n.            *  @param outputBlobs contains all output blobs for each layer specified in @p outBlobNames.\n.            *  @param outBlobNames names for layers which outputs are needed to get"},
    {"forwardAsync", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_forwardAsync, 0), "forwardAsync([, outputName]) -> retval\n.   @brief Runs forward pass to compute output of layer with name @p outputName.\n.            *  @param outputName name for layer which output is needed to get\n.            *  @details By default runs forward pass for the whole network.\n.            *\n.            *  This is an asynchronous version of forward(const String&).\n.            *  dnn::DNN_BACKEND_INFERENCE_ENGINE backend is required."},
    {"getFLOPS", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getFLOPS, 0), "getFLOPS(netInputShapes) -> retval\n.   @brief Computes FLOP for whole loaded model with specified input shapes.\n.            * @param netInputShapes vector of shapes for all net inputs.\n.            * @returns computed FLOP.\n\n\n\ngetFLOPS(netInputShape) -> retval\n.   @overload\n\n\n\ngetFLOPS(layerId, netInputShapes) -> retval\n.   @overload\n\n\n\ngetFLOPS(layerId, netInputShape) -> retval\n.   @overload"},
    {"getInputDetails", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getInputDetails, 0), "getInputDetails() -> scales, zeropoints\n.   @brief Returns input scale and zeropoint for a quantized Net.\n.            *  @param scales output parameter for returning input scales.\n.            *  @param zeropoints output parameter for returning input zeropoints."},
    {"getLayer", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayer, 0), "getLayer(layerId) -> retval\n.   @brief Returns pointer to layer with specified id or name which the network use.\n\n\n\ngetLayer(layerName) -> retval\n.   @overload\n.            *  @deprecated Use int getLayerId(const String &layer)"},
    {"getLayerId", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayerId, 0), "getLayerId(layer) -> retval\n.   @brief Converts string name of the layer to the integer identifier.\n.            *  @returns id of the layer, or -1 if the layer wasn't found."},
    {"getLayerNames", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayerNames, 0), "getLayerNames() -> retval\n."},
    {"getLayerTypes", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayerTypes, 0), "getLayerTypes() -> layersTypes\n.   @brief Returns list of types for layer used in model.\n.            * @param layersTypes output parameter for returning types."},
    {"getLayersCount", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayersCount, 0), "getLayersCount(layerType) -> retval\n.   @brief Returns count of layers of specified type.\n.            * @param layerType type.\n.            * @returns count of layers"},
    {"getLayersShapes", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayersShapes, 0), "getLayersShapes(netInputShapes) -> layersIds, inLayersShapes, outLayersShapes\n.   @brief Returns input and output shapes for all layers in loaded model;\n.            *  preliminary inferencing isn't necessary.\n.            *  @param netInputShapes shapes for all input blobs in net input layer.\n.            *  @param layersIds output parameter for layer IDs.\n.            *  @param inLayersShapes output parameter for input layers shapes;\n.            * order is the same as in layersIds\n.            *  @param outLayersShapes output parameter for output layers shapes;\n.            * order is the same as in layersIds\n\n\n\ngetLayersShapes(netInputShape) -> layersIds, inLayersShapes, outLayersShapes\n.   @overload"},
    {"getMemoryConsumption", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getMemoryConsumption, 0), "getMemoryConsumption(netInputShape) -> weights, blobs\n.   @overload\n\n\n\ngetMemoryConsumption(layerId, netInputShapes) -> weights, blobs\n.   @overload\n\n\n\ngetMemoryConsumption(layerId, netInputShape) -> weights, blobs\n.   @overload"},
    {"getOutputDetails", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getOutputDetails, 0), "getOutputDetails() -> scales, zeropoints\n.   @brief Returns output scale and zeropoint for a quantized Net.\n.            *  @param scales output parameter for returning output scales.\n.            *  @param zeropoints output parameter for returning output zeropoints."},
    {"getParam", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getParam, 0), "getParam(layer[, numParam]) -> retval\n.   @brief Returns parameter blob of the layer.\n.            *  @param layer name or id of the layer.\n.            *  @param numParam index of the layer parameter in the Layer::blobs array.\n.            *  @see Layer::blobs\n\n\n\ngetParam(layerName[, numParam]) -> retval\n."},
    {"getPerfProfile", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getPerfProfile, 0), "getPerfProfile() -> retval, timings\n.   @brief Returns overall time for inference and timings (in ticks) for layers.\n.            *\n.            * Indexes in returned vector correspond to layers ids. Some layers can be fused with others,\n.            * in this case zero ticks count will be return for that skipped layers. Supported by DNN_BACKEND_OPENCV on DNN_TARGET_CPU only.\n.            *\n.            * @param[out] timings vector for tick timings for all layers.\n.            * @return overall ticks for model inference."},
    {"getUnconnectedOutLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getUnconnectedOutLayers, 0), "getUnconnectedOutLayers() -> retval\n.   @brief Returns indexes of layers with unconnected outputs.\n.            *\n.            * FIXIT: Rework API to registerOutput() approach, deprecate this call"},
    {"getUnconnectedOutLayersNames", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getUnconnectedOutLayersNames, 0), "getUnconnectedOutLayersNames() -> retval\n.   @brief Returns names of layers with unconnected outputs.\n.            *\n.            * FIXIT: Rework API to registerOutput() approach, deprecate this call"},
    {"quantize", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_quantize, 0), "quantize(calibData, inputsDtype, outputsDtype[, perChannel]) -> retval\n.   @brief Returns a quantized Net from a floating-point Net.\n.            *  @param calibData Calibration data to compute the quantization parameters.\n.            *  @param inputsDtype Datatype of quantized net's inputs. Can be CV_32F or CV_8S.\n.            *  @param outputsDtype Datatype of quantized net's outputs. Can be CV_32F or CV_8S.\n.            *  @param perChannel Quantization granularity of quantized Net. The default is true, that means quantize model\n.            *  in per-channel way (channel-wise). Set it false to quantize model in per-tensor way (or tensor-wise)."},
    {"readFromModelOptimizer", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_readFromModelOptimizer_static, METH_STATIC), "readFromModelOptimizer(xml, bin) -> retval\n.   @brief Create a network from Intel's Model Optimizer intermediate representation (IR).\n.            *  @param[in] xml XML configuration file with network's topology.\n.            *  @param[in] bin Binary file with trained weights.\n.            *  Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine\n.            *  backend.\n\n\n\nreadFromModelOptimizer(bufferModelConfig, bufferWeights) -> retval\n.   @brief Create a network from Intel's Model Optimizer in-memory buffers with intermediate representation (IR).\n.            *  @param[in] bufferModelConfig buffer with model's configuration.\n.            *  @param[in] bufferWeights buffer with model's trained weights.\n.            *  @returns Net object."},
    {"setHalideScheduler", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setHalideScheduler, 0), "setHalideScheduler(scheduler) -> None\n.   * @brief Compile Halide layers.\n.            * @param[in] scheduler Path to YAML file with scheduling directives.\n.            * @see setPreferableBackend\n.            *\n.            * Schedule layers that support Halide backend. Then compile them for\n.            * specific target. For layers that not represented in scheduling file\n.            * or if no manual scheduling used at all, automatic scheduling will be applied."},
    {"setInput", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setInput, 0), "setInput(blob[, name[, scalefactor[, mean]]]) -> None\n.   @brief Sets the new input value for the network\n.            *  @param blob        A new blob. Should have CV_32F or CV_8U depth.\n.            *  @param name        A name of input layer.\n.            *  @param scalefactor An optional normalization scale.\n.            *  @param mean        An optional mean subtraction values.\n.            *  @see connect(String, String) to know format of the descriptor.\n.            *\n.            *  If scale or mean values are specified, a final input blob is computed\n.            *  as:\n.            * \\f[input(n,c,h,w) = scalefactor \\times (blob(n,c,h,w) - mean_c)\\f]"},
    {"setInputShape", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setInputShape, 0), "setInputShape(inputName, shape) -> None\n.   @brief Specify shape of network input."},
    {"setInputsNames", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setInputsNames, 0), "setInputsNames(inputBlobNames) -> None\n.   @brief Sets outputs names of the network input pseudo layer.\n.            *\n.            * Each net always has special own the network input pseudo layer with id=0.\n.            * This layer stores the user blobs only and don't make any computations.\n.            * In fact, this layer provides the only way to pass user data into the network.\n.            * As any other layer, this layer can label its outputs and this function provides an easy way to do this."},
    {"setParam", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setParam, 0), "setParam(layer, numParam, blob) -> None\n.   @brief Sets the new value for the learned param of the layer.\n.            *  @param layer name or id of the layer.\n.            *  @param numParam index of the layer parameter in the Layer::blobs array.\n.            *  @param blob the new value.\n.            *  @see Layer::blobs\n.            *  @note If shape of the new blob differs from the previous shape,\n.            *  then the following forward pass may fail.\n\n\n\nsetParam(layerName, numParam, blob) -> None\n."},
    {"setPreferableBackend", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setPreferableBackend, 0), "setPreferableBackend(backendId) -> None\n.   * @brief Ask network to use specific computation backend where it supported.\n.            * @param[in] backendId backend identifier.\n.            * @see Backend\n.            *\n.            * If OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT\n.            * means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV."},
    {"setPreferableTarget", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setPreferableTarget, 0), "setPreferableTarget(targetId) -> None\n.   * @brief Ask network to make computations on specific target device.\n.            * @param[in] targetId target identifier.\n.            * @see Target\n.            *\n.            * List of supported combinations backend / target:\n.            * |                        | DNN_BACKEND_OPENCV | DNN_BACKEND_INFERENCE_ENGINE | DNN_BACKEND_HALIDE |  DNN_BACKEND_CUDA |\n.            * |------------------------|--------------------|------------------------------|--------------------|-------------------|\n.            * | DNN_TARGET_CPU         |                  + |                            + |                  + |                   |\n.            * | DNN_TARGET_OPENCL      |                  + |                            + |                  + |                   |\n.            * | DNN_TARGET_OPENCL_FP16 |                  + |                            + |                    |                   |\n.            * | DNN_TARGET_MYRIAD      |                    |                            + |                    |                   |\n.            * | DNN_TARGET_FPGA        |                    |                            + |                    |                   |\n.            * | DNN_TARGET_CUDA        |                    |                              |                    |                 + |\n.            * | DNN_TARGET_CUDA_FP16   |                    |                              |                    |                 + |\n.            * | DNN_TARGET_HDDL        |                    |                            + |                    |                   |"},

    {NULL,          NULL}
};

// Converter (dnn_Net)

template<>
struct PyOpenCV_Converter< cv::dnn::Net >
{
    static PyObject* from(const cv::dnn::Net& r)
    {
        return pyopencv_dnn_Net_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::Net& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::Net * dst_;
        if (pyopencv_dnn_Net_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::Net for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_SegmentationModel (Generic)
//================================================================================

// GetSet (dnn_SegmentationModel)



// Methods (dnn_SegmentationModel)

static int pyopencv_cv_dnn_dnn_SegmentationModel_SegmentationModel(pyopencv_dnn_SegmentationModel_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_model = NULL;
    String model;
    Napi::Value* pyobj_config = NULL;
    String config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:SegmentationModel", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::SegmentationModel(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:SegmentationModel", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::SegmentationModel(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("SegmentationModel");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_SegmentationModel_segment(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::SegmentationModel * self1 = 0;
    if (!pyopencv_dnn_SegmentationModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_SegmentationModel' or its derivative)");
    cv::dnn::SegmentationModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "frame", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_SegmentationModel.segment", (char**)keywords, &pyobj_frame, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->segment(frame, mask));
        return jsopencv_from(mask);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "frame", "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_SegmentationModel.segment", (char**)keywords, &pyobj_frame, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->segment(frame, mask));
        return jsopencv_from(mask);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("segment");

    return NULL;
}



// Tables (dnn_SegmentationModel)

static PyGetSetDef pyopencv_dnn_SegmentationModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_SegmentationModel_methods[] =
{
    {"segment", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_SegmentationModel_segment, 0), "segment(frame[, mask]) -> mask\n.   @brief Given the @p input frame, create input blob, run net\n.             *  @param[in]  frame  The input image.\n.             *  @param[out] mask Allocated class prediction for each pixel"},

    {NULL,          NULL}
};

// Converter (dnn_SegmentationModel)

template<>
struct PyOpenCV_Converter< cv::dnn::SegmentationModel >
{
    static PyObject* from(const cv::dnn::SegmentationModel& r)
    {
        return pyopencv_dnn_SegmentationModel_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::SegmentationModel& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::SegmentationModel * dst_;
        if (pyopencv_dnn_SegmentationModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::SegmentationModel for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_TextDetectionModel (Generic)
//================================================================================

// GetSet (dnn_TextDetectionModel)



// Methods (dnn_TextDetectionModel)

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel' or its derivative)");
    cv::dnn::TextDetectionModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    vector_vector_Point detections;
    vector_float confidences;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detect", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(frame, detections, confidences));
        return Py_BuildValue("(NN)", jsopencv_from(detections), jsopencv_from(confidences));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    vector_vector_Point detections;
    vector_float confidences;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detect", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(frame, detections, confidences));
        return Py_BuildValue("(NN)", jsopencv_from(detections), jsopencv_from(confidences));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    vector_vector_Point detections;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detect", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(frame, detections));
        return jsopencv_from(detections);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    vector_vector_Point detections;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detect", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(frame, detections));
        return jsopencv_from(detections);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_detectTextRectangles(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel' or its derivative)");
    cv::dnn::TextDetectionModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    vector_RotatedRect detections;
    vector_float confidences;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detectTextRectangles", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectTextRectangles(frame, detections, confidences));
        return Py_BuildValue("(NN)", jsopencv_from(detections), jsopencv_from(confidences));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    vector_RotatedRect detections;
    vector_float confidences;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detectTextRectangles", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectTextRectangles(frame, detections, confidences));
        return Py_BuildValue("(NN)", jsopencv_from(detections), jsopencv_from(confidences));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    vector_RotatedRect detections;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detectTextRectangles", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectTextRectangles(frame, detections));
        return jsopencv_from(detections);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    vector_RotatedRect detections;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel.detectTextRectangles", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectTextRectangles(frame, detections));
        return jsopencv_from(detections);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectTextRectangles");

    return NULL;
}



// Tables (dnn_TextDetectionModel)

static PyGetSetDef pyopencv_dnn_TextDetectionModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_TextDetectionModel_methods[] =
{
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_detect, 0), "detect(frame) -> detections, confidences\n.   @brief Performs detection\n.        *\n.        * Given the input @p frame, prepare network input, run network inference, post-process network output and return result detections.\n.        *\n.        * Each result is quadrangle's 4 points in this order:\n.        * - bottom-left\n.        * - top-left\n.        * - top-right\n.        * - bottom-right\n.        *\n.        * Use cv::getPerspectiveTransform function to retrieve image region without perspective transformations.\n.        *\n.        * @note If DL model doesn't support that kind of output then result may be derived from detectTextRectangles() output.\n.        *\n.        * @param[in] frame The input image\n.        * @param[out] detections array with detections' quadrangles (4 points per result)\n.        * @param[out] confidences array with detection confidences\n\n\n\ndetect(frame) -> detections\n.   @overload"},
    {"detectTextRectangles", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_detectTextRectangles, 0), "detectTextRectangles(frame) -> detections, confidences\n.   @brief Performs detection\n.        *\n.        * Given the input @p frame, prepare network input, run network inference, post-process network output and return result detections.\n.        *\n.        * Each result is rotated rectangle.\n.        *\n.        * @note Result may be inaccurate in case of strong perspective transformations.\n.        *\n.        * @param[in] frame the input image\n.        * @param[out] detections array with detections' RotationRect results\n.        * @param[out] confidences array with detection confidences\n\n\n\ndetectTextRectangles(frame) -> detections\n.   @overload"},

    {NULL,          NULL}
};

// Converter (dnn_TextDetectionModel)

template<>
struct PyOpenCV_Converter< cv::dnn::TextDetectionModel >
{
    static PyObject* from(const cv::dnn::TextDetectionModel& r)
    {
        return pyopencv_dnn_TextDetectionModel_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::TextDetectionModel& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::TextDetectionModel * dst_;
        if (pyopencv_dnn_TextDetectionModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::TextDetectionModel for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_TextDetectionModel_DB (Generic)
//================================================================================

// GetSet (dnn_TextDetectionModel_DB)



// Methods (dnn_TextDetectionModel_DB)

static int pyopencv_cv_dnn_dnn_TextDetectionModel_DB_TextDetectionModel_DB(pyopencv_dnn_TextDetectionModel_DB_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TextDetectionModel_DB", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::TextDetectionModel_DB(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_model = NULL;
    std::string model;
    Napi::Value* pyobj_config = NULL;
    std::string config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:TextDetectionModel_DB", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::TextDetectionModel_DB(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("TextDetectionModel_DB");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getBinaryThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBinaryThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getMaxCandidates(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxCandidates());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getPolygonThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPolygonThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getUnclipRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUnclipRatio());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setBinaryThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    Napi::Value* pyobj_binaryThreshold = NULL;
    float binaryThreshold=0.f;
    TextDetectionModel_DB retval;

    const char* keywords[] = { "binaryThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel_DB.setBinaryThreshold", (char**)keywords, &pyobj_binaryThreshold) &&
        jsopencv_to_safe(info, pyobj_binaryThreshold, binaryThreshold, ArgInfo("binaryThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setBinaryThreshold(binaryThreshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setMaxCandidates(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    Napi::Value* pyobj_maxCandidates = NULL;
    int maxCandidates=0;
    TextDetectionModel_DB retval;

    const char* keywords[] = { "maxCandidates", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel_DB.setMaxCandidates", (char**)keywords, &pyobj_maxCandidates) &&
        jsopencv_to_safe(info, pyobj_maxCandidates, maxCandidates, ArgInfo("maxCandidates", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setMaxCandidates(maxCandidates));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setPolygonThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    Napi::Value* pyobj_polygonThreshold = NULL;
    float polygonThreshold=0.f;
    TextDetectionModel_DB retval;

    const char* keywords[] = { "polygonThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel_DB.setPolygonThreshold", (char**)keywords, &pyobj_polygonThreshold) &&
        jsopencv_to_safe(info, pyobj_polygonThreshold, polygonThreshold, ArgInfo("polygonThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setPolygonThreshold(polygonThreshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setUnclipRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_DB * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_DB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_DB' or its derivative)");
    cv::dnn::TextDetectionModel_DB* _self_ = (self1);
    Napi::Value* pyobj_unclipRatio = NULL;
    double unclipRatio=0;
    TextDetectionModel_DB retval;

    const char* keywords[] = { "unclipRatio", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel_DB.setUnclipRatio", (char**)keywords, &pyobj_unclipRatio) &&
        jsopencv_to_safe(info, pyobj_unclipRatio, unclipRatio, ArgInfo("unclipRatio", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setUnclipRatio(unclipRatio));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_TextDetectionModel_DB)

static PyGetSetDef pyopencv_dnn_TextDetectionModel_DB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_TextDetectionModel_DB_methods[] =
{
    {"getBinaryThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getBinaryThreshold, 0), "getBinaryThreshold() -> retval\n."},
    {"getMaxCandidates", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getMaxCandidates, 0), "getMaxCandidates() -> retval\n."},
    {"getPolygonThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getPolygonThreshold, 0), "getPolygonThreshold() -> retval\n."},
    {"getUnclipRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_getUnclipRatio, 0), "getUnclipRatio() -> retval\n."},
    {"setBinaryThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setBinaryThreshold, 0), "setBinaryThreshold(binaryThreshold) -> retval\n."},
    {"setMaxCandidates", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setMaxCandidates, 0), "setMaxCandidates(maxCandidates) -> retval\n."},
    {"setPolygonThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setPolygonThreshold, 0), "setPolygonThreshold(polygonThreshold) -> retval\n."},
    {"setUnclipRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_DB_setUnclipRatio, 0), "setUnclipRatio(unclipRatio) -> retval\n."},

    {NULL,          NULL}
};

// Converter (dnn_TextDetectionModel_DB)

template<>
struct PyOpenCV_Converter< cv::dnn::TextDetectionModel_DB >
{
    static PyObject* from(const cv::dnn::TextDetectionModel_DB& r)
    {
        return pyopencv_dnn_TextDetectionModel_DB_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::TextDetectionModel_DB& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::TextDetectionModel_DB * dst_;
        if (pyopencv_dnn_TextDetectionModel_DB_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::TextDetectionModel_DB for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_TextDetectionModel_EAST (Generic)
//================================================================================

// GetSet (dnn_TextDetectionModel_EAST)



// Methods (dnn_TextDetectionModel_EAST)

static int pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_TextDetectionModel_EAST(pyopencv_dnn_TextDetectionModel_EAST_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TextDetectionModel_EAST", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::TextDetectionModel_EAST(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_model = NULL;
    std::string model;
    Napi::Value* pyobj_config = NULL;
    std::string config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:TextDetectionModel_EAST", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::TextDetectionModel_EAST(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("TextDetectionModel_EAST");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_getConfidenceThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_EAST * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_EAST_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_EAST' or its derivative)");
    cv::dnn::TextDetectionModel_EAST* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getConfidenceThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_getNMSThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_EAST * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_EAST_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_EAST' or its derivative)");
    cv::dnn::TextDetectionModel_EAST* _self_ = (self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNMSThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_setConfidenceThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_EAST * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_EAST_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_EAST' or its derivative)");
    cv::dnn::TextDetectionModel_EAST* _self_ = (self1);
    Napi::Value* pyobj_confThreshold = NULL;
    float confThreshold=0.f;
    TextDetectionModel_EAST retval;

    const char* keywords[] = { "confThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel_EAST.setConfidenceThreshold", (char**)keywords, &pyobj_confThreshold) &&
        jsopencv_to_safe(info, pyobj_confThreshold, confThreshold, ArgInfo("confThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setConfidenceThreshold(confThreshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_setNMSThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextDetectionModel_EAST * self1 = 0;
    if (!pyopencv_dnn_TextDetectionModel_EAST_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextDetectionModel_EAST' or its derivative)");
    cv::dnn::TextDetectionModel_EAST* _self_ = (self1);
    Napi::Value* pyobj_nmsThreshold = NULL;
    float nmsThreshold=0.f;
    TextDetectionModel_EAST retval;

    const char* keywords[] = { "nmsThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextDetectionModel_EAST.setNMSThreshold", (char**)keywords, &pyobj_nmsThreshold) &&
        jsopencv_to_safe(info, pyobj_nmsThreshold, nmsThreshold, ArgInfo("nmsThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setNMSThreshold(nmsThreshold));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_TextDetectionModel_EAST)

static PyGetSetDef pyopencv_dnn_TextDetectionModel_EAST_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_TextDetectionModel_EAST_methods[] =
{
    {"getConfidenceThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_getConfidenceThreshold, 0), "getConfidenceThreshold() -> retval\n.   * @brief Get the detection confidence threshold"},
    {"getNMSThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_getNMSThreshold, 0), "getNMSThreshold() -> retval\n.   * @brief Get the detection confidence threshold"},
    {"setConfidenceThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_setConfidenceThreshold, 0), "setConfidenceThreshold(confThreshold) -> retval\n.   * @brief Set the detection confidence threshold\n.        * @param[in] confThreshold A threshold used to filter boxes by confidences"},
    {"setNMSThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextDetectionModel_EAST_setNMSThreshold, 0), "setNMSThreshold(nmsThreshold) -> retval\n.   * @brief Set the detection NMS filter threshold\n.        * @param[in] nmsThreshold A threshold used in non maximum suppression"},

    {NULL,          NULL}
};

// Converter (dnn_TextDetectionModel_EAST)

template<>
struct PyOpenCV_Converter< cv::dnn::TextDetectionModel_EAST >
{
    static PyObject* from(const cv::dnn::TextDetectionModel_EAST& r)
    {
        return pyopencv_dnn_TextDetectionModel_EAST_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::TextDetectionModel_EAST& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::TextDetectionModel_EAST * dst_;
        if (pyopencv_dnn_TextDetectionModel_EAST_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::TextDetectionModel_EAST for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_TextRecognitionModel (Generic)
//================================================================================

// GetSet (dnn_TextRecognitionModel)



// Methods (dnn_TextRecognitionModel)

static int pyopencv_cv_dnn_dnn_TextRecognitionModel_TextRecognitionModel(pyopencv_dnn_TextRecognitionModel_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::dnn;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_network = NULL;
    Net network;

    const char* keywords[] = { "network", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:TextRecognitionModel", (char**)keywords, &pyobj_network) &&
        jsopencv_to_safe(info, pyobj_network, network, ArgInfo("network", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::TextRecognitionModel(network));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_model = NULL;
    std::string model;
    Napi::Value* pyobj_config = NULL;
    std::string config="";

    const char* keywords[] = { "model", "config", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:TextRecognitionModel", (char**)keywords, &pyobj_model, &pyobj_config) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_config, config, ArgInfo("config", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::dnn::TextRecognitionModel(model, config));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("TextRecognitionModel");

    return -1;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextRecognitionModel_getDecodeType(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextRecognitionModel * self1 = 0;
    if (!pyopencv_dnn_TextRecognitionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextRecognitionModel' or its derivative)");
    cv::dnn::TextRecognitionModel* _self_ = (self1);
    std::string retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDecodeType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextRecognitionModel_getVocabulary(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextRecognitionModel * self1 = 0;
    if (!pyopencv_dnn_TextRecognitionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextRecognitionModel' or its derivative)");
    cv::dnn::TextRecognitionModel* _self_ = (self1);
    std::vector<std::string> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVocabulary());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextRecognitionModel_recognize(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextRecognitionModel * self1 = 0;
    if (!pyopencv_dnn_TextRecognitionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextRecognitionModel' or its derivative)");
    cv::dnn::TextRecognitionModel* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    std::string retval;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextRecognitionModel.recognize", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->recognize(frame));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    std::string retval;

    const char* keywords[] = { "frame", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextRecognitionModel.recognize", (char**)keywords, &pyobj_frame) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->recognize(frame));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    Mat frame;
    Napi::Value* pyobj_roiRects = NULL;
    vector_Mat roiRects;
    vector_string results;

    const char* keywords[] = { "frame", "roiRects", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_TextRecognitionModel.recognize", (char**)keywords, &pyobj_frame, &pyobj_roiRects) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_roiRects, roiRects, ArgInfo("roiRects", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->recognize(frame, roiRects, results));
        return jsopencv_from(results);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_frame = NULL;
    UMat frame;
    Napi::Value* pyobj_roiRects = NULL;
    vector_UMat roiRects;
    vector_string results;

    const char* keywords[] = { "frame", "roiRects", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_TextRecognitionModel.recognize", (char**)keywords, &pyobj_frame, &pyobj_roiRects) &&
        jsopencv_to_safe(info, pyobj_frame, frame, ArgInfo("frame", 0)) &&
        jsopencv_to_safe(info, pyobj_roiRects, roiRects, ArgInfo("roiRects", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->recognize(frame, roiRects, results));
        return jsopencv_from(results);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("recognize");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextRecognitionModel_setDecodeOptsCTCPrefixBeamSearch(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextRecognitionModel * self1 = 0;
    if (!pyopencv_dnn_TextRecognitionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextRecognitionModel' or its derivative)");
    cv::dnn::TextRecognitionModel* _self_ = (self1);
    Napi::Value* pyobj_beamSize = NULL;
    int beamSize=0;
    Napi::Value* pyobj_vocPruneSize = NULL;
    int vocPruneSize=0;
    TextRecognitionModel retval;

    const char* keywords[] = { "beamSize", "vocPruneSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_TextRecognitionModel.setDecodeOptsCTCPrefixBeamSearch", (char**)keywords, &pyobj_beamSize, &pyobj_vocPruneSize) &&
        jsopencv_to_safe(info, pyobj_beamSize, beamSize, ArgInfo("beamSize", 0)) &&
        jsopencv_to_safe(info, pyobj_vocPruneSize, vocPruneSize, ArgInfo("vocPruneSize", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setDecodeOptsCTCPrefixBeamSearch(beamSize, vocPruneSize));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextRecognitionModel_setDecodeType(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextRecognitionModel * self1 = 0;
    if (!pyopencv_dnn_TextRecognitionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextRecognitionModel' or its derivative)");
    cv::dnn::TextRecognitionModel* _self_ = (self1);
    Napi::Value* pyobj_decodeType = NULL;
    std::string decodeType;
    TextRecognitionModel retval;

    const char* keywords[] = { "decodeType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextRecognitionModel.setDecodeType", (char**)keywords, &pyobj_decodeType) &&
        jsopencv_to_safe(info, pyobj_decodeType, decodeType, ArgInfo("decodeType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setDecodeType(decodeType));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_dnn_TextRecognitionModel_setVocabulary(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn;


    cv::dnn::TextRecognitionModel * self1 = 0;
    if (!pyopencv_dnn_TextRecognitionModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_TextRecognitionModel' or its derivative)");
    cv::dnn::TextRecognitionModel* _self_ = (self1);
    Napi::Value* pyobj_vocabulary = NULL;
    vector_string vocabulary;
    TextRecognitionModel retval;

    const char* keywords[] = { "vocabulary", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_TextRecognitionModel.setVocabulary", (char**)keywords, &pyobj_vocabulary) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setVocabulary(vocabulary));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (dnn_TextRecognitionModel)

static PyGetSetDef pyopencv_dnn_TextRecognitionModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_TextRecognitionModel_methods[] =
{
    {"getDecodeType", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextRecognitionModel_getDecodeType, 0), "getDecodeType() -> retval\n.   * @brief Get the decoding method\n.        * @return the decoding method"},
    {"getVocabulary", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextRecognitionModel_getVocabulary, 0), "getVocabulary() -> retval\n.   * @brief Get the vocabulary for recognition.\n.        * @return vocabulary the associated vocabulary"},
    {"recognize", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextRecognitionModel_recognize, 0), "recognize(frame) -> retval\n.   * @brief Given the @p input frame, create input blob, run net and return recognition result\n.        * @param[in] frame The input image\n.        * @return The text recognition result\n\n\n\nrecognize(frame, roiRects) -> results\n.   * @brief Given the @p input frame, create input blob, run net and return recognition result\n.        * @param[in] frame The input image\n.        * @param[in] roiRects List of text detection regions of interest (cv::Rect, CV_32SC4). ROIs is be cropped as the network inputs\n.        * @param[out] results A set of text recognition results."},
    {"setDecodeOptsCTCPrefixBeamSearch", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextRecognitionModel_setDecodeOptsCTCPrefixBeamSearch, 0), "setDecodeOptsCTCPrefixBeamSearch(beamSize[, vocPruneSize]) -> retval\n.   * @brief Set the decoding method options for `\"CTC-prefix-beam-search\"` decode usage\n.        * @param[in] beamSize Beam size for search\n.        * @param[in] vocPruneSize Parameter to optimize big vocabulary search,\n.        * only take top @p vocPruneSize tokens in each search step, @p vocPruneSize <= 0 stands for disable this prune."},
    {"setDecodeType", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextRecognitionModel_setDecodeType, 0), "setDecodeType(decodeType) -> retval\n.   * @brief Set the decoding method of translating the network output into string\n.        * @param[in] decodeType The decoding method of translating the network output into string, currently supported type:\n.        *    - `\"CTC-greedy\"` greedy decoding for the output of CTC-based methods\n.        *    - `\"CTC-prefix-beam-search\"` Prefix beam search decoding for the output of CTC-based methods"},
    {"setVocabulary", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_dnn_TextRecognitionModel_setVocabulary, 0), "setVocabulary(vocabulary) -> retval\n.   * @brief Set the vocabulary for recognition.\n.        * @param[in] vocabulary the associated vocabulary of the network."},

    {NULL,          NULL}
};

// Converter (dnn_TextRecognitionModel)

template<>
struct PyOpenCV_Converter< cv::dnn::TextRecognitionModel >
{
    static PyObject* from(const cv::dnn::TextRecognitionModel& r)
    {
        return pyopencv_dnn_TextRecognitionModel_Instance(r);
    }
    static bool to(PyObject* src, cv::dnn::TextRecognitionModel& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::dnn::TextRecognitionModel * dst_;
        if (pyopencv_dnn_TextRecognitionModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::dnn::TextRecognitionModel for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// dnn_superres_DnnSuperResImpl (Generic)
//================================================================================

// GetSet (dnn_superres_DnnSuperResImpl)



// Methods (dnn_superres_DnnSuperResImpl)

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;

    Ptr<DnnSuperResImpl> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::dnn_superres::DnnSuperResImpl::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_getAlgorithm(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlgorithm());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_getScale(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_readModel(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    Napi::Value* pyobj_path = NULL;
    String path;

    const char* keywords[] = { "path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_superres_DnnSuperResImpl.readModel", (char**)keywords, &pyobj_path) &&
        jsopencv_to_safe(info, pyobj_path, path, ArgInfo("path", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->readModel(path));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_setModel(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    Napi::Value* pyobj_algo = NULL;
    String algo;
    Napi::Value* pyobj_scale = NULL;
    int scale=0;

    const char* keywords[] = { "algo", "scale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:dnn_superres_DnnSuperResImpl.setModel", (char**)keywords, &pyobj_algo, &pyobj_scale) &&
        jsopencv_to_safe(info, pyobj_algo, algo, ArgInfo("algo", 0)) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setModel(algo, scale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_setPreferableBackend(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    Napi::Value* pyobj_backendId = NULL;
    int backendId=0;

    const char* keywords[] = { "backendId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_superres_DnnSuperResImpl.setPreferableBackend", (char**)keywords, &pyobj_backendId) &&
        jsopencv_to_safe(info, pyobj_backendId, backendId, ArgInfo("backendId", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreferableBackend(backendId));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_setPreferableTarget(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    Napi::Value* pyobj_targetId = NULL;
    int targetId=0;

    const char* keywords[] = { "targetId", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:dnn_superres_DnnSuperResImpl.setPreferableTarget", (char**)keywords, &pyobj_targetId) &&
        jsopencv_to_safe(info, pyobj_targetId, targetId, ArgInfo("targetId", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPreferableTarget(targetId));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_upsample(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_result = NULL;
    Mat result;

    const char* keywords[] = { "img", "result", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_superres_DnnSuperResImpl.upsample", (char**)keywords, &pyobj_img, &pyobj_result) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_result, result, ArgInfo("result", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->upsample(img, result));
        return jsopencv_from(result);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_result = NULL;
    UMat result;

    const char* keywords[] = { "img", "result", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:dnn_superres_DnnSuperResImpl.upsample", (char**)keywords, &pyobj_img, &pyobj_result) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_result, result, ArgInfo("result", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->upsample(img, result));
        return jsopencv_from(result);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("upsample");

    return NULL;
}

static Napi::Value pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_upsampleMultioutput(const Napi::CallbackInfo &info)
{
    using namespace cv::dnn_superres;


    Ptr<cv::dnn_superres::DnnSuperResImpl> * self1 = 0;
    if (!pyopencv_dnn_superres_DnnSuperResImpl_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'dnn_superres_DnnSuperResImpl' or its derivative)");
    Ptr<cv::dnn_superres::DnnSuperResImpl> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_imgs_new = NULL;
    vector_Mat imgs_new;
    Napi::Value* pyobj_scale_factors = NULL;
    vector_int scale_factors;
    Napi::Value* pyobj_node_names = NULL;
    vector_String node_names;

    const char* keywords[] = { "img", "imgs_new", "scale_factors", "node_names", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:dnn_superres_DnnSuperResImpl.upsampleMultioutput", (char**)keywords, &pyobj_img, &pyobj_imgs_new, &pyobj_scale_factors, &pyobj_node_names) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_imgs_new, imgs_new, ArgInfo("imgs_new", 0)) &&
        jsopencv_to_safe(info, pyobj_scale_factors, scale_factors, ArgInfo("scale_factors", 0)) &&
        jsopencv_to_safe(info, pyobj_node_names, node_names, ArgInfo("node_names", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upsampleMultioutput(img, imgs_new, scale_factors, node_names));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_imgs_new = NULL;
    vector_Mat imgs_new;
    Napi::Value* pyobj_scale_factors = NULL;
    vector_int scale_factors;
    Napi::Value* pyobj_node_names = NULL;
    vector_String node_names;

    const char* keywords[] = { "img", "imgs_new", "scale_factors", "node_names", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:dnn_superres_DnnSuperResImpl.upsampleMultioutput", (char**)keywords, &pyobj_img, &pyobj_imgs_new, &pyobj_scale_factors, &pyobj_node_names) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_imgs_new, imgs_new, ArgInfo("imgs_new", 0)) &&
        jsopencv_to_safe(info, pyobj_scale_factors, scale_factors, ArgInfo("scale_factors", 0)) &&
        jsopencv_to_safe(info, pyobj_node_names, node_names, ArgInfo("node_names", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->upsampleMultioutput(img, imgs_new, scale_factors, node_names));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("upsampleMultioutput");

    return NULL;
}



// Tables (dnn_superres_DnnSuperResImpl)

static PyGetSetDef pyopencv_dnn_superres_DnnSuperResImpl_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_dnn_superres_DnnSuperResImpl_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_create_static, METH_STATIC), "create() -> retval\n.   @brief Empty constructor for python"},
    {"getAlgorithm", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_getAlgorithm, 0), "getAlgorithm() -> retval\n.   @brief Returns the scale factor of the model:\n.       @return Current algorithm."},
    {"getScale", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_getScale, 0), "getScale() -> retval\n.   @brief Returns the scale factor of the model:\n.       @return Current scale factor."},
    {"readModel", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_readModel, 0), "readModel(path) -> None\n.   @brief Read the model from the given path\n.       @param path Path to the model file."},
    {"setModel", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_setModel, 0), "setModel(algo, scale) -> None\n.   @brief Set desired model\n.       @param algo String containing one of the desired models:\n.           - __edsr__\n.           - __espcn__\n.           - __fsrcnn__\n.           - __lapsrn__\n.       @param scale Integer specifying the upscale factor"},
    {"setPreferableBackend", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_setPreferableBackend, 0), "setPreferableBackend(backendId) -> None\n.   @brief Set computation backend"},
    {"setPreferableTarget", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_setPreferableTarget, 0), "setPreferableTarget(targetId) -> None\n.   @brief Set computation target"},
    {"upsample", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_upsample, 0), "upsample(img[, result]) -> result\n.   @brief Upsample via neural network\n.       @param img Image to upscale\n.       @param result Destination upscaled image"},
    {"upsampleMultioutput", CV_JS_FN_WITH_KW_(pyopencv_cv_dnn_superres_dnn_superres_DnnSuperResImpl_upsampleMultioutput, 0), "upsampleMultioutput(img, imgs_new, scale_factors, node_names) -> None\n.   @brief Upsample via neural network of multiple outputs\n.       @param img Image to upscale\n.       @param imgs_new Destination upscaled images\n.       @param scale_factors Scaling factors of the output nodes\n.       @param node_names Names of the output nodes in the neural network"},

    {NULL,          NULL}
};

// Converter (dnn_superres_DnnSuperResImpl)

template<>
struct PyOpenCV_Converter< Ptr<cv::dnn_superres::DnnSuperResImpl> >
{
    static PyObject* from(const Ptr<cv::dnn_superres::DnnSuperResImpl>& r)
    {
        return pyopencv_dnn_superres_DnnSuperResImpl_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::dnn_superres::DnnSuperResImpl>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::dnn_superres::DnnSuperResImpl> * dst_;
        if (pyopencv_dnn_superres_DnnSuperResImpl_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::dnn_superres::DnnSuperResImpl> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_BIF (Generic)
//================================================================================

// GetSet (face_BIF)



// Methods (face_BIF)

static Napi::Value pyopencv_cv_face_face_BIF_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BIF> * self1 = 0;
    if (!pyopencv_face_BIF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BIF' or its derivative)");
    Ptr<cv::face::BIF> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_features = NULL;
    Mat features;

    const char* keywords[] = { "image", "features", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:face_BIF.compute", (char**)keywords, &pyobj_image, &pyobj_features) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(image, features));
        return jsopencv_from(features);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_features = NULL;
    UMat features;

    const char* keywords[] = { "image", "features", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:face_BIF.compute", (char**)keywords, &pyobj_image, &pyobj_features) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(image, features));
        return jsopencv_from(features);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BIF_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_num_bands = NULL;
    int num_bands=8;
    Napi::Value* pyobj_num_rotations = NULL;
    int num_rotations=12;
    Ptr<BIF> retval;

    const char* keywords[] = { "num_bands", "num_rotations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:face_BIF.create", (char**)keywords, &pyobj_num_bands, &pyobj_num_rotations) &&
        jsopencv_to_safe(info, pyobj_num_bands, num_bands, ArgInfo("num_bands", 0)) &&
        jsopencv_to_safe(info, pyobj_num_rotations, num_rotations, ArgInfo("num_rotations", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::BIF::create(num_bands, num_rotations));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BIF_getNumBands(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BIF> * self1 = 0;
    if (!pyopencv_face_BIF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BIF' or its derivative)");
    Ptr<cv::face::BIF> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumBands());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BIF_getNumRotations(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BIF> * self1 = 0;
    if (!pyopencv_face_BIF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BIF' or its derivative)");
    Ptr<cv::face::BIF> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumRotations());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (face_BIF)

static PyGetSetDef pyopencv_face_BIF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_BIF_methods[] =
{
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BIF_compute, 0), "compute(image[, features]) -> features\n.   Computes features sby input image.\n.        *  @param image Input image (CV_32FC1).\n.        *  @param features Feature vector (CV_32FC1)."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BIF_create_static, METH_STATIC), "create([, num_bands[, num_rotations]]) -> retval\n.   * @param num_bands The number of filter bands (<=8) used for computing BIF.\n.        * @param num_rotations The number of image rotations for computing BIF.\n.        * @returns Object for computing BIF."},
    {"getNumBands", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BIF_getNumBands, 0), "getNumBands() -> retval\n.   @returns The number of filter bands used for computing BIF."},
    {"getNumRotations", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BIF_getNumRotations, 0), "getNumRotations() -> retval\n.   @returns The number of image rotations."},

    {NULL,          NULL}
};

// Converter (face_BIF)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::BIF> >
{
    static PyObject* from(const Ptr<cv::face::BIF>& r)
    {
        return pyopencv_face_BIF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::BIF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::BIF> * dst_;
        if (pyopencv_face_BIF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::BIF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_BasicFaceRecognizer (Generic)
//================================================================================

// GetSet (face_BasicFaceRecognizer)



// Methods (face_BasicFaceRecognizer)

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getEigenValues(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEigenValues());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getEigenVectors(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEigenVectors());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLabels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getMean(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMean());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getNumComponents(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumComponents());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getProjections(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    std::vector<cv::Mat> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getProjections());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_setNumComponents(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_BasicFaceRecognizer.setNumComponents", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumComponents(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_BasicFaceRecognizer_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::BasicFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_BasicFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    Ptr<cv::face::BasicFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_BasicFaceRecognizer.setThreshold", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (face_BasicFaceRecognizer)

static PyGetSetDef pyopencv_face_BasicFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_BasicFaceRecognizer_methods[] =
{
    {"getEigenValues", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getEigenValues, 0), "getEigenValues() -> retval\n."},
    {"getEigenVectors", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getEigenVectors, 0), "getEigenVectors() -> retval\n."},
    {"getLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getLabels, 0), "getLabels() -> retval\n."},
    {"getMean", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getMean, 0), "getMean() -> retval\n."},
    {"getNumComponents", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getNumComponents, 0), "getNumComponents() -> retval\n.   @see setNumComponents"},
    {"getProjections", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getProjections, 0), "getProjections() -> retval\n."},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getThreshold, 0), "getThreshold() -> retval\n.   @see setThreshold"},
    {"setNumComponents", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_setNumComponents, 0), "setNumComponents(val) -> None\n.   @copybrief getNumComponents @see getNumComponents"},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_setThreshold, 0), "setThreshold(val) -> None\n.   @copybrief getThreshold @see getThreshold"},

    {NULL,          NULL}
};

// Converter (face_BasicFaceRecognizer)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::BasicFaceRecognizer> >
{
    static PyObject* from(const Ptr<cv::face::BasicFaceRecognizer>& r)
    {
        return pyopencv_face_BasicFaceRecognizer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::BasicFaceRecognizer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::BasicFaceRecognizer> * dst_;
        if (pyopencv_face_BasicFaceRecognizer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::BasicFaceRecognizer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_EigenFaceRecognizer (Generic)
//================================================================================

// GetSet (face_EigenFaceRecognizer)



// Methods (face_EigenFaceRecognizer)

static Napi::Value pyopencv_cv_face_face_EigenFaceRecognizer_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_num_components = NULL;
    int num_components=0;
    Napi::Value* pyobj_threshold = NULL;
    double threshold=DBL_MAX;
    Ptr<EigenFaceRecognizer> retval;

    const char* keywords[] = { "num_components", "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:face_EigenFaceRecognizer.create", (char**)keywords, &pyobj_num_components, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_num_components, num_components, ArgInfo("num_components", 0)) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::EigenFaceRecognizer::create(num_components, threshold));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (face_EigenFaceRecognizer)

static PyGetSetDef pyopencv_face_EigenFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_EigenFaceRecognizer_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_EigenFaceRecognizer_create_static, METH_STATIC), "create([, num_components[, threshold]]) -> retval\n.   @param num_components The number of components (read: Eigenfaces) kept for this Principal\n.       Component Analysis. As a hint: There's no rule how many components (read: Eigenfaces) should be\n.       kept for good reconstruction capabilities. It is based on your input data, so experiment with the\n.       number. Keeping 80 components should almost always be sufficient.\n.       @param threshold The threshold applied in the prediction.\n.   \n.       ### Notes:\n.   \n.       -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n.           color spaces.\n.       -   **THE EIGENFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n.           SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n.           input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n.           the images.\n.       -   This model does not support updating.\n.   \n.       ### Model internal data:\n.   \n.       -   num_components see EigenFaceRecognizer::create.\n.       -   threshold see EigenFaceRecognizer::create.\n.       -   eigenvalues The eigenvalues for this Principal Component Analysis (ordered descending).\n.       -   eigenvectors The eigenvectors for this Principal Component Analysis (ordered by their\n.           eigenvalue).\n.       -   mean The sample mean calculated from the training data.\n.       -   projections The projections of the training data.\n.       -   labels The threshold applied in the prediction. If the distance to the nearest neighbor is\n.           larger than the threshold, this method returns -1."},

    {NULL,          NULL}
};

// Converter (face_EigenFaceRecognizer)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::EigenFaceRecognizer> >
{
    static PyObject* from(const Ptr<cv::face::EigenFaceRecognizer>& r)
    {
        return pyopencv_face_EigenFaceRecognizer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::EigenFaceRecognizer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::EigenFaceRecognizer> * dst_;
        if (pyopencv_face_EigenFaceRecognizer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::EigenFaceRecognizer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_FaceRecognizer (Generic)
//================================================================================

// GetSet (face_FaceRecognizer)



// Methods (face_FaceRecognizer)

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_getLabelInfo(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_label = NULL;
    int label=0;
    String retval;

    const char* keywords[] = { "label", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.getLabelInfo", (char**)keywords, &pyobj_label) &&
        jsopencv_to_safe(info, pyobj_label, label, ArgInfo("label", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLabelInfo(label));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_getLabelsByString(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_str = NULL;
    String str;
    std::vector<int> retval;

    const char* keywords[] = { "str", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.getLabelsByString", (char**)keywords, &pyobj_str) &&
        jsopencv_to_safe(info, pyobj_str, str, ArgInfo("str", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLabelsByString(str));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_predict(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    int label;
    double confidence;

    const char* keywords[] = { "src", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.predict", (char**)keywords, &pyobj_src) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->predict(src, label, confidence));
        return Py_BuildValue("(NN)", jsopencv_from(label), jsopencv_from(confidence));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    int label;
    double confidence;

    const char* keywords[] = { "src", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.predict", (char**)keywords, &pyobj_src) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->predict(src, label, confidence));
        return Py_BuildValue("(NN)", jsopencv_from(label), jsopencv_from(confidence));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_predict_collect(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_collector = NULL;
    Ptr<PredictCollector> collector;

    const char* keywords[] = { "src", "collector", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.predict_collect", (char**)keywords, &pyobj_src, &pyobj_collector) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_collector, collector, ArgInfo("collector", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->predict(src, collector));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_collector = NULL;
    Ptr<PredictCollector> collector;

    const char* keywords[] = { "src", "collector", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.predict_collect", (char**)keywords, &pyobj_src, &pyobj_collector) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_collector, collector, ArgInfo("collector", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->predict(src, collector));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict_collect");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_predict_label(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    int retval;

    const char* keywords[] = { "src", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.predict_label", (char**)keywords, &pyobj_src) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(src));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    int retval;

    const char* keywords[] = { "src", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.predict_label", (char**)keywords, &pyobj_src) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(src));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict_label");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_read(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.read", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->read(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_setLabelInfo(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_label = NULL;
    int label=0;
    Napi::Value* pyobj_strInfo = NULL;
    String strInfo;

    const char* keywords[] = { "label", "strInfo", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.setLabelInfo", (char**)keywords, &pyobj_label, &pyobj_strInfo) &&
        jsopencv_to_safe(info, pyobj_label, label, ArgInfo("label", 0)) &&
        jsopencv_to_safe(info, pyobj_strInfo, strInfo, ArgInfo("strInfo", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLabelInfo(label, strInfo));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_train(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_labels = NULL;
    Mat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.train", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->train(src, labels));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_labels = NULL;
    UMat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.train", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->train(src, labels));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("train");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_update(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    vector_Mat src;
    Napi::Value* pyobj_labels = NULL;
    Mat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.update", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->update(src, labels));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    vector_UMat src;
    Napi::Value* pyobj_labels = NULL;
    UMat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:face_FaceRecognizer.update", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->update(src, labels));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("update");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_FaceRecognizer_write(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::FaceRecognizer> * self1 = 0;
    if (!pyopencv_face_FaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    Ptr<cv::face::FaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_FaceRecognizer.write", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->write(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (face_FaceRecognizer)

static PyGetSetDef pyopencv_face_FaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_FaceRecognizer_methods[] =
{
    {"getLabelInfo", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_getLabelInfo, 0), "getLabelInfo(label) -> retval\n.   @brief Gets string information by label.\n.   \n.       If an unknown label id is provided or there is no label information associated with the specified\n.       label id the method returns an empty string."},
    {"getLabelsByString", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_getLabelsByString, 0), "getLabelsByString(str) -> retval\n.   @brief Gets vector of labels by string.\n.   \n.       The function searches for the labels containing the specified sub-string in the associated string\n.       info."},
    {"predict", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_predict, 0), "predict(src) -> label, confidence\n.   @brief Predicts a label and associated confidence (e.g. distance) for a given input image.\n.   \n.       @param src Sample image to get a prediction from.\n.       @param label The predicted label for the given image.\n.       @param confidence Associated confidence (e.g. distance) for the predicted label.\n.   \n.       The suffix const means that prediction does not affect the internal model state, so the method can\n.       be safely called from within different threads.\n.   \n.       The following example shows how to get a prediction from a trained model:\n.   \n.       @code\n.       using namespace cv;\n.       // Do your initialization here (create the cv::FaceRecognizer model) ...\n.       // ...\n.       // Read in a sample image:\n.       Mat img = imread(\"person1/3.jpg\", IMREAD_GRAYSCALE);\n.       // And get a prediction from the cv::FaceRecognizer:\n.       int predicted = model->predict(img);\n.       @endcode\n.   \n.       Or to get a prediction and the associated confidence (e.g. distance):\n.   \n.       @code\n.       using namespace cv;\n.       // Do your initialization here (create the cv::FaceRecognizer model) ...\n.       // ...\n.       Mat img = imread(\"person1/3.jpg\", IMREAD_GRAYSCALE);\n.       // Some variables for the predicted label and associated confidence (e.g. distance):\n.       int predicted_label = -1;\n.       double predicted_confidence = 0.0;\n.       // Get the prediction and associated confidence from the model\n.       model->predict(img, predicted_label, predicted_confidence);\n.       @endcode"},
    {"predict_collect", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_predict_collect, 0), "predict_collect(src, collector) -> None\n.   @brief - if implemented - send all result of prediction to collector that can be used for somehow custom result handling\n.       @param src Sample image to get a prediction from.\n.       @param collector User-defined collector object that accepts all results\n.   \n.       To implement this method u just have to do same internal cycle as in predict(InputArray src, CV_OUT int &label, CV_OUT double &confidence) but\n.       not try to get \"best@ result, just resend it to caller side with given collector"},
    {"predict_label", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_predict_label, 0), "predict_label(src) -> retval\n.   @overload"},
    {"read", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_read, 0), "read(filename) -> None\n.   @brief Loads a FaceRecognizer and its model state.\n.   \n.       Loads a persisted model and state from a given XML or YAML file . Every FaceRecognizer has to\n.       overwrite FaceRecognizer::load(FileStorage& fs) to enable loading the model state.\n.       FaceRecognizer::load(FileStorage& fs) in turn gets called by\n.       FaceRecognizer::load(const String& filename), to ease saving a model."},
    {"setLabelInfo", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_setLabelInfo, 0), "setLabelInfo(label, strInfo) -> None\n.   @brief Sets string info for the specified model's label.\n.   \n.       The string info is replaced by the provided value if it was set before for the specified label."},
    {"train", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_train, 0), "train(src, labels) -> None\n.   @brief Trains a FaceRecognizer with given data and associated labels.\n.   \n.       @param src The training images, that means the faces you want to learn. The data has to be\n.       given as a vector\\<Mat\\>.\n.       @param labels The labels corresponding to the images have to be given either as a vector\\<int\\>\n.       or a Mat of type CV_32SC1.\n.   \n.       The following source code snippet shows you how to learn a Fisherfaces model on a given set of\n.       images. The images are read with imread and pushed into a std::vector\\<Mat\\>. The labels of each\n.       image are stored within a std::vector\\<int\\> (you could also use a Mat of type CV_32SC1). Think of\n.       the label as the subject (the person) this image belongs to, so same subjects (persons) should have\n.       the same label. For the available FaceRecognizer you don't have to pay any attention to the order of\n.       the labels, just make sure same persons have the same label:\n.   \n.       @code\n.       // holds images and labels\n.       vector<Mat> images;\n.       vector<int> labels;\n.       // using Mat of type CV_32SC1\n.       // Mat labels(number_of_samples, 1, CV_32SC1);\n.       // images for first person\n.       images.push_back(imread(\"person0/0.jpg\", IMREAD_GRAYSCALE)); labels.push_back(0);\n.       images.push_back(imread(\"person0/1.jpg\", IMREAD_GRAYSCALE)); labels.push_back(0);\n.       images.push_back(imread(\"person0/2.jpg\", IMREAD_GRAYSCALE)); labels.push_back(0);\n.       // images for second person\n.       images.push_back(imread(\"person1/0.jpg\", IMREAD_GRAYSCALE)); labels.push_back(1);\n.       images.push_back(imread(\"person1/1.jpg\", IMREAD_GRAYSCALE)); labels.push_back(1);\n.       images.push_back(imread(\"person1/2.jpg\", IMREAD_GRAYSCALE)); labels.push_back(1);\n.       @endcode\n.   \n.       Now that you have read some images, we can create a new FaceRecognizer. In this example I'll create\n.       a Fisherfaces model and decide to keep all of the possible Fisherfaces:\n.   \n.       @code\n.       // Create a new Fisherfaces model and retain all available Fisherfaces,\n.       // this is the most common usage of this specific FaceRecognizer:\n.       //\n.       Ptr<FaceRecognizer> model =  FisherFaceRecognizer::create();\n.       @endcode\n.   \n.       And finally train it on the given dataset (the face images and labels):\n.   \n.       @code\n.       // This is the common interface to train all of the available cv::FaceRecognizer\n.       // implementations:\n.       //\n.       model->train(images, labels);\n.       @endcode"},
    {"update", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_update, 0), "update(src, labels) -> None\n.   @brief Updates a FaceRecognizer with given data and associated labels.\n.   \n.       @param src The training images, that means the faces you want to learn. The data has to be given\n.       as a vector\\<Mat\\>.\n.       @param labels The labels corresponding to the images have to be given either as a vector\\<int\\> or\n.       a Mat of type CV_32SC1.\n.   \n.       This method updates a (probably trained) FaceRecognizer, but only if the algorithm supports it. The\n.       Local Binary Patterns Histograms (LBPH) recognizer (see createLBPHFaceRecognizer) can be updated.\n.       For the Eigenfaces and Fisherfaces method, this is algorithmically not possible and you have to\n.       re-estimate the model with FaceRecognizer::train. In any case, a call to train empties the existing\n.       model and learns a new model, while update does not delete any model data.\n.   \n.       @code\n.       // Create a new LBPH model (it can be updated) and use the default parameters,\n.       // this is the most common usage of this specific FaceRecognizer:\n.       //\n.       Ptr<FaceRecognizer> model =  LBPHFaceRecognizer::create();\n.       // This is the common interface to train all of the available cv::FaceRecognizer\n.       // implementations:\n.       //\n.       model->train(images, labels);\n.       // Some containers to hold new image:\n.       vector<Mat> newImages;\n.       vector<int> newLabels;\n.       // You should add some images to the containers:\n.       //\n.       // ...\n.       //\n.       // Now updating the model is as easy as calling:\n.       model->update(newImages,newLabels);\n.       // This will preserve the old model data and extend the existing model\n.       // with the new features extracted from newImages!\n.       @endcode\n.   \n.       Calling update on an Eigenfaces model (see EigenFaceRecognizer::create), which doesn't support\n.       updating, will throw an error similar to:\n.   \n.       @code\n.       OpenCV Error: The function/feature is not implemented (This FaceRecognizer (FaceRecognizer.Eigenfaces) does not support updating, you have to use FaceRecognizer::train to update it.) in update, file /home/philipp/git/opencv/modules/contrib/src/facerec.cpp, line 305\n.       terminate called after throwing an instance of 'cv::Exception'\n.       @endcode\n.   \n.       @note The FaceRecognizer does not store your training images, because this would be very\n.       memory intense and it's not the responsibility of te FaceRecognizer to do so. The caller is\n.       responsible for maintaining the dataset, he want to work with."},
    {"write", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_write, 0), "write(filename) -> None\n.   @brief Saves a FaceRecognizer and its model state.\n.   \n.       Saves this model to a given filename, either as XML or YAML.\n.       @param filename The filename to store this FaceRecognizer to (either XML/YAML).\n.   \n.       Every FaceRecognizer overwrites FaceRecognizer::save(FileStorage& fs) to save the internal model\n.       state. FaceRecognizer::save(const String& filename) saves the state of a model to the given\n.       filename.\n.   \n.       The suffix const means that prediction does not affect the internal model state, so the method can\n.       be safely called from within different threads."},

    {NULL,          NULL}
};

// Converter (face_FaceRecognizer)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::FaceRecognizer> >
{
    static PyObject* from(const Ptr<cv::face::FaceRecognizer>& r)
    {
        return pyopencv_face_FaceRecognizer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::FaceRecognizer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::FaceRecognizer> * dst_;
        if (pyopencv_face_FaceRecognizer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::FaceRecognizer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_Facemark (Generic)
//================================================================================

// GetSet (face_Facemark)



// Methods (face_Facemark)

static Napi::Value pyopencv_cv_face_face_Facemark_fit(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::Facemark> * self1 = 0;
    if (!pyopencv_face_Facemark_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_Facemark' or its derivative)");
    Ptr<cv::face::Facemark> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_faces = NULL;
    Mat faces;
    Napi::Value* pyobj_landmarks = NULL;
    vector_Mat landmarks;
    bool retval;

    const char* keywords[] = { "image", "faces", "landmarks", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:face_Facemark.fit", (char**)keywords, &pyobj_image, &pyobj_faces, &pyobj_landmarks) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_faces, faces, ArgInfo("faces", 0)) &&
        jsopencv_to_safe(info, pyobj_landmarks, landmarks, ArgInfo("landmarks", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->fit(image, faces, landmarks));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(landmarks));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_faces = NULL;
    UMat faces;
    Napi::Value* pyobj_landmarks = NULL;
    vector_UMat landmarks;
    bool retval;

    const char* keywords[] = { "image", "faces", "landmarks", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:face_Facemark.fit", (char**)keywords, &pyobj_image, &pyobj_faces, &pyobj_landmarks) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_faces, faces, ArgInfo("faces", 0)) &&
        jsopencv_to_safe(info, pyobj_landmarks, landmarks, ArgInfo("landmarks", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->fit(image, faces, landmarks));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(landmarks));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("fit");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_Facemark_loadModel(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::Facemark> * self1 = 0;
    if (!pyopencv_face_Facemark_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_Facemark' or its derivative)");
    Ptr<cv::face::Facemark> _self_ = *(self1);
    Napi::Value* pyobj_model = NULL;
    String model;

    const char* keywords[] = { "model", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_Facemark.loadModel", (char**)keywords, &pyobj_model) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->loadModel(model));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (face_Facemark)

static PyGetSetDef pyopencv_face_Facemark_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_Facemark_methods[] =
{
    {"fit", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_Facemark_fit, 0), "fit(image, faces[, landmarks]) -> retval, landmarks\n.   @brief Detect facial landmarks from an image.\n.       @param image Input image.\n.       @param faces Output of the function which represent region of interest of the detected faces.\n.       Each face is stored in cv::Rect container.\n.       @param landmarks The detected landmark points for each faces.\n.   \n.       <B>Example of usage</B>\n.       @code\n.       Mat image = imread(\"image.jpg\");\n.       std::vector<Rect> faces;\n.       std::vector<std::vector<Point2f> > landmarks;\n.       facemark->fit(image, faces, landmarks);\n.       @endcode"},
    {"loadModel", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_Facemark_loadModel, 0), "loadModel(model) -> None\n.   @brief A function to load the trained model before the fitting process.\n.       @param model A string represent the filename of a trained model.\n.   \n.       <B>Example of usage</B>\n.       @code\n.       facemark->loadModel(\"../data/lbf.model\");\n.       @endcode"},

    {NULL,          NULL}
};

// Converter (face_Facemark)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::Facemark> >
{
    static PyObject* from(const Ptr<cv::face::Facemark>& r)
    {
        return pyopencv_face_Facemark_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::Facemark>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::Facemark> * dst_;
        if (pyopencv_face_Facemark_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::Facemark> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_FacemarkAAM (Generic)
//================================================================================

// GetSet (face_FacemarkAAM)



// Methods (face_FacemarkAAM)



// Tables (face_FacemarkAAM)

static PyGetSetDef pyopencv_face_FacemarkAAM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_FacemarkAAM_methods[] =
{

    {NULL,          NULL}
};

// Converter (face_FacemarkAAM)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::FacemarkAAM> >
{
    static PyObject* from(const Ptr<cv::face::FacemarkAAM>& r)
    {
        return pyopencv_face_FacemarkAAM_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::FacemarkAAM>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::FacemarkAAM> * dst_;
        if (pyopencv_face_FacemarkAAM_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::FacemarkAAM> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_FacemarkKazemi (Generic)
//================================================================================

// GetSet (face_FacemarkKazemi)



// Methods (face_FacemarkKazemi)



// Tables (face_FacemarkKazemi)

static PyGetSetDef pyopencv_face_FacemarkKazemi_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_FacemarkKazemi_methods[] =
{

    {NULL,          NULL}
};

// Converter (face_FacemarkKazemi)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::FacemarkKazemi> >
{
    static PyObject* from(const Ptr<cv::face::FacemarkKazemi>& r)
    {
        return pyopencv_face_FacemarkKazemi_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::FacemarkKazemi>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::FacemarkKazemi> * dst_;
        if (pyopencv_face_FacemarkKazemi_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::FacemarkKazemi> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_FacemarkLBF (Generic)
//================================================================================

// GetSet (face_FacemarkLBF)



// Methods (face_FacemarkLBF)



// Tables (face_FacemarkLBF)

static PyGetSetDef pyopencv_face_FacemarkLBF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_FacemarkLBF_methods[] =
{

    {NULL,          NULL}
};

// Converter (face_FacemarkLBF)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::FacemarkLBF> >
{
    static PyObject* from(const Ptr<cv::face::FacemarkLBF>& r)
    {
        return pyopencv_face_FacemarkLBF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::FacemarkLBF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::FacemarkLBF> * dst_;
        if (pyopencv_face_FacemarkLBF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::FacemarkLBF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_FacemarkTrain (Generic)
//================================================================================

// GetSet (face_FacemarkTrain)



// Methods (face_FacemarkTrain)



// Tables (face_FacemarkTrain)

static PyGetSetDef pyopencv_face_FacemarkTrain_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_FacemarkTrain_methods[] =
{

    {NULL,          NULL}
};

// Converter (face_FacemarkTrain)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::FacemarkTrain> >
{
    static PyObject* from(const Ptr<cv::face::FacemarkTrain>& r)
    {
        return pyopencv_face_FacemarkTrain_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::FacemarkTrain>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::FacemarkTrain> * dst_;
        if (pyopencv_face_FacemarkTrain_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::FacemarkTrain> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_FisherFaceRecognizer (Generic)
//================================================================================

// GetSet (face_FisherFaceRecognizer)



// Methods (face_FisherFaceRecognizer)

static Napi::Value pyopencv_cv_face_face_FisherFaceRecognizer_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_num_components = NULL;
    int num_components=0;
    Napi::Value* pyobj_threshold = NULL;
    double threshold=DBL_MAX;
    Ptr<FisherFaceRecognizer> retval;

    const char* keywords[] = { "num_components", "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:face_FisherFaceRecognizer.create", (char**)keywords, &pyobj_num_components, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_num_components, num_components, ArgInfo("num_components", 0)) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::FisherFaceRecognizer::create(num_components, threshold));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (face_FisherFaceRecognizer)

static PyGetSetDef pyopencv_face_FisherFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_FisherFaceRecognizer_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_FisherFaceRecognizer_create_static, METH_STATIC), "create([, num_components[, threshold]]) -> retval\n.   @param num_components The number of components (read: Fisherfaces) kept for this Linear\n.       Discriminant Analysis with the Fisherfaces criterion. It's useful to keep all components, that\n.       means the number of your classes c (read: subjects, persons you want to recognize). If you leave\n.       this at the default (0) or set it to a value less-equal 0 or greater (c-1), it will be set to the\n.       correct number (c-1) automatically.\n.       @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n.       is larger than the threshold, this method returns -1.\n.   \n.       ### Notes:\n.   \n.       -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n.           color spaces.\n.       -   **THE FISHERFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n.           SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n.           input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n.           the images.\n.       -   This model does not support updating.\n.   \n.       ### Model internal data:\n.   \n.       -   num_components see FisherFaceRecognizer::create.\n.       -   threshold see FisherFaceRecognizer::create.\n.       -   eigenvalues The eigenvalues for this Linear Discriminant Analysis (ordered descending).\n.       -   eigenvectors The eigenvectors for this Linear Discriminant Analysis (ordered by their\n.           eigenvalue).\n.       -   mean The sample mean calculated from the training data.\n.       -   projections The projections of the training data.\n.       -   labels The labels corresponding to the projections."},

    {NULL,          NULL}
};

// Converter (face_FisherFaceRecognizer)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::FisherFaceRecognizer> >
{
    static PyObject* from(const Ptr<cv::face::FisherFaceRecognizer>& r)
    {
        return pyopencv_face_FisherFaceRecognizer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::FisherFaceRecognizer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::FisherFaceRecognizer> * dst_;
        if (pyopencv_face_FisherFaceRecognizer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::FisherFaceRecognizer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_LBPHFaceRecognizer (Generic)
//================================================================================

// GetSet (face_LBPHFaceRecognizer)



// Methods (face_LBPHFaceRecognizer)

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_radius = NULL;
    int radius=1;
    Napi::Value* pyobj_neighbors = NULL;
    int neighbors=8;
    Napi::Value* pyobj_grid_x = NULL;
    int grid_x=8;
    Napi::Value* pyobj_grid_y = NULL;
    int grid_y=8;
    Napi::Value* pyobj_threshold = NULL;
    double threshold=DBL_MAX;
    Ptr<LBPHFaceRecognizer> retval;

    const char* keywords[] = { "radius", "neighbors", "grid_x", "grid_y", "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:face_LBPHFaceRecognizer.create", (char**)keywords, &pyobj_radius, &pyobj_neighbors, &pyobj_grid_x, &pyobj_grid_y, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)) &&
        jsopencv_to_safe(info, pyobj_neighbors, neighbors, ArgInfo("neighbors", 0)) &&
        jsopencv_to_safe(info, pyobj_grid_x, grid_x, ArgInfo("grid_x", 0)) &&
        jsopencv_to_safe(info, pyobj_grid_y, grid_y, ArgInfo("grid_y", 0)) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::LBPHFaceRecognizer::create(radius, neighbors, grid_x, grid_y, threshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getGridX(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGridX());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getGridY(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGridY());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getHistograms(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    std::vector<cv::Mat> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getHistograms());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLabels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getNeighbors(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNeighbors());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_getThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_setGridX(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_LBPHFaceRecognizer.setGridX", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGridX(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_setGridY(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_LBPHFaceRecognizer.setGridY", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGridY(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_setNeighbors(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_LBPHFaceRecognizer.setNeighbors", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNeighbors(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_setRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_LBPHFaceRecognizer.setRadius", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRadius(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_LBPHFaceRecognizer_setThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::LBPHFaceRecognizer> * self1 = 0;
    if (!pyopencv_face_LBPHFaceRecognizer_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    Ptr<cv::face::LBPHFaceRecognizer> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_LBPHFaceRecognizer.setThreshold", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (face_LBPHFaceRecognizer)

static PyGetSetDef pyopencv_face_LBPHFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_LBPHFaceRecognizer_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_create_static, METH_STATIC), "create([, radius[, neighbors[, grid_x[, grid_y[, threshold]]]]]) -> retval\n.   @param radius The radius used for building the Circular Local Binary Pattern. The greater the\n.       radius, the smoother the image but more spatial information you can get.\n.       @param neighbors The number of sample points to build a Circular Local Binary Pattern from. An\n.       appropriate value is to use `8` sample points. Keep in mind: the more sample points you include,\n.       the higher the computational cost.\n.       @param grid_x The number of cells in the horizontal direction, 8 is a common value used in\n.       publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n.       feature vector.\n.       @param grid_y The number of cells in the vertical direction, 8 is a common value used in\n.       publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n.       feature vector.\n.       @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n.       is larger than the threshold, this method returns -1.\n.   \n.       ### Notes:\n.   \n.       -   The Circular Local Binary Patterns (used in training and prediction) expect the data given as\n.           grayscale images, use cvtColor to convert between the color spaces.\n.       -   This model supports updating.\n.   \n.       ### Model internal data:\n.   \n.       -   radius see LBPHFaceRecognizer::create.\n.       -   neighbors see LBPHFaceRecognizer::create.\n.       -   grid_x see LLBPHFaceRecognizer::create.\n.       -   grid_y see LBPHFaceRecognizer::create.\n.       -   threshold see LBPHFaceRecognizer::create.\n.       -   histograms Local Binary Patterns Histograms calculated from the given training data (empty if\n.           none was given).\n.       -   labels Labels corresponding to the calculated Local Binary Patterns Histograms."},
    {"getGridX", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getGridX, 0), "getGridX() -> retval\n.   @see setGridX"},
    {"getGridY", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getGridY, 0), "getGridY() -> retval\n.   @see setGridY"},
    {"getHistograms", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getHistograms, 0), "getHistograms() -> retval\n."},
    {"getLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getLabels, 0), "getLabels() -> retval\n."},
    {"getNeighbors", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getNeighbors, 0), "getNeighbors() -> retval\n.   @see setNeighbors"},
    {"getRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getRadius, 0), "getRadius() -> retval\n.   @see setRadius"},
    {"getThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getThreshold, 0), "getThreshold() -> retval\n.   @see setThreshold"},
    {"setGridX", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setGridX, 0), "setGridX(val) -> None\n.   @copybrief getGridX @see getGridX"},
    {"setGridY", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setGridY, 0), "setGridY(val) -> None\n.   @copybrief getGridY @see getGridY"},
    {"setNeighbors", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setNeighbors, 0), "setNeighbors(val) -> None\n.   @copybrief getNeighbors @see getNeighbors"},
    {"setRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setRadius, 0), "setRadius(val) -> None\n.   @copybrief getRadius @see getRadius"},
    {"setThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setThreshold, 0), "setThreshold(val) -> None\n.   @copybrief getThreshold @see getThreshold"},

    {NULL,          NULL}
};

// Converter (face_LBPHFaceRecognizer)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::LBPHFaceRecognizer> >
{
    static PyObject* from(const Ptr<cv::face::LBPHFaceRecognizer>& r)
    {
        return pyopencv_face_LBPHFaceRecognizer_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::LBPHFaceRecognizer>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::LBPHFaceRecognizer> * dst_;
        if (pyopencv_face_LBPHFaceRecognizer_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::LBPHFaceRecognizer> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_MACE (Generic)
//================================================================================

// GetSet (face_MACE)



// Methods (face_MACE)

static Napi::Value pyopencv_cv_face_face_MACE_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_IMGSIZE = NULL;
    int IMGSIZE=64;
    cv::Ptr<MACE> retval;

    const char* keywords[] = { "IMGSIZE", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:face_MACE.create", (char**)keywords, &pyobj_IMGSIZE) &&
        jsopencv_to_safe(info, pyobj_IMGSIZE, IMGSIZE, ArgInfo("IMGSIZE", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::MACE::create(IMGSIZE));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_MACE_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_objname = NULL;
    String objname;
    cv::Ptr<MACE> retval;

    const char* keywords[] = { "filename", "objname", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:face_MACE.load", (char**)keywords, &pyobj_filename, &pyobj_objname) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_objname, objname, ArgInfo("objname", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::MACE::load(filename, objname));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_MACE_salt(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::MACE> * self1 = 0;
    if (!pyopencv_face_MACE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_MACE' or its derivative)");
    Ptr<cv::face::MACE> _self_ = *(self1);
    Napi::Value* pyobj_passphrase = NULL;
    String passphrase;

    const char* keywords[] = { "passphrase", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_MACE.salt", (char**)keywords, &pyobj_passphrase) &&
        jsopencv_to_safe(info, pyobj_passphrase, passphrase, ArgInfo("passphrase", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->salt(passphrase));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_MACE_same(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::MACE> * self1 = 0;
    if (!pyopencv_face_MACE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_MACE' or its derivative)");
    Ptr<cv::face::MACE> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_query = NULL;
    Mat query;
    bool retval;

    const char* keywords[] = { "query", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_MACE.same", (char**)keywords, &pyobj_query) &&
        jsopencv_to_safe(info, pyobj_query, query, ArgInfo("query", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->same(query));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_query = NULL;
    UMat query;
    bool retval;

    const char* keywords[] = { "query", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_MACE.same", (char**)keywords, &pyobj_query) &&
        jsopencv_to_safe(info, pyobj_query, query, ArgInfo("query", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->same(query));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("same");

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_MACE_train(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::MACE> * self1 = 0;
    if (!pyopencv_face_MACE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_MACE' or its derivative)");
    Ptr<cv::face::MACE> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_images = NULL;
    vector_Mat images;

    const char* keywords[] = { "images", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_MACE.train", (char**)keywords, &pyobj_images) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->train(images));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_images = NULL;
    vector_UMat images;

    const char* keywords[] = { "images", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:face_MACE.train", (char**)keywords, &pyobj_images) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->train(images));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("train");

    return NULL;
}



// Tables (face_MACE)

static PyGetSetDef pyopencv_face_MACE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_MACE_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_MACE_create_static, METH_STATIC), "create([, IMGSIZE]) -> retval\n.   @brief constructor\n.       @param IMGSIZE  images will get resized to this (should be an even number)"},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_MACE_load_static, METH_STATIC), "load(filename[, objname]) -> retval\n.   @brief constructor\n.       @param filename  build a new MACE instance from a pre-serialized FileStorage\n.       @param objname (optional) top-level node in the FileStorage"},
    {"salt", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_MACE_salt, 0), "salt(passphrase) -> None\n.   @brief optionally encrypt images with random convolution\n.       @param passphrase a crc64 random seed will get generated from this"},
    {"same", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_MACE_same, 0), "same(query) -> retval\n.   @brief correlate query img and threshold to min class value\n.       @param query  a Mat with query image"},
    {"train", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_MACE_train, 0), "train(images) -> None\n.   @brief train it on positive features\n.          compute the mace filter: `h = D(-1) * X * (X(+) * D(-1) * X)(-1) * C`\n.          also calculate a minimal threshold for this class, the smallest self-similarity from the train images\n.       @param images  a vector<Mat> with the train images"},

    {NULL,          NULL}
};

// Converter (face_MACE)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::MACE> >
{
    static PyObject* from(const Ptr<cv::face::MACE>& r)
    {
        return pyopencv_face_MACE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::MACE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::MACE> * dst_;
        if (pyopencv_face_MACE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::MACE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_PredictCollector (Generic)
//================================================================================

// GetSet (face_PredictCollector)



// Methods (face_PredictCollector)



// Tables (face_PredictCollector)

static PyGetSetDef pyopencv_face_PredictCollector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_PredictCollector_methods[] =
{

    {NULL,          NULL}
};

// Converter (face_PredictCollector)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::PredictCollector> >
{
    static PyObject* from(const Ptr<cv::face::PredictCollector>& r)
    {
        return pyopencv_face_PredictCollector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::PredictCollector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::PredictCollector> * dst_;
        if (pyopencv_face_PredictCollector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::PredictCollector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// face_StandardCollector (Generic)
//================================================================================

// GetSet (face_StandardCollector)



// Methods (face_StandardCollector)

static Napi::Value pyopencv_cv_face_face_StandardCollector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::face;

    Napi::Value* pyobj_threshold = NULL;
    double threshold=DBL_MAX;
    Ptr<StandardCollector> retval;

    const char* keywords[] = { "threshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:face_StandardCollector.create", (char**)keywords, &pyobj_threshold) &&
        jsopencv_to_safe(info, pyobj_threshold, threshold, ArgInfo("threshold", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::face::StandardCollector::create(threshold));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_StandardCollector_getMinDist(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::StandardCollector> * self1 = 0;
    if (!pyopencv_face_StandardCollector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_StandardCollector' or its derivative)");
    Ptr<cv::face::StandardCollector> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinDist());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_StandardCollector_getMinLabel(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::StandardCollector> * self1 = 0;
    if (!pyopencv_face_StandardCollector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_StandardCollector' or its derivative)");
    Ptr<cv::face::StandardCollector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinLabel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_face_face_StandardCollector_getResults(const Napi::CallbackInfo &info)
{
    using namespace cv::face;


    Ptr<cv::face::StandardCollector> * self1 = 0;
    if (!pyopencv_face_StandardCollector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'face_StandardCollector' or its derivative)");
    Ptr<cv::face::StandardCollector> _self_ = *(self1);
    Napi::Value* pyobj_sorted = NULL;
    bool sorted=false;
    std::vector< std::pair<int, double> > retval;

    const char* keywords[] = { "sorted", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:face_StandardCollector.getResults", (char**)keywords, &pyobj_sorted) &&
        jsopencv_to_safe(info, pyobj_sorted, sorted, ArgInfo("sorted", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getResults(sorted));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (face_StandardCollector)

static PyGetSetDef pyopencv_face_StandardCollector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_face_StandardCollector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_create_static, METH_STATIC), "create([, threshold]) -> retval\n.   @brief Static constructor\n.       @param threshold set threshold"},
    {"getMinDist", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_getMinDist, 0), "getMinDist() -> retval\n.   @brief Returns minimal distance value"},
    {"getMinLabel", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_getMinLabel, 0), "getMinLabel() -> retval\n.   @brief Returns label with minimal distance"},
    {"getResults", CV_JS_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_getResults, 0), "getResults([, sorted]) -> retval\n.   @brief Return results as vector\n.       @param sorted If set, results will be sorted by distance\n.       Each values is a pair of label and distance."},

    {NULL,          NULL}
};

// Converter (face_StandardCollector)

template<>
struct PyOpenCV_Converter< Ptr<cv::face::StandardCollector> >
{
    static PyObject* from(const Ptr<cv::face::StandardCollector>& r)
    {
        return pyopencv_face_StandardCollector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::face::StandardCollector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::face::StandardCollector> * dst_;
        if (pyopencv_face_StandardCollector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::face::StandardCollector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// flann_Index (Generic)
//================================================================================

// GetSet (flann_Index)



// Methods (flann_Index)

static int pyopencv_cv_flann_flann_Index_Index(pyopencv_flann_Index_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::flann;

    pyPrepareArgumentConversionErrorsStorage(3);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::flann::Index()));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_features = NULL;
    Mat features;
    Napi::Value* pyobj_params = NULL;
    IndexParams params;
    Napi::Value* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Index", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)) &&
        jsopencv_to_safe(info, pyobj_distType, distType, ArgInfo("distType", 0)))
    {
        new (&(self->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::flann::Index(features, params, distType)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_features = NULL;
    UMat features;
    Napi::Value* pyobj_params = NULL;
    IndexParams params;
    Napi::Value* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:Index", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)) &&
        jsopencv_to_safe(info, pyobj_distType, distType, ArgInfo("distType", 0)))
    {
        new (&(self->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::flann::Index(features, params, distType)));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Index");

    return -1;
}

static Napi::Value pyopencv_cv_flann_flann_Index_build(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_features = NULL;
    Mat features;
    Napi::Value* pyobj_params = NULL;
    IndexParams params;
    Napi::Value* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:flann_Index.build", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)) &&
        jsopencv_to_safe(info, pyobj_distType, distType, ArgInfo("distType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->build(features, params, distType));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_features = NULL;
    UMat features;
    Napi::Value* pyobj_params = NULL;
    IndexParams params;
    Napi::Value* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:flann_Index.build", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)) &&
        jsopencv_to_safe(info, pyobj_distType, distType, ArgInfo("distType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->build(features, params, distType));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("build");

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_getAlgorithm(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    cvflann::flann_algorithm_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlgorithm());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_getDistance(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    cvflann::flann_distance_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDistance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_knnSearch(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_query = NULL;
    Mat query;
    Napi::Value* pyobj_indices = NULL;
    Mat indices;
    Napi::Value* pyobj_dists = NULL;
    Mat dists;
    Napi::Value* pyobj_knn = NULL;
    int knn=0;
    Napi::Value* pyobj_params = NULL;
    SearchParams params;

    const char* keywords[] = { "query", "knn", "indices", "dists", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:flann_Index.knnSearch", (char**)keywords, &pyobj_query, &pyobj_knn, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_query, query, ArgInfo("query", 0)) &&
        jsopencv_to_safe(info, pyobj_indices, indices, ArgInfo("indices", 1)) &&
        jsopencv_to_safe(info, pyobj_dists, dists, ArgInfo("dists", 1)) &&
        jsopencv_to_safe(info, pyobj_knn, knn, ArgInfo("knn", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->knnSearch(query, indices, dists, knn, params));
        return Py_BuildValue("(NN)", jsopencv_from(indices), jsopencv_from(dists));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_query = NULL;
    UMat query;
    Napi::Value* pyobj_indices = NULL;
    UMat indices;
    Napi::Value* pyobj_dists = NULL;
    UMat dists;
    Napi::Value* pyobj_knn = NULL;
    int knn=0;
    Napi::Value* pyobj_params = NULL;
    SearchParams params;

    const char* keywords[] = { "query", "knn", "indices", "dists", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:flann_Index.knnSearch", (char**)keywords, &pyobj_query, &pyobj_knn, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_query, query, ArgInfo("query", 0)) &&
        jsopencv_to_safe(info, pyobj_indices, indices, ArgInfo("indices", 1)) &&
        jsopencv_to_safe(info, pyobj_dists, dists, ArgInfo("dists", 1)) &&
        jsopencv_to_safe(info, pyobj_knn, knn, ArgInfo("knn", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->knnSearch(query, indices, dists, knn, params));
        return Py_BuildValue("(NN)", jsopencv_from(indices), jsopencv_from(dists));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("knnSearch");

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_load(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_features = NULL;
    Mat features;
    Napi::Value* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "features", "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:flann_Index.load", (char**)keywords, &pyobj_features, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->load(features, filename));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_features = NULL;
    UMat features;
    Napi::Value* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "features", "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:flann_Index.load", (char**)keywords, &pyobj_features, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->load(features, filename));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("load");

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_radiusSearch(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_query = NULL;
    Mat query;
    Napi::Value* pyobj_indices = NULL;
    Mat indices;
    Napi::Value* pyobj_dists = NULL;
    Mat dists;
    Napi::Value* pyobj_radius = NULL;
    double radius=0;
    Napi::Value* pyobj_maxResults = NULL;
    int maxResults=0;
    Napi::Value* pyobj_params = NULL;
    SearchParams params;
    int retval;

    const char* keywords[] = { "query", "radius", "maxResults", "indices", "dists", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOO:flann_Index.radiusSearch", (char**)keywords, &pyobj_query, &pyobj_radius, &pyobj_maxResults, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_query, query, ArgInfo("query", 0)) &&
        jsopencv_to_safe(info, pyobj_indices, indices, ArgInfo("indices", 1)) &&
        jsopencv_to_safe(info, pyobj_dists, dists, ArgInfo("dists", 1)) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)) &&
        jsopencv_to_safe(info, pyobj_maxResults, maxResults, ArgInfo("maxResults", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->radiusSearch(query, indices, dists, radius, maxResults, params));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(indices), jsopencv_from(dists));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_query = NULL;
    UMat query;
    Napi::Value* pyobj_indices = NULL;
    UMat indices;
    Napi::Value* pyobj_dists = NULL;
    UMat dists;
    Napi::Value* pyobj_radius = NULL;
    double radius=0;
    Napi::Value* pyobj_maxResults = NULL;
    int maxResults=0;
    Napi::Value* pyobj_params = NULL;
    SearchParams params;
    int retval;

    const char* keywords[] = { "query", "radius", "maxResults", "indices", "dists", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOO:flann_Index.radiusSearch", (char**)keywords, &pyobj_query, &pyobj_radius, &pyobj_maxResults, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_query, query, ArgInfo("query", 0)) &&
        jsopencv_to_safe(info, pyobj_indices, indices, ArgInfo("indices", 1)) &&
        jsopencv_to_safe(info, pyobj_dists, dists, ArgInfo("dists", 1)) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)) &&
        jsopencv_to_safe(info, pyobj_maxResults, maxResults, ArgInfo("maxResults", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->radiusSearch(query, indices, dists, radius, maxResults, params));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(indices), jsopencv_from(dists));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("radiusSearch");

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_release(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_flann_flann_Index_save(const Napi::CallbackInfo &info)
{
    using namespace cv::flann;


    Ptr<cv::flann::Index> * self1 = 0;
    if (!pyopencv_flann_Index_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    Ptr<cv::flann::Index> _self_ = *(self1);
    Napi::Value* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:flann_Index.save", (char**)keywords, &pyobj_filename) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->save(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (flann_Index)

static PyGetSetDef pyopencv_flann_Index_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_flann_Index_methods[] =
{
    {"build", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_build, 0), "build(features, params[, distType]) -> None\n."},
    {"getAlgorithm", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_getAlgorithm, 0), "getAlgorithm() -> retval\n."},
    {"getDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_getDistance, 0), "getDistance() -> retval\n."},
    {"knnSearch", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_knnSearch, 0), "knnSearch(query, knn[, indices[, dists[, params]]]) -> indices, dists\n."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_load, 0), "load(features, filename) -> retval\n."},
    {"radiusSearch", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_radiusSearch, 0), "radiusSearch(query, radius, maxResults[, indices[, dists[, params]]]) -> retval, indices, dists\n."},
    {"release", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_release, 0), "release() -> None\n."},
    {"save", CV_JS_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_save, 0), "save(filename) -> None\n."},

    {NULL,          NULL}
};

// Converter (flann_Index)

template<>
struct PyOpenCV_Converter< Ptr<cv::flann::Index> >
{
    static PyObject* from(const Ptr<cv::flann::Index>& r)
    {
        return pyopencv_flann_Index_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::flann::Index>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::flann::Index> * dst_;
        if (pyopencv_flann_Index_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::flann::Index> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_GNetPackage (Generic)
//================================================================================

// GetSet (gapi_GNetPackage)



// Methods (gapi_GNetPackage)

static int pyopencv_cv_gapi_gapi_GNetPackage_GNetPackage(pyopencv_gapi_GNetPackage_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::GNetPackage());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_nets = NULL;
    vector_GNetParam nets;

    const char* keywords[] = { "nets", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GNetPackage", (char**)keywords, &pyobj_nets) &&
        jsopencv_to_safe(info, pyobj_nets, nets, ArgInfo("nets", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::GNetPackage(nets));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("GNetPackage");

    return -1;
}



// Tables (gapi_GNetPackage)

static PyGetSetDef pyopencv_gapi_GNetPackage_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_GNetPackage_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_GNetPackage)

template<>
struct PyOpenCV_Converter< cv::gapi::GNetPackage >
{
    static PyObject* from(const cv::gapi::GNetPackage& r)
    {
        return pyopencv_gapi_GNetPackage_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::GNetPackage& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::GNetPackage * dst_;
        if (pyopencv_gapi_GNetPackage_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::GNetPackage for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_GNetParam (Generic)
//================================================================================

// GetSet (gapi_GNetParam)



// Methods (gapi_GNetParam)



// Tables (gapi_GNetParam)

static PyGetSetDef pyopencv_gapi_GNetParam_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_GNetParam_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_GNetParam)

template<>
struct PyOpenCV_Converter< cv::gapi::GNetParam >
{
    static PyObject* from(const cv::gapi::GNetParam& r)
    {
        return pyopencv_gapi_GNetParam_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::GNetParam& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::GNetParam * dst_;
        if (pyopencv_gapi_GNetParam_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::GNetParam for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_ie_PyParams (Generic)
//================================================================================

// GetSet (gapi_ie_PyParams)



// Methods (gapi_ie_PyParams)

static int pyopencv_cv_gapi_ie_gapi_ie_PyParams_PyParams(pyopencv_gapi_ie_PyParams_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::ie;

    pyPrepareArgumentConversionErrorsStorage(3);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::ie::PyParams());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_tag = NULL;
    std::string tag;
    Napi::Value* pyobj_model = NULL;
    std::string model;
    Napi::Value* pyobj_weights = NULL;
    std::string weights;
    Napi::Value* pyobj_device = NULL;
    std::string device;

    const char* keywords[] = { "tag", "model", "weights", "device", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO:PyParams", (char**)keywords, &pyobj_tag, &pyobj_model, &pyobj_weights, &pyobj_device) &&
        jsopencv_to_safe(info, pyobj_tag, tag, ArgInfo("tag", 0)) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_weights, weights, ArgInfo("weights", 0)) &&
        jsopencv_to_safe(info, pyobj_device, device, ArgInfo("device", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::ie::PyParams(tag, model, weights, device));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_tag = NULL;
    std::string tag;
    Napi::Value* pyobj_model = NULL;
    std::string model;
    Napi::Value* pyobj_device = NULL;
    std::string device;

    const char* keywords[] = { "tag", "model", "device", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:PyParams", (char**)keywords, &pyobj_tag, &pyobj_model, &pyobj_device) &&
        jsopencv_to_safe(info, pyobj_tag, tag, ArgInfo("tag", 0)) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_device, device, ArgInfo("device", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::ie::PyParams(tag, model, device));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("PyParams");

    return -1;
}

static Napi::Value pyopencv_cv_gapi_ie_gapi_ie_PyParams_cfgBatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::ie;


    cv::gapi::ie::PyParams * self1 = 0;
    if (!pyopencv_gapi_ie_PyParams_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_ie_PyParams' or its derivative)");
    cv::gapi::ie::PyParams* _self_ = (self1);
    Napi::Value* pyobj_size = NULL;
    size_t size=0;
    PyParams retval;

    const char* keywords[] = { "size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:gapi_ie_PyParams.cfgBatchSize", (char**)keywords, &pyobj_size) &&
        jsopencv_to_safe(info, pyobj_size, size, ArgInfo("size", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cfgBatchSize(size));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_gapi_ie_gapi_ie_PyParams_cfgNumRequests(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::ie;


    cv::gapi::ie::PyParams * self1 = 0;
    if (!pyopencv_gapi_ie_PyParams_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_ie_PyParams' or its derivative)");
    cv::gapi::ie::PyParams* _self_ = (self1);
    Napi::Value* pyobj_nireq = NULL;
    size_t nireq=0;
    PyParams retval;

    const char* keywords[] = { "nireq", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:gapi_ie_PyParams.cfgNumRequests", (char**)keywords, &pyobj_nireq) &&
        jsopencv_to_safe(info, pyobj_nireq, nireq, ArgInfo("nireq", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->cfgNumRequests(nireq));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_gapi_ie_gapi_ie_PyParams_constInput(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::ie;


    cv::gapi::ie::PyParams * self1 = 0;
    if (!pyopencv_gapi_ie_PyParams_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_ie_PyParams' or its derivative)");
    cv::gapi::ie::PyParams* _self_ = (self1);
    Napi::Value* pyobj_layer_name = NULL;
    std::string layer_name;
    Napi::Value* pyobj_data = NULL;
    Mat data;
    Napi::Value* pyobj_hint = NULL;
    TraitAs hint=TraitAs::TENSOR;
    PyParams retval;

    const char* keywords[] = { "layer_name", "data", "hint", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:gapi_ie_PyParams.constInput", (char**)keywords, &pyobj_layer_name, &pyobj_data, &pyobj_hint) &&
        jsopencv_to_safe(info, pyobj_layer_name, layer_name, ArgInfo("layer_name", 0)) &&
        jsopencv_to_safe(info, pyobj_data, data, ArgInfo("data", 0)) &&
        jsopencv_to_safe(info, pyobj_hint, hint, ArgInfo("hint", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->constInput(layer_name, data, hint));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (gapi_ie_PyParams)

static PyGetSetDef pyopencv_gapi_ie_PyParams_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_ie_PyParams_methods[] =
{
    {"cfgBatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_ie_gapi_ie_PyParams_cfgBatchSize, 0), "cfgBatchSize(size) -> retval\n."},
    {"cfgNumRequests", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_ie_gapi_ie_PyParams_cfgNumRequests, 0), "cfgNumRequests(nireq) -> retval\n."},
    {"constInput", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_ie_gapi_ie_PyParams_constInput, 0), "constInput(layer_name, data[, hint]) -> retval\n."},

    {NULL,          NULL}
};

// Converter (gapi_ie_PyParams)

template<>
struct PyOpenCV_Converter< cv::gapi::ie::PyParams >
{
    static PyObject* from(const cv::gapi::ie::PyParams& r)
    {
        return pyopencv_gapi_ie_PyParams_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::ie::PyParams& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::ie::PyParams * dst_;
        if (pyopencv_gapi_ie_PyParams_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::ie::PyParams for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_onnx_PyParams (Generic)
//================================================================================

// GetSet (gapi_onnx_PyParams)



// Methods (gapi_onnx_PyParams)

static int pyopencv_cv_gapi_onnx_gapi_onnx_PyParams_PyParams(pyopencv_gapi_onnx_PyParams_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::onnx;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::onnx::PyParams());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_tag = NULL;
    std::string tag;
    Napi::Value* pyobj_model_path = NULL;
    std::string model_path;

    const char* keywords[] = { "tag", "model_path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:PyParams", (char**)keywords, &pyobj_tag, &pyobj_model_path) &&
        jsopencv_to_safe(info, pyobj_tag, tag, ArgInfo("tag", 0)) &&
        jsopencv_to_safe(info, pyobj_model_path, model_path, ArgInfo("model_path", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::onnx::PyParams(tag, model_path));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("PyParams");

    return -1;
}



// Tables (gapi_onnx_PyParams)

static PyGetSetDef pyopencv_gapi_onnx_PyParams_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_onnx_PyParams_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_onnx_PyParams)

template<>
struct PyOpenCV_Converter< cv::gapi::onnx::PyParams >
{
    static PyObject* from(const cv::gapi::onnx::PyParams& r)
    {
        return pyopencv_gapi_onnx_PyParams_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::onnx::PyParams& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::onnx::PyParams * dst_;
        if (pyopencv_gapi_onnx_PyParams_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::onnx::PyParams for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_streaming_queue_capacity (Generic)
//================================================================================

// GetSet (gapi_streaming_queue_capacity)


static PyObject* pyopencv_gapi_streaming_queue_capacity_get_capacity(pyopencv_gapi_streaming_queue_capacity_t* p, void *closure)
{
    return jsopencv_from(p->v.capacity);
}

static int pyopencv_gapi_streaming_queue_capacity_set_capacity(pyopencv_gapi_streaming_queue_capacity_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the capacity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.capacity, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_streaming_queue_capacity)

static int pyopencv_cv_gapi_streaming_gapi_streaming_queue_capacity_queue_capacity(pyopencv_gapi_streaming_queue_capacity_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::streaming;

    Napi::Value* pyobj_cap = NULL;
    size_t cap=1;

    const char* keywords[] = { "cap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:queue_capacity", (char**)keywords, &pyobj_cap) &&
        jsopencv_to_safe(info, pyobj_cap, cap, ArgInfo("cap", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::streaming::queue_capacity(cap));
        return 0;
    }

    return -1;
}



// Tables (gapi_streaming_queue_capacity)

static PyGetSetDef pyopencv_gapi_streaming_queue_capacity_getseters[] =
{
    {(char*)"capacity", (getter)pyopencv_gapi_streaming_queue_capacity_get_capacity, (setter)pyopencv_gapi_streaming_queue_capacity_set_capacity, (char*)"capacity", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_streaming_queue_capacity_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_streaming_queue_capacity)

template<>
struct PyOpenCV_Converter< cv::gapi::streaming::queue_capacity >
{
    static PyObject* from(const cv::gapi::streaming::queue_capacity& r)
    {
        return pyopencv_gapi_streaming_queue_capacity_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::streaming::queue_capacity& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::streaming::queue_capacity * dst_;
        if (pyopencv_gapi_streaming_queue_capacity_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::streaming::queue_capacity for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_GOutputs (Generic)
//================================================================================

// GetSet (gapi_wip_GOutputs)



// Methods (gapi_wip_GOutputs)

static Napi::Value pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGArray(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::wip;


    cv::gapi::wip::GOutputs * self1 = 0;
    if (!pyopencv_gapi_wip_GOutputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_wip_GOutputs' or its derivative)");
    cv::gapi::wip::GOutputs* _self_ = (self1);
    Napi::Value* pyobj_type = NULL;
    gapi_ArgType type=static_cast<gapi_ArgType>(0);
    cv::GArrayT retval;

    const char* keywords[] = { "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:gapi_wip_GOutputs.getGArray", (char**)keywords, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGArray(type));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGMat(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::wip;


    cv::gapi::wip::GOutputs * self1 = 0;
    if (!pyopencv_gapi_wip_GOutputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_wip_GOutputs' or its derivative)");
    cv::gapi::wip::GOutputs* _self_ = (self1);
    cv::GMat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGMat());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGOpaque(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::wip;


    cv::gapi::wip::GOutputs * self1 = 0;
    if (!pyopencv_gapi_wip_GOutputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_wip_GOutputs' or its derivative)");
    cv::gapi::wip::GOutputs* _self_ = (self1);
    Napi::Value* pyobj_type = NULL;
    gapi_ArgType type=static_cast<gapi_ArgType>(0);
    cv::GOpaqueT retval;

    const char* keywords[] = { "type", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:gapi_wip_GOutputs.getGOpaque", (char**)keywords, &pyobj_type) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGOpaque(type));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGScalar(const Napi::CallbackInfo &info)
{
    using namespace cv::gapi::wip;


    cv::gapi::wip::GOutputs * self1 = 0;
    if (!pyopencv_gapi_wip_GOutputs_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'gapi_wip_GOutputs' or its derivative)");
    cv::gapi::wip::GOutputs* _self_ = (self1);
    cv::GScalar retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGScalar());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (gapi_wip_GOutputs)

static PyGetSetDef pyopencv_gapi_wip_GOutputs_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_GOutputs_methods[] =
{
    {"getGArray", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGArray, 0), "getGArray(type) -> retval\n."},
    {"getGMat", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGMat, 0), "getGMat() -> retval\n."},
    {"getGOpaque", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGOpaque, 0), "getGOpaque(type) -> retval\n."},
    {"getGScalar", CV_JS_FN_WITH_KW_(pyopencv_cv_gapi_wip_gapi_wip_GOutputs_getGScalar, 0), "getGScalar() -> retval\n."},

    {NULL,          NULL}
};

// Converter (gapi_wip_GOutputs)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::GOutputs >
{
    static PyObject* from(const cv::gapi::wip::GOutputs& r)
    {
        return pyopencv_gapi_wip_GOutputs_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::GOutputs& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::GOutputs * dst_;
        if (pyopencv_gapi_wip_GOutputs_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::GOutputs for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_IStreamSource (Generic)
//================================================================================

// GetSet (gapi_wip_IStreamSource)



// Methods (gapi_wip_IStreamSource)



// Tables (gapi_wip_IStreamSource)

static PyGetSetDef pyopencv_gapi_wip_IStreamSource_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_IStreamSource_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_IStreamSource)

template<>
struct PyOpenCV_Converter< Ptr<cv::gapi::wip::IStreamSource> >
{
    static PyObject* from(const Ptr<cv::gapi::wip::IStreamSource>& r)
    {
        return pyopencv_gapi_wip_IStreamSource_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::gapi::wip::IStreamSource>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::gapi::wip::IStreamSource> * dst_;
        if (pyopencv_gapi_wip_IStreamSource_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::gapi::wip::IStreamSource> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Circle (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Circle)


static PyObject* pyopencv_gapi_wip_draw_Circle_get_center(pyopencv_gapi_wip_draw_Circle_t* p, void *closure)
{
    return jsopencv_from(p->v.center);
}

static int pyopencv_gapi_wip_draw_Circle_set_center(pyopencv_gapi_wip_draw_Circle_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the center attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.center, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Circle_get_color(pyopencv_gapi_wip_draw_Circle_t* p, void *closure)
{
    return jsopencv_from(p->v.color);
}

static int pyopencv_gapi_wip_draw_Circle_set_color(pyopencv_gapi_wip_draw_Circle_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the color attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.color, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Circle_get_lt(pyopencv_gapi_wip_draw_Circle_t* p, void *closure)
{
    return jsopencv_from(p->v.lt);
}

static int pyopencv_gapi_wip_draw_Circle_set_lt(pyopencv_gapi_wip_draw_Circle_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lt, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Circle_get_radius(pyopencv_gapi_wip_draw_Circle_t* p, void *closure)
{
    return jsopencv_from(p->v.radius);
}

static int pyopencv_gapi_wip_draw_Circle_set_radius(pyopencv_gapi_wip_draw_Circle_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the radius attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.radius, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Circle_get_shift(pyopencv_gapi_wip_draw_Circle_t* p, void *closure)
{
    return jsopencv_from(p->v.shift);
}

static int pyopencv_gapi_wip_draw_Circle_set_shift(pyopencv_gapi_wip_draw_Circle_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the shift attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.shift, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Circle_get_thick(pyopencv_gapi_wip_draw_Circle_t* p, void *closure)
{
    return jsopencv_from(p->v.thick);
}

static int pyopencv_gapi_wip_draw_Circle_set_thick(pyopencv_gapi_wip_draw_Circle_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thick attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.thick, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Circle)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Circle_Circle(pyopencv_gapi_wip_draw_Circle_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_center_ = NULL;
    Point center_;
    Napi::Value* pyobj_radius_ = NULL;
    int radius_=0;
    Napi::Value* pyobj_color_ = NULL;
    Scalar color_;
    Napi::Value* pyobj_thick_ = NULL;
    int thick_=1;
    Napi::Value* pyobj_lt_ = NULL;
    int lt_=8;
    Napi::Value* pyobj_shift_ = NULL;
    int shift_=0;

    const char* keywords[] = { "center_", "radius_", "color_", "thick_", "lt_", "shift_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOO:Circle", (char**)keywords, &pyobj_center_, &pyobj_radius_, &pyobj_color_, &pyobj_thick_, &pyobj_lt_, &pyobj_shift_) &&
        jsopencv_to_safe(info, pyobj_center_, center_, ArgInfo("center_", 0)) &&
        jsopencv_to_safe(info, pyobj_radius_, radius_, ArgInfo("radius_", 0)) &&
        jsopencv_to_safe(info, pyobj_color_, color_, ArgInfo("color_", 0)) &&
        jsopencv_to_safe(info, pyobj_thick_, thick_, ArgInfo("thick_", 0)) &&
        jsopencv_to_safe(info, pyobj_lt_, lt_, ArgInfo("lt_", 0)) &&
        jsopencv_to_safe(info, pyobj_shift_, shift_, ArgInfo("shift_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Circle(center_, radius_, color_, thick_, lt_, shift_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Circle());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Circle");

    return -1;
}



// Tables (gapi_wip_draw_Circle)

static PyGetSetDef pyopencv_gapi_wip_draw_Circle_getseters[] =
{
    {(char*)"center", (getter)pyopencv_gapi_wip_draw_Circle_get_center, (setter)pyopencv_gapi_wip_draw_Circle_set_center, (char*)"center", NULL},
    {(char*)"color", (getter)pyopencv_gapi_wip_draw_Circle_get_color, (setter)pyopencv_gapi_wip_draw_Circle_set_color, (char*)"color", NULL},
    {(char*)"lt", (getter)pyopencv_gapi_wip_draw_Circle_get_lt, (setter)pyopencv_gapi_wip_draw_Circle_set_lt, (char*)"lt", NULL},
    {(char*)"radius", (getter)pyopencv_gapi_wip_draw_Circle_get_radius, (setter)pyopencv_gapi_wip_draw_Circle_set_radius, (char*)"radius", NULL},
    {(char*)"shift", (getter)pyopencv_gapi_wip_draw_Circle_get_shift, (setter)pyopencv_gapi_wip_draw_Circle_set_shift, (char*)"shift", NULL},
    {(char*)"thick", (getter)pyopencv_gapi_wip_draw_Circle_get_thick, (setter)pyopencv_gapi_wip_draw_Circle_set_thick, (char*)"thick", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Circle_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Circle)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Circle >
{
    static PyObject* from(const cv::gapi::wip::draw::Circle& r)
    {
        return pyopencv_gapi_wip_draw_Circle_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Circle& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Circle * dst_;
        if (pyopencv_gapi_wip_draw_Circle_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Circle for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Image (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Image)


static PyObject* pyopencv_gapi_wip_draw_Image_get_alpha(pyopencv_gapi_wip_draw_Image_t* p, void *closure)
{
    return jsopencv_from(p->v.alpha);
}

static int pyopencv_gapi_wip_draw_Image_set_alpha(pyopencv_gapi_wip_draw_Image_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the alpha attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.alpha, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Image_get_img(pyopencv_gapi_wip_draw_Image_t* p, void *closure)
{
    return jsopencv_from(p->v.img);
}

static int pyopencv_gapi_wip_draw_Image_set_img(pyopencv_gapi_wip_draw_Image_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the img attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.img, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Image_get_org(pyopencv_gapi_wip_draw_Image_t* p, void *closure)
{
    return jsopencv_from(p->v.org);
}

static int pyopencv_gapi_wip_draw_Image_set_org(pyopencv_gapi_wip_draw_Image_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the org attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.org, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Image)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Image_Image(pyopencv_gapi_wip_draw_Image_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_org_ = NULL;
    Point org_;
    Napi::Value* pyobj_img_ = NULL;
    Mat img_;
    Napi::Value* pyobj_alpha_ = NULL;
    Mat alpha_;

    const char* keywords[] = { "org_", "img_", "alpha_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:Image", (char**)keywords, &pyobj_org_, &pyobj_img_, &pyobj_alpha_) &&
        jsopencv_to_safe(info, pyobj_org_, org_, ArgInfo("org_", 0)) &&
        jsopencv_to_safe(info, pyobj_img_, img_, ArgInfo("img_", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha_, alpha_, ArgInfo("alpha_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Image(org_, img_, alpha_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Image());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Image");

    return -1;
}



// Tables (gapi_wip_draw_Image)

static PyGetSetDef pyopencv_gapi_wip_draw_Image_getseters[] =
{
    {(char*)"alpha", (getter)pyopencv_gapi_wip_draw_Image_get_alpha, (setter)pyopencv_gapi_wip_draw_Image_set_alpha, (char*)"alpha", NULL},
    {(char*)"img", (getter)pyopencv_gapi_wip_draw_Image_get_img, (setter)pyopencv_gapi_wip_draw_Image_set_img, (char*)"img", NULL},
    {(char*)"org", (getter)pyopencv_gapi_wip_draw_Image_get_org, (setter)pyopencv_gapi_wip_draw_Image_set_org, (char*)"org", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Image_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Image)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Image >
{
    static PyObject* from(const cv::gapi::wip::draw::Image& r)
    {
        return pyopencv_gapi_wip_draw_Image_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Image& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Image * dst_;
        if (pyopencv_gapi_wip_draw_Image_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Image for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Line (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Line)


static PyObject* pyopencv_gapi_wip_draw_Line_get_color(pyopencv_gapi_wip_draw_Line_t* p, void *closure)
{
    return jsopencv_from(p->v.color);
}

static int pyopencv_gapi_wip_draw_Line_set_color(pyopencv_gapi_wip_draw_Line_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the color attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.color, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Line_get_lt(pyopencv_gapi_wip_draw_Line_t* p, void *closure)
{
    return jsopencv_from(p->v.lt);
}

static int pyopencv_gapi_wip_draw_Line_set_lt(pyopencv_gapi_wip_draw_Line_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lt, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Line_get_pt1(pyopencv_gapi_wip_draw_Line_t* p, void *closure)
{
    return jsopencv_from(p->v.pt1);
}

static int pyopencv_gapi_wip_draw_Line_set_pt1(pyopencv_gapi_wip_draw_Line_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the pt1 attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.pt1, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Line_get_pt2(pyopencv_gapi_wip_draw_Line_t* p, void *closure)
{
    return jsopencv_from(p->v.pt2);
}

static int pyopencv_gapi_wip_draw_Line_set_pt2(pyopencv_gapi_wip_draw_Line_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the pt2 attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.pt2, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Line_get_shift(pyopencv_gapi_wip_draw_Line_t* p, void *closure)
{
    return jsopencv_from(p->v.shift);
}

static int pyopencv_gapi_wip_draw_Line_set_shift(pyopencv_gapi_wip_draw_Line_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the shift attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.shift, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Line_get_thick(pyopencv_gapi_wip_draw_Line_t* p, void *closure)
{
    return jsopencv_from(p->v.thick);
}

static int pyopencv_gapi_wip_draw_Line_set_thick(pyopencv_gapi_wip_draw_Line_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thick attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.thick, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Line)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Line_Line(pyopencv_gapi_wip_draw_Line_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_pt1_ = NULL;
    Point pt1_;
    Napi::Value* pyobj_pt2_ = NULL;
    Point pt2_;
    Napi::Value* pyobj_color_ = NULL;
    Scalar color_;
    Napi::Value* pyobj_thick_ = NULL;
    int thick_=1;
    Napi::Value* pyobj_lt_ = NULL;
    int lt_=8;
    Napi::Value* pyobj_shift_ = NULL;
    int shift_=0;

    const char* keywords[] = { "pt1_", "pt2_", "color_", "thick_", "lt_", "shift_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOO:Line", (char**)keywords, &pyobj_pt1_, &pyobj_pt2_, &pyobj_color_, &pyobj_thick_, &pyobj_lt_, &pyobj_shift_) &&
        jsopencv_to_safe(info, pyobj_pt1_, pt1_, ArgInfo("pt1_", 0)) &&
        jsopencv_to_safe(info, pyobj_pt2_, pt2_, ArgInfo("pt2_", 0)) &&
        jsopencv_to_safe(info, pyobj_color_, color_, ArgInfo("color_", 0)) &&
        jsopencv_to_safe(info, pyobj_thick_, thick_, ArgInfo("thick_", 0)) &&
        jsopencv_to_safe(info, pyobj_lt_, lt_, ArgInfo("lt_", 0)) &&
        jsopencv_to_safe(info, pyobj_shift_, shift_, ArgInfo("shift_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Line(pt1_, pt2_, color_, thick_, lt_, shift_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Line());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Line");

    return -1;
}



// Tables (gapi_wip_draw_Line)

static PyGetSetDef pyopencv_gapi_wip_draw_Line_getseters[] =
{
    {(char*)"color", (getter)pyopencv_gapi_wip_draw_Line_get_color, (setter)pyopencv_gapi_wip_draw_Line_set_color, (char*)"color", NULL},
    {(char*)"lt", (getter)pyopencv_gapi_wip_draw_Line_get_lt, (setter)pyopencv_gapi_wip_draw_Line_set_lt, (char*)"lt", NULL},
    {(char*)"pt1", (getter)pyopencv_gapi_wip_draw_Line_get_pt1, (setter)pyopencv_gapi_wip_draw_Line_set_pt1, (char*)"pt1", NULL},
    {(char*)"pt2", (getter)pyopencv_gapi_wip_draw_Line_get_pt2, (setter)pyopencv_gapi_wip_draw_Line_set_pt2, (char*)"pt2", NULL},
    {(char*)"shift", (getter)pyopencv_gapi_wip_draw_Line_get_shift, (setter)pyopencv_gapi_wip_draw_Line_set_shift, (char*)"shift", NULL},
    {(char*)"thick", (getter)pyopencv_gapi_wip_draw_Line_get_thick, (setter)pyopencv_gapi_wip_draw_Line_set_thick, (char*)"thick", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Line_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Line)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Line >
{
    static PyObject* from(const cv::gapi::wip::draw::Line& r)
    {
        return pyopencv_gapi_wip_draw_Line_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Line& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Line * dst_;
        if (pyopencv_gapi_wip_draw_Line_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Line for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Mosaic (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Mosaic)


static PyObject* pyopencv_gapi_wip_draw_Mosaic_get_cellSz(pyopencv_gapi_wip_draw_Mosaic_t* p, void *closure)
{
    return jsopencv_from(p->v.cellSz);
}

static int pyopencv_gapi_wip_draw_Mosaic_set_cellSz(pyopencv_gapi_wip_draw_Mosaic_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cellSz attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.cellSz, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Mosaic_get_decim(pyopencv_gapi_wip_draw_Mosaic_t* p, void *closure)
{
    return jsopencv_from(p->v.decim);
}

static int pyopencv_gapi_wip_draw_Mosaic_set_decim(pyopencv_gapi_wip_draw_Mosaic_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the decim attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.decim, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Mosaic_get_mos(pyopencv_gapi_wip_draw_Mosaic_t* p, void *closure)
{
    return jsopencv_from(p->v.mos);
}

static int pyopencv_gapi_wip_draw_Mosaic_set_mos(pyopencv_gapi_wip_draw_Mosaic_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the mos attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.mos, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Mosaic)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Mosaic_Mosaic(pyopencv_gapi_wip_draw_Mosaic_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Mosaic());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_mos_ = NULL;
    Rect2i mos_;
    Napi::Value* pyobj_cellSz_ = NULL;
    int cellSz_=0;
    Napi::Value* pyobj_decim_ = NULL;
    int decim_=0;

    const char* keywords[] = { "mos_", "cellSz_", "decim_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:Mosaic", (char**)keywords, &pyobj_mos_, &pyobj_cellSz_, &pyobj_decim_) &&
        jsopencv_to_safe(info, pyobj_mos_, mos_, ArgInfo("mos_", 0)) &&
        jsopencv_to_safe(info, pyobj_cellSz_, cellSz_, ArgInfo("cellSz_", 0)) &&
        jsopencv_to_safe(info, pyobj_decim_, decim_, ArgInfo("decim_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Mosaic(mos_, cellSz_, decim_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Mosaic");

    return -1;
}



// Tables (gapi_wip_draw_Mosaic)

static PyGetSetDef pyopencv_gapi_wip_draw_Mosaic_getseters[] =
{
    {(char*)"cellSz", (getter)pyopencv_gapi_wip_draw_Mosaic_get_cellSz, (setter)pyopencv_gapi_wip_draw_Mosaic_set_cellSz, (char*)"cellSz", NULL},
    {(char*)"decim", (getter)pyopencv_gapi_wip_draw_Mosaic_get_decim, (setter)pyopencv_gapi_wip_draw_Mosaic_set_decim, (char*)"decim", NULL},
    {(char*)"mos", (getter)pyopencv_gapi_wip_draw_Mosaic_get_mos, (setter)pyopencv_gapi_wip_draw_Mosaic_set_mos, (char*)"mos", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Mosaic_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Mosaic)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Mosaic >
{
    static PyObject* from(const cv::gapi::wip::draw::Mosaic& r)
    {
        return pyopencv_gapi_wip_draw_Mosaic_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Mosaic& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Mosaic * dst_;
        if (pyopencv_gapi_wip_draw_Mosaic_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Mosaic for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Poly (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Poly)


static PyObject* pyopencv_gapi_wip_draw_Poly_get_color(pyopencv_gapi_wip_draw_Poly_t* p, void *closure)
{
    return jsopencv_from(p->v.color);
}

static int pyopencv_gapi_wip_draw_Poly_set_color(pyopencv_gapi_wip_draw_Poly_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the color attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.color, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Poly_get_lt(pyopencv_gapi_wip_draw_Poly_t* p, void *closure)
{
    return jsopencv_from(p->v.lt);
}

static int pyopencv_gapi_wip_draw_Poly_set_lt(pyopencv_gapi_wip_draw_Poly_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lt, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Poly_get_points(pyopencv_gapi_wip_draw_Poly_t* p, void *closure)
{
    return jsopencv_from(p->v.points);
}

static int pyopencv_gapi_wip_draw_Poly_set_points(pyopencv_gapi_wip_draw_Poly_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the points attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.points, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Poly_get_shift(pyopencv_gapi_wip_draw_Poly_t* p, void *closure)
{
    return jsopencv_from(p->v.shift);
}

static int pyopencv_gapi_wip_draw_Poly_set_shift(pyopencv_gapi_wip_draw_Poly_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the shift attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.shift, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Poly_get_thick(pyopencv_gapi_wip_draw_Poly_t* p, void *closure)
{
    return jsopencv_from(p->v.thick);
}

static int pyopencv_gapi_wip_draw_Poly_set_thick(pyopencv_gapi_wip_draw_Poly_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thick attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.thick, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Poly)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Poly_Poly(pyopencv_gapi_wip_draw_Poly_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_points_ = NULL;
    vector_Point points_;
    Napi::Value* pyobj_color_ = NULL;
    Scalar color_;
    Napi::Value* pyobj_thick_ = NULL;
    int thick_=1;
    Napi::Value* pyobj_lt_ = NULL;
    int lt_=8;
    Napi::Value* pyobj_shift_ = NULL;
    int shift_=0;

    const char* keywords[] = { "points_", "color_", "thick_", "lt_", "shift_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:Poly", (char**)keywords, &pyobj_points_, &pyobj_color_, &pyobj_thick_, &pyobj_lt_, &pyobj_shift_) &&
        jsopencv_to_safe(info, pyobj_points_, points_, ArgInfo("points_", 0)) &&
        jsopencv_to_safe(info, pyobj_color_, color_, ArgInfo("color_", 0)) &&
        jsopencv_to_safe(info, pyobj_thick_, thick_, ArgInfo("thick_", 0)) &&
        jsopencv_to_safe(info, pyobj_lt_, lt_, ArgInfo("lt_", 0)) &&
        jsopencv_to_safe(info, pyobj_shift_, shift_, ArgInfo("shift_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Poly(points_, color_, thick_, lt_, shift_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Poly());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Poly");

    return -1;
}



// Tables (gapi_wip_draw_Poly)

static PyGetSetDef pyopencv_gapi_wip_draw_Poly_getseters[] =
{
    {(char*)"color", (getter)pyopencv_gapi_wip_draw_Poly_get_color, (setter)pyopencv_gapi_wip_draw_Poly_set_color, (char*)"color", NULL},
    {(char*)"lt", (getter)pyopencv_gapi_wip_draw_Poly_get_lt, (setter)pyopencv_gapi_wip_draw_Poly_set_lt, (char*)"lt", NULL},
    {(char*)"points", (getter)pyopencv_gapi_wip_draw_Poly_get_points, (setter)pyopencv_gapi_wip_draw_Poly_set_points, (char*)"points", NULL},
    {(char*)"shift", (getter)pyopencv_gapi_wip_draw_Poly_get_shift, (setter)pyopencv_gapi_wip_draw_Poly_set_shift, (char*)"shift", NULL},
    {(char*)"thick", (getter)pyopencv_gapi_wip_draw_Poly_get_thick, (setter)pyopencv_gapi_wip_draw_Poly_set_thick, (char*)"thick", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Poly_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Poly)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Poly >
{
    static PyObject* from(const cv::gapi::wip::draw::Poly& r)
    {
        return pyopencv_gapi_wip_draw_Poly_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Poly& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Poly * dst_;
        if (pyopencv_gapi_wip_draw_Poly_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Poly for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Rect (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Rect)


static PyObject* pyopencv_gapi_wip_draw_Rect_get_color(pyopencv_gapi_wip_draw_Rect_t* p, void *closure)
{
    return jsopencv_from(p->v.color);
}

static int pyopencv_gapi_wip_draw_Rect_set_color(pyopencv_gapi_wip_draw_Rect_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the color attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.color, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Rect_get_lt(pyopencv_gapi_wip_draw_Rect_t* p, void *closure)
{
    return jsopencv_from(p->v.lt);
}

static int pyopencv_gapi_wip_draw_Rect_set_lt(pyopencv_gapi_wip_draw_Rect_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lt, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Rect_get_rect(pyopencv_gapi_wip_draw_Rect_t* p, void *closure)
{
    return jsopencv_from(p->v.rect);
}

static int pyopencv_gapi_wip_draw_Rect_set_rect(pyopencv_gapi_wip_draw_Rect_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the rect attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.rect, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Rect_get_shift(pyopencv_gapi_wip_draw_Rect_t* p, void *closure)
{
    return jsopencv_from(p->v.shift);
}

static int pyopencv_gapi_wip_draw_Rect_set_shift(pyopencv_gapi_wip_draw_Rect_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the shift attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.shift, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Rect_get_thick(pyopencv_gapi_wip_draw_Rect_t* p, void *closure)
{
    return jsopencv_from(p->v.thick);
}

static int pyopencv_gapi_wip_draw_Rect_set_thick(pyopencv_gapi_wip_draw_Rect_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thick attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.thick, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Rect)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Rect_Rect(pyopencv_gapi_wip_draw_Rect_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Rect());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_rect_ = NULL;
    Rect2i rect_;
    Napi::Value* pyobj_color_ = NULL;
    Scalar color_;
    Napi::Value* pyobj_thick_ = NULL;
    int thick_=1;
    Napi::Value* pyobj_lt_ = NULL;
    int lt_=8;
    Napi::Value* pyobj_shift_ = NULL;
    int shift_=0;

    const char* keywords[] = { "rect_", "color_", "thick_", "lt_", "shift_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:Rect", (char**)keywords, &pyobj_rect_, &pyobj_color_, &pyobj_thick_, &pyobj_lt_, &pyobj_shift_) &&
        jsopencv_to_safe(info, pyobj_rect_, rect_, ArgInfo("rect_", 0)) &&
        jsopencv_to_safe(info, pyobj_color_, color_, ArgInfo("color_", 0)) &&
        jsopencv_to_safe(info, pyobj_thick_, thick_, ArgInfo("thick_", 0)) &&
        jsopencv_to_safe(info, pyobj_lt_, lt_, ArgInfo("lt_", 0)) &&
        jsopencv_to_safe(info, pyobj_shift_, shift_, ArgInfo("shift_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Rect(rect_, color_, thick_, lt_, shift_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Rect");

    return -1;
}



// Tables (gapi_wip_draw_Rect)

static PyGetSetDef pyopencv_gapi_wip_draw_Rect_getseters[] =
{
    {(char*)"color", (getter)pyopencv_gapi_wip_draw_Rect_get_color, (setter)pyopencv_gapi_wip_draw_Rect_set_color, (char*)"color", NULL},
    {(char*)"lt", (getter)pyopencv_gapi_wip_draw_Rect_get_lt, (setter)pyopencv_gapi_wip_draw_Rect_set_lt, (char*)"lt", NULL},
    {(char*)"rect", (getter)pyopencv_gapi_wip_draw_Rect_get_rect, (setter)pyopencv_gapi_wip_draw_Rect_set_rect, (char*)"rect", NULL},
    {(char*)"shift", (getter)pyopencv_gapi_wip_draw_Rect_get_shift, (setter)pyopencv_gapi_wip_draw_Rect_set_shift, (char*)"shift", NULL},
    {(char*)"thick", (getter)pyopencv_gapi_wip_draw_Rect_get_thick, (setter)pyopencv_gapi_wip_draw_Rect_set_thick, (char*)"thick", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Rect_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Rect)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Rect >
{
    static PyObject* from(const cv::gapi::wip::draw::Rect& r)
    {
        return pyopencv_gapi_wip_draw_Rect_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Rect& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Rect * dst_;
        if (pyopencv_gapi_wip_draw_Rect_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Rect for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_draw_Text (Generic)
//================================================================================

// GetSet (gapi_wip_draw_Text)


static PyObject* pyopencv_gapi_wip_draw_Text_get_bottom_left_origin(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.bottom_left_origin);
}

static int pyopencv_gapi_wip_draw_Text_set_bottom_left_origin(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the bottom_left_origin attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.bottom_left_origin, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_color(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.color);
}

static int pyopencv_gapi_wip_draw_Text_set_color(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the color attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.color, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_ff(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.ff);
}

static int pyopencv_gapi_wip_draw_Text_set_ff(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the ff attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.ff, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_fs(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.fs);
}

static int pyopencv_gapi_wip_draw_Text_set_fs(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the fs attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.fs, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_lt(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.lt);
}

static int pyopencv_gapi_wip_draw_Text_set_lt(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lt attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lt, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_org(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.org);
}

static int pyopencv_gapi_wip_draw_Text_set_org(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the org attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.org, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_text(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.text);
}

static int pyopencv_gapi_wip_draw_Text_set_text(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the text attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.text, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_gapi_wip_draw_Text_get_thick(pyopencv_gapi_wip_draw_Text_t* p, void *closure)
{
    return jsopencv_from(p->v.thick);
}

static int pyopencv_gapi_wip_draw_Text_set_thick(pyopencv_gapi_wip_draw_Text_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thick attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.thick, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (gapi_wip_draw_Text)

static int pyopencv_cv_gapi_wip_draw_gapi_wip_draw_Text_Text(pyopencv_gapi_wip_draw_Text_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::draw;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_text_ = NULL;
    std::string text_;
    Napi::Value* pyobj_org_ = NULL;
    Point org_;
    Napi::Value* pyobj_ff_ = NULL;
    int ff_=0;
    Napi::Value* pyobj_fs_ = NULL;
    double fs_=0;
    Napi::Value* pyobj_color_ = NULL;
    Scalar color_;
    Napi::Value* pyobj_thick_ = NULL;
    int thick_=1;
    Napi::Value* pyobj_lt_ = NULL;
    int lt_=8;
    Napi::Value* pyobj_bottom_left_origin_ = NULL;
    bool bottom_left_origin_=false;

    const char* keywords[] = { "text_", "org_", "ff_", "fs_", "color_", "thick_", "lt_", "bottom_left_origin_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOO|OOO:Text", (char**)keywords, &pyobj_text_, &pyobj_org_, &pyobj_ff_, &pyobj_fs_, &pyobj_color_, &pyobj_thick_, &pyobj_lt_, &pyobj_bottom_left_origin_) &&
        jsopencv_to_safe(info, pyobj_text_, text_, ArgInfo("text_", 0)) &&
        jsopencv_to_safe(info, pyobj_org_, org_, ArgInfo("org_", 0)) &&
        jsopencv_to_safe(info, pyobj_ff_, ff_, ArgInfo("ff_", 0)) &&
        jsopencv_to_safe(info, pyobj_fs_, fs_, ArgInfo("fs_", 0)) &&
        jsopencv_to_safe(info, pyobj_color_, color_, ArgInfo("color_", 0)) &&
        jsopencv_to_safe(info, pyobj_thick_, thick_, ArgInfo("thick_", 0)) &&
        jsopencv_to_safe(info, pyobj_lt_, lt_, ArgInfo("lt_", 0)) &&
        jsopencv_to_safe(info, pyobj_bottom_left_origin_, bottom_left_origin_, ArgInfo("bottom_left_origin_", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Text(text_, org_, ff_, fs_, color_, thick_, lt_, bottom_left_origin_));
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::gapi::wip::draw::Text());
        return 0;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("Text");

    return -1;
}



// Tables (gapi_wip_draw_Text)

static PyGetSetDef pyopencv_gapi_wip_draw_Text_getseters[] =
{
    {(char*)"bottom_left_origin", (getter)pyopencv_gapi_wip_draw_Text_get_bottom_left_origin, (setter)pyopencv_gapi_wip_draw_Text_set_bottom_left_origin, (char*)"bottom_left_origin", NULL},
    {(char*)"color", (getter)pyopencv_gapi_wip_draw_Text_get_color, (setter)pyopencv_gapi_wip_draw_Text_set_color, (char*)"color", NULL},
    {(char*)"ff", (getter)pyopencv_gapi_wip_draw_Text_get_ff, (setter)pyopencv_gapi_wip_draw_Text_set_ff, (char*)"ff", NULL},
    {(char*)"fs", (getter)pyopencv_gapi_wip_draw_Text_get_fs, (setter)pyopencv_gapi_wip_draw_Text_set_fs, (char*)"fs", NULL},
    {(char*)"lt", (getter)pyopencv_gapi_wip_draw_Text_get_lt, (setter)pyopencv_gapi_wip_draw_Text_set_lt, (char*)"lt", NULL},
    {(char*)"org", (getter)pyopencv_gapi_wip_draw_Text_get_org, (setter)pyopencv_gapi_wip_draw_Text_set_org, (char*)"org", NULL},
    {(char*)"text", (getter)pyopencv_gapi_wip_draw_Text_get_text, (setter)pyopencv_gapi_wip_draw_Text_set_text, (char*)"text", NULL},
    {(char*)"thick", (getter)pyopencv_gapi_wip_draw_Text_get_thick, (setter)pyopencv_gapi_wip_draw_Text_set_thick, (char*)"thick", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_draw_Text_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_draw_Text)

template<>
struct PyOpenCV_Converter< cv::gapi::wip::draw::Text >
{
    static PyObject* from(const cv::gapi::wip::draw::Text& r)
    {
        return pyopencv_gapi_wip_draw_Text_Instance(r);
    }
    static bool to(PyObject* src, cv::gapi::wip::draw::Text& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::gapi::wip::draw::Text * dst_;
        if (pyopencv_gapi_wip_draw_Text_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::gapi::wip::draw::Text for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// gapi_wip_gst_GStreamerPipeline (Generic)
//================================================================================

// GetSet (gapi_wip_gst_GStreamerPipeline)



// Methods (gapi_wip_gst_GStreamerPipeline)

static int pyopencv_cv_gapi_wip_gst_gapi_wip_gst_GStreamerPipeline_GStreamerPipeline(pyopencv_gapi_wip_gst_GStreamerPipeline_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::gapi::wip::gst;

    Napi::Value* pyobj_pipeline = NULL;
    std::string pipeline;

    const char* keywords[] = { "pipeline", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:GStreamerPipeline", (char**)keywords, &pyobj_pipeline) &&
        jsopencv_to_safe(info, pyobj_pipeline, pipeline, ArgInfo("pipeline", 0)))
    {
        new (&(self->v)) Ptr<cv::gapi::wip::gst::GStreamerPipeline>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::gapi::wip::gst::GStreamerPipeline(pipeline)));
        return 0;
    }

    return -1;
}



// Tables (gapi_wip_gst_GStreamerPipeline)

static PyGetSetDef pyopencv_gapi_wip_gst_GStreamerPipeline_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_gapi_wip_gst_GStreamerPipeline_methods[] =
{

    {NULL,          NULL}
};

// Converter (gapi_wip_gst_GStreamerPipeline)

template<>
struct PyOpenCV_Converter< Ptr<cv::gapi::wip::gst::GStreamerPipeline> >
{
    static PyObject* from(const Ptr<cv::gapi::wip::gst::GStreamerPipeline>& r)
    {
        return pyopencv_gapi_wip_gst_GStreamerPipeline_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::gapi::wip::gst::GStreamerPipeline>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::gapi::wip::gst::GStreamerPipeline> * dst_;
        if (pyopencv_gapi_wip_gst_GStreamerPipeline_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::gapi::wip::gst::GStreamerPipeline> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_AverageHash (Generic)
//================================================================================

// GetSet (img_hash_AverageHash)



// Methods (img_hash_AverageHash)

static Napi::Value pyopencv_cv_img_hash_img_hash_AverageHash_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;

    Ptr<AverageHash> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::img_hash::AverageHash::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (img_hash_AverageHash)

static PyGetSetDef pyopencv_img_hash_AverageHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_AverageHash_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_AverageHash_create_static, METH_STATIC), "create() -> retval\n."},

    {NULL,          NULL}
};

// Converter (img_hash_AverageHash)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::AverageHash> >
{
    static PyObject* from(const Ptr<cv::img_hash::AverageHash>& r)
    {
        return pyopencv_img_hash_AverageHash_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::AverageHash>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::AverageHash> * dst_;
        if (pyopencv_img_hash_AverageHash_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::AverageHash> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_BlockMeanHash (Generic)
//================================================================================

// GetSet (img_hash_BlockMeanHash)



// Methods (img_hash_BlockMeanHash)

static Napi::Value pyopencv_cv_img_hash_img_hash_BlockMeanHash_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;

    Napi::Value* pyobj_mode = NULL;
    int mode=BLOCK_MEAN_HASH_MODE_0;
    Ptr<BlockMeanHash> retval;

    const char* keywords[] = { "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:img_hash_BlockMeanHash.create", (char**)keywords, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::img_hash::BlockMeanHash::create(mode));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_BlockMeanHash_getMean(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::BlockMeanHash> * self1 = 0;
    if (!pyopencv_img_hash_BlockMeanHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_BlockMeanHash' or its derivative)");
    Ptr<cv::img_hash::BlockMeanHash> _self_ = *(self1);
    std::vector<double> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMean());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_BlockMeanHash_setMode(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::BlockMeanHash> * self1 = 0;
    if (!pyopencv_img_hash_BlockMeanHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_BlockMeanHash' or its derivative)");
    Ptr<cv::img_hash::BlockMeanHash> _self_ = *(self1);
    Napi::Value* pyobj_mode = NULL;
    int mode=0;

    const char* keywords[] = { "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:img_hash_BlockMeanHash.setMode", (char**)keywords, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMode(mode));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (img_hash_BlockMeanHash)

static PyGetSetDef pyopencv_img_hash_BlockMeanHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_BlockMeanHash_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_BlockMeanHash_create_static, METH_STATIC), "create([, mode]) -> retval\n."},
    {"getMean", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_BlockMeanHash_getMean, 0), "getMean() -> retval\n."},
    {"setMode", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_BlockMeanHash_setMode, 0), "setMode(mode) -> None\n.   @brief Create BlockMeanHash object\n.           @param mode the mode"},

    {NULL,          NULL}
};

// Converter (img_hash_BlockMeanHash)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::BlockMeanHash> >
{
    static PyObject* from(const Ptr<cv::img_hash::BlockMeanHash>& r)
    {
        return pyopencv_img_hash_BlockMeanHash_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::BlockMeanHash>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::BlockMeanHash> * dst_;
        if (pyopencv_img_hash_BlockMeanHash_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::BlockMeanHash> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_ColorMomentHash (Generic)
//================================================================================

// GetSet (img_hash_ColorMomentHash)



// Methods (img_hash_ColorMomentHash)

static Napi::Value pyopencv_cv_img_hash_img_hash_ColorMomentHash_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;

    Ptr<ColorMomentHash> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::img_hash::ColorMomentHash::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (img_hash_ColorMomentHash)

static PyGetSetDef pyopencv_img_hash_ColorMomentHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_ColorMomentHash_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_ColorMomentHash_create_static, METH_STATIC), "create() -> retval\n."},

    {NULL,          NULL}
};

// Converter (img_hash_ColorMomentHash)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::ColorMomentHash> >
{
    static PyObject* from(const Ptr<cv::img_hash::ColorMomentHash>& r)
    {
        return pyopencv_img_hash_ColorMomentHash_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::ColorMomentHash>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::ColorMomentHash> * dst_;
        if (pyopencv_img_hash_ColorMomentHash_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::ColorMomentHash> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_ImgHashBase (Generic)
//================================================================================

// GetSet (img_hash_ImgHashBase)



// Methods (img_hash_ImgHashBase)

static Napi::Value pyopencv_cv_img_hash_img_hash_ImgHashBase_compare(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::ImgHashBase> * self1 = 0;
    if (!pyopencv_img_hash_ImgHashBase_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_ImgHashBase' or its derivative)");
    Ptr<cv::img_hash::ImgHashBase> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_hashOne = NULL;
    Mat hashOne;
    Napi::Value* pyobj_hashTwo = NULL;
    Mat hashTwo;
    double retval;

    const char* keywords[] = { "hashOne", "hashTwo", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:img_hash_ImgHashBase.compare", (char**)keywords, &pyobj_hashOne, &pyobj_hashTwo) &&
        jsopencv_to_safe(info, pyobj_hashOne, hashOne, ArgInfo("hashOne", 0)) &&
        jsopencv_to_safe(info, pyobj_hashTwo, hashTwo, ArgInfo("hashTwo", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compare(hashOne, hashTwo));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_hashOne = NULL;
    UMat hashOne;
    Napi::Value* pyobj_hashTwo = NULL;
    UMat hashTwo;
    double retval;

    const char* keywords[] = { "hashOne", "hashTwo", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:img_hash_ImgHashBase.compare", (char**)keywords, &pyobj_hashOne, &pyobj_hashTwo) &&
        jsopencv_to_safe(info, pyobj_hashOne, hashOne, ArgInfo("hashOne", 0)) &&
        jsopencv_to_safe(info, pyobj_hashTwo, hashTwo, ArgInfo("hashTwo", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compare(hashOne, hashTwo));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compare");

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_ImgHashBase_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::ImgHashBase> * self1 = 0;
    if (!pyopencv_img_hash_ImgHashBase_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_ImgHashBase' or its derivative)");
    Ptr<cv::img_hash::ImgHashBase> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_inputArr = NULL;
    Mat inputArr;
    Napi::Value* pyobj_outputArr = NULL;
    Mat outputArr;

    const char* keywords[] = { "inputArr", "outputArr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:img_hash_ImgHashBase.compute", (char**)keywords, &pyobj_inputArr, &pyobj_outputArr) &&
        jsopencv_to_safe(info, pyobj_inputArr, inputArr, ArgInfo("inputArr", 0)) &&
        jsopencv_to_safe(info, pyobj_outputArr, outputArr, ArgInfo("outputArr", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(inputArr, outputArr));
        return jsopencv_from(outputArr);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_inputArr = NULL;
    UMat inputArr;
    Napi::Value* pyobj_outputArr = NULL;
    UMat outputArr;

    const char* keywords[] = { "inputArr", "outputArr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:img_hash_ImgHashBase.compute", (char**)keywords, &pyobj_inputArr, &pyobj_outputArr) &&
        jsopencv_to_safe(info, pyobj_inputArr, inputArr, ArgInfo("inputArr", 0)) &&
        jsopencv_to_safe(info, pyobj_outputArr, outputArr, ArgInfo("outputArr", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->compute(inputArr, outputArr));
        return jsopencv_from(outputArr);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}



// Tables (img_hash_ImgHashBase)

static PyGetSetDef pyopencv_img_hash_ImgHashBase_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_ImgHashBase_methods[] =
{
    {"compare", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_ImgHashBase_compare, 0), "compare(hashOne, hashTwo) -> retval\n.   @brief Compare the hash value between inOne and inTwo\n.           @param hashOne Hash value one\n.           @param hashTwo Hash value two\n.           @return value indicate similarity between inOne and inTwo, the meaning\n.           of the value vary from algorithms to algorithms"},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_ImgHashBase_compute, 0), "compute(inputArr[, outputArr]) -> outputArr\n.   @brief Computes hash of the input image\n.           @param inputArr input image want to compute hash value\n.           @param outputArr hash of the image"},

    {NULL,          NULL}
};

// Converter (img_hash_ImgHashBase)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::ImgHashBase> >
{
    static PyObject* from(const Ptr<cv::img_hash::ImgHashBase>& r)
    {
        return pyopencv_img_hash_ImgHashBase_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::ImgHashBase>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::ImgHashBase> * dst_;
        if (pyopencv_img_hash_ImgHashBase_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::ImgHashBase> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_MarrHildrethHash (Generic)
//================================================================================

// GetSet (img_hash_MarrHildrethHash)



// Methods (img_hash_MarrHildrethHash)

static Napi::Value pyopencv_cv_img_hash_img_hash_MarrHildrethHash_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;

    Napi::Value* pyobj_alpha = NULL;
    float alpha=2.0f;
    Napi::Value* pyobj_scale = NULL;
    float scale=1.0f;
    Ptr<MarrHildrethHash> retval;

    const char* keywords[] = { "alpha", "scale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:img_hash_MarrHildrethHash.create", (char**)keywords, &pyobj_alpha, &pyobj_scale) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::img_hash::MarrHildrethHash::create(alpha, scale));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::MarrHildrethHash> * self1 = 0;
    if (!pyopencv_img_hash_MarrHildrethHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_MarrHildrethHash' or its derivative)");
    Ptr<cv::img_hash::MarrHildrethHash> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlpha());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getScale(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::MarrHildrethHash> * self1 = 0;
    if (!pyopencv_img_hash_MarrHildrethHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_MarrHildrethHash' or its derivative)");
    Ptr<cv::img_hash::MarrHildrethHash> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_MarrHildrethHash_setKernelParam(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::MarrHildrethHash> * self1 = 0;
    if (!pyopencv_img_hash_MarrHildrethHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_MarrHildrethHash' or its derivative)");
    Ptr<cv::img_hash::MarrHildrethHash> _self_ = *(self1);
    Napi::Value* pyobj_alpha = NULL;
    float alpha=0.f;
    Napi::Value* pyobj_scale = NULL;
    float scale=0.f;

    const char* keywords[] = { "alpha", "scale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:img_hash_MarrHildrethHash.setKernelParam", (char**)keywords, &pyobj_alpha, &pyobj_scale) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setKernelParam(alpha, scale));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (img_hash_MarrHildrethHash)

static PyGetSetDef pyopencv_img_hash_MarrHildrethHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_MarrHildrethHash_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_create_static, METH_STATIC), "create([, alpha[, scale]]) -> retval\n.   @param alpha int scale factor for marr wavelet (default=2).\n.           @param scale int level of scale factor (default = 1)"},
    {"getAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getAlpha, 0), "getAlpha() -> retval\n.   * @brief self explain"},
    {"getScale", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getScale, 0), "getScale() -> retval\n.   * @brief self explain"},
    {"setKernelParam", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_setKernelParam, 0), "setKernelParam(alpha, scale) -> None\n.   @brief Set Mh kernel parameters\n.           @param alpha int scale factor for marr wavelet (default=2).\n.           @param scale int level of scale factor (default = 1)"},

    {NULL,          NULL}
};

// Converter (img_hash_MarrHildrethHash)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::MarrHildrethHash> >
{
    static PyObject* from(const Ptr<cv::img_hash::MarrHildrethHash>& r)
    {
        return pyopencv_img_hash_MarrHildrethHash_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::MarrHildrethHash>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::MarrHildrethHash> * dst_;
        if (pyopencv_img_hash_MarrHildrethHash_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::MarrHildrethHash> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_PHash (Generic)
//================================================================================

// GetSet (img_hash_PHash)



// Methods (img_hash_PHash)

static Napi::Value pyopencv_cv_img_hash_img_hash_PHash_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;

    Ptr<PHash> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::img_hash::PHash::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (img_hash_PHash)

static PyGetSetDef pyopencv_img_hash_PHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_PHash_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_PHash_create_static, METH_STATIC), "create() -> retval\n."},

    {NULL,          NULL}
};

// Converter (img_hash_PHash)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::PHash> >
{
    static PyObject* from(const Ptr<cv::img_hash::PHash>& r)
    {
        return pyopencv_img_hash_PHash_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::PHash>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::PHash> * dst_;
        if (pyopencv_img_hash_PHash_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::PHash> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// img_hash_RadialVarianceHash (Generic)
//================================================================================

// GetSet (img_hash_RadialVarianceHash)



// Methods (img_hash_RadialVarianceHash)

static Napi::Value pyopencv_cv_img_hash_img_hash_RadialVarianceHash_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;

    Napi::Value* pyobj_sigma = NULL;
    double sigma=1;
    Napi::Value* pyobj_numOfAngleLine = NULL;
    int numOfAngleLine=180;
    Ptr<RadialVarianceHash> retval;

    const char* keywords[] = { "sigma", "numOfAngleLine", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:img_hash_RadialVarianceHash.create", (char**)keywords, &pyobj_sigma, &pyobj_numOfAngleLine) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)) &&
        jsopencv_to_safe(info, pyobj_numOfAngleLine, numOfAngleLine, ArgInfo("numOfAngleLine", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::img_hash::RadialVarianceHash::create(sigma, numOfAngleLine));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getNumOfAngleLine(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::RadialVarianceHash> * self1 = 0;
    if (!pyopencv_img_hash_RadialVarianceHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    Ptr<cv::img_hash::RadialVarianceHash> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumOfAngleLine());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::RadialVarianceHash> * self1 = 0;
    if (!pyopencv_img_hash_RadialVarianceHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    Ptr<cv::img_hash::RadialVarianceHash> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setNumOfAngleLine(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::RadialVarianceHash> * self1 = 0;
    if (!pyopencv_img_hash_RadialVarianceHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    Ptr<cv::img_hash::RadialVarianceHash> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    int value=0;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:img_hash_RadialVarianceHash.setNumOfAngleLine", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumOfAngleLine(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::img_hash;


    Ptr<cv::img_hash::RadialVarianceHash> * self1 = 0;
    if (!pyopencv_img_hash_RadialVarianceHash_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    Ptr<cv::img_hash::RadialVarianceHash> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    double value=0;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:img_hash_RadialVarianceHash.setSigma", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigma(value));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (img_hash_RadialVarianceHash)

static PyGetSetDef pyopencv_img_hash_RadialVarianceHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_img_hash_RadialVarianceHash_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_create_static, METH_STATIC), "create([, sigma[, numOfAngleLine]]) -> retval\n."},
    {"getNumOfAngleLine", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getNumOfAngleLine, 0), "getNumOfAngleLine() -> retval\n."},
    {"getSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getSigma, 0), "getSigma() -> retval\n."},
    {"setNumOfAngleLine", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setNumOfAngleLine, 0), "setNumOfAngleLine(value) -> None\n."},
    {"setSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setSigma, 0), "setSigma(value) -> None\n."},

    {NULL,          NULL}
};

// Converter (img_hash_RadialVarianceHash)

template<>
struct PyOpenCV_Converter< Ptr<cv::img_hash::RadialVarianceHash> >
{
    static PyObject* from(const Ptr<cv::img_hash::RadialVarianceHash>& r)
    {
        return pyopencv_img_hash_RadialVarianceHash_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::img_hash::RadialVarianceHash>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::img_hash::RadialVarianceHash> * dst_;
        if (pyopencv_img_hash_RadialVarianceHash_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::img_hash::RadialVarianceHash> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_MultiTracker (Generic)
//================================================================================

// GetSet (legacy_MultiTracker)



// Methods (legacy_MultiTracker)

static int pyopencv_cv_legacy_legacy_MultiTracker_MultiTracker(pyopencv_legacy_MultiTracker_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::legacy;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::legacy::MultiTracker>(); // init Ptr with placement new
        if(self) ERRWRAP2_NAPI(info, self->v.reset(new cv::legacy::MultiTracker()));
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_legacy_legacy_MultiTracker_add(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;


    Ptr<cv::legacy::MultiTracker> * self1 = 0;
    if (!pyopencv_legacy_MultiTracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'legacy_MultiTracker' or its derivative)");
    Ptr<cv::legacy::MultiTracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_newTracker = NULL;
    Ptr<legacy::Tracker> newTracker;
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "newTracker", "image", "boundingBox", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:legacy_MultiTracker.add", (char**)keywords, &pyobj_newTracker, &pyobj_image, &pyobj_boundingBox) &&
        jsopencv_to_safe(info, pyobj_newTracker, newTracker, ArgInfo("newTracker", 0)) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->add(newTracker, image, boundingBox));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_newTracker = NULL;
    Ptr<legacy::Tracker> newTracker;
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "newTracker", "image", "boundingBox", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:legacy_MultiTracker.add", (char**)keywords, &pyobj_newTracker, &pyobj_image, &pyobj_boundingBox) &&
        jsopencv_to_safe(info, pyobj_newTracker, newTracker, ArgInfo("newTracker", 0)) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->add(newTracker, image, boundingBox));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("add");

    return NULL;
}

static Napi::Value pyopencv_cv_legacy_legacy_MultiTracker_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<MultiTracker> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::MultiTracker::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_legacy_legacy_MultiTracker_getObjects(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;


    Ptr<cv::legacy::MultiTracker> * self1 = 0;
    if (!pyopencv_legacy_MultiTracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'legacy_MultiTracker' or its derivative)");
    Ptr<cv::legacy::MultiTracker> _self_ = *(self1);
    std::vector<Rect2d> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getObjects());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_legacy_legacy_MultiTracker_update(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;


    Ptr<cv::legacy::MultiTracker> * self1 = 0;
    if (!pyopencv_legacy_MultiTracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'legacy_MultiTracker' or its derivative)");
    Ptr<cv::legacy::MultiTracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    vector_Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:legacy_MultiTracker.update", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(boundingBox));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    vector_Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:legacy_MultiTracker.update", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(boundingBox));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("update");

    return NULL;
}



// Tables (legacy_MultiTracker)

static PyGetSetDef pyopencv_legacy_MultiTracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_MultiTracker_methods[] =
{
    {"add", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_MultiTracker_add, 0), "add(newTracker, image, boundingBox) -> retval\n.   * \\brief Add a new object to be tracked.\n.     *\n.     * @param newTracker tracking algorithm to be used\n.     * @param image input image\n.     * @param boundingBox a rectangle represents ROI of the tracked object"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_MultiTracker_create_static, METH_STATIC), "create() -> retval\n.   * \\brief Returns a pointer to a new instance of MultiTracker"},
    {"getObjects", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_MultiTracker_getObjects, 0), "getObjects() -> retval\n.   * \\brief Returns a reference to a storage for the tracked objects, each object corresponds to one tracker algorithm"},
    {"update", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_MultiTracker_update, 0), "update(image) -> retval, boundingBox\n.   * \\brief Update the current tracking status.\n.     * @param image input image\n.     * @param boundingBox the tracking result, represent a list of ROIs of the tracked objects."},

    {NULL,          NULL}
};

// Converter (legacy_MultiTracker)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::MultiTracker> >
{
    static PyObject* from(const Ptr<cv::legacy::MultiTracker>& r)
    {
        return pyopencv_legacy_MultiTracker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::MultiTracker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::MultiTracker> * dst_;
        if (pyopencv_legacy_MultiTracker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::MultiTracker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_Tracker (Generic)
//================================================================================

// GetSet (legacy_Tracker)



// Methods (legacy_Tracker)

static Napi::Value pyopencv_cv_legacy_legacy_Tracker_init(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;


    Ptr<cv::legacy::Tracker> * self1 = 0;
    if (!pyopencv_legacy_Tracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'legacy_Tracker' or its derivative)");
    Ptr<cv::legacy::Tracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", "boundingBox", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:legacy_Tracker.init", (char**)keywords, &pyobj_image, &pyobj_boundingBox) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->init(image, boundingBox));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", "boundingBox", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:legacy_Tracker.init", (char**)keywords, &pyobj_image, &pyobj_boundingBox) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->init(image, boundingBox));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("init");

    return NULL;
}

static Napi::Value pyopencv_cv_legacy_legacy_Tracker_update(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;


    Ptr<cv::legacy::Tracker> * self1 = 0;
    if (!pyopencv_legacy_Tracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'legacy_Tracker' or its derivative)");
    Ptr<cv::legacy::Tracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:legacy_Tracker.update", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(boundingBox));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:legacy_Tracker.update", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(boundingBox));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("update");

    return NULL;
}



// Tables (legacy_Tracker)

static PyGetSetDef pyopencv_legacy_Tracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_Tracker_methods[] =
{
    {"init", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_Tracker_init, 0), "init(image, boundingBox) -> retval\n.   @brief Initialize the tracker with a known bounding box that surrounded the target\n.       @param image The initial frame\n.       @param boundingBox The initial bounding box\n.   \n.       @return True if initialization went succesfully, false otherwise"},
    {"update", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_Tracker_update, 0), "update(image) -> retval, boundingBox\n.   @brief Update the tracker, find the new most likely bounding box for the target\n.       @param image The current frame\n.       @param boundingBox The bounding box that represent the new target location, if true was returned, not\n.       modified otherwise\n.   \n.       @return True means that target was located and false means that tracker cannot locate target in\n.       current frame. Note, that latter *does not* imply that tracker has failed, maybe target is indeed\n.       missing from the frame (say, out of sight)"},

    {NULL,          NULL}
};

// Converter (legacy_Tracker)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::Tracker> >
{
    static PyObject* from(const Ptr<cv::legacy::Tracker>& r)
    {
        return pyopencv_legacy_Tracker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::Tracker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::Tracker> * dst_;
        if (pyopencv_legacy_Tracker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::Tracker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerBoosting (Generic)
//================================================================================

// GetSet (legacy_TrackerBoosting)



// Methods (legacy_TrackerBoosting)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerBoosting_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerBoosting> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerBoosting::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (legacy_TrackerBoosting)

static PyGetSetDef pyopencv_legacy_TrackerBoosting_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerBoosting_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerBoosting_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor\n.       @param parameters BOOSTING parameters TrackerBoosting::Params"},

    {NULL,          NULL}
};

// Converter (legacy_TrackerBoosting)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerBoosting> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerBoosting>& r)
    {
        return pyopencv_legacy_TrackerBoosting_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerBoosting>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerBoosting> * dst_;
        if (pyopencv_legacy_TrackerBoosting_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerBoosting> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerCSRT (Generic)
//================================================================================

// GetSet (legacy_TrackerCSRT)



// Methods (legacy_TrackerCSRT)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerCSRT_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerCSRT> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerCSRT::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_legacy_legacy_TrackerCSRT_setInitialMask(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;


    Ptr<cv::legacy::TrackerCSRT> * self1 = 0;
    if (!pyopencv_legacy_TrackerCSRT_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'legacy_TrackerCSRT' or its derivative)");
    Ptr<cv::legacy::TrackerCSRT> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:legacy_TrackerCSRT.setInitialMask", (char**)keywords, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitialMask(mask));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "mask", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:legacy_TrackerCSRT.setInitialMask", (char**)keywords, &pyobj_mask) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitialMask(mask));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setInitialMask");

    return NULL;
}



// Tables (legacy_TrackerCSRT)

static PyGetSetDef pyopencv_legacy_TrackerCSRT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerCSRT_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerCSRT_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor\n.     @param parameters CSRT parameters TrackerCSRT::Params"},
    {"setInitialMask", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerCSRT_setInitialMask, 0), "setInitialMask(mask) -> None\n."},

    {NULL,          NULL}
};

// Converter (legacy_TrackerCSRT)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerCSRT> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerCSRT>& r)
    {
        return pyopencv_legacy_TrackerCSRT_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerCSRT>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerCSRT> * dst_;
        if (pyopencv_legacy_TrackerCSRT_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerCSRT> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerKCF (Generic)
//================================================================================

// GetSet (legacy_TrackerKCF)



// Methods (legacy_TrackerKCF)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerKCF_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerKCF> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerKCF::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (legacy_TrackerKCF)

static PyGetSetDef pyopencv_legacy_TrackerKCF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerKCF_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerKCF_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor\n.     @param parameters KCF parameters TrackerKCF::Params"},

    {NULL,          NULL}
};

// Converter (legacy_TrackerKCF)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerKCF> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerKCF>& r)
    {
        return pyopencv_legacy_TrackerKCF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerKCF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerKCF> * dst_;
        if (pyopencv_legacy_TrackerKCF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerKCF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerMIL (Generic)
//================================================================================

// GetSet (legacy_TrackerMIL)



// Methods (legacy_TrackerMIL)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerMIL_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerMIL> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerMIL::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (legacy_TrackerMIL)

static PyGetSetDef pyopencv_legacy_TrackerMIL_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerMIL_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerMIL_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor\n.       @param parameters MIL parameters TrackerMIL::Params"},

    {NULL,          NULL}
};

// Converter (legacy_TrackerMIL)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerMIL> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerMIL>& r)
    {
        return pyopencv_legacy_TrackerMIL_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerMIL>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerMIL> * dst_;
        if (pyopencv_legacy_TrackerMIL_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerMIL> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerMOSSE (Generic)
//================================================================================

// GetSet (legacy_TrackerMOSSE)



// Methods (legacy_TrackerMOSSE)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerMOSSE_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerMOSSE> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerMOSSE::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (legacy_TrackerMOSSE)

static PyGetSetDef pyopencv_legacy_TrackerMOSSE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerMOSSE_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerMOSSE_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor"},

    {NULL,          NULL}
};

// Converter (legacy_TrackerMOSSE)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerMOSSE> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerMOSSE>& r)
    {
        return pyopencv_legacy_TrackerMOSSE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerMOSSE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerMOSSE> * dst_;
        if (pyopencv_legacy_TrackerMOSSE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerMOSSE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerMedianFlow (Generic)
//================================================================================

// GetSet (legacy_TrackerMedianFlow)



// Methods (legacy_TrackerMedianFlow)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerMedianFlow_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerMedianFlow> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerMedianFlow::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (legacy_TrackerMedianFlow)

static PyGetSetDef pyopencv_legacy_TrackerMedianFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerMedianFlow_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerMedianFlow_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor\n.       @param parameters Median Flow parameters TrackerMedianFlow::Params"},

    {NULL,          NULL}
};

// Converter (legacy_TrackerMedianFlow)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerMedianFlow> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerMedianFlow>& r)
    {
        return pyopencv_legacy_TrackerMedianFlow_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerMedianFlow>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerMedianFlow> * dst_;
        if (pyopencv_legacy_TrackerMedianFlow_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerMedianFlow> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// legacy_TrackerTLD (Generic)
//================================================================================

// GetSet (legacy_TrackerTLD)



// Methods (legacy_TrackerTLD)

static Napi::Value pyopencv_cv_legacy_legacy_TrackerTLD_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::legacy;

    Ptr<legacy::TrackerTLD> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::legacy::TrackerTLD::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (legacy_TrackerTLD)

static PyGetSetDef pyopencv_legacy_TrackerTLD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_legacy_TrackerTLD_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_legacy_legacy_TrackerTLD_create_static, METH_STATIC), "create() -> retval\n.   @brief Constructor\n.       @param parameters TLD parameters TrackerTLD::Params"},

    {NULL,          NULL}
};

// Converter (legacy_TrackerTLD)

template<>
struct PyOpenCV_Converter< Ptr<cv::legacy::TrackerTLD> >
{
    static PyObject* from(const Ptr<cv::legacy::TrackerTLD>& r)
    {
        return pyopencv_legacy_TrackerTLD_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::legacy::TrackerTLD>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::legacy::TrackerTLD> * dst_;
        if (pyopencv_legacy_TrackerTLD_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::legacy::TrackerTLD> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// mcc_CChecker (Generic)
//================================================================================

// GetSet (mcc_CChecker)



// Methods (mcc_CChecker)

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;

    Ptr<CChecker> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::mcc::CChecker::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_getBox(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    std::vector<Point2f> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBox());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_getCenter(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Point2f retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCenter());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_getChartsRGB(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getChartsRGB());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_getChartsYCbCr(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getChartsYCbCr());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_getCost(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCost());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_getTarget(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    TYPECHART retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTarget());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_setBox(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Napi::Value* pyobj__box = NULL;
    vector_Point2f _box;

    const char* keywords[] = { "_box", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CChecker.setBox", (char**)keywords, &pyobj__box) &&
        jsopencv_to_safe(info, pyobj__box, _box, ArgInfo("_box", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBox(_box));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_setCenter(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Napi::Value* pyobj__center = NULL;
    Point2f _center;

    const char* keywords[] = { "_center", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CChecker.setCenter", (char**)keywords, &pyobj__center) &&
        jsopencv_to_safe(info, pyobj__center, _center, ArgInfo("_center", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCenter(_center));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_setChartsRGB(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Napi::Value* pyobj__chartsRGB = NULL;
    Mat _chartsRGB;

    const char* keywords[] = { "_chartsRGB", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CChecker.setChartsRGB", (char**)keywords, &pyobj__chartsRGB) &&
        jsopencv_to_safe(info, pyobj__chartsRGB, _chartsRGB, ArgInfo("_chartsRGB", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setChartsRGB(_chartsRGB));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_setChartsYCbCr(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Napi::Value* pyobj__chartsYCbCr = NULL;
    Mat _chartsYCbCr;

    const char* keywords[] = { "_chartsYCbCr", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CChecker.setChartsYCbCr", (char**)keywords, &pyobj__chartsYCbCr) &&
        jsopencv_to_safe(info, pyobj__chartsYCbCr, _chartsYCbCr, ArgInfo("_chartsYCbCr", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setChartsYCbCr(_chartsYCbCr));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_setCost(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Napi::Value* pyobj__cost = NULL;
    float _cost=0.f;

    const char* keywords[] = { "_cost", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CChecker.setCost", (char**)keywords, &pyobj__cost) &&
        jsopencv_to_safe(info, pyobj__cost, _cost, ArgInfo("_cost", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCost(_cost));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CChecker_setTarget(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CChecker> * self1 = 0;
    if (!pyopencv_mcc_CChecker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CChecker' or its derivative)");
    Ptr<cv::mcc::CChecker> _self_ = *(self1);
    Napi::Value* pyobj__target = NULL;
    TYPECHART _target=static_cast<TYPECHART>(0);

    const char* keywords[] = { "_target", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CChecker.setTarget", (char**)keywords, &pyobj__target) &&
        jsopencv_to_safe(info, pyobj__target, _target, ArgInfo("_target", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTarget(_target));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (mcc_CChecker)

static PyGetSetDef pyopencv_mcc_CChecker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_mcc_CChecker_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_create_static, METH_STATIC), "create() -> retval\n.   \\brief Create a new CChecker object.\n.       * \\return A pointer to the implementation of the CChecker"},
    {"getBox", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_getBox, 0), "getBox() -> retval\n."},
    {"getCenter", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_getCenter, 0), "getCenter() -> retval\n."},
    {"getChartsRGB", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_getChartsRGB, 0), "getChartsRGB() -> retval\n."},
    {"getChartsYCbCr", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_getChartsYCbCr, 0), "getChartsYCbCr() -> retval\n."},
    {"getCost", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_getCost, 0), "getCost() -> retval\n."},
    {"getTarget", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_getTarget, 0), "getTarget() -> retval\n."},
    {"setBox", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_setBox, 0), "setBox(_box) -> None\n."},
    {"setCenter", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_setCenter, 0), "setCenter(_center) -> None\n."},
    {"setChartsRGB", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_setChartsRGB, 0), "setChartsRGB(_chartsRGB) -> None\n."},
    {"setChartsYCbCr", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_setChartsYCbCr, 0), "setChartsYCbCr(_chartsYCbCr) -> None\n."},
    {"setCost", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_setCost, 0), "setCost(_cost) -> None\n."},
    {"setTarget", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CChecker_setTarget, 0), "setTarget(_target) -> None\n."},

    {NULL,          NULL}
};

// Converter (mcc_CChecker)

template<>
struct PyOpenCV_Converter< Ptr<cv::mcc::CChecker> >
{
    static PyObject* from(const Ptr<cv::mcc::CChecker>& r)
    {
        return pyopencv_mcc_CChecker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::mcc::CChecker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::mcc::CChecker> * dst_;
        if (pyopencv_mcc_CChecker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::mcc::CChecker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// mcc_CCheckerDetector (Generic)
//================================================================================

// GetSet (mcc_CCheckerDetector)



// Methods (mcc_CCheckerDetector)

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;

    Ptr<CCheckerDetector> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::mcc::CCheckerDetector::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDetector_getBestColorChecker(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CCheckerDetector> * self1 = 0;
    if (!pyopencv_mcc_CCheckerDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CCheckerDetector' or its derivative)");
    Ptr<cv::mcc::CCheckerDetector> _self_ = *(self1);
    Ptr<mcc::CChecker> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBestColorChecker());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDetector_getListColorChecker(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CCheckerDetector> * self1 = 0;
    if (!pyopencv_mcc_CCheckerDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CCheckerDetector' or its derivative)");
    Ptr<cv::mcc::CCheckerDetector> _self_ = *(self1);
    std::vector<Ptr<CChecker>> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getListColorChecker());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDetector_process(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CCheckerDetector> * self1 = 0;
    if (!pyopencv_mcc_CCheckerDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CCheckerDetector' or its derivative)");
    Ptr<cv::mcc::CCheckerDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_chartType = NULL;
    TYPECHART chartType=static_cast<TYPECHART>(0);
    Napi::Value* pyobj_nc = NULL;
    int nc=1;
    Napi::Value* pyobj_useNet = NULL;
    bool useNet=false;
    Napi::Value* pyobj_params = NULL;
    Ptr<DetectorParameters> params=DetectorParameters::create();
    bool retval;

    const char* keywords[] = { "image", "chartType", "nc", "useNet", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:mcc_CCheckerDetector.process", (char**)keywords, &pyobj_image, &pyobj_chartType, &pyobj_nc, &pyobj_useNet, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_chartType, chartType, ArgInfo("chartType", 0)) &&
        jsopencv_to_safe(info, pyobj_nc, nc, ArgInfo("nc", 0)) &&
        jsopencv_to_safe(info, pyobj_useNet, useNet, ArgInfo("useNet", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->process(image, chartType, nc, useNet, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_chartType = NULL;
    TYPECHART chartType=static_cast<TYPECHART>(0);
    Napi::Value* pyobj_nc = NULL;
    int nc=1;
    Napi::Value* pyobj_useNet = NULL;
    bool useNet=false;
    Napi::Value* pyobj_params = NULL;
    Ptr<DetectorParameters> params=DetectorParameters::create();
    bool retval;

    const char* keywords[] = { "image", "chartType", "nc", "useNet", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:mcc_CCheckerDetector.process", (char**)keywords, &pyobj_image, &pyobj_chartType, &pyobj_nc, &pyobj_useNet, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_chartType, chartType, ArgInfo("chartType", 0)) &&
        jsopencv_to_safe(info, pyobj_nc, nc, ArgInfo("nc", 0)) &&
        jsopencv_to_safe(info, pyobj_useNet, useNet, ArgInfo("useNet", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->process(image, chartType, nc, useNet, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("process");

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDetector_processWithROI(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CCheckerDetector> * self1 = 0;
    if (!pyopencv_mcc_CCheckerDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CCheckerDetector' or its derivative)");
    Ptr<cv::mcc::CCheckerDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_chartType = NULL;
    TYPECHART chartType=static_cast<TYPECHART>(0);
    Napi::Value* pyobj_regionsOfInterest = NULL;
    vector_Rect regionsOfInterest;
    Napi::Value* pyobj_nc = NULL;
    int nc=1;
    Napi::Value* pyobj_useNet = NULL;
    bool useNet=false;
    Napi::Value* pyobj_params = NULL;
    Ptr<DetectorParameters> params=DetectorParameters::create();
    bool retval;

    const char* keywords[] = { "image", "chartType", "regionsOfInterest", "nc", "useNet", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOO:mcc_CCheckerDetector.processWithROI", (char**)keywords, &pyobj_image, &pyobj_chartType, &pyobj_regionsOfInterest, &pyobj_nc, &pyobj_useNet, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_chartType, chartType, ArgInfo("chartType", 0)) &&
        jsopencv_to_safe(info, pyobj_regionsOfInterest, regionsOfInterest, ArgInfo("regionsOfInterest", 0)) &&
        jsopencv_to_safe(info, pyobj_nc, nc, ArgInfo("nc", 0)) &&
        jsopencv_to_safe(info, pyobj_useNet, useNet, ArgInfo("useNet", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->process(image, chartType, regionsOfInterest, nc, useNet, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_chartType = NULL;
    TYPECHART chartType=static_cast<TYPECHART>(0);
    Napi::Value* pyobj_regionsOfInterest = NULL;
    vector_Rect regionsOfInterest;
    Napi::Value* pyobj_nc = NULL;
    int nc=1;
    Napi::Value* pyobj_useNet = NULL;
    bool useNet=false;
    Napi::Value* pyobj_params = NULL;
    Ptr<DetectorParameters> params=DetectorParameters::create();
    bool retval;

    const char* keywords[] = { "image", "chartType", "regionsOfInterest", "nc", "useNet", "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOO:mcc_CCheckerDetector.processWithROI", (char**)keywords, &pyobj_image, &pyobj_chartType, &pyobj_regionsOfInterest, &pyobj_nc, &pyobj_useNet, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_chartType, chartType, ArgInfo("chartType", 0)) &&
        jsopencv_to_safe(info, pyobj_regionsOfInterest, regionsOfInterest, ArgInfo("regionsOfInterest", 0)) &&
        jsopencv_to_safe(info, pyobj_nc, nc, ArgInfo("nc", 0)) &&
        jsopencv_to_safe(info, pyobj_useNet, useNet, ArgInfo("useNet", 0)) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->process(image, chartType, regionsOfInterest, nc, useNet, params));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("processWithROI");

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDetector_setNet(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CCheckerDetector> * self1 = 0;
    if (!pyopencv_mcc_CCheckerDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CCheckerDetector' or its derivative)");
    Ptr<cv::mcc::CCheckerDetector> _self_ = *(self1);
    Napi::Value* pyobj_net = NULL;
    cv::dnn::Net net;
    bool retval;

    const char* keywords[] = { "net", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CCheckerDetector.setNet", (char**)keywords, &pyobj_net) &&
        jsopencv_to_safe(info, pyobj_net, net, ArgInfo("net", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setNet(net));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (mcc_CCheckerDetector)

static PyGetSetDef pyopencv_mcc_CCheckerDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_mcc_CCheckerDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDetector_create_static, METH_STATIC), "create() -> retval\n.   \\brief Returns the implementation of the CCheckerDetector.\n.       *"},
    {"getBestColorChecker", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDetector_getBestColorChecker, 0), "getBestColorChecker() -> retval\n.   \\brief Get the best color checker. By the best it means the one\n.       *         detected with the highest confidence.\n.       * \\return checker A single colorchecker, if atleast one colorchecker\n.       *                 was detected, 'nullptr' otherwise."},
    {"getListColorChecker", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDetector_getListColorChecker, 0), "getListColorChecker() -> retval\n.   \\brief Get the list of all detected colorcheckers\n.       * \\return checkers vector of colorcheckers"},
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDetector_process, 0), "process(image, chartType[, nc[, useNet[, params]]]) -> retval\n.   \\brief Find the ColorCharts in the given image.\n.       *\n.       * Differs from the above one only in the arguments.\n.       *\n.       * This version searches for the chart in the full image.\n.       *\n.       * The found charts are not returned but instead stored in the\n.       * detector, these can be accessed later on using getBestColorChecker()\n.       * and getListColorChecker()\n.       * \\param image image in color space BGR\n.       * \\param chartType type of the chart to detect\n.       * \\param nc number of charts in the image, if you don't know the exact\n.       *           then keeping this number high helps.\n.       * \\param useNet if it is true the network provided using the setNet()\n.       *               is used for preliminary search for regions where chart\n.       *               could be present, inside the regionsOfInterest provied.\n.       * \\param params parameters of the detection system. More information\n.       *               about them can be found in the struct DetectorParameters.\n.       * \\return true if atleast one chart is detected otherwise false"},
    {"processWithROI", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDetector_processWithROI, 0), "processWithROI(image, chartType, regionsOfInterest[, nc[, useNet[, params]]]) -> retval\n.   \\brief Find the ColorCharts in the given image.\n.       *\n.       * The found charts are not returned but instead stored in the\n.       * detector, these can be accessed later on using getBestColorChecker()\n.       * and getListColorChecker()\n.       * \\param image image in color space BGR\n.       * \\param chartType type of the chart to detect\n.       * \\param regionsOfInterest regions of image to look for the chart, if\n.       *                          it is empty, charts are looked for in the\n.       *                          entire image\n.       * \\param nc number of charts in the image, if you don't know the exact\n.       *           then keeping this number high helps.\n.       * \\param useNet if it is true the network provided using the setNet()\n.       *               is used for preliminary search for regions where chart\n.       *               could be present, inside the regionsOfInterest provied.\n.       * \\param params parameters of the detection system. More information\n.       *               about them can be found in the struct DetectorParameters.\n.       * \\return true if atleast one chart is detected otherwise false"},
    {"setNet", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDetector_setNet, 0), "setNet(net) -> retval\n.   \\brief Set the net which will be used to find the approximate\n.       *         bounding boxes for the color charts.\n.       *\n.       * It is not necessary to use this, but this usually results in\n.       * better detection rate.\n.       *\n.       * \\param net the neural network, if the network in empty, then\n.       *            the function will return false.\n.       * \\return true if it was able to set the detector's network,\n.       *         false otherwise."},

    {NULL,          NULL}
};

// Converter (mcc_CCheckerDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::mcc::CCheckerDetector> >
{
    static PyObject* from(const Ptr<cv::mcc::CCheckerDetector>& r)
    {
        return pyopencv_mcc_CCheckerDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::mcc::CCheckerDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::mcc::CCheckerDetector> * dst_;
        if (pyopencv_mcc_CCheckerDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::mcc::CCheckerDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// mcc_CCheckerDraw (Generic)
//================================================================================

// GetSet (mcc_CCheckerDraw)



// Methods (mcc_CCheckerDraw)

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDraw_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;

    Napi::Value* pyobj_pChecker = NULL;
    Ptr<CChecker> pChecker;
    Napi::Value* pyobj_color = NULL;
    Scalar color=CV_RGB(0, 250, 0);
    Napi::Value* pyobj_thickness = NULL;
    int thickness=2;
    Ptr<CCheckerDraw> retval;

    const char* keywords[] = { "pChecker", "color", "thickness", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:mcc_CCheckerDraw.create", (char**)keywords, &pyobj_pChecker, &pyobj_color, &pyobj_thickness) &&
        jsopencv_to_safe(info, pyobj_pChecker, pChecker, ArgInfo("pChecker", 0)) &&
        jsopencv_to_safe(info, pyobj_color, color, ArgInfo("color", 0)) &&
        jsopencv_to_safe(info, pyobj_thickness, thickness, ArgInfo("thickness", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::mcc::CCheckerDraw::create(pChecker, color, thickness));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_mcc_mcc_CCheckerDraw_draw(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;


    Ptr<cv::mcc::CCheckerDraw> * self1 = 0;
    if (!pyopencv_mcc_CCheckerDraw_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'mcc_CCheckerDraw' or its derivative)");
    Ptr<cv::mcc::CCheckerDraw> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CCheckerDraw.draw", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->draw(img));
        return jsopencv_from(img);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:mcc_CCheckerDraw.draw", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->draw(img));
        return jsopencv_from(img);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("draw");

    return NULL;
}



// Tables (mcc_CCheckerDraw)

static PyGetSetDef pyopencv_mcc_CCheckerDraw_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_mcc_CCheckerDraw_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDraw_create_static, METH_STATIC), "create(pChecker[, color[, thickness]]) -> retval\n.   \\brief Create a new CCheckerDraw object.\n.       * \\param pChecker The checker which will be drawn by this object.\n.       * \\param color The color by with which the squares of the checker\n.       *              will be drawn\n.       * \\param thickness The thickness with which the sqaures will be\n.       *                  drawn\n.       * \\return A pointer to the implementation of the CCheckerDraw"},
    {"draw", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_CCheckerDraw_draw, 0), "draw(img) -> img\n.   \\brief Draws the checker to the given image.\n.       * \\param img image in color space BGR\n.       * \\return void"},

    {NULL,          NULL}
};

// Converter (mcc_CCheckerDraw)

template<>
struct PyOpenCV_Converter< Ptr<cv::mcc::CCheckerDraw> >
{
    static PyObject* from(const Ptr<cv::mcc::CCheckerDraw>& r)
    {
        return pyopencv_mcc_CCheckerDraw_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::mcc::CCheckerDraw>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::mcc::CCheckerDraw> * dst_;
        if (pyopencv_mcc_CCheckerDraw_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::mcc::CCheckerDraw> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// mcc_DetectorParameters (Generic)
//================================================================================

// GetSet (mcc_DetectorParameters)


static PyObject* pyopencv_mcc_DetectorParameters_get_B0factor(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->B0factor);
}

static int pyopencv_mcc_DetectorParameters_set_B0factor(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the B0factor attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->B0factor, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_adaptiveThreshConstant(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->adaptiveThreshConstant);
}

static int pyopencv_mcc_DetectorParameters_set_adaptiveThreshConstant(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshConstant attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->adaptiveThreshConstant, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_adaptiveThreshWinSizeMax(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->adaptiveThreshWinSizeMax);
}

static int pyopencv_mcc_DetectorParameters_set_adaptiveThreshWinSizeMax(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeMax attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->adaptiveThreshWinSizeMax, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_adaptiveThreshWinSizeMin(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->adaptiveThreshWinSizeMin);
}

static int pyopencv_mcc_DetectorParameters_set_adaptiveThreshWinSizeMin(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeMin attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->adaptiveThreshWinSizeMin, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_adaptiveThreshWinSizeStep(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->adaptiveThreshWinSizeStep);
}

static int pyopencv_mcc_DetectorParameters_set_adaptiveThreshWinSizeStep(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeStep attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->adaptiveThreshWinSizeStep, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_borderWidth(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->borderWidth);
}

static int pyopencv_mcc_DetectorParameters_set_borderWidth(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the borderWidth attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->borderWidth, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_confidenceThreshold(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->confidenceThreshold);
}

static int pyopencv_mcc_DetectorParameters_set_confidenceThreshold(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the confidenceThreshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->confidenceThreshold, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_findCandidatesApproxPolyDPEpsMultiplier(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->findCandidatesApproxPolyDPEpsMultiplier);
}

static int pyopencv_mcc_DetectorParameters_set_findCandidatesApproxPolyDPEpsMultiplier(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the findCandidatesApproxPolyDPEpsMultiplier attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->findCandidatesApproxPolyDPEpsMultiplier, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_maxError(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->maxError);
}

static int pyopencv_mcc_DetectorParameters_set_maxError(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxError attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->maxError, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minContourLengthAllowed(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minContourLengthAllowed);
}

static int pyopencv_mcc_DetectorParameters_set_minContourLengthAllowed(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minContourLengthAllowed attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minContourLengthAllowed, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minContourPointsAllowed(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minContourPointsAllowed);
}

static int pyopencv_mcc_DetectorParameters_set_minContourPointsAllowed(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minContourPointsAllowed attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minContourPointsAllowed, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minContourSolidity(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minContourSolidity);
}

static int pyopencv_mcc_DetectorParameters_set_minContourSolidity(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minContourSolidity attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minContourSolidity, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minContoursArea(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minContoursArea);
}

static int pyopencv_mcc_DetectorParameters_set_minContoursArea(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minContoursArea attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minContoursArea, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minContoursAreaRate(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minContoursAreaRate);
}

static int pyopencv_mcc_DetectorParameters_set_minContoursAreaRate(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minContoursAreaRate attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minContoursAreaRate, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minGroupSize(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minGroupSize);
}

static int pyopencv_mcc_DetectorParameters_set_minGroupSize(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minGroupSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minGroupSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minImageSize(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minImageSize);
}

static int pyopencv_mcc_DetectorParameters_set_minImageSize(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minImageSize attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minImageSize, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minInterCheckerDistance(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minInterCheckerDistance);
}

static int pyopencv_mcc_DetectorParameters_set_minInterCheckerDistance(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minInterCheckerDistance attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minInterCheckerDistance, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_mcc_DetectorParameters_get_minInterContourDistance(pyopencv_mcc_DetectorParameters_t* p, void *closure)
{
    return jsopencv_from(p->v->minInterContourDistance);
}

static int pyopencv_mcc_DetectorParameters_set_minInterContourDistance(pyopencv_mcc_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minInterContourDistance attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minInterContourDistance, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (mcc_DetectorParameters)

static Napi::Value pyopencv_cv_mcc_mcc_DetectorParameters_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::mcc;

    Ptr<DetectorParameters> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::mcc::DetectorParameters::create());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (mcc_DetectorParameters)

static PyGetSetDef pyopencv_mcc_DetectorParameters_getseters[] =
{
    {(char*)"B0factor", (getter)pyopencv_mcc_DetectorParameters_get_B0factor, (setter)pyopencv_mcc_DetectorParameters_set_B0factor, (char*)"B0factor", NULL},
    {(char*)"adaptiveThreshConstant", (getter)pyopencv_mcc_DetectorParameters_get_adaptiveThreshConstant, (setter)pyopencv_mcc_DetectorParameters_set_adaptiveThreshConstant, (char*)"adaptiveThreshConstant", NULL},
    {(char*)"adaptiveThreshWinSizeMax", (getter)pyopencv_mcc_DetectorParameters_get_adaptiveThreshWinSizeMax, (setter)pyopencv_mcc_DetectorParameters_set_adaptiveThreshWinSizeMax, (char*)"adaptiveThreshWinSizeMax", NULL},
    {(char*)"adaptiveThreshWinSizeMin", (getter)pyopencv_mcc_DetectorParameters_get_adaptiveThreshWinSizeMin, (setter)pyopencv_mcc_DetectorParameters_set_adaptiveThreshWinSizeMin, (char*)"adaptiveThreshWinSizeMin", NULL},
    {(char*)"adaptiveThreshWinSizeStep", (getter)pyopencv_mcc_DetectorParameters_get_adaptiveThreshWinSizeStep, (setter)pyopencv_mcc_DetectorParameters_set_adaptiveThreshWinSizeStep, (char*)"adaptiveThreshWinSizeStep", NULL},
    {(char*)"borderWidth", (getter)pyopencv_mcc_DetectorParameters_get_borderWidth, (setter)pyopencv_mcc_DetectorParameters_set_borderWidth, (char*)"borderWidth", NULL},
    {(char*)"confidenceThreshold", (getter)pyopencv_mcc_DetectorParameters_get_confidenceThreshold, (setter)pyopencv_mcc_DetectorParameters_set_confidenceThreshold, (char*)"confidenceThreshold", NULL},
    {(char*)"findCandidatesApproxPolyDPEpsMultiplier", (getter)pyopencv_mcc_DetectorParameters_get_findCandidatesApproxPolyDPEpsMultiplier, (setter)pyopencv_mcc_DetectorParameters_set_findCandidatesApproxPolyDPEpsMultiplier, (char*)"findCandidatesApproxPolyDPEpsMultiplier", NULL},
    {(char*)"maxError", (getter)pyopencv_mcc_DetectorParameters_get_maxError, (setter)pyopencv_mcc_DetectorParameters_set_maxError, (char*)"maxError", NULL},
    {(char*)"minContourLengthAllowed", (getter)pyopencv_mcc_DetectorParameters_get_minContourLengthAllowed, (setter)pyopencv_mcc_DetectorParameters_set_minContourLengthAllowed, (char*)"minContourLengthAllowed", NULL},
    {(char*)"minContourPointsAllowed", (getter)pyopencv_mcc_DetectorParameters_get_minContourPointsAllowed, (setter)pyopencv_mcc_DetectorParameters_set_minContourPointsAllowed, (char*)"minContourPointsAllowed", NULL},
    {(char*)"minContourSolidity", (getter)pyopencv_mcc_DetectorParameters_get_minContourSolidity, (setter)pyopencv_mcc_DetectorParameters_set_minContourSolidity, (char*)"minContourSolidity", NULL},
    {(char*)"minContoursArea", (getter)pyopencv_mcc_DetectorParameters_get_minContoursArea, (setter)pyopencv_mcc_DetectorParameters_set_minContoursArea, (char*)"minContoursArea", NULL},
    {(char*)"minContoursAreaRate", (getter)pyopencv_mcc_DetectorParameters_get_minContoursAreaRate, (setter)pyopencv_mcc_DetectorParameters_set_minContoursAreaRate, (char*)"minContoursAreaRate", NULL},
    {(char*)"minGroupSize", (getter)pyopencv_mcc_DetectorParameters_get_minGroupSize, (setter)pyopencv_mcc_DetectorParameters_set_minGroupSize, (char*)"minGroupSize", NULL},
    {(char*)"minImageSize", (getter)pyopencv_mcc_DetectorParameters_get_minImageSize, (setter)pyopencv_mcc_DetectorParameters_set_minImageSize, (char*)"minImageSize", NULL},
    {(char*)"minInterCheckerDistance", (getter)pyopencv_mcc_DetectorParameters_get_minInterCheckerDistance, (setter)pyopencv_mcc_DetectorParameters_set_minInterCheckerDistance, (char*)"minInterCheckerDistance", NULL},
    {(char*)"minInterContourDistance", (getter)pyopencv_mcc_DetectorParameters_get_minInterContourDistance, (setter)pyopencv_mcc_DetectorParameters_set_minInterContourDistance, (char*)"minInterContourDistance", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_mcc_DetectorParameters_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_mcc_mcc_DetectorParameters_create_static, METH_STATIC), "create() -> retval\n."},

    {NULL,          NULL}
};

// Converter (mcc_DetectorParameters)

template<>
struct PyOpenCV_Converter< Ptr<cv::mcc::DetectorParameters> >
{
    static PyObject* from(const Ptr<cv::mcc::DetectorParameters>& r)
    {
        return pyopencv_mcc_DetectorParameters_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::mcc::DetectorParameters>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::mcc::DetectorParameters> * dst_;
        if (pyopencv_mcc_DetectorParameters_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::mcc::DetectorParameters> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_ANN_MLP (Generic)
//================================================================================

// GetSet (ml_ANN_MLP)



// Methods (ml_ANN_MLP)

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<ANN_MLP> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::ANN_MLP::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getAnnealCoolingRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAnnealCoolingRatio());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getAnnealFinalT(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAnnealFinalT());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getAnnealInitialT(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAnnealInitialT());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getAnnealItePerStep(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAnnealItePerStep());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getBackpropMomentumScale(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBackpropMomentumScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getBackpropWeightScale(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBackpropWeightScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getLayerSizes(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayerSizes());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getRpropDW0(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRpropDW0());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMax(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRpropDWMax());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMin(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRpropDWMin());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMinus(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRpropDWMinus());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getRpropDWPlus(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRpropDWPlus());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getTrainMethod(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainMethod());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_getWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_layerIdx = NULL;
    int layerIdx=0;
    Mat retval;

    const char* keywords[] = { "layerIdx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.getWeights", (char**)keywords, &pyobj_layerIdx) &&
        jsopencv_to_safe(info, pyobj_layerIdx, layerIdx, ArgInfo("layerIdx", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeights(layerIdx));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Ptr<ANN_MLP> retval;

    const char* keywords[] = { "filepath", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.load", (char**)keywords, &pyobj_filepath) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::ANN_MLP::load(filepath));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setActivationFunction(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_type = NULL;
    int type=0;
    Napi::Value* pyobj_param1 = NULL;
    double param1=0;
    Napi::Value* pyobj_param2 = NULL;
    double param2=0;

    const char* keywords[] = { "type", "param1", "param2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_ANN_MLP.setActivationFunction", (char**)keywords, &pyobj_type, &pyobj_param1, &pyobj_param2) &&
        jsopencv_to_safe(info, pyobj_type, type, ArgInfo("type", 0)) &&
        jsopencv_to_safe(info, pyobj_param1, param1, ArgInfo("param1", 0)) &&
        jsopencv_to_safe(info, pyobj_param2, param2, ArgInfo("param2", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setActivationFunction(type, param1, param2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setAnnealCoolingRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setAnnealCoolingRatio", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAnnealCoolingRatio(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setAnnealFinalT(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setAnnealFinalT", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAnnealFinalT(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setAnnealInitialT(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setAnnealInitialT", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAnnealInitialT(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setAnnealItePerStep(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setAnnealItePerStep", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAnnealItePerStep(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setBackpropMomentumScale(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setBackpropMomentumScale", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBackpropMomentumScale(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setBackpropWeightScale(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setBackpropWeightScale", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBackpropWeightScale(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setLayerSizes(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj__layer_sizes = NULL;
    Mat _layer_sizes;

    const char* keywords[] = { "_layer_sizes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setLayerSizes", (char**)keywords, &pyobj__layer_sizes) &&
        jsopencv_to_safe(info, pyobj__layer_sizes, _layer_sizes, ArgInfo("_layer_sizes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLayerSizes(_layer_sizes));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__layer_sizes = NULL;
    UMat _layer_sizes;

    const char* keywords[] = { "_layer_sizes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setLayerSizes", (char**)keywords, &pyobj__layer_sizes) &&
        jsopencv_to_safe(info, pyobj__layer_sizes, _layer_sizes, ArgInfo("_layer_sizes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLayerSizes(_layer_sizes));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setLayerSizes");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setRpropDW0(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setRpropDW0", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRpropDW0(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMax(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setRpropDWMax", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRpropDWMax(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMin(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setRpropDWMin", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRpropDWMin(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMinus(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setRpropDWMinus", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRpropDWMinus(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setRpropDWPlus(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setRpropDWPlus", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRpropDWPlus(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_ANN_MLP.setTermCriteria", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_ANN_MLP_setTrainMethod(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::ANN_MLP> * self1 = 0;
    if (!pyopencv_ml_ANN_MLP_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    Ptr<cv::ml::ANN_MLP> _self_ = *(self1);
    Napi::Value* pyobj_method = NULL;
    int method=0;
    Napi::Value* pyobj_param1 = NULL;
    double param1=0;
    Napi::Value* pyobj_param2 = NULL;
    double param2=0;

    const char* keywords[] = { "method", "param1", "param2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_ANN_MLP.setTrainMethod", (char**)keywords, &pyobj_method, &pyobj_param1, &pyobj_param2) &&
        jsopencv_to_safe(info, pyobj_method, method, ArgInfo("method", 0)) &&
        jsopencv_to_safe(info, pyobj_param1, param1, ArgInfo("param1", 0)) &&
        jsopencv_to_safe(info, pyobj_param2, param2, ArgInfo("param2", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTrainMethod(method, param1, param2));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_ANN_MLP)

static PyGetSetDef pyopencv_ml_ANN_MLP_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_ANN_MLP_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_create_static, METH_STATIC), "create() -> retval\n.   @brief Creates empty model\n.   \n.       Use StatModel::train to train the model, Algorithm::load\\<ANN_MLP\\>(filename) to load the pre-trained model.\n.       Note that the train method has optional flags: ANN_MLP::TrainFlags."},
    {"getAnnealCoolingRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealCoolingRatio, 0), "getAnnealCoolingRatio() -> retval\n.   @see setAnnealCoolingRatio"},
    {"getAnnealFinalT", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealFinalT, 0), "getAnnealFinalT() -> retval\n.   @see setAnnealFinalT"},
    {"getAnnealInitialT", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealInitialT, 0), "getAnnealInitialT() -> retval\n.   @see setAnnealInitialT"},
    {"getAnnealItePerStep", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealItePerStep, 0), "getAnnealItePerStep() -> retval\n.   @see setAnnealItePerStep"},
    {"getBackpropMomentumScale", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getBackpropMomentumScale, 0), "getBackpropMomentumScale() -> retval\n.   @see setBackpropMomentumScale"},
    {"getBackpropWeightScale", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getBackpropWeightScale, 0), "getBackpropWeightScale() -> retval\n.   @see setBackpropWeightScale"},
    {"getLayerSizes", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getLayerSizes, 0), "getLayerSizes() -> retval\n.   Integer vector specifying the number of neurons in each layer including the input and output layers.\n.       The very first element specifies the number of elements in the input layer.\n.       The last element - number of elements in the output layer.\n.   @sa setLayerSizes"},
    {"getRpropDW0", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDW0, 0), "getRpropDW0() -> retval\n.   @see setRpropDW0"},
    {"getRpropDWMax", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMax, 0), "getRpropDWMax() -> retval\n.   @see setRpropDWMax"},
    {"getRpropDWMin", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMin, 0), "getRpropDWMin() -> retval\n.   @see setRpropDWMin"},
    {"getRpropDWMinus", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMinus, 0), "getRpropDWMinus() -> retval\n.   @see setRpropDWMinus"},
    {"getRpropDWPlus", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWPlus, 0), "getRpropDWPlus() -> retval\n.   @see setRpropDWPlus"},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getTrainMethod", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getTrainMethod, 0), "getTrainMethod() -> retval\n.   Returns current training method"},
    {"getWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getWeights, 0), "getWeights(layerIdx) -> retval\n."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_load_static, METH_STATIC), "load(filepath) -> retval\n.   @brief Loads and creates a serialized ANN from a file\n.        *\n.        * Use ANN::save to serialize and store an ANN to disk.\n.        * Load the ANN from this file again, by calling this function with the path to the file.\n.        *\n.        * @param filepath path to serialized ANN"},
    {"setActivationFunction", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setActivationFunction, 0), "setActivationFunction(type[, param1[, param2]]) -> None\n.   Initialize the activation function for each neuron.\n.       Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.\n.       @param type The type of activation function. See ANN_MLP::ActivationFunctions.\n.       @param param1 The first parameter of the activation function, \\f$\\alpha\\f$. Default value is 0.\n.       @param param2 The second parameter of the activation function, \\f$\\beta\\f$. Default value is 0."},
    {"setAnnealCoolingRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealCoolingRatio, 0), "setAnnealCoolingRatio(val) -> None\n.   @copybrief getAnnealCoolingRatio @see getAnnealCoolingRatio"},
    {"setAnnealFinalT", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealFinalT, 0), "setAnnealFinalT(val) -> None\n.   @copybrief getAnnealFinalT @see getAnnealFinalT"},
    {"setAnnealInitialT", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealInitialT, 0), "setAnnealInitialT(val) -> None\n.   @copybrief getAnnealInitialT @see getAnnealInitialT"},
    {"setAnnealItePerStep", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealItePerStep, 0), "setAnnealItePerStep(val) -> None\n.   @copybrief getAnnealItePerStep @see getAnnealItePerStep"},
    {"setBackpropMomentumScale", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setBackpropMomentumScale, 0), "setBackpropMomentumScale(val) -> None\n.   @copybrief getBackpropMomentumScale @see getBackpropMomentumScale"},
    {"setBackpropWeightScale", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setBackpropWeightScale, 0), "setBackpropWeightScale(val) -> None\n.   @copybrief getBackpropWeightScale @see getBackpropWeightScale"},
    {"setLayerSizes", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setLayerSizes, 0), "setLayerSizes(_layer_sizes) -> None\n.   Integer vector specifying the number of neurons in each layer including the input and output layers.\n.       The very first element specifies the number of elements in the input layer.\n.       The last element - number of elements in the output layer. Default value is empty Mat.\n.   @sa getLayerSizes"},
    {"setRpropDW0", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDW0, 0), "setRpropDW0(val) -> None\n.   @copybrief getRpropDW0 @see getRpropDW0"},
    {"setRpropDWMax", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMax, 0), "setRpropDWMax(val) -> None\n.   @copybrief getRpropDWMax @see getRpropDWMax"},
    {"setRpropDWMin", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMin, 0), "setRpropDWMin(val) -> None\n.   @copybrief getRpropDWMin @see getRpropDWMin"},
    {"setRpropDWMinus", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMinus, 0), "setRpropDWMinus(val) -> None\n.   @copybrief getRpropDWMinus @see getRpropDWMinus"},
    {"setRpropDWPlus", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWPlus, 0), "setRpropDWPlus(val) -> None\n.   @copybrief getRpropDWPlus @see getRpropDWPlus"},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"setTrainMethod", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setTrainMethod, 0), "setTrainMethod(method[, param1[, param2]]) -> None\n.   Sets training method and common parameters.\n.       @param method Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.\n.       @param param1 passed to setRpropDW0 for ANN_MLP::RPROP and to setBackpropWeightScale for ANN_MLP::BACKPROP and to initialT for ANN_MLP::ANNEAL.\n.       @param param2 passed to setRpropDWMin for ANN_MLP::RPROP and to setBackpropMomentumScale for ANN_MLP::BACKPROP and to finalT for ANN_MLP::ANNEAL."},

    {NULL,          NULL}
};

// Converter (ml_ANN_MLP)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::ANN_MLP> >
{
    static PyObject* from(const Ptr<cv::ml::ANN_MLP>& r)
    {
        return pyopencv_ml_ANN_MLP_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::ANN_MLP>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::ANN_MLP> * dst_;
        if (pyopencv_ml_ANN_MLP_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::ANN_MLP> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_Boost (Generic)
//================================================================================

// GetSet (ml_Boost)



// Methods (ml_Boost)

static Napi::Value pyopencv_cv_ml_ml_Boost_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<Boost> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::Boost::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_getBoostType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::Boost> * self1 = 0;
    if (!pyopencv_ml_Boost_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    Ptr<cv::ml::Boost> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBoostType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_getWeakCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::Boost> * self1 = 0;
    if (!pyopencv_ml_Boost_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    Ptr<cv::ml::Boost> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeakCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_getWeightTrimRate(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::Boost> * self1 = 0;
    if (!pyopencv_ml_Boost_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    Ptr<cv::ml::Boost> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightTrimRate());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<Boost> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_Boost.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::Boost::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_setBoostType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::Boost> * self1 = 0;
    if (!pyopencv_ml_Boost_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    Ptr<cv::ml::Boost> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_Boost.setBoostType", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBoostType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_setWeakCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::Boost> * self1 = 0;
    if (!pyopencv_ml_Boost_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    Ptr<cv::ml::Boost> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_Boost.setWeakCount", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeakCount(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_Boost_setWeightTrimRate(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::Boost> * self1 = 0;
    if (!pyopencv_ml_Boost_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    Ptr<cv::ml::Boost> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_Boost.setWeightTrimRate", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightTrimRate(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_Boost)

static PyGetSetDef pyopencv_ml_Boost_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_Boost_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_create_static, METH_STATIC), "create() -> retval\n.   Creates the empty model.\n.   Use StatModel::train to train the model, Algorithm::load\\<Boost\\>(filename) to load the pre-trained model."},
    {"getBoostType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_getBoostType, 0), "getBoostType() -> retval\n.   @see setBoostType"},
    {"getWeakCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_getWeakCount, 0), "getWeakCount() -> retval\n.   @see setWeakCount"},
    {"getWeightTrimRate", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_getWeightTrimRate, 0), "getWeightTrimRate() -> retval\n.   @see setWeightTrimRate"},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized Boost from a file\n.        *\n.        * Use Boost::save to serialize and store an RTree to disk.\n.        * Load the Boost from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized Boost\n.        * @param nodeName name of node containing the classifier"},
    {"setBoostType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_setBoostType, 0), "setBoostType(val) -> None\n.   @copybrief getBoostType @see getBoostType"},
    {"setWeakCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_setWeakCount, 0), "setWeakCount(val) -> None\n.   @copybrief getWeakCount @see getWeakCount"},
    {"setWeightTrimRate", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_setWeightTrimRate, 0), "setWeightTrimRate(val) -> None\n.   @copybrief getWeightTrimRate @see getWeightTrimRate"},

    {NULL,          NULL}
};

// Converter (ml_Boost)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::Boost> >
{
    static PyObject* from(const Ptr<cv::ml::Boost>& r)
    {
        return pyopencv_ml_Boost_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::Boost>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::Boost> * dst_;
        if (pyopencv_ml_Boost_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::Boost> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_DTrees (Generic)
//================================================================================

// GetSet (ml_DTrees)



// Methods (ml_DTrees)

static Napi::Value pyopencv_cv_ml_ml_DTrees_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<DTrees> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::DTrees::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getCVFolds(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCVFolds());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getMaxCategories(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxCategories());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getMaxDepth(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxDepth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getMinSampleCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinSampleCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getPriors(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPriors());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getRegressionAccuracy(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRegressionAccuracy());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getTruncatePrunedTree(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTruncatePrunedTree());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getUse1SERule(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUse1SERule());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_getUseSurrogates(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseSurrogates());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<DTrees> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_DTrees.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::DTrees::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setCVFolds(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setCVFolds", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCVFolds(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setMaxCategories(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setMaxCategories", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxCategories(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setMaxDepth(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setMaxDepth", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setMinSampleCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setMinSampleCount", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinSampleCount(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setPriors(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setPriors", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPriors(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setRegressionAccuracy(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setRegressionAccuracy", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRegressionAccuracy(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setTruncatePrunedTree(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setTruncatePrunedTree", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTruncatePrunedTree(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setUse1SERule(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setUse1SERule", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUse1SERule(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_DTrees_setUseSurrogates(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::DTrees> * self1 = 0;
    if (!pyopencv_ml_DTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    Ptr<cv::ml::DTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_DTrees.setUseSurrogates", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseSurrogates(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_DTrees)

static PyGetSetDef pyopencv_ml_DTrees_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_DTrees_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_create_static, METH_STATIC), "create() -> retval\n.   @brief Creates the empty model\n.   \n.       The static method creates empty decision tree with the specified parameters. It should be then\n.       trained using train method (see StatModel::train). Alternatively, you can load the model from\n.       file using Algorithm::load\\<DTrees\\>(filename)."},
    {"getCVFolds", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getCVFolds, 0), "getCVFolds() -> retval\n.   @see setCVFolds"},
    {"getMaxCategories", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getMaxCategories, 0), "getMaxCategories() -> retval\n.   @see setMaxCategories"},
    {"getMaxDepth", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getMaxDepth, 0), "getMaxDepth() -> retval\n.   @see setMaxDepth"},
    {"getMinSampleCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getMinSampleCount, 0), "getMinSampleCount() -> retval\n.   @see setMinSampleCount"},
    {"getPriors", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getPriors, 0), "getPriors() -> retval\n.   @see setPriors"},
    {"getRegressionAccuracy", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getRegressionAccuracy, 0), "getRegressionAccuracy() -> retval\n.   @see setRegressionAccuracy"},
    {"getTruncatePrunedTree", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getTruncatePrunedTree, 0), "getTruncatePrunedTree() -> retval\n.   @see setTruncatePrunedTree"},
    {"getUse1SERule", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getUse1SERule, 0), "getUse1SERule() -> retval\n.   @see setUse1SERule"},
    {"getUseSurrogates", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getUseSurrogates, 0), "getUseSurrogates() -> retval\n.   @see setUseSurrogates"},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized DTrees from a file\n.        *\n.        * Use DTree::save to serialize and store an DTree to disk.\n.        * Load the DTree from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized DTree\n.        * @param nodeName name of node containing the classifier"},
    {"setCVFolds", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setCVFolds, 0), "setCVFolds(val) -> None\n.   @copybrief getCVFolds @see getCVFolds"},
    {"setMaxCategories", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setMaxCategories, 0), "setMaxCategories(val) -> None\n.   @copybrief getMaxCategories @see getMaxCategories"},
    {"setMaxDepth", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setMaxDepth, 0), "setMaxDepth(val) -> None\n.   @copybrief getMaxDepth @see getMaxDepth"},
    {"setMinSampleCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setMinSampleCount, 0), "setMinSampleCount(val) -> None\n.   @copybrief getMinSampleCount @see getMinSampleCount"},
    {"setPriors", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setPriors, 0), "setPriors(val) -> None\n.   @copybrief getPriors @see getPriors"},
    {"setRegressionAccuracy", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setRegressionAccuracy, 0), "setRegressionAccuracy(val) -> None\n.   @copybrief getRegressionAccuracy @see getRegressionAccuracy"},
    {"setTruncatePrunedTree", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setTruncatePrunedTree, 0), "setTruncatePrunedTree(val) -> None\n.   @copybrief getTruncatePrunedTree @see getTruncatePrunedTree"},
    {"setUse1SERule", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setUse1SERule, 0), "setUse1SERule(val) -> None\n.   @copybrief getUse1SERule @see getUse1SERule"},
    {"setUseSurrogates", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setUseSurrogates, 0), "setUseSurrogates(val) -> None\n.   @copybrief getUseSurrogates @see getUseSurrogates"},

    {NULL,          NULL}
};

// Converter (ml_DTrees)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::DTrees> >
{
    static PyObject* from(const Ptr<cv::ml::DTrees>& r)
    {
        return pyopencv_ml_DTrees_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::DTrees>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::DTrees> * dst_;
        if (pyopencv_ml_DTrees_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::DTrees> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_EM (Generic)
//================================================================================

// GetSet (ml_EM)



// Methods (ml_EM)

static Napi::Value pyopencv_cv_ml_ml_EM_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<EM> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::EM::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_getClustersNumber(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getClustersNumber());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_getCovarianceMatrixType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCovarianceMatrixType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_getCovs(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    Napi::Value* pyobj_covs = NULL;
    vector_Mat covs;

    const char* keywords[] = { "covs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ml_EM.getCovs", (char**)keywords, &pyobj_covs) &&
        jsopencv_to_safe(info, pyobj_covs, covs, ArgInfo("covs", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getCovs(covs));
        return jsopencv_from(covs);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_getMeans(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMeans());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_getWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<EM> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_EM.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::EM::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_predict(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_results = NULL;
    Mat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_EM.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(results));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_results = NULL;
    UMat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_EM.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(results));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_predict2(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_sample = NULL;
    Mat sample;
    Napi::Value* pyobj_probs = NULL;
    Mat probs;
    Vec2d retval;

    const char* keywords[] = { "sample", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_EM.predict2", (char**)keywords, &pyobj_sample, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_sample, sample, ArgInfo("sample", 0)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict2(sample, probs));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_sample = NULL;
    UMat sample;
    Napi::Value* pyobj_probs = NULL;
    UMat probs;
    Vec2d retval;

    const char* keywords[] = { "sample", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_EM.predict2", (char**)keywords, &pyobj_sample, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_sample, sample, ArgInfo("sample", 0)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict2(sample, probs));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict2");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_setClustersNumber(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_EM.setClustersNumber", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setClustersNumber(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_setCovarianceMatrixType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_EM.setCovarianceMatrixType", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCovarianceMatrixType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_EM.setTermCriteria", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_trainE(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_means0 = NULL;
    Mat means0;
    Napi::Value* pyobj_covs0 = NULL;
    Mat covs0;
    Napi::Value* pyobj_weights0 = NULL;
    Mat weights0;
    Napi::Value* pyobj_logLikelihoods = NULL;
    Mat logLikelihoods;
    Napi::Value* pyobj_labels = NULL;
    Mat labels;
    Napi::Value* pyobj_probs = NULL;
    Mat probs;
    bool retval;

    const char* keywords[] = { "samples", "means0", "covs0", "weights0", "logLikelihoods", "labels", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOOOO:ml_EM.trainE", (char**)keywords, &pyobj_samples, &pyobj_means0, &pyobj_covs0, &pyobj_weights0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_means0, means0, ArgInfo("means0", 0)) &&
        jsopencv_to_safe(info, pyobj_covs0, covs0, ArgInfo("covs0", 0)) &&
        jsopencv_to_safe(info, pyobj_weights0, weights0, ArgInfo("weights0", 0)) &&
        jsopencv_to_safe(info, pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 1)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainE(samples, means0, covs0, weights0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(logLikelihoods), jsopencv_from(labels), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_means0 = NULL;
    UMat means0;
    Napi::Value* pyobj_covs0 = NULL;
    UMat covs0;
    Napi::Value* pyobj_weights0 = NULL;
    UMat weights0;
    Napi::Value* pyobj_logLikelihoods = NULL;
    UMat logLikelihoods;
    Napi::Value* pyobj_labels = NULL;
    UMat labels;
    Napi::Value* pyobj_probs = NULL;
    UMat probs;
    bool retval;

    const char* keywords[] = { "samples", "means0", "covs0", "weights0", "logLikelihoods", "labels", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOOOO:ml_EM.trainE", (char**)keywords, &pyobj_samples, &pyobj_means0, &pyobj_covs0, &pyobj_weights0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_means0, means0, ArgInfo("means0", 0)) &&
        jsopencv_to_safe(info, pyobj_covs0, covs0, ArgInfo("covs0", 0)) &&
        jsopencv_to_safe(info, pyobj_weights0, weights0, ArgInfo("weights0", 0)) &&
        jsopencv_to_safe(info, pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 1)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainE(samples, means0, covs0, weights0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(logLikelihoods), jsopencv_from(labels), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("trainE");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_trainEM(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_logLikelihoods = NULL;
    Mat logLikelihoods;
    Napi::Value* pyobj_labels = NULL;
    Mat labels;
    Napi::Value* pyobj_probs = NULL;
    Mat probs;
    bool retval;

    const char* keywords[] = { "samples", "logLikelihoods", "labels", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:ml_EM.trainEM", (char**)keywords, &pyobj_samples, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 1)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainEM(samples, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(logLikelihoods), jsopencv_from(labels), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_logLikelihoods = NULL;
    UMat logLikelihoods;
    Napi::Value* pyobj_labels = NULL;
    UMat labels;
    Napi::Value* pyobj_probs = NULL;
    UMat probs;
    bool retval;

    const char* keywords[] = { "samples", "logLikelihoods", "labels", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:ml_EM.trainEM", (char**)keywords, &pyobj_samples, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 1)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainEM(samples, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(logLikelihoods), jsopencv_from(labels), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("trainEM");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_EM_trainM(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::EM> * self1 = 0;
    if (!pyopencv_ml_EM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Ptr<cv::ml::EM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_probs0 = NULL;
    Mat probs0;
    Napi::Value* pyobj_logLikelihoods = NULL;
    Mat logLikelihoods;
    Napi::Value* pyobj_labels = NULL;
    Mat labels;
    Napi::Value* pyobj_probs = NULL;
    Mat probs;
    bool retval;

    const char* keywords[] = { "samples", "probs0", "logLikelihoods", "labels", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:ml_EM.trainM", (char**)keywords, &pyobj_samples, &pyobj_probs0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_probs0, probs0, ArgInfo("probs0", 0)) &&
        jsopencv_to_safe(info, pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 1)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainM(samples, probs0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(logLikelihoods), jsopencv_from(labels), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_probs0 = NULL;
    UMat probs0;
    Napi::Value* pyobj_logLikelihoods = NULL;
    UMat logLikelihoods;
    Napi::Value* pyobj_labels = NULL;
    UMat labels;
    Napi::Value* pyobj_probs = NULL;
    UMat probs;
    bool retval;

    const char* keywords[] = { "samples", "probs0", "logLikelihoods", "labels", "probs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:ml_EM.trainM", (char**)keywords, &pyobj_samples, &pyobj_probs0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_probs0, probs0, ArgInfo("probs0", 0)) &&
        jsopencv_to_safe(info, pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        jsopencv_to_safe(info, pyobj_labels, labels, ArgInfo("labels", 1)) &&
        jsopencv_to_safe(info, pyobj_probs, probs, ArgInfo("probs", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainM(samples, probs0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(logLikelihoods), jsopencv_from(labels), jsopencv_from(probs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("trainM");

    return NULL;
}



// Tables (ml_EM)

static PyGetSetDef pyopencv_ml_EM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_EM_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_create_static, METH_STATIC), "create() -> retval\n.   Creates empty %EM model.\n.       The model should be trained then using StatModel::train(traindata, flags) method. Alternatively, you\n.       can use one of the EM::train\\* methods or load it from file using Algorithm::load\\<EM\\>(filename)."},
    {"getClustersNumber", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getClustersNumber, 0), "getClustersNumber() -> retval\n.   @see setClustersNumber"},
    {"getCovarianceMatrixType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getCovarianceMatrixType, 0), "getCovarianceMatrixType() -> retval\n.   @see setCovarianceMatrixType"},
    {"getCovs", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getCovs, 0), "getCovs([, covs]) -> covs\n.   @brief Returns covariation matrices\n.   \n.       Returns vector of covariation matrices. Number of matrices is the number of gaussian mixtures,\n.       each matrix is a square floating-point matrix NxN, where N is the space dimensionality."},
    {"getMeans", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getMeans, 0), "getMeans() -> retval\n.   @brief Returns the cluster centers (means of the Gaussian mixture)\n.   \n.       Returns matrix with the number of rows equal to the number of mixtures and number of columns\n.       equal to the space dimensionality."},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getWeights, 0), "getWeights() -> retval\n.   @brief Returns weights of the mixtures\n.   \n.       Returns vector with the number of elements equal to the number of mixtures."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized EM from a file\n.        *\n.        * Use EM::save to serialize and store an EM to disk.\n.        * Load the EM from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized EM\n.        * @param nodeName name of node containing the classifier"},
    {"predict", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_predict, 0), "predict(samples[, results[, flags]]) -> retval, results\n.   @brief Returns posterior probabilities for the provided samples\n.   \n.       @param samples The input samples, floating-point matrix\n.       @param results The optional output \\f$ nSamples \\times nClusters\\f$ matrix of results. It contains\n.       posterior probabilities for each sample from the input\n.       @param flags This parameter will be ignored"},
    {"predict2", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_predict2, 0), "predict2(sample[, probs]) -> retval, probs\n.   @brief Returns a likelihood logarithm value and an index of the most probable mixture component\n.       for the given sample.\n.   \n.       @param sample A sample for classification. It should be a one-channel matrix of\n.           \\f$1 \\times dims\\f$ or \\f$dims \\times 1\\f$ size.\n.       @param probs Optional output matrix that contains posterior probabilities of each component\n.           given the sample. It has \\f$1 \\times nclusters\\f$ size and CV_64FC1 type.\n.   \n.       The method returns a two-element double vector. Zero element is a likelihood logarithm value for\n.       the sample. First element is an index of the most probable mixture component for the given\n.       sample."},
    {"setClustersNumber", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_setClustersNumber, 0), "setClustersNumber(val) -> None\n.   @copybrief getClustersNumber @see getClustersNumber"},
    {"setCovarianceMatrixType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_setCovarianceMatrixType, 0), "setCovarianceMatrixType(val) -> None\n.   @copybrief getCovarianceMatrixType @see getCovarianceMatrixType"},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"trainE", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_trainE, 0), "trainE(samples, means0[, covs0[, weights0[, logLikelihoods[, labels[, probs]]]]]) -> retval, logLikelihoods, labels, probs\n.   @brief Estimate the Gaussian mixture parameters from a samples set.\n.   \n.       This variation starts with Expectation step. You need to provide initial means \\f$a_k\\f$ of\n.       mixture components. Optionally you can pass initial weights \\f$\\pi_k\\f$ and covariance matrices\n.       \\f$S_k\\f$ of mixture components.\n.   \n.       @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n.           one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n.           it will be converted to the inner matrix of such type for the further computing.\n.       @param means0 Initial means \\f$a_k\\f$ of mixture components. It is a one-channel matrix of\n.           \\f$nclusters \\times dims\\f$ size. If the matrix does not have CV_64F type it will be\n.           converted to the inner matrix of such type for the further computing.\n.       @param covs0 The vector of initial covariance matrices \\f$S_k\\f$ of mixture components. Each of\n.           covariance matrices is a one-channel matrix of \\f$dims \\times dims\\f$ size. If the matrices\n.           do not have CV_64F type they will be converted to the inner matrices of such type for the\n.           further computing.\n.       @param weights0 Initial weights \\f$\\pi_k\\f$ of mixture components. It should be a one-channel\n.           floating-point matrix with \\f$1 \\times nclusters\\f$ or \\f$nclusters \\times 1\\f$ size.\n.       @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n.           each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n.       @param labels The optional output \"class label\" for each sample:\n.           \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n.           mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n.       @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n.           mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n.           CV_64FC1 type."},
    {"trainEM", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_trainEM, 0), "trainEM(samples[, logLikelihoods[, labels[, probs]]]) -> retval, logLikelihoods, labels, probs\n.   @brief Estimate the Gaussian mixture parameters from a samples set.\n.   \n.       This variation starts with Expectation step. Initial values of the model parameters will be\n.       estimated by the k-means algorithm.\n.   \n.       Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take\n.       responses (class labels or function values) as input. Instead, it computes the *Maximum\n.       Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the\n.       parameters inside the structure: \\f$p_{i,k}\\f$ in probs, \\f$a_k\\f$ in means , \\f$S_k\\f$ in\n.       covs[k], \\f$\\pi_k\\f$ in weights , and optionally computes the output \"class label\" for each\n.       sample: \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most\n.       probable mixture component for each sample).\n.   \n.       The trained model can be used further for prediction, just like any other classifier. The\n.       trained model is similar to the NormalBayesClassifier.\n.   \n.       @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n.           one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n.           it will be converted to the inner matrix of such type for the further computing.\n.       @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n.           each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n.       @param labels The optional output \"class label\" for each sample:\n.           \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n.           mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n.       @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n.           mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n.           CV_64FC1 type."},
    {"trainM", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_trainM, 0), "trainM(samples, probs0[, logLikelihoods[, labels[, probs]]]) -> retval, logLikelihoods, labels, probs\n.   @brief Estimate the Gaussian mixture parameters from a samples set.\n.   \n.       This variation starts with Maximization step. You need to provide initial probabilities\n.       \\f$p_{i,k}\\f$ to use this option.\n.   \n.       @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n.           one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n.           it will be converted to the inner matrix of such type for the further computing.\n.       @param probs0 the probabilities\n.       @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n.           each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n.       @param labels The optional output \"class label\" for each sample:\n.           \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n.           mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n.       @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n.           mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n.           CV_64FC1 type."},

    {NULL,          NULL}
};

// Converter (ml_EM)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::EM> >
{
    static PyObject* from(const Ptr<cv::ml::EM>& r)
    {
        return pyopencv_ml_EM_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::EM>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::EM> * dst_;
        if (pyopencv_ml_EM_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::EM> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_KNearest (Generic)
//================================================================================

// GetSet (ml_KNearest)



// Methods (ml_KNearest)

static Napi::Value pyopencv_cv_ml_ml_KNearest_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<KNearest> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::KNearest::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_findNearest(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_k = NULL;
    int k=0;
    Napi::Value* pyobj_results = NULL;
    Mat results;
    Napi::Value* pyobj_neighborResponses = NULL;
    Mat neighborResponses;
    Napi::Value* pyobj_dist = NULL;
    Mat dist;
    float retval;

    const char* keywords[] = { "samples", "k", "results", "neighborResponses", "dist", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:ml_KNearest.findNearest", (char**)keywords, &pyobj_samples, &pyobj_k, &pyobj_results, &pyobj_neighborResponses, &pyobj_dist) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_neighborResponses, neighborResponses, ArgInfo("neighborResponses", 1)) &&
        jsopencv_to_safe(info, pyobj_dist, dist, ArgInfo("dist", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->findNearest(samples, k, results, neighborResponses, dist));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(results), jsopencv_from(neighborResponses), jsopencv_from(dist));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_k = NULL;
    int k=0;
    Napi::Value* pyobj_results = NULL;
    UMat results;
    Napi::Value* pyobj_neighborResponses = NULL;
    UMat neighborResponses;
    Napi::Value* pyobj_dist = NULL;
    UMat dist;
    float retval;

    const char* keywords[] = { "samples", "k", "results", "neighborResponses", "dist", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:ml_KNearest.findNearest", (char**)keywords, &pyobj_samples, &pyobj_k, &pyobj_results, &pyobj_neighborResponses, &pyobj_dist) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_neighborResponses, neighborResponses, ArgInfo("neighborResponses", 1)) &&
        jsopencv_to_safe(info, pyobj_dist, dist, ArgInfo("dist", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->findNearest(samples, k, results, neighborResponses, dist));
        return Py_BuildValue("(NNNN)", jsopencv_from(retval), jsopencv_from(results), jsopencv_from(neighborResponses), jsopencv_from(dist));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("findNearest");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_getAlgorithmType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlgorithmType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_getDefaultK(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultK());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_getEmax(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEmax());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_getIsClassifier(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIsClassifier());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Ptr<KNearest> retval;

    const char* keywords[] = { "filepath", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_KNearest.load", (char**)keywords, &pyobj_filepath) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::KNearest::load(filepath));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_setAlgorithmType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_KNearest.setAlgorithmType", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAlgorithmType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_setDefaultK(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_KNearest.setDefaultK", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDefaultK(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_setEmax(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_KNearest.setEmax", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEmax(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_KNearest_setIsClassifier(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::KNearest> * self1 = 0;
    if (!pyopencv_ml_KNearest_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    Ptr<cv::ml::KNearest> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_KNearest.setIsClassifier", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setIsClassifier(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_KNearest)

static PyGetSetDef pyopencv_ml_KNearest_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_KNearest_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_create_static, METH_STATIC), "create() -> retval\n.   @brief Creates the empty model\n.   \n.       The static method creates empty %KNearest classifier. It should be then trained using StatModel::train method."},
    {"findNearest", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_findNearest, 0), "findNearest(samples, k[, results[, neighborResponses[, dist]]]) -> retval, results, neighborResponses, dist\n.   @brief Finds the neighbors and predicts responses for input vectors.\n.   \n.       @param samples Input samples stored by rows. It is a single-precision floating-point matrix of\n.           `<number_of_samples> * k` size.\n.       @param k Number of used nearest neighbors. Should be greater than 1.\n.       @param results Vector with results of prediction (regression or classification) for each input\n.           sample. It is a single-precision floating-point vector with `<number_of_samples>` elements.\n.       @param neighborResponses Optional output values for corresponding neighbors. It is a single-\n.           precision floating-point matrix of `<number_of_samples> * k` size.\n.       @param dist Optional output distances from the input vectors to the corresponding neighbors. It\n.           is a single-precision floating-point matrix of `<number_of_samples> * k` size.\n.   \n.       For each input vector (a row of the matrix samples), the method finds the k nearest neighbors.\n.       In case of regression, the predicted result is a mean value of the particular vector's neighbor\n.       responses. In case of classification, the class is determined by voting.\n.   \n.       For each input vector, the neighbors are sorted by their distances to the vector.\n.   \n.       In case of C++ interface you can use output pointers to empty matrices and the function will\n.       allocate memory itself.\n.   \n.       If only a single input vector is passed, all output matrices are optional and the predicted\n.       value is returned by the method.\n.   \n.       The function is parallelized with the TBB library."},
    {"getAlgorithmType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getAlgorithmType, 0), "getAlgorithmType() -> retval\n.   @see setAlgorithmType"},
    {"getDefaultK", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getDefaultK, 0), "getDefaultK() -> retval\n.   @see setDefaultK"},
    {"getEmax", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getEmax, 0), "getEmax() -> retval\n.   @see setEmax"},
    {"getIsClassifier", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getIsClassifier, 0), "getIsClassifier() -> retval\n.   @see setIsClassifier"},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_load_static, METH_STATIC), "load(filepath) -> retval\n.   @brief Loads and creates a serialized knearest from a file\n.        *\n.        * Use KNearest::save to serialize and store an KNearest to disk.\n.        * Load the KNearest from this file again, by calling this function with the path to the file.\n.        *\n.        * @param filepath path to serialized KNearest"},
    {"setAlgorithmType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setAlgorithmType, 0), "setAlgorithmType(val) -> None\n.   @copybrief getAlgorithmType @see getAlgorithmType"},
    {"setDefaultK", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setDefaultK, 0), "setDefaultK(val) -> None\n.   @copybrief getDefaultK @see getDefaultK"},
    {"setEmax", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setEmax, 0), "setEmax(val) -> None\n.   @copybrief getEmax @see getEmax"},
    {"setIsClassifier", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setIsClassifier, 0), "setIsClassifier(val) -> None\n.   @copybrief getIsClassifier @see getIsClassifier"},

    {NULL,          NULL}
};

// Converter (ml_KNearest)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::KNearest> >
{
    static PyObject* from(const Ptr<cv::ml::KNearest>& r)
    {
        return pyopencv_ml_KNearest_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::KNearest>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::KNearest> * dst_;
        if (pyopencv_ml_KNearest_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::KNearest> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_LogisticRegression (Generic)
//================================================================================

// GetSet (ml_LogisticRegression)



// Methods (ml_LogisticRegression)

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<LogisticRegression> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::LogisticRegression::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_getIterations(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIterations());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_getLearningRate(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLearningRate());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_getMiniBatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMiniBatchSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_getRegularization(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRegularization());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_getTrainMethod(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainMethod());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_get_learnt_thetas(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get_learnt_thetas());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<LogisticRegression> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_LogisticRegression.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::LogisticRegression::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_predict(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_results = NULL;
    Mat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_LogisticRegression.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(results));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_results = NULL;
    UMat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_LogisticRegression.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(results));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_setIterations(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_LogisticRegression.setIterations", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_setLearningRate(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_LogisticRegression.setLearningRate", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLearningRate(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_setMiniBatchSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_LogisticRegression.setMiniBatchSize", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMiniBatchSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_setRegularization(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_LogisticRegression.setRegularization", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRegularization(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_LogisticRegression.setTermCriteria", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_LogisticRegression_setTrainMethod(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::LogisticRegression> * self1 = 0;
    if (!pyopencv_ml_LogisticRegression_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Ptr<cv::ml::LogisticRegression> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_LogisticRegression.setTrainMethod", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTrainMethod(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_LogisticRegression)

static PyGetSetDef pyopencv_ml_LogisticRegression_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_LogisticRegression_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_create_static, METH_STATIC), "create() -> retval\n.   @brief Creates empty model.\n.   \n.       Creates Logistic Regression model with parameters given."},
    {"getIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getIterations, 0), "getIterations() -> retval\n.   @see setIterations"},
    {"getLearningRate", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getLearningRate, 0), "getLearningRate() -> retval\n.   @see setLearningRate"},
    {"getMiniBatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getMiniBatchSize, 0), "getMiniBatchSize() -> retval\n.   @see setMiniBatchSize"},
    {"getRegularization", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getRegularization, 0), "getRegularization() -> retval\n.   @see setRegularization"},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getTrainMethod", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getTrainMethod, 0), "getTrainMethod() -> retval\n.   @see setTrainMethod"},
    {"get_learnt_thetas", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_get_learnt_thetas, 0), "get_learnt_thetas() -> retval\n.   @brief This function returns the trained parameters arranged across rows.\n.   \n.       For a two class classification problem, it returns a row matrix. It returns learnt parameters of\n.       the Logistic Regression as a matrix of type CV_32F."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized LogisticRegression from a file\n.        *\n.        * Use LogisticRegression::save to serialize and store an LogisticRegression to disk.\n.        * Load the LogisticRegression from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized LogisticRegression\n.        * @param nodeName name of node containing the classifier"},
    {"predict", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_predict, 0), "predict(samples[, results[, flags]]) -> retval, results\n.   @brief Predicts responses for input samples and returns a float type.\n.   \n.       @param samples The input data for the prediction algorithm. Matrix [m x n], where each row\n.           contains variables (features) of one object being classified. Should have data type CV_32F.\n.       @param results Predicted labels as a column matrix of type CV_32S.\n.       @param flags Not used."},
    {"setIterations", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setIterations, 0), "setIterations(val) -> None\n.   @copybrief getIterations @see getIterations"},
    {"setLearningRate", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setLearningRate, 0), "setLearningRate(val) -> None\n.   @copybrief getLearningRate @see getLearningRate"},
    {"setMiniBatchSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setMiniBatchSize, 0), "setMiniBatchSize(val) -> None\n.   @copybrief getMiniBatchSize @see getMiniBatchSize"},
    {"setRegularization", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setRegularization, 0), "setRegularization(val) -> None\n.   @copybrief getRegularization @see getRegularization"},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"setTrainMethod", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setTrainMethod, 0), "setTrainMethod(val) -> None\n.   @copybrief getTrainMethod @see getTrainMethod"},

    {NULL,          NULL}
};

// Converter (ml_LogisticRegression)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::LogisticRegression> >
{
    static PyObject* from(const Ptr<cv::ml::LogisticRegression>& r)
    {
        return pyopencv_ml_LogisticRegression_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::LogisticRegression>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::LogisticRegression> * dst_;
        if (pyopencv_ml_LogisticRegression_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::LogisticRegression> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_NormalBayesClassifier (Generic)
//================================================================================

// GetSet (ml_NormalBayesClassifier)



// Methods (ml_NormalBayesClassifier)

static Napi::Value pyopencv_cv_ml_ml_NormalBayesClassifier_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<NormalBayesClassifier> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::NormalBayesClassifier::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_NormalBayesClassifier_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<NormalBayesClassifier> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_NormalBayesClassifier.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::NormalBayesClassifier::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_NormalBayesClassifier_predictProb(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::NormalBayesClassifier> * self1 = 0;
    if (!pyopencv_ml_NormalBayesClassifier_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_NormalBayesClassifier' or its derivative)");
    Ptr<cv::ml::NormalBayesClassifier> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_inputs = NULL;
    Mat inputs;
    Napi::Value* pyobj_outputs = NULL;
    Mat outputs;
    Napi::Value* pyobj_outputProbs = NULL;
    Mat outputProbs;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "inputs", "outputs", "outputProbs", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:ml_NormalBayesClassifier.predictProb", (char**)keywords, &pyobj_inputs, &pyobj_outputs, &pyobj_outputProbs, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        jsopencv_to_safe(info, pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        jsopencv_to_safe(info, pyobj_outputProbs, outputProbs, ArgInfo("outputProbs", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predictProb(inputs, outputs, outputProbs, flags));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(outputs), jsopencv_from(outputProbs));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_inputs = NULL;
    UMat inputs;
    Napi::Value* pyobj_outputs = NULL;
    UMat outputs;
    Napi::Value* pyobj_outputProbs = NULL;
    UMat outputProbs;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "inputs", "outputs", "outputProbs", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OOO:ml_NormalBayesClassifier.predictProb", (char**)keywords, &pyobj_inputs, &pyobj_outputs, &pyobj_outputProbs, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        jsopencv_to_safe(info, pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        jsopencv_to_safe(info, pyobj_outputProbs, outputProbs, ArgInfo("outputProbs", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predictProb(inputs, outputs, outputProbs, flags));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(outputs), jsopencv_from(outputProbs));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predictProb");

    return NULL;
}



// Tables (ml_NormalBayesClassifier)

static PyGetSetDef pyopencv_ml_NormalBayesClassifier_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_NormalBayesClassifier_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_NormalBayesClassifier_create_static, METH_STATIC), "create() -> retval\n.   Creates empty model\n.   Use StatModel::train to train the model after creation."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_NormalBayesClassifier_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized NormalBayesClassifier from a file\n.        *\n.        * Use NormalBayesClassifier::save to serialize and store an NormalBayesClassifier to disk.\n.        * Load the NormalBayesClassifier from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized NormalBayesClassifier\n.        * @param nodeName name of node containing the classifier"},
    {"predictProb", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_NormalBayesClassifier_predictProb, 0), "predictProb(inputs[, outputs[, outputProbs[, flags]]]) -> retval, outputs, outputProbs\n.   @brief Predicts the response for sample(s).\n.   \n.       The method estimates the most probable classes for input vectors. Input vectors (one or more)\n.       are stored as rows of the matrix inputs. In case of multiple input vectors, there should be one\n.       output vector outputs. The predicted class for a single input vector is returned by the method.\n.       The vector outputProbs contains the output probabilities corresponding to each element of\n.       result."},

    {NULL,          NULL}
};

// Converter (ml_NormalBayesClassifier)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::NormalBayesClassifier> >
{
    static PyObject* from(const Ptr<cv::ml::NormalBayesClassifier>& r)
    {
        return pyopencv_ml_NormalBayesClassifier_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::NormalBayesClassifier>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::NormalBayesClassifier> * dst_;
        if (pyopencv_ml_NormalBayesClassifier_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::NormalBayesClassifier> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_ParamGrid (Generic)
//================================================================================

// GetSet (ml_ParamGrid)


static PyObject* pyopencv_ml_ParamGrid_get_logStep(pyopencv_ml_ParamGrid_t* p, void *closure)
{
    return jsopencv_from(p->v->logStep);
}

static int pyopencv_ml_ParamGrid_set_logStep(pyopencv_ml_ParamGrid_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the logStep attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->logStep, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ml_ParamGrid_get_maxVal(pyopencv_ml_ParamGrid_t* p, void *closure)
{
    return jsopencv_from(p->v->maxVal);
}

static int pyopencv_ml_ParamGrid_set_maxVal(pyopencv_ml_ParamGrid_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxVal attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->maxVal, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ml_ParamGrid_get_minVal(pyopencv_ml_ParamGrid_t* p, void *closure)
{
    return jsopencv_from(p->v->minVal);
}

static int pyopencv_ml_ParamGrid_set_minVal(pyopencv_ml_ParamGrid_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minVal attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v->minVal, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (ml_ParamGrid)

static Napi::Value pyopencv_cv_ml_ml_ParamGrid_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_minVal = NULL;
    double minVal=0.;
    Napi::Value* pyobj_maxVal = NULL;
    double maxVal=0.;
    Napi::Value* pyobj_logstep = NULL;
    double logstep=1.;
    Ptr<ParamGrid> retval;

    const char* keywords[] = { "minVal", "maxVal", "logstep", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:ml_ParamGrid.create", (char**)keywords, &pyobj_minVal, &pyobj_maxVal, &pyobj_logstep) &&
        jsopencv_to_safe(info, pyobj_minVal, minVal, ArgInfo("minVal", 0)) &&
        jsopencv_to_safe(info, pyobj_maxVal, maxVal, ArgInfo("maxVal", 0)) &&
        jsopencv_to_safe(info, pyobj_logstep, logstep, ArgInfo("logstep", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::ParamGrid::create(minVal, maxVal, logstep));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (ml_ParamGrid)

static PyGetSetDef pyopencv_ml_ParamGrid_getseters[] =
{
    {(char*)"logStep", (getter)pyopencv_ml_ParamGrid_get_logStep, (setter)pyopencv_ml_ParamGrid_set_logStep, (char*)"logStep", NULL},
    {(char*)"maxVal", (getter)pyopencv_ml_ParamGrid_get_maxVal, (setter)pyopencv_ml_ParamGrid_set_maxVal, (char*)"maxVal", NULL},
    {(char*)"minVal", (getter)pyopencv_ml_ParamGrid_get_minVal, (setter)pyopencv_ml_ParamGrid_set_minVal, (char*)"minVal", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_ParamGrid_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_ParamGrid_create_static, METH_STATIC), "create([, minVal[, maxVal[, logstep]]]) -> retval\n.   @brief Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method\n.   \n.       @param minVal minimum value of the parameter grid\n.       @param maxVal maximum value of the parameter grid\n.       @param logstep Logarithmic step for iterating the statmodel parameter"},

    {NULL,          NULL}
};

// Converter (ml_ParamGrid)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::ParamGrid> >
{
    static PyObject* from(const Ptr<cv::ml::ParamGrid>& r)
    {
        return pyopencv_ml_ParamGrid_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::ParamGrid>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::ParamGrid> * dst_;
        if (pyopencv_ml_ParamGrid_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::ParamGrid> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_RTrees (Generic)
//================================================================================

// GetSet (ml_RTrees)



// Methods (ml_RTrees)

static Napi::Value pyopencv_cv_ml_ml_RTrees_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<RTrees> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::RTrees::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_getActiveVarCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getActiveVarCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_getCalculateVarImportance(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCalculateVarImportance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_getOOBError(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getOOBError());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_getVarImportance(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarImportance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_getVotes(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_results = NULL;
    Mat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;

    const char* keywords[] = { "samples", "flags", "results", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ml_RTrees.getVotes", (char**)keywords, &pyobj_samples, &pyobj_flags, &pyobj_results) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getVotes(samples, results, flags));
        return jsopencv_from(results);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_results = NULL;
    UMat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;

    const char* keywords[] = { "samples", "flags", "results", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ml_RTrees.getVotes", (char**)keywords, &pyobj_samples, &pyobj_flags, &pyobj_results) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getVotes(samples, results, flags));
        return jsopencv_from(results);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getVotes");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<RTrees> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_RTrees.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::RTrees::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_setActiveVarCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_RTrees.setActiveVarCount", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setActiveVarCount(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_setCalculateVarImportance(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_RTrees.setCalculateVarImportance", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCalculateVarImportance(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_RTrees_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::RTrees> * self1 = 0;
    if (!pyopencv_ml_RTrees_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Ptr<cv::ml::RTrees> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_RTrees.setTermCriteria", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_RTrees)

static PyGetSetDef pyopencv_ml_RTrees_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_RTrees_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_create_static, METH_STATIC), "create() -> retval\n.   Creates the empty model.\n.       Use StatModel::train to train the model, StatModel::train to create and train the model,\n.       Algorithm::load to load the pre-trained model."},
    {"getActiveVarCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getActiveVarCount, 0), "getActiveVarCount() -> retval\n.   @see setActiveVarCount"},
    {"getCalculateVarImportance", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getCalculateVarImportance, 0), "getCalculateVarImportance() -> retval\n.   @see setCalculateVarImportance"},
    {"getOOBError", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getOOBError, 0), "getOOBError() -> retval\n.   Returns the OOB error value, computed at the training stage when calcOOBError is set to true.\n.        * If this flag was set to false, 0 is returned. The OOB error is also scaled by sample weighting."},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getVarImportance", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getVarImportance, 0), "getVarImportance() -> retval\n.   Returns the variable importance array.\n.       The method returns the variable importance vector, computed at the training stage when\n.       CalculateVarImportance is set to true. If this flag was set to false, the empty matrix is\n.       returned."},
    {"getVotes", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getVotes, 0), "getVotes(samples, flags[, results]) -> results\n.   Returns the result of each individual tree in the forest.\n.       In case the model is a regression problem, the method will return each of the trees'\n.       results for each of the sample cases. If the model is a classifier, it will return\n.       a Mat with samples + 1 rows, where the first row gives the class number and the\n.       following rows return the votes each class had for each sample.\n.           @param samples Array containing the samples for which votes will be calculated.\n.           @param results Array where the result of the calculation will be written.\n.           @param flags Flags for defining the type of RTrees."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized RTree from a file\n.        *\n.        * Use RTree::save to serialize and store an RTree to disk.\n.        * Load the RTree from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized RTree\n.        * @param nodeName name of node containing the classifier"},
    {"setActiveVarCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_setActiveVarCount, 0), "setActiveVarCount(val) -> None\n.   @copybrief getActiveVarCount @see getActiveVarCount"},
    {"setCalculateVarImportance", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_setCalculateVarImportance, 0), "setCalculateVarImportance(val) -> None\n.   @copybrief getCalculateVarImportance @see getCalculateVarImportance"},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},

    {NULL,          NULL}
};

// Converter (ml_RTrees)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::RTrees> >
{
    static PyObject* from(const Ptr<cv::ml::RTrees>& r)
    {
        return pyopencv_ml_RTrees_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::RTrees>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::RTrees> * dst_;
        if (pyopencv_ml_RTrees_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::RTrees> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_SVM (Generic)
//================================================================================

// GetSet (ml_SVM)



// Methods (ml_SVM)

static Napi::Value pyopencv_cv_ml_ml_SVM_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<SVM> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::SVM::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getC(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getC());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getClassWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getClassWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getCoef0(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCoef0());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getDecisionFunction(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_i = NULL;
    int i=0;
    Napi::Value* pyobj_alpha = NULL;
    Mat alpha;
    Napi::Value* pyobj_svidx = NULL;
    Mat svidx;
    double retval;

    const char* keywords[] = { "i", "alpha", "svidx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_SVM.getDecisionFunction", (char**)keywords, &pyobj_i, &pyobj_alpha, &pyobj_svidx) &&
        jsopencv_to_safe(info, pyobj_i, i, ArgInfo("i", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 1)) &&
        jsopencv_to_safe(info, pyobj_svidx, svidx, ArgInfo("svidx", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDecisionFunction(i, alpha, svidx));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(alpha), jsopencv_from(svidx));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_i = NULL;
    int i=0;
    Napi::Value* pyobj_alpha = NULL;
    UMat alpha;
    Napi::Value* pyobj_svidx = NULL;
    UMat svidx;
    double retval;

    const char* keywords[] = { "i", "alpha", "svidx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_SVM.getDecisionFunction", (char**)keywords, &pyobj_i, &pyobj_alpha, &pyobj_svidx) &&
        jsopencv_to_safe(info, pyobj_i, i, ArgInfo("i", 0)) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 1)) &&
        jsopencv_to_safe(info, pyobj_svidx, svidx, ArgInfo("svidx", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDecisionFunction(i, alpha, svidx));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(alpha), jsopencv_from(svidx));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getDecisionFunction");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getDefaultGridPtr_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_param_id = NULL;
    int param_id=0;
    Ptr<ParamGrid> retval;

    const char* keywords[] = { "param_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.getDefaultGridPtr", (char**)keywords, &pyobj_param_id) &&
        jsopencv_to_safe(info, pyobj_param_id, param_id, ArgInfo("param_id", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::SVM::getDefaultGridPtr(param_id));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getDegree(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDegree());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getGamma(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGamma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getKernelType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getKernelType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getNu(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNu());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getP(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getP());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getSupportVectors(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSupportVectors());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    cv::TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_getUncompressedSupportVectors(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUncompressedSupportVectors());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Ptr<SVM> retval;

    const char* keywords[] = { "filepath", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.load", (char**)keywords, &pyobj_filepath) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::SVM::load(filepath));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setC(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setC", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setC(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setClassWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setClassWeights", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setClassWeights(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setCoef0(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setCoef0", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCoef0(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setDegree(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setDegree", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDegree(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setGamma(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setGamma", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setKernel(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_kernelType = NULL;
    int kernelType=0;

    const char* keywords[] = { "kernelType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setKernel", (char**)keywords, &pyobj_kernelType) &&
        jsopencv_to_safe(info, pyobj_kernelType, kernelType, ArgInfo("kernelType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setKernel(kernelType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setNu(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setNu", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNu(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setP(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setP", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setP(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setTermCriteria", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_setType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    int val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVM.setType", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVM_trainAuto(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVM> * self1 = 0;
    if (!pyopencv_ml_SVM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Ptr<cv::ml::SVM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Napi::Value* pyobj_responses = NULL;
    Mat responses;
    Napi::Value* pyobj_kFold = NULL;
    int kFold=10;
    Napi::Value* pyobj_Cgrid = NULL;
    Ptr<ParamGrid> Cgrid=SVM::getDefaultGridPtr(SVM::C);
    Napi::Value* pyobj_gammaGrid = NULL;
    Ptr<ParamGrid> gammaGrid=SVM::getDefaultGridPtr(SVM::GAMMA);
    Napi::Value* pyobj_pGrid = NULL;
    Ptr<ParamGrid> pGrid=SVM::getDefaultGridPtr(SVM::P);
    Napi::Value* pyobj_nuGrid = NULL;
    Ptr<ParamGrid> nuGrid=SVM::getDefaultGridPtr(SVM::NU);
    Napi::Value* pyobj_coeffGrid = NULL;
    Ptr<ParamGrid> coeffGrid=SVM::getDefaultGridPtr(SVM::COEF);
    Napi::Value* pyobj_degreeGrid = NULL;
    Ptr<ParamGrid> degreeGrid=SVM::getDefaultGridPtr(SVM::DEGREE);
    Napi::Value* pyobj_balanced = NULL;
    bool balanced=false;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", "kFold", "Cgrid", "gammaGrid", "pGrid", "nuGrid", "coeffGrid", "degreeGrid", "balanced", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOOOOOOO:ml_SVM.trainAuto", (char**)keywords, &pyobj_samples, &pyobj_layout, &pyobj_responses, &pyobj_kFold, &pyobj_Cgrid, &pyobj_gammaGrid, &pyobj_pGrid, &pyobj_nuGrid, &pyobj_coeffGrid, &pyobj_degreeGrid, &pyobj_balanced) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_responses, responses, ArgInfo("responses", 0)) &&
        jsopencv_to_safe(info, pyobj_kFold, kFold, ArgInfo("kFold", 0)) &&
        jsopencv_to_safe(info, pyobj_Cgrid, Cgrid, ArgInfo("Cgrid", 0)) &&
        jsopencv_to_safe(info, pyobj_gammaGrid, gammaGrid, ArgInfo("gammaGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_pGrid, pGrid, ArgInfo("pGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_nuGrid, nuGrid, ArgInfo("nuGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_coeffGrid, coeffGrid, ArgInfo("coeffGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_degreeGrid, degreeGrid, ArgInfo("degreeGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_balanced, balanced, ArgInfo("balanced", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainAuto(samples, layout, responses, kFold, Cgrid, gammaGrid, pGrid, nuGrid, coeffGrid, degreeGrid, balanced));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Napi::Value* pyobj_responses = NULL;
    UMat responses;
    Napi::Value* pyobj_kFold = NULL;
    int kFold=10;
    Napi::Value* pyobj_Cgrid = NULL;
    Ptr<ParamGrid> Cgrid=SVM::getDefaultGridPtr(SVM::C);
    Napi::Value* pyobj_gammaGrid = NULL;
    Ptr<ParamGrid> gammaGrid=SVM::getDefaultGridPtr(SVM::GAMMA);
    Napi::Value* pyobj_pGrid = NULL;
    Ptr<ParamGrid> pGrid=SVM::getDefaultGridPtr(SVM::P);
    Napi::Value* pyobj_nuGrid = NULL;
    Ptr<ParamGrid> nuGrid=SVM::getDefaultGridPtr(SVM::NU);
    Napi::Value* pyobj_coeffGrid = NULL;
    Ptr<ParamGrid> coeffGrid=SVM::getDefaultGridPtr(SVM::COEF);
    Napi::Value* pyobj_degreeGrid = NULL;
    Ptr<ParamGrid> degreeGrid=SVM::getDefaultGridPtr(SVM::DEGREE);
    Napi::Value* pyobj_balanced = NULL;
    bool balanced=false;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", "kFold", "Cgrid", "gammaGrid", "pGrid", "nuGrid", "coeffGrid", "degreeGrid", "balanced", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOOOOOOO:ml_SVM.trainAuto", (char**)keywords, &pyobj_samples, &pyobj_layout, &pyobj_responses, &pyobj_kFold, &pyobj_Cgrid, &pyobj_gammaGrid, &pyobj_pGrid, &pyobj_nuGrid, &pyobj_coeffGrid, &pyobj_degreeGrid, &pyobj_balanced) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_responses, responses, ArgInfo("responses", 0)) &&
        jsopencv_to_safe(info, pyobj_kFold, kFold, ArgInfo("kFold", 0)) &&
        jsopencv_to_safe(info, pyobj_Cgrid, Cgrid, ArgInfo("Cgrid", 0)) &&
        jsopencv_to_safe(info, pyobj_gammaGrid, gammaGrid, ArgInfo("gammaGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_pGrid, pGrid, ArgInfo("pGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_nuGrid, nuGrid, ArgInfo("nuGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_coeffGrid, coeffGrid, ArgInfo("coeffGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_degreeGrid, degreeGrid, ArgInfo("degreeGrid", 0)) &&
        jsopencv_to_safe(info, pyobj_balanced, balanced, ArgInfo("balanced", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->trainAuto(samples, layout, responses, kFold, Cgrid, gammaGrid, pGrid, nuGrid, coeffGrid, degreeGrid, balanced));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("trainAuto");

    return NULL;
}



// Tables (ml_SVM)

static PyGetSetDef pyopencv_ml_SVM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_SVM_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_create_static, METH_STATIC), "create() -> retval\n.   Creates empty model.\n.       Use StatModel::train to train the model. Since %SVM has several parameters, you may want to\n.   find the best parameters for your problem, it can be done with SVM::trainAuto."},
    {"getC", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getC, 0), "getC() -> retval\n.   @see setC"},
    {"getClassWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getClassWeights, 0), "getClassWeights() -> retval\n.   @see setClassWeights"},
    {"getCoef0", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getCoef0, 0), "getCoef0() -> retval\n.   @see setCoef0"},
    {"getDecisionFunction", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getDecisionFunction, 0), "getDecisionFunction(i[, alpha[, svidx]]) -> retval, alpha, svidx\n.   @brief Retrieves the decision function\n.   \n.       @param i the index of the decision function. If the problem solved is regression, 1-class or\n.           2-class classification, then there will be just one decision function and the index should\n.           always be 0. Otherwise, in the case of N-class classification, there will be \\f$N(N-1)/2\\f$\n.           decision functions.\n.       @param alpha the optional output vector for weights, corresponding to different support vectors.\n.           In the case of linear %SVM all the alpha's will be 1's.\n.       @param svidx the optional output vector of indices of support vectors within the matrix of\n.           support vectors (which can be retrieved by SVM::getSupportVectors). In the case of linear\n.           %SVM each decision function consists of a single \"compressed\" support vector.\n.   \n.       The method returns rho parameter of the decision function, a scalar subtracted from the weighted\n.       sum of kernel responses."},
    {"getDefaultGridPtr", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getDefaultGridPtr_static, METH_STATIC), "getDefaultGridPtr(param_id) -> retval\n.   @brief Generates a grid for %SVM parameters.\n.   \n.       @param param_id %SVM parameters IDs that must be one of the SVM::ParamTypes. The grid is\n.       generated for the parameter with this ID.\n.   \n.       The function generates a grid pointer for the specified parameter of the %SVM algorithm.\n.       The grid may be passed to the function SVM::trainAuto."},
    {"getDegree", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getDegree, 0), "getDegree() -> retval\n.   @see setDegree"},
    {"getGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getGamma, 0), "getGamma() -> retval\n.   @see setGamma"},
    {"getKernelType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getKernelType, 0), "getKernelType() -> retval\n.   Type of a %SVM kernel.\n.   See SVM::KernelTypes. Default value is SVM::RBF."},
    {"getNu", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getNu, 0), "getNu() -> retval\n.   @see setNu"},
    {"getP", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getP, 0), "getP() -> retval\n.   @see setP"},
    {"getSupportVectors", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getSupportVectors, 0), "getSupportVectors() -> retval\n.   @brief Retrieves all the support vectors\n.   \n.       The method returns all the support vectors as a floating-point matrix, where support vectors are\n.       stored as matrix rows."},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getType, 0), "getType() -> retval\n.   @see setType"},
    {"getUncompressedSupportVectors", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getUncompressedSupportVectors, 0), "getUncompressedSupportVectors() -> retval\n.   @brief Retrieves all the uncompressed support vectors of a linear %SVM\n.   \n.       The method returns all the uncompressed support vectors of a linear %SVM that the compressed\n.       support vector, used for prediction, was derived from. They are returned in a floating-point\n.       matrix, where the support vectors are stored as matrix rows."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_load_static, METH_STATIC), "load(filepath) -> retval\n.   @brief Loads and creates a serialized svm from a file\n.        *\n.        * Use SVM::save to serialize and store an SVM to disk.\n.        * Load the SVM from this file again, by calling this function with the path to the file.\n.        *\n.        * @param filepath path to serialized svm"},
    {"setC", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setC, 0), "setC(val) -> None\n.   @copybrief getC @see getC"},
    {"setClassWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setClassWeights, 0), "setClassWeights(val) -> None\n.   @copybrief getClassWeights @see getClassWeights"},
    {"setCoef0", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setCoef0, 0), "setCoef0(val) -> None\n.   @copybrief getCoef0 @see getCoef0"},
    {"setDegree", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setDegree, 0), "setDegree(val) -> None\n.   @copybrief getDegree @see getDegree"},
    {"setGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setGamma, 0), "setGamma(val) -> None\n.   @copybrief getGamma @see getGamma"},
    {"setKernel", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setKernel, 0), "setKernel(kernelType) -> None\n.   Initialize with one of predefined kernels.\n.   See SVM::KernelTypes."},
    {"setNu", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setNu, 0), "setNu(val) -> None\n.   @copybrief getNu @see getNu"},
    {"setP", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setP, 0), "setP(val) -> None\n.   @copybrief getP @see getP"},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"setType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setType, 0), "setType(val) -> None\n.   @copybrief getType @see getType"},
    {"trainAuto", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_trainAuto, 0), "trainAuto(samples, layout, responses[, kFold[, Cgrid[, gammaGrid[, pGrid[, nuGrid[, coeffGrid[, degreeGrid[, balanced]]]]]]]]) -> retval\n.   @brief Trains an %SVM with optimal parameters\n.   \n.       @param samples training samples\n.       @param layout See ml::SampleTypes.\n.       @param responses vector of responses associated with the training samples.\n.       @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One\n.           subset is used to test the model, the others form the train set. So, the %SVM algorithm is\n.       @param Cgrid grid for C\n.       @param gammaGrid grid for gamma\n.       @param pGrid grid for p\n.       @param nuGrid grid for nu\n.       @param coeffGrid grid for coeff\n.       @param degreeGrid grid for degree\n.       @param balanced If true and the problem is 2-class classification then the method creates more\n.           balanced cross-validation subsets that is proportions between classes in subsets are close\n.           to such proportion in the whole train dataset.\n.   \n.       The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,\n.       nu, coef0, degree. Parameters are considered optimal when the cross-validation\n.       estimate of the test set error is minimal.\n.   \n.       This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only\n.       offers rudimentary parameter options.\n.   \n.       This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the\n.       regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and\n.       the usual %SVM with parameters specified in params is executed."},

    {NULL,          NULL}
};

// Converter (ml_SVM)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::SVM> >
{
    static PyObject* from(const Ptr<cv::ml::SVM>& r)
    {
        return pyopencv_ml_SVM_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::SVM>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::SVM> * dst_;
        if (pyopencv_ml_SVM_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::SVM> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_SVMSGD (Generic)
//================================================================================

// GetSet (ml_SVMSGD)



// Methods (ml_SVMSGD)

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Ptr<SVMSGD> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::SVMSGD::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getInitialStepSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getInitialStepSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getMarginRegularization(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMarginRegularization());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getMarginType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMarginType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getShift(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getShift());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getStepDecreasingPower(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getStepDecreasingPower());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getSvmsgdType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSvmsgdType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    TermCriteria retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTermCriteria());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_getWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_load_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_filepath = NULL;
    String filepath;
    Napi::Value* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<SVMSGD> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_SVMSGD.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        jsopencv_to_safe(info, pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        jsopencv_to_safe(info, pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::SVMSGD::load(filepath, nodeName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setInitialStepSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_InitialStepSize = NULL;
    float InitialStepSize=0.f;

    const char* keywords[] = { "InitialStepSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVMSGD.setInitialStepSize", (char**)keywords, &pyobj_InitialStepSize) &&
        jsopencv_to_safe(info, pyobj_InitialStepSize, InitialStepSize, ArgInfo("InitialStepSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitialStepSize(InitialStepSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setMarginRegularization(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_marginRegularization = NULL;
    float marginRegularization=0.f;

    const char* keywords[] = { "marginRegularization", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVMSGD.setMarginRegularization", (char**)keywords, &pyobj_marginRegularization) &&
        jsopencv_to_safe(info, pyobj_marginRegularization, marginRegularization, ArgInfo("marginRegularization", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMarginRegularization(marginRegularization));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setMarginType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_marginType = NULL;
    int marginType=0;

    const char* keywords[] = { "marginType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVMSGD.setMarginType", (char**)keywords, &pyobj_marginType) &&
        jsopencv_to_safe(info, pyobj_marginType, marginType, ArgInfo("marginType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMarginType(marginType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setOptimalParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_svmsgdType = NULL;
    int svmsgdType=SVMSGD::ASGD;
    Napi::Value* pyobj_marginType = NULL;
    int marginType=SVMSGD::SOFT_MARGIN;

    const char* keywords[] = { "svmsgdType", "marginType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ml_SVMSGD.setOptimalParameters", (char**)keywords, &pyobj_svmsgdType, &pyobj_marginType) &&
        jsopencv_to_safe(info, pyobj_svmsgdType, svmsgdType, ArgInfo("svmsgdType", 0)) &&
        jsopencv_to_safe(info, pyobj_marginType, marginType, ArgInfo("marginType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setOptimalParameters(svmsgdType, marginType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setStepDecreasingPower(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_stepDecreasingPower = NULL;
    float stepDecreasingPower=0.f;

    const char* keywords[] = { "stepDecreasingPower", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVMSGD.setStepDecreasingPower", (char**)keywords, &pyobj_stepDecreasingPower) &&
        jsopencv_to_safe(info, pyobj_stepDecreasingPower, stepDecreasingPower, ArgInfo("stepDecreasingPower", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setStepDecreasingPower(stepDecreasingPower));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setSvmsgdType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_svmsgdType = NULL;
    int svmsgdType=0;

    const char* keywords[] = { "svmsgdType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVMSGD.setSvmsgdType", (char**)keywords, &pyobj_svmsgdType) &&
        jsopencv_to_safe(info, pyobj_svmsgdType, svmsgdType, ArgInfo("svmsgdType", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSvmsgdType(svmsgdType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_SVMSGD_setTermCriteria(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::SVMSGD> * self1 = 0;
    if (!pyopencv_ml_SVMSGD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Ptr<cv::ml::SVMSGD> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_SVMSGD.setTermCriteria", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_SVMSGD)

static PyGetSetDef pyopencv_ml_SVMSGD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_SVMSGD_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_create_static, METH_STATIC), "create() -> retval\n.   @brief Creates empty model.\n.        * Use StatModel::train to train the model. Since %SVMSGD has several parameters, you may want to\n.        * find the best parameters for your problem or use setOptimalParameters() to set some default parameters."},
    {"getInitialStepSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getInitialStepSize, 0), "getInitialStepSize() -> retval\n.   @see setInitialStepSize"},
    {"getMarginRegularization", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getMarginRegularization, 0), "getMarginRegularization() -> retval\n.   @see setMarginRegularization"},
    {"getMarginType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getMarginType, 0), "getMarginType() -> retval\n.   @see setMarginType"},
    {"getShift", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getShift, 0), "getShift() -> retval\n.   * @return the shift of the trained model (decision function f(x) = weights * x + shift)."},
    {"getStepDecreasingPower", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getStepDecreasingPower, 0), "getStepDecreasingPower() -> retval\n.   @see setStepDecreasingPower"},
    {"getSvmsgdType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getSvmsgdType, 0), "getSvmsgdType() -> retval\n.   @see setSvmsgdType"},
    {"getTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getWeights, 0), "getWeights() -> retval\n.   * @return the weights of the trained model (decision function f(x) = weights * x + shift)."},
    {"load", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_load_static, METH_STATIC), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized SVMSGD from a file\n.        *\n.        * Use SVMSGD::save to serialize and store an SVMSGD to disk.\n.        * Load the SVMSGD from this file again, by calling this function with the path to the file.\n.        * Optionally specify the node for the file containing the classifier\n.        *\n.        * @param filepath path to serialized SVMSGD\n.        * @param nodeName name of node containing the classifier"},
    {"setInitialStepSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setInitialStepSize, 0), "setInitialStepSize(InitialStepSize) -> None\n.   @copybrief getInitialStepSize @see getInitialStepSize"},
    {"setMarginRegularization", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setMarginRegularization, 0), "setMarginRegularization(marginRegularization) -> None\n.   @copybrief getMarginRegularization @see getMarginRegularization"},
    {"setMarginType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setMarginType, 0), "setMarginType(marginType) -> None\n.   @copybrief getMarginType @see getMarginType"},
    {"setOptimalParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setOptimalParameters, 0), "setOptimalParameters([, svmsgdType[, marginType]]) -> None\n.   @brief Function sets optimal parameters values for chosen SVM SGD model.\n.        * @param svmsgdType is the type of SVMSGD classifier.\n.        * @param marginType is the type of margin constraint."},
    {"setStepDecreasingPower", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setStepDecreasingPower, 0), "setStepDecreasingPower(stepDecreasingPower) -> None\n.   @copybrief getStepDecreasingPower @see getStepDecreasingPower"},
    {"setSvmsgdType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setSvmsgdType, 0), "setSvmsgdType(svmsgdType) -> None\n.   @copybrief getSvmsgdType @see getSvmsgdType"},
    {"setTermCriteria", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},

    {NULL,          NULL}
};

// Converter (ml_SVMSGD)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::SVMSGD> >
{
    static PyObject* from(const Ptr<cv::ml::SVMSGD>& r)
    {
        return pyopencv_ml_SVMSGD_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::SVMSGD>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::SVMSGD> * dst_;
        if (pyopencv_ml_SVMSGD_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::SVMSGD> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_StatModel (Generic)
//================================================================================

// GetSet (ml_StatModel)



// Methods (ml_StatModel)

static Napi::Value pyopencv_cv_ml_ml_StatModel_calcError(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_data = NULL;
    Ptr<TrainData> data;
    Napi::Value* pyobj_test = NULL;
    bool test=0;
    Napi::Value* pyobj_resp = NULL;
    Mat resp;
    float retval;

    const char* keywords[] = { "data", "test", "resp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ml_StatModel.calcError", (char**)keywords, &pyobj_data, &pyobj_test, &pyobj_resp) &&
        jsopencv_to_safe(info, pyobj_data, data, ArgInfo("data", 0)) &&
        jsopencv_to_safe(info, pyobj_test, test, ArgInfo("test", 0)) &&
        jsopencv_to_safe(info, pyobj_resp, resp, ArgInfo("resp", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->calcError(data, test, resp));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(resp));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_data = NULL;
    Ptr<TrainData> data;
    Napi::Value* pyobj_test = NULL;
    bool test=0;
    Napi::Value* pyobj_resp = NULL;
    UMat resp;
    float retval;

    const char* keywords[] = { "data", "test", "resp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ml_StatModel.calcError", (char**)keywords, &pyobj_data, &pyobj_test, &pyobj_resp) &&
        jsopencv_to_safe(info, pyobj_data, data, ArgInfo("data", 0)) &&
        jsopencv_to_safe(info, pyobj_test, test, ArgInfo("test", 0)) &&
        jsopencv_to_safe(info, pyobj_resp, resp, ArgInfo("resp", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->calcError(data, test, resp));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(resp));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("calcError");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_StatModel_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_StatModel_getVarCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_StatModel_isClassifier(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isClassifier());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_StatModel_isTrained(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isTrained());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_StatModel_predict(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_results = NULL;
    Mat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_StatModel.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(results));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_results = NULL;
    UMat results;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ml_StatModel.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_results, results, ArgInfo("results", 1)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(results));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("predict");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_StatModel_train(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::StatModel> * self1 = 0;
    if (!pyopencv_ml_StatModel_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    Ptr<cv::ml::StatModel> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_trainData = NULL;
    Ptr<TrainData> trainData;
    Napi::Value* pyobj_flags = NULL;
    int flags=0;
    bool retval;

    const char* keywords[] = { "trainData", "flags", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_StatModel.train", (char**)keywords, &pyobj_trainData, &pyobj_flags) &&
        jsopencv_to_safe(info, pyobj_trainData, trainData, ArgInfo("trainData", 0)) &&
        jsopencv_to_safe(info, pyobj_flags, flags, ArgInfo("flags", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->train(trainData, flags));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Napi::Value* pyobj_responses = NULL;
    Mat responses;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_StatModel.train", (char**)keywords, &pyobj_samples, &pyobj_layout, &pyobj_responses) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_responses, responses, ArgInfo("responses", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->train(samples, layout, responses));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Napi::Value* pyobj_responses = NULL;
    UMat responses;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_StatModel.train", (char**)keywords, &pyobj_samples, &pyobj_layout, &pyobj_responses) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_responses, responses, ArgInfo("responses", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->train(samples, layout, responses));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("train");

    return NULL;
}



// Tables (ml_StatModel)

static PyGetSetDef pyopencv_ml_StatModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_StatModel_methods[] =
{
    {"calcError", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_calcError, 0), "calcError(data, test[, resp]) -> retval, resp\n.   @brief Computes error on the training or test dataset\n.   \n.       @param data the training data\n.       @param test if true, the error is computed over the test subset of the data, otherwise it's\n.           computed over the training subset of the data. Please note that if you loaded a completely\n.           different dataset to evaluate already trained classifier, you will probably want not to set\n.           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n.           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n.       @param resp the optional output responses.\n.   \n.       The method uses StatModel::predict to compute the error. For regression models the error is\n.       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%)."},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_empty, 0), "empty() -> retval\n."},
    {"getVarCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_getVarCount, 0), "getVarCount() -> retval\n.   @brief Returns the number of variables in training samples"},
    {"isClassifier", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_isClassifier, 0), "isClassifier() -> retval\n.   @brief Returns true if the model is classifier"},
    {"isTrained", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_isTrained, 0), "isTrained() -> retval\n.   @brief Returns true if the model is trained"},
    {"predict", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_predict, 0), "predict(samples[, results[, flags]]) -> retval, results\n.   @brief Predicts response(s) for the provided sample(s)\n.   \n.       @param samples The input samples, floating-point matrix\n.       @param results The optional output matrix of results.\n.       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags."},
    {"train", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_train, 0), "train(trainData[, flags]) -> retval\n.   @brief Trains the statistical model\n.   \n.       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n.           created with TrainData::create.\n.       @param flags optional flags, depending on the model. Some of the models can be updated with the\n.           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n\n\n\ntrain(samples, layout, responses) -> retval\n.   @brief Trains the statistical model\n.   \n.       @param samples training samples\n.       @param layout See ml::SampleTypes.\n.       @param responses vector of responses associated with the training samples."},

    {NULL,          NULL}
};

// Converter (ml_StatModel)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::StatModel> >
{
    static PyObject* from(const Ptr<cv::ml::StatModel>& r)
    {
        return pyopencv_ml_StatModel_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::StatModel>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::StatModel> * dst_;
        if (pyopencv_ml_StatModel_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::StatModel> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ml_TrainData (Generic)
//================================================================================

// GetSet (ml_TrainData)



// Methods (ml_TrainData)

static Napi::Value pyopencv_cv_ml_ml_TrainData_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_samples = NULL;
    Mat samples;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Napi::Value* pyobj_responses = NULL;
    Mat responses;
    Napi::Value* pyobj_varIdx = NULL;
    Mat varIdx;
    Napi::Value* pyobj_sampleIdx = NULL;
    Mat sampleIdx;
    Napi::Value* pyobj_sampleWeights = NULL;
    Mat sampleWeights;
    Napi::Value* pyobj_varType = NULL;
    Mat varType;
    Ptr<TrainData> retval;

    const char* keywords[] = { "samples", "layout", "responses", "varIdx", "sampleIdx", "sampleWeights", "varType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOOO:ml_TrainData.create", (char**)keywords, &pyobj_samples, &pyobj_layout, &pyobj_responses, &pyobj_varIdx, &pyobj_sampleIdx, &pyobj_sampleWeights, &pyobj_varType) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_responses, responses, ArgInfo("responses", 0)) &&
        jsopencv_to_safe(info, pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) &&
        jsopencv_to_safe(info, pyobj_sampleIdx, sampleIdx, ArgInfo("sampleIdx", 0)) &&
        jsopencv_to_safe(info, pyobj_sampleWeights, sampleWeights, ArgInfo("sampleWeights", 0)) &&
        jsopencv_to_safe(info, pyobj_varType, varType, ArgInfo("varType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::TrainData::create(samples, layout, responses, varIdx, sampleIdx, sampleWeights, varType));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_samples = NULL;
    UMat samples;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Napi::Value* pyobj_responses = NULL;
    UMat responses;
    Napi::Value* pyobj_varIdx = NULL;
    UMat varIdx;
    Napi::Value* pyobj_sampleIdx = NULL;
    UMat sampleIdx;
    Napi::Value* pyobj_sampleWeights = NULL;
    UMat sampleWeights;
    Napi::Value* pyobj_varType = NULL;
    UMat varType;
    Ptr<TrainData> retval;

    const char* keywords[] = { "samples", "layout", "responses", "varIdx", "sampleIdx", "sampleWeights", "varType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|OOOO:ml_TrainData.create", (char**)keywords, &pyobj_samples, &pyobj_layout, &pyobj_responses, &pyobj_varIdx, &pyobj_sampleIdx, &pyobj_sampleWeights, &pyobj_varType) &&
        jsopencv_to_safe(info, pyobj_samples, samples, ArgInfo("samples", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_responses, responses, ArgInfo("responses", 0)) &&
        jsopencv_to_safe(info, pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) &&
        jsopencv_to_safe(info, pyobj_sampleIdx, sampleIdx, ArgInfo("sampleIdx", 0)) &&
        jsopencv_to_safe(info, pyobj_sampleWeights, sampleWeights, ArgInfo("sampleWeights", 0)) &&
        jsopencv_to_safe(info, pyobj_varType, varType, ArgInfo("varType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::TrainData::create(samples, layout, responses, varIdx, sampleIdx, sampleWeights, varType));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getCatCount(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Napi::Value* pyobj_vi = NULL;
    int vi=0;
    int retval;

    const char* keywords[] = { "vi", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_TrainData.getCatCount", (char**)keywords, &pyobj_vi) &&
        jsopencv_to_safe(info, pyobj_vi, vi, ArgInfo("vi", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCatCount(vi));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getCatMap(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCatMap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getCatOfs(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCatOfs());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getClassLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getClassLabels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getDefaultSubstValues(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultSubstValues());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getLayout(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLayout());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getMissing(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMissing());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNAllVars(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNAllVars());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNSamples(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNTestSamples(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNTestSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNTrainSamples(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNTrainSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNVars(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNVars());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNames(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Napi::Value* pyobj_names = NULL;
    vector_String names;

    const char* keywords[] = { "names", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ml_TrainData.getNames", (char**)keywords, &pyobj_names) &&
        jsopencv_to_safe(info, pyobj_names, names, ArgInfo("names", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getNames(names));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getNormCatResponses(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNormCatResponses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getResponseType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getResponseType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getResponses(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getResponses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getSample(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_varIdx = NULL;
    Mat varIdx;
    Napi::Value* pyobj_sidx = NULL;
    int sidx=0;
    Napi::Value* pyobj_buf = NULL;
    float buf=0.f;

    const char* keywords[] = { "varIdx", "sidx", "buf", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_TrainData.getSample", (char**)keywords, &pyobj_varIdx, &pyobj_sidx, &pyobj_buf) &&
        jsopencv_to_safe(info, pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) &&
        jsopencv_to_safe(info, pyobj_sidx, sidx, ArgInfo("sidx", 0)) &&
        jsopencv_to_safe(info, pyobj_buf, buf, ArgInfo("buf", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getSample(varIdx, sidx, &buf));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_varIdx = NULL;
    UMat varIdx;
    Napi::Value* pyobj_sidx = NULL;
    int sidx=0;
    Napi::Value* pyobj_buf = NULL;
    float buf=0.f;

    const char* keywords[] = { "varIdx", "sidx", "buf", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_TrainData.getSample", (char**)keywords, &pyobj_varIdx, &pyobj_sidx, &pyobj_buf) &&
        jsopencv_to_safe(info, pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) &&
        jsopencv_to_safe(info, pyobj_sidx, sidx, ArgInfo("sidx", 0)) &&
        jsopencv_to_safe(info, pyobj_buf, buf, ArgInfo("buf", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getSample(varIdx, sidx, &buf));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getSample");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getSampleWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSampleWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getSamples(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getSubMatrix_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_matrix = NULL;
    Mat matrix;
    Napi::Value* pyobj_idx = NULL;
    Mat idx;
    Napi::Value* pyobj_layout = NULL;
    int layout=0;
    Mat retval;

    const char* keywords[] = { "matrix", "idx", "layout", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_TrainData.getSubMatrix", (char**)keywords, &pyobj_matrix, &pyobj_idx, &pyobj_layout) &&
        jsopencv_to_safe(info, pyobj_matrix, matrix, ArgInfo("matrix", 0)) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::TrainData::getSubMatrix(matrix, idx, layout));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getSubVector_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;

    Napi::Value* pyobj_vec = NULL;
    Mat vec;
    Napi::Value* pyobj_idx = NULL;
    Mat idx;
    Mat retval;

    const char* keywords[] = { "vec", "idx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:ml_TrainData.getSubVector", (char**)keywords, &pyobj_vec, &pyobj_idx) &&
        jsopencv_to_safe(info, pyobj_vec, vec, ArgInfo("vec", 0)) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ml::TrainData::getSubVector(vec, idx));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTestNormCatResponses(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTestNormCatResponses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTestResponses(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTestResponses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTestSampleIdx(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTestSampleIdx());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTestSampleWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTestSampleWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTestSamples(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTestSamples());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTrainNormCatResponses(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainNormCatResponses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTrainResponses(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainResponses());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTrainSampleIdx(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainSampleIdx());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTrainSampleWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainSampleWeights());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getTrainSamples(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Napi::Value* pyobj_layout = NULL;
    int layout=ROW_SAMPLE;
    Napi::Value* pyobj_compressSamples = NULL;
    bool compressSamples=true;
    Napi::Value* pyobj_compressVars = NULL;
    bool compressVars=true;
    Mat retval;

    const char* keywords[] = { "layout", "compressSamples", "compressVars", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:ml_TrainData.getTrainSamples", (char**)keywords, &pyobj_layout, &pyobj_compressSamples, &pyobj_compressVars) &&
        jsopencv_to_safe(info, pyobj_layout, layout, ArgInfo("layout", 0)) &&
        jsopencv_to_safe(info, pyobj_compressSamples, compressSamples, ArgInfo("compressSamples", 0)) &&
        jsopencv_to_safe(info, pyobj_compressVars, compressVars, ArgInfo("compressVars", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getTrainSamples(layout, compressSamples, compressVars));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getValues(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_vi = NULL;
    int vi=0;
    Napi::Value* pyobj_sidx = NULL;
    Mat sidx;
    Napi::Value* pyobj_values = NULL;
    float values=0.f;

    const char* keywords[] = { "vi", "sidx", "values", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_TrainData.getValues", (char**)keywords, &pyobj_vi, &pyobj_sidx, &pyobj_values) &&
        jsopencv_to_safe(info, pyobj_vi, vi, ArgInfo("vi", 0)) &&
        jsopencv_to_safe(info, pyobj_sidx, sidx, ArgInfo("sidx", 0)) &&
        jsopencv_to_safe(info, pyobj_values, values, ArgInfo("values", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getValues(vi, sidx, &values));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_vi = NULL;
    int vi=0;
    Napi::Value* pyobj_sidx = NULL;
    UMat sidx;
    Napi::Value* pyobj_values = NULL;
    float values=0.f;

    const char* keywords[] = { "vi", "sidx", "values", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:ml_TrainData.getValues", (char**)keywords, &pyobj_vi, &pyobj_sidx, &pyobj_values) &&
        jsopencv_to_safe(info, pyobj_vi, vi, ArgInfo("vi", 0)) &&
        jsopencv_to_safe(info, pyobj_sidx, sidx, ArgInfo("sidx", 0)) &&
        jsopencv_to_safe(info, pyobj_values, values, ArgInfo("values", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getValues(vi, sidx, &values));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getValues");

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getVarIdx(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarIdx());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getVarSymbolFlags(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarSymbolFlags());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_getVarType(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getVarType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_setTrainTestSplit(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Napi::Value* pyobj_count = NULL;
    int count=0;
    Napi::Value* pyobj_shuffle = NULL;
    bool shuffle=true;

    const char* keywords[] = { "count", "shuffle", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_TrainData.setTrainTestSplit", (char**)keywords, &pyobj_count, &pyobj_shuffle) &&
        jsopencv_to_safe(info, pyobj_count, count, ArgInfo("count", 0)) &&
        jsopencv_to_safe(info, pyobj_shuffle, shuffle, ArgInfo("shuffle", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTrainTestSplit(count, shuffle));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_setTrainTestSplitRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);
    Napi::Value* pyobj_ratio = NULL;
    double ratio=0;
    Napi::Value* pyobj_shuffle = NULL;
    bool shuffle=true;

    const char* keywords[] = { "ratio", "shuffle", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ml_TrainData.setTrainTestSplitRatio", (char**)keywords, &pyobj_ratio, &pyobj_shuffle) &&
        jsopencv_to_safe(info, pyobj_ratio, ratio, ArgInfo("ratio", 0)) &&
        jsopencv_to_safe(info, pyobj_shuffle, shuffle, ArgInfo("shuffle", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTrainTestSplitRatio(ratio, shuffle));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ml_ml_TrainData_shuffleTrainTest(const Napi::CallbackInfo &info)
{
    using namespace cv::ml;


    Ptr<cv::ml::TrainData> * self1 = 0;
    if (!pyopencv_ml_TrainData_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Ptr<cv::ml::TrainData> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->shuffleTrainTest());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ml_TrainData)

static PyGetSetDef pyopencv_ml_TrainData_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ml_TrainData_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_create_static, METH_STATIC), "create(samples, layout, responses[, varIdx[, sampleIdx[, sampleWeights[, varType]]]]) -> retval\n.   @brief Creates training data from in-memory arrays.\n.   \n.       @param samples matrix of samples. It should have CV_32F type.\n.       @param layout see ml::SampleTypes.\n.       @param responses matrix of responses. If the responses are scalar, they should be stored as a\n.           single row or as a single column. The matrix should have type CV_32F or CV_32S (in the\n.           former case the responses are considered as ordered by default; in the latter case - as\n.           categorical)\n.       @param varIdx vector specifying which variables to use for training. It can be an integer vector\n.           (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of\n.           active variables.\n.       @param sampleIdx vector specifying which samples to use for training. It can be an integer\n.           vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask\n.           of training samples.\n.       @param sampleWeights optional vector with weights for each sample. It should have CV_32F type.\n.       @param varType optional vector of type CV_8U and size `<number_of_variables_in_samples> +\n.           <number_of_variables_in_responses>`, containing types of each input and output variable. See\n.           ml::VariableTypes."},
    {"getCatCount", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getCatCount, 0), "getCatCount(vi) -> retval\n."},
    {"getCatMap", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getCatMap, 0), "getCatMap() -> retval\n."},
    {"getCatOfs", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getCatOfs, 0), "getCatOfs() -> retval\n."},
    {"getClassLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getClassLabels, 0), "getClassLabels() -> retval\n.   @brief Returns the vector of class labels\n.   \n.       The function returns vector of unique labels occurred in the responses."},
    {"getDefaultSubstValues", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getDefaultSubstValues, 0), "getDefaultSubstValues() -> retval\n."},
    {"getLayout", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getLayout, 0), "getLayout() -> retval\n."},
    {"getMissing", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getMissing, 0), "getMissing() -> retval\n."},
    {"getNAllVars", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNAllVars, 0), "getNAllVars() -> retval\n."},
    {"getNSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNSamples, 0), "getNSamples() -> retval\n."},
    {"getNTestSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNTestSamples, 0), "getNTestSamples() -> retval\n."},
    {"getNTrainSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNTrainSamples, 0), "getNTrainSamples() -> retval\n."},
    {"getNVars", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNVars, 0), "getNVars() -> retval\n."},
    {"getNames", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNames, 0), "getNames(names) -> None\n.   @brief Returns vector of symbolic names captured in loadFromCSV()"},
    {"getNormCatResponses", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNormCatResponses, 0), "getNormCatResponses() -> retval\n."},
    {"getResponseType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getResponseType, 0), "getResponseType() -> retval\n."},
    {"getResponses", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getResponses, 0), "getResponses() -> retval\n."},
    {"getSample", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSample, 0), "getSample(varIdx, sidx, buf) -> None\n."},
    {"getSampleWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSampleWeights, 0), "getSampleWeights() -> retval\n."},
    {"getSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSamples, 0), "getSamples() -> retval\n."},
    {"getSubMatrix", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSubMatrix_static, METH_STATIC), "getSubMatrix(matrix, idx, layout) -> retval\n.   @brief Extract from matrix rows/cols specified by passed indexes.\n.       @param matrix input matrix (supported types: CV_32S, CV_32F, CV_64F)\n.       @param idx 1D index vector\n.       @param layout specifies to extract rows (cv::ml::ROW_SAMPLES) or to extract columns (cv::ml::COL_SAMPLES)"},
    {"getSubVector", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSubVector_static, METH_STATIC), "getSubVector(vec, idx) -> retval\n.   @brief Extract from 1D vector elements specified by passed indexes.\n.       @param vec input vector (supported types: CV_32S, CV_32F, CV_64F)\n.       @param idx 1D index vector"},
    {"getTestNormCatResponses", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestNormCatResponses, 0), "getTestNormCatResponses() -> retval\n."},
    {"getTestResponses", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestResponses, 0), "getTestResponses() -> retval\n."},
    {"getTestSampleIdx", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestSampleIdx, 0), "getTestSampleIdx() -> retval\n."},
    {"getTestSampleWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestSampleWeights, 0), "getTestSampleWeights() -> retval\n."},
    {"getTestSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestSamples, 0), "getTestSamples() -> retval\n.   @brief Returns matrix of test samples"},
    {"getTrainNormCatResponses", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainNormCatResponses, 0), "getTrainNormCatResponses() -> retval\n.   @brief Returns the vector of normalized categorical responses\n.   \n.       The function returns vector of responses. Each response is integer from `0` to `<number of\n.       classes>-1`. The actual label value can be retrieved then from the class label vector, see\n.       TrainData::getClassLabels."},
    {"getTrainResponses", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainResponses, 0), "getTrainResponses() -> retval\n.   @brief Returns the vector of responses\n.   \n.       The function returns ordered or the original categorical responses. Usually it's used in\n.       regression algorithms."},
    {"getTrainSampleIdx", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainSampleIdx, 0), "getTrainSampleIdx() -> retval\n."},
    {"getTrainSampleWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainSampleWeights, 0), "getTrainSampleWeights() -> retval\n."},
    {"getTrainSamples", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainSamples, 0), "getTrainSamples([, layout[, compressSamples[, compressVars]]]) -> retval\n.   @brief Returns matrix of train samples\n.   \n.       @param layout The requested layout. If it's different from the initial one, the matrix is\n.           transposed. See ml::SampleTypes.\n.       @param compressSamples if true, the function returns only the training samples (specified by\n.           sampleIdx)\n.       @param compressVars if true, the function returns the shorter training samples, containing only\n.           the active variables.\n.   \n.       In current implementation the function tries to avoid physical data copying and returns the\n.       matrix stored inside TrainData (unless the transposition or compression is needed)."},
    {"getValues", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getValues, 0), "getValues(vi, sidx, values) -> None\n."},
    {"getVarIdx", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getVarIdx, 0), "getVarIdx() -> retval\n."},
    {"getVarSymbolFlags", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getVarSymbolFlags, 0), "getVarSymbolFlags() -> retval\n."},
    {"getVarType", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getVarType, 0), "getVarType() -> retval\n."},
    {"setTrainTestSplit", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_setTrainTestSplit, 0), "setTrainTestSplit(count[, shuffle]) -> None\n.   @brief Splits the training data into the training and test parts\n.       @sa TrainData::setTrainTestSplitRatio"},
    {"setTrainTestSplitRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_setTrainTestSplitRatio, 0), "setTrainTestSplitRatio(ratio[, shuffle]) -> None\n.   @brief Splits the training data into the training and test parts\n.   \n.       The function selects a subset of specified relative size and then returns it as the training\n.       set. If the function is not called, all the data is used for training. Please, note that for\n.       each of TrainData::getTrain\\* there is corresponding TrainData::getTest\\*, so that the test\n.       subset can be retrieved and processed as well.\n.       @sa TrainData::setTrainTestSplit"},
    {"shuffleTrainTest", CV_JS_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_shuffleTrainTest, 0), "shuffleTrainTest() -> None\n."},

    {NULL,          NULL}
};

// Converter (ml_TrainData)

template<>
struct PyOpenCV_Converter< Ptr<cv::ml::TrainData> >
{
    static PyObject* from(const Ptr<cv::ml::TrainData>& r)
    {
        return pyopencv_ml_TrainData_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ml::TrainData>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ml::TrainData> * dst_;
        if (pyopencv_ml_TrainData_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ml::TrainData> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ocl_Device (Generic)
//================================================================================

// GetSet (ocl_Device)



// Methods (ocl_Device)

static int pyopencv_cv_ocl_ocl_Device_Device(pyopencv_ocl_Device_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::ocl;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::ocl::Device());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_OpenCLVersion(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->OpenCLVersion());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_OpenCL_C_Version(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->OpenCL_C_Version());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_addressBits(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->addressBits());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_available(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->available());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_compilerAvailable(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compilerAvailable());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_deviceVersionMajor(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->deviceVersionMajor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_deviceVersionMinor(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->deviceVersionMinor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_doubleFPConfig(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->doubleFPConfig());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_driverVersion(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->driverVersion());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_endianLittle(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->endianLittle());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_errorCorrectionSupport(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->errorCorrectionSupport());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_executionCapabilities(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->executionCapabilities());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_extensions(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->extensions());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_getDefault_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;

    Device retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ocl::Device::getDefault());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_globalMemCacheLineSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->globalMemCacheLineSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_globalMemCacheSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->globalMemCacheSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_globalMemCacheType(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->globalMemCacheType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_globalMemSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->globalMemSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_halfFPConfig(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->halfFPConfig());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_hostUnifiedMemory(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->hostUnifiedMemory());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_image2DMaxHeight(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->image2DMaxHeight());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_image2DMaxWidth(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->image2DMaxWidth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_image3DMaxDepth(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->image3DMaxDepth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_image3DMaxHeight(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->image3DMaxHeight());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_image3DMaxWidth(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->image3DMaxWidth());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_imageFromBufferSupport(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->imageFromBufferSupport());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_imageMaxArraySize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->imageMaxArraySize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_imageMaxBufferSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->imageMaxBufferSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_imageSupport(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->imageSupport());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_intelSubgroupsSupport(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->intelSubgroupsSupport());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_isAMD(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isAMD());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_isExtensionSupported(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    Napi::Value* pyobj_extensionName = NULL;
    String extensionName;
    bool retval;

    const char* keywords[] = { "extensionName", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ocl_Device.isExtensionSupported", (char**)keywords, &pyobj_extensionName) &&
        jsopencv_to_safe(info, pyobj_extensionName, extensionName, ArgInfo("extensionName", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isExtensionSupported(extensionName));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_isIntel(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isIntel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_isNVidia(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->isNVidia());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_linkerAvailable(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->linkerAvailable());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_localMemSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->localMemSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_localMemType(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->localMemType());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxClockFrequency(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxClockFrequency());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxComputeUnits(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxComputeUnits());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxConstantArgs(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxConstantArgs());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxConstantBufferSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxConstantBufferSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxMemAllocSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxMemAllocSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxParameterSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxParameterSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxReadImageArgs(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxReadImageArgs());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxSamplers(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxSamplers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxWorkGroupSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxWorkGroupSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxWorkItemDims(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxWorkItemDims());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_maxWriteImageArgs(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->maxWriteImageArgs());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_memBaseAddrAlign(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->memBaseAddrAlign());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_name(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->name());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthChar(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthChar());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthDouble(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthDouble());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthFloat(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthFloat());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthHalf(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthHalf());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthInt(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthInt());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthLong(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthLong());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_nativeVectorWidthShort(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->nativeVectorWidthShort());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthChar(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthChar());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthDouble(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthDouble());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthFloat(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthFloat());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthHalf(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthHalf());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthInt(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthInt());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthLong(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthLong());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_preferredVectorWidthShort(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->preferredVectorWidthShort());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_printfBufferSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->printfBufferSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_profilingTimerResolution(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    size_t retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->profilingTimerResolution());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_singleFPConfig(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->singleFPConfig());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_type(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->type());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_vendorID(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->vendorID());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_vendorName(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->vendorName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ocl_ocl_Device_version(const Napi::CallbackInfo &info)
{
    using namespace cv::ocl;


    cv::ocl::Device * self1 = 0;
    if (!pyopencv_ocl_Device_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ocl_Device' or its derivative)");
    cv::ocl::Device* _self_ = (self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->version());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (ocl_Device)

static PyGetSetDef pyopencv_ocl_Device_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ocl_Device_methods[] =
{
    {"OpenCLVersion", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_OpenCLVersion, 0), "OpenCLVersion() -> retval\n."},
    {"OpenCL_C_Version", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_OpenCL_C_Version, 0), "OpenCL_C_Version() -> retval\n."},
    {"addressBits", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_addressBits, 0), "addressBits() -> retval\n."},
    {"available", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_available, 0), "available() -> retval\n."},
    {"compilerAvailable", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_compilerAvailable, 0), "compilerAvailable() -> retval\n."},
    {"deviceVersionMajor", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_deviceVersionMajor, 0), "deviceVersionMajor() -> retval\n."},
    {"deviceVersionMinor", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_deviceVersionMinor, 0), "deviceVersionMinor() -> retval\n."},
    {"doubleFPConfig", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_doubleFPConfig, 0), "doubleFPConfig() -> retval\n."},
    {"driverVersion", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_driverVersion, 0), "driverVersion() -> retval\n."},
    {"endianLittle", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_endianLittle, 0), "endianLittle() -> retval\n."},
    {"errorCorrectionSupport", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_errorCorrectionSupport, 0), "errorCorrectionSupport() -> retval\n."},
    {"executionCapabilities", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_executionCapabilities, 0), "executionCapabilities() -> retval\n."},
    {"extensions", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_extensions, 0), "extensions() -> retval\n."},
    {"getDefault", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_getDefault_static, METH_STATIC), "getDefault() -> retval\n."},
    {"globalMemCacheLineSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_globalMemCacheLineSize, 0), "globalMemCacheLineSize() -> retval\n."},
    {"globalMemCacheSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_globalMemCacheSize, 0), "globalMemCacheSize() -> retval\n."},
    {"globalMemCacheType", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_globalMemCacheType, 0), "globalMemCacheType() -> retval\n."},
    {"globalMemSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_globalMemSize, 0), "globalMemSize() -> retval\n."},
    {"halfFPConfig", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_halfFPConfig, 0), "halfFPConfig() -> retval\n."},
    {"hostUnifiedMemory", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_hostUnifiedMemory, 0), "hostUnifiedMemory() -> retval\n."},
    {"image2DMaxHeight", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_image2DMaxHeight, 0), "image2DMaxHeight() -> retval\n."},
    {"image2DMaxWidth", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_image2DMaxWidth, 0), "image2DMaxWidth() -> retval\n."},
    {"image3DMaxDepth", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_image3DMaxDepth, 0), "image3DMaxDepth() -> retval\n."},
    {"image3DMaxHeight", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_image3DMaxHeight, 0), "image3DMaxHeight() -> retval\n."},
    {"image3DMaxWidth", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_image3DMaxWidth, 0), "image3DMaxWidth() -> retval\n."},
    {"imageFromBufferSupport", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_imageFromBufferSupport, 0), "imageFromBufferSupport() -> retval\n."},
    {"imageMaxArraySize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_imageMaxArraySize, 0), "imageMaxArraySize() -> retval\n."},
    {"imageMaxBufferSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_imageMaxBufferSize, 0), "imageMaxBufferSize() -> retval\n."},
    {"imageSupport", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_imageSupport, 0), "imageSupport() -> retval\n."},
    {"intelSubgroupsSupport", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_intelSubgroupsSupport, 0), "intelSubgroupsSupport() -> retval\n."},
    {"isAMD", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_isAMD, 0), "isAMD() -> retval\n."},
    {"isExtensionSupported", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_isExtensionSupported, 0), "isExtensionSupported(extensionName) -> retval\n."},
    {"isIntel", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_isIntel, 0), "isIntel() -> retval\n."},
    {"isNVidia", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_isNVidia, 0), "isNVidia() -> retval\n."},
    {"linkerAvailable", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_linkerAvailable, 0), "linkerAvailable() -> retval\n."},
    {"localMemSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_localMemSize, 0), "localMemSize() -> retval\n."},
    {"localMemType", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_localMemType, 0), "localMemType() -> retval\n."},
    {"maxClockFrequency", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxClockFrequency, 0), "maxClockFrequency() -> retval\n."},
    {"maxComputeUnits", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxComputeUnits, 0), "maxComputeUnits() -> retval\n."},
    {"maxConstantArgs", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxConstantArgs, 0), "maxConstantArgs() -> retval\n."},
    {"maxConstantBufferSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxConstantBufferSize, 0), "maxConstantBufferSize() -> retval\n."},
    {"maxMemAllocSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxMemAllocSize, 0), "maxMemAllocSize() -> retval\n."},
    {"maxParameterSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxParameterSize, 0), "maxParameterSize() -> retval\n."},
    {"maxReadImageArgs", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxReadImageArgs, 0), "maxReadImageArgs() -> retval\n."},
    {"maxSamplers", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxSamplers, 0), "maxSamplers() -> retval\n."},
    {"maxWorkGroupSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxWorkGroupSize, 0), "maxWorkGroupSize() -> retval\n."},
    {"maxWorkItemDims", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxWorkItemDims, 0), "maxWorkItemDims() -> retval\n."},
    {"maxWriteImageArgs", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_maxWriteImageArgs, 0), "maxWriteImageArgs() -> retval\n."},
    {"memBaseAddrAlign", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_memBaseAddrAlign, 0), "memBaseAddrAlign() -> retval\n."},
    {"name", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_name, 0), "name() -> retval\n."},
    {"nativeVectorWidthChar", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthChar, 0), "nativeVectorWidthChar() -> retval\n."},
    {"nativeVectorWidthDouble", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthDouble, 0), "nativeVectorWidthDouble() -> retval\n."},
    {"nativeVectorWidthFloat", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthFloat, 0), "nativeVectorWidthFloat() -> retval\n."},
    {"nativeVectorWidthHalf", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthHalf, 0), "nativeVectorWidthHalf() -> retval\n."},
    {"nativeVectorWidthInt", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthInt, 0), "nativeVectorWidthInt() -> retval\n."},
    {"nativeVectorWidthLong", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthLong, 0), "nativeVectorWidthLong() -> retval\n."},
    {"nativeVectorWidthShort", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_nativeVectorWidthShort, 0), "nativeVectorWidthShort() -> retval\n."},
    {"preferredVectorWidthChar", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthChar, 0), "preferredVectorWidthChar() -> retval\n."},
    {"preferredVectorWidthDouble", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthDouble, 0), "preferredVectorWidthDouble() -> retval\n."},
    {"preferredVectorWidthFloat", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthFloat, 0), "preferredVectorWidthFloat() -> retval\n."},
    {"preferredVectorWidthHalf", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthHalf, 0), "preferredVectorWidthHalf() -> retval\n."},
    {"preferredVectorWidthInt", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthInt, 0), "preferredVectorWidthInt() -> retval\n."},
    {"preferredVectorWidthLong", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthLong, 0), "preferredVectorWidthLong() -> retval\n."},
    {"preferredVectorWidthShort", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_preferredVectorWidthShort, 0), "preferredVectorWidthShort() -> retval\n."},
    {"printfBufferSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_printfBufferSize, 0), "printfBufferSize() -> retval\n."},
    {"profilingTimerResolution", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_profilingTimerResolution, 0), "profilingTimerResolution() -> retval\n."},
    {"singleFPConfig", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_singleFPConfig, 0), "singleFPConfig() -> retval\n."},
    {"type", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_type, 0), "type() -> retval\n."},
    {"vendorID", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_vendorID, 0), "vendorID() -> retval\n."},
    {"vendorName", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_vendorName, 0), "vendorName() -> retval\n."},
    {"version", CV_JS_FN_WITH_KW_(pyopencv_cv_ocl_ocl_Device_version, 0), "version() -> retval\n."},

    {NULL,          NULL}
};

// Converter (ocl_Device)

template<>
struct PyOpenCV_Converter< cv::ocl::Device >
{
    static PyObject* from(const cv::ocl::Device& r)
    {
        return pyopencv_ocl_Device_Instance(r);
    }
    static bool to(PyObject* src, cv::ocl::Device& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::ocl::Device * dst_;
        if (pyopencv_ocl_Device_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::ocl::Device for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ocl_OpenCLExecutionContext (Generic)
//================================================================================

// GetSet (ocl_OpenCLExecutionContext)



// Methods (ocl_OpenCLExecutionContext)



// Tables (ocl_OpenCLExecutionContext)

static PyGetSetDef pyopencv_ocl_OpenCLExecutionContext_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ocl_OpenCLExecutionContext_methods[] =
{

    {NULL,          NULL}
};

// Converter (ocl_OpenCLExecutionContext)

template<>
struct PyOpenCV_Converter< Ptr<cv::ocl::OpenCLExecutionContext> >
{
    static PyObject* from(const Ptr<cv::ocl::OpenCLExecutionContext>& r)
    {
        return pyopencv_ocl_OpenCLExecutionContext_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ocl::OpenCLExecutionContext>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ocl::OpenCLExecutionContext> * dst_;
        if (pyopencv_ocl_OpenCLExecutionContext_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ocl::OpenCLExecutionContext> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// plot_Plot2d (Generic)
//================================================================================

// GetSet (plot_Plot2d)



// Methods (plot_Plot2d)

static Napi::Value pyopencv_cv_plot_plot_Plot2d_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;

    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_data = NULL;
    Mat data;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "data", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.create", (char**)keywords, &pyobj_data) &&
        jsopencv_to_safe(info, pyobj_data, data, ArgInfo("data", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::plot::Plot2d::create(data));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_data = NULL;
    UMat data;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "data", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.create", (char**)keywords, &pyobj_data) &&
        jsopencv_to_safe(info, pyobj_data, data, ArgInfo("data", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::plot::Plot2d::create(data));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dataX = NULL;
    Mat dataX;
    Napi::Value* pyobj_dataY = NULL;
    Mat dataY;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "dataX", "dataY", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:plot_Plot2d.create", (char**)keywords, &pyobj_dataX, &pyobj_dataY) &&
        jsopencv_to_safe(info, pyobj_dataX, dataX, ArgInfo("dataX", 0)) &&
        jsopencv_to_safe(info, pyobj_dataY, dataY, ArgInfo("dataY", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::plot::Plot2d::create(dataX, dataY));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dataX = NULL;
    UMat dataX;
    Napi::Value* pyobj_dataY = NULL;
    UMat dataY;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "dataX", "dataY", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:plot_Plot2d.create", (char**)keywords, &pyobj_dataX, &pyobj_dataY) &&
        jsopencv_to_safe(info, pyobj_dataX, dataX, ArgInfo("dataX", 0)) &&
        jsopencv_to_safe(info, pyobj_dataY, dataY, ArgInfo("dataY", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::plot::Plot2d::create(dataX, dataY));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_render(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj__plotResult = NULL;
    Mat _plotResult;

    const char* keywords[] = { "_plotResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:plot_Plot2d.render", (char**)keywords, &pyobj__plotResult) &&
        jsopencv_to_safe(info, pyobj__plotResult, _plotResult, ArgInfo("_plotResult", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->render(_plotResult));
        return jsopencv_from(_plotResult);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__plotResult = NULL;
    UMat _plotResult;

    const char* keywords[] = { "_plotResult", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:plot_Plot2d.render", (char**)keywords, &pyobj__plotResult) &&
        jsopencv_to_safe(info, pyobj__plotResult, _plotResult, ArgInfo("_plotResult", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->render(_plotResult));
        return jsopencv_from(_plotResult);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("render");

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setGridLinesNumber(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj_gridLinesNumber = NULL;
    int gridLinesNumber=0;

    const char* keywords[] = { "gridLinesNumber", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setGridLinesNumber", (char**)keywords, &pyobj_gridLinesNumber) &&
        jsopencv_to_safe(info, pyobj_gridLinesNumber, gridLinesNumber, ArgInfo("gridLinesNumber", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGridLinesNumber(gridLinesNumber));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setInvertOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__invertOrientation = NULL;
    bool _invertOrientation=0;

    const char* keywords[] = { "_invertOrientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setInvertOrientation", (char**)keywords, &pyobj__invertOrientation) &&
        jsopencv_to_safe(info, pyobj__invertOrientation, _invertOrientation, ArgInfo("_invertOrientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInvertOrientation(_invertOrientation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setMaxX(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotMaxX = NULL;
    double _plotMaxX=0;

    const char* keywords[] = { "_plotMaxX", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setMaxX", (char**)keywords, &pyobj__plotMaxX) &&
        jsopencv_to_safe(info, pyobj__plotMaxX, _plotMaxX, ArgInfo("_plotMaxX", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxX(_plotMaxX));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setMaxY(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotMaxY = NULL;
    double _plotMaxY=0;

    const char* keywords[] = { "_plotMaxY", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setMaxY", (char**)keywords, &pyobj__plotMaxY) &&
        jsopencv_to_safe(info, pyobj__plotMaxY, _plotMaxY, ArgInfo("_plotMaxY", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxY(_plotMaxY));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setMinX(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotMinX = NULL;
    double _plotMinX=0;

    const char* keywords[] = { "_plotMinX", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setMinX", (char**)keywords, &pyobj__plotMinX) &&
        jsopencv_to_safe(info, pyobj__plotMinX, _plotMinX, ArgInfo("_plotMinX", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinX(_plotMinX));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setMinY(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotMinY = NULL;
    double _plotMinY=0;

    const char* keywords[] = { "_plotMinY", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setMinY", (char**)keywords, &pyobj__plotMinY) &&
        jsopencv_to_safe(info, pyobj__plotMinY, _plotMinY, ArgInfo("_plotMinY", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinY(_plotMinY));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setNeedPlotLine(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__needPlotLine = NULL;
    bool _needPlotLine=0;

    const char* keywords[] = { "_needPlotLine", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setNeedPlotLine", (char**)keywords, &pyobj__needPlotLine) &&
        jsopencv_to_safe(info, pyobj__needPlotLine, _needPlotLine, ArgInfo("_needPlotLine", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNeedPlotLine(_needPlotLine));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotAxisColor(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotAxisColor = NULL;
    Scalar _plotAxisColor;

    const char* keywords[] = { "_plotAxisColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPlotAxisColor", (char**)keywords, &pyobj__plotAxisColor) &&
        jsopencv_to_safe(info, pyobj__plotAxisColor, _plotAxisColor, ArgInfo("_plotAxisColor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotAxisColor(_plotAxisColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotBackgroundColor(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotBackgroundColor = NULL;
    Scalar _plotBackgroundColor;

    const char* keywords[] = { "_plotBackgroundColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPlotBackgroundColor", (char**)keywords, &pyobj__plotBackgroundColor) &&
        jsopencv_to_safe(info, pyobj__plotBackgroundColor, _plotBackgroundColor, ArgInfo("_plotBackgroundColor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotBackgroundColor(_plotBackgroundColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotGridColor(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotGridColor = NULL;
    Scalar _plotGridColor;

    const char* keywords[] = { "_plotGridColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPlotGridColor", (char**)keywords, &pyobj__plotGridColor) &&
        jsopencv_to_safe(info, pyobj__plotGridColor, _plotGridColor, ArgInfo("_plotGridColor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotGridColor(_plotGridColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotLineColor(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotLineColor = NULL;
    Scalar _plotLineColor;

    const char* keywords[] = { "_plotLineColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPlotLineColor", (char**)keywords, &pyobj__plotLineColor) &&
        jsopencv_to_safe(info, pyobj__plotLineColor, _plotLineColor, ArgInfo("_plotLineColor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotLineColor(_plotLineColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotLineWidth(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotLineWidth = NULL;
    int _plotLineWidth=0;

    const char* keywords[] = { "_plotLineWidth", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPlotLineWidth", (char**)keywords, &pyobj__plotLineWidth) &&
        jsopencv_to_safe(info, pyobj__plotLineWidth, _plotLineWidth, ArgInfo("_plotLineWidth", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotLineWidth(_plotLineWidth));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotSize(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotSizeWidth = NULL;
    int _plotSizeWidth=0;
    Napi::Value* pyobj__plotSizeHeight = NULL;
    int _plotSizeHeight=0;

    const char* keywords[] = { "_plotSizeWidth", "_plotSizeHeight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:plot_Plot2d.setPlotSize", (char**)keywords, &pyobj__plotSizeWidth, &pyobj__plotSizeHeight) &&
        jsopencv_to_safe(info, pyobj__plotSizeWidth, _plotSizeWidth, ArgInfo("_plotSizeWidth", 0)) &&
        jsopencv_to_safe(info, pyobj__plotSizeHeight, _plotSizeHeight, ArgInfo("_plotSizeHeight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotSize(_plotSizeWidth, _plotSizeHeight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPlotTextColor(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj__plotTextColor = NULL;
    Scalar _plotTextColor;

    const char* keywords[] = { "_plotTextColor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPlotTextColor", (char**)keywords, &pyobj__plotTextColor) &&
        jsopencv_to_safe(info, pyobj__plotTextColor, _plotTextColor, ArgInfo("_plotTextColor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPlotTextColor(_plotTextColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setPointIdxToPrint(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj_pointIdx = NULL;
    int pointIdx=0;

    const char* keywords[] = { "pointIdx", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setPointIdxToPrint", (char**)keywords, &pyobj_pointIdx) &&
        jsopencv_to_safe(info, pyobj_pointIdx, pointIdx, ArgInfo("pointIdx", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPointIdxToPrint(pointIdx));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setShowGrid(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj_needShowGrid = NULL;
    bool needShowGrid=0;

    const char* keywords[] = { "needShowGrid", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setShowGrid", (char**)keywords, &pyobj_needShowGrid) &&
        jsopencv_to_safe(info, pyobj_needShowGrid, needShowGrid, ArgInfo("needShowGrid", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setShowGrid(needShowGrid));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_plot_plot_Plot2d_setShowText(const Napi::CallbackInfo &info)
{
    using namespace cv::plot;


    Ptr<cv::plot::Plot2d> * self1 = 0;
    if (!pyopencv_plot_Plot2d_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    Ptr<cv::plot::Plot2d> _self_ = *(self1);
    Napi::Value* pyobj_needShowText = NULL;
    bool needShowText=0;

    const char* keywords[] = { "needShowText", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:plot_Plot2d.setShowText", (char**)keywords, &pyobj_needShowText) &&
        jsopencv_to_safe(info, pyobj_needShowText, needShowText, ArgInfo("needShowText", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setShowText(needShowText));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (plot_Plot2d)

static PyGetSetDef pyopencv_plot_Plot2d_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_plot_Plot2d_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_create_static, METH_STATIC), "create(data) -> retval\n.   * @brief Creates Plot2d object\n.                *\n.                * @param data \\f$1xN\\f$ or \\f$Nx1\\f$ matrix containing \\f$Y\\f$ values of points to plot. \\f$X\\f$ values\n.                * will be equal to indexes of correspondind elements in data matrix.\n\n\n\ncreate(dataX, dataY) -> retval\n.   * @brief Creates Plot2d object\n.                *\n.                * @param dataX \\f$1xN\\f$ or \\f$Nx1\\f$ matrix \\f$X\\f$ values of points to plot.\n.                * @param dataY \\f$1xN\\f$ or \\f$Nx1\\f$ matrix containing \\f$Y\\f$ values of points to plot."},
    {"render", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_render, 0), "render([, _plotResult]) -> _plotResult\n."},
    {"setGridLinesNumber", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setGridLinesNumber, 0), "setGridLinesNumber(gridLinesNumber) -> None\n."},
    {"setInvertOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setInvertOrientation, 0), "setInvertOrientation(_invertOrientation) -> None\n."},
    {"setMaxX", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMaxX, 0), "setMaxX(_plotMaxX) -> None\n."},
    {"setMaxY", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMaxY, 0), "setMaxY(_plotMaxY) -> None\n."},
    {"setMinX", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMinX, 0), "setMinX(_plotMinX) -> None\n."},
    {"setMinY", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMinY, 0), "setMinY(_plotMinY) -> None\n."},
    {"setNeedPlotLine", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setNeedPlotLine, 0), "setNeedPlotLine(_needPlotLine) -> None\n.   * @brief Switches data visualization mode\n.                *\n.                * @param _needPlotLine if true then neighbour plot points will be connected by lines.\n.                * In other case data will be plotted as a set of standalone points."},
    {"setPlotAxisColor", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotAxisColor, 0), "setPlotAxisColor(_plotAxisColor) -> None\n."},
    {"setPlotBackgroundColor", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotBackgroundColor, 0), "setPlotBackgroundColor(_plotBackgroundColor) -> None\n."},
    {"setPlotGridColor", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotGridColor, 0), "setPlotGridColor(_plotGridColor) -> None\n."},
    {"setPlotLineColor", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotLineColor, 0), "setPlotLineColor(_plotLineColor) -> None\n."},
    {"setPlotLineWidth", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotLineWidth, 0), "setPlotLineWidth(_plotLineWidth) -> None\n."},
    {"setPlotSize", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotSize, 0), "setPlotSize(_plotSizeWidth, _plotSizeHeight) -> None\n."},
    {"setPlotTextColor", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotTextColor, 0), "setPlotTextColor(_plotTextColor) -> None\n."},
    {"setPointIdxToPrint", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPointIdxToPrint, 0), "setPointIdxToPrint(pointIdx) -> None\n.   * @brief Sets the index of a point which coordinates will be printed on the top left corner of the plot (if ShowText flag is true).\n.                *\n.                * @param pointIdx index of the required point in data array."},
    {"setShowGrid", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setShowGrid, 0), "setShowGrid(needShowGrid) -> None\n."},
    {"setShowText", CV_JS_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setShowText, 0), "setShowText(needShowText) -> None\n."},

    {NULL,          NULL}
};

// Converter (plot_Plot2d)

template<>
struct PyOpenCV_Converter< Ptr<cv::plot::Plot2d> >
{
    static PyObject* from(const Ptr<cv::plot::Plot2d>& r)
    {
        return pyopencv_plot_Plot2d_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::plot::Plot2d>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::plot::Plot2d> * dst_;
        if (pyopencv_plot_Plot2d_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::plot::Plot2d> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// quality_QualityBRISQUE (Generic)
//================================================================================

// GetSet (quality_QualityBRISQUE)



// Methods (quality_QualityBRISQUE)

static Napi::Value pyopencv_cv_quality_quality_QualityBRISQUE_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityBRISQUE> * self1 = 0;
    if (!pyopencv_quality_QualityBRISQUE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityBRISQUE' or its derivative)");
    Ptr<cv::quality::QualityBRISQUE> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    cv::Scalar retval;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityBRISQUE.compute", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    cv::Scalar retval;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityBRISQUE.compute", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_model_file_path = NULL;
    String model_file_path;
    Napi::Value* pyobj_range_file_path = NULL;
    String range_file_path;
    cv::Scalar retval;

    const char* keywords[] = { "img", "model_file_path", "range_file_path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:quality_QualityBRISQUE.compute", (char**)keywords, &pyobj_img, &pyobj_model_file_path, &pyobj_range_file_path) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_model_file_path, model_file_path, ArgInfo("model_file_path", 0)) &&
        jsopencv_to_safe(info, pyobj_range_file_path, range_file_path, ArgInfo("range_file_path", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img, model_file_path, range_file_path));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_model_file_path = NULL;
    String model_file_path;
    Napi::Value* pyobj_range_file_path = NULL;
    String range_file_path;
    cv::Scalar retval;

    const char* keywords[] = { "img", "model_file_path", "range_file_path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:quality_QualityBRISQUE.compute", (char**)keywords, &pyobj_img, &pyobj_model_file_path, &pyobj_range_file_path) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_model_file_path, model_file_path, ArgInfo("model_file_path", 0)) &&
        jsopencv_to_safe(info, pyobj_range_file_path, range_file_path, ArgInfo("range_file_path", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img, model_file_path, range_file_path));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityBRISQUE_computeFeatures_static(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_features = NULL;
    Mat features;

    const char* keywords[] = { "img", "features", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:quality_QualityBRISQUE.computeFeatures", (char**)keywords, &pyobj_img, &pyobj_features) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 1)))
    {
        ERRWRAP2_NAPI(info, cv::quality::QualityBRISQUE::computeFeatures(img, features));
        return jsopencv_from(features);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_features = NULL;
    UMat features;

    const char* keywords[] = { "img", "features", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:quality_QualityBRISQUE.computeFeatures", (char**)keywords, &pyobj_img, &pyobj_features) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 1)))
    {
        ERRWRAP2_NAPI(info, cv::quality::QualityBRISQUE::computeFeatures(img, features));
        return jsopencv_from(features);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("computeFeatures");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityBRISQUE_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_model_file_path = NULL;
    String model_file_path;
    Napi::Value* pyobj_range_file_path = NULL;
    String range_file_path;
    Ptr<QualityBRISQUE> retval;

    const char* keywords[] = { "model_file_path", "range_file_path", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:quality_QualityBRISQUE.create", (char**)keywords, &pyobj_model_file_path, &pyobj_range_file_path) &&
        jsopencv_to_safe(info, pyobj_model_file_path, model_file_path, ArgInfo("model_file_path", 0)) &&
        jsopencv_to_safe(info, pyobj_range_file_path, range_file_path, ArgInfo("range_file_path", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityBRISQUE::create(model_file_path, range_file_path));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_model = NULL;
    Ptr<ml::SVM> model;
    Napi::Value* pyobj_range = NULL;
    Mat range;
    Ptr<QualityBRISQUE> retval;

    const char* keywords[] = { "model", "range", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:quality_QualityBRISQUE.create", (char**)keywords, &pyobj_model, &pyobj_range) &&
        jsopencv_to_safe(info, pyobj_model, model, ArgInfo("model", 0)) &&
        jsopencv_to_safe(info, pyobj_range, range, ArgInfo("range", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityBRISQUE::create(model, range));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}



// Tables (quality_QualityBRISQUE)

static PyGetSetDef pyopencv_quality_QualityBRISQUE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_quality_QualityBRISQUE_methods[] =
{
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBRISQUE_compute, 0), "compute(img) -> retval\n.   @brief Computes BRISQUE quality score for input image\n.       @param img Image for which to compute quality\n.       @returns cv::Scalar with the score in the first element.  The score ranges from 0 (best quality) to 100 (worst quality)\n\n\n\ncompute(img, model_file_path, range_file_path) -> retval\n.   @brief static method for computing quality\n.       @param img image for which to compute quality\n.       @param model_file_path cv::String which contains a path to the BRISQUE model data, eg. /path/to/brisque_model_live.yml\n.       @param range_file_path cv::String which contains a path to the BRISQUE range data, eg. /path/to/brisque_range_live.yml\n.       @returns cv::Scalar with the score in the first element.  The score ranges from 0 (best quality) to 100 (worst quality)"},
    {"computeFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBRISQUE_computeFeatures_static, METH_STATIC), "computeFeatures(img[, features]) -> features\n.   @brief static method for computing image features used by the BRISQUE algorithm\n.       @param img image (BGR(A) or grayscale) for which to compute features\n.       @param features output row vector of features to cv::Mat or cv::UMat"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBRISQUE_create_static, METH_STATIC), "create(model_file_path, range_file_path) -> retval\n.   @brief Create an object which calculates quality\n.       @param model_file_path cv::String which contains a path to the BRISQUE model data, eg. /path/to/brisque_model_live.yml\n.       @param range_file_path cv::String which contains a path to the BRISQUE range data, eg. /path/to/brisque_range_live.yml\n\n\n\ncreate(model, range) -> retval\n.   @brief Create an object which calculates quality\n.       @param model cv::Ptr<cv::ml::SVM> which contains a loaded BRISQUE model\n.       @param range cv::Mat which contains BRISQUE range data"},

    {NULL,          NULL}
};

// Converter (quality_QualityBRISQUE)

template<>
struct PyOpenCV_Converter< Ptr<cv::quality::QualityBRISQUE> >
{
    static PyObject* from(const Ptr<cv::quality::QualityBRISQUE>& r)
    {
        return pyopencv_quality_QualityBRISQUE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::quality::QualityBRISQUE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::quality::QualityBRISQUE> * dst_;
        if (pyopencv_quality_QualityBRISQUE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::quality::QualityBRISQUE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// quality_QualityBase (Generic)
//================================================================================

// GetSet (quality_QualityBase)



// Methods (quality_QualityBase)

static Napi::Value pyopencv_cv_quality_quality_QualityBase_clear(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityBase> * self1 = 0;
    if (!pyopencv_quality_QualityBase_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityBase' or its derivative)");
    Ptr<cv::quality::QualityBase> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityBase_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityBase> * self1 = 0;
    if (!pyopencv_quality_QualityBase_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityBase' or its derivative)");
    Ptr<cv::quality::QualityBase> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    cv::Scalar retval;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityBase.compute", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    cv::Scalar retval;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityBase.compute", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityBase_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityBase> * self1 = 0;
    if (!pyopencv_quality_QualityBase_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityBase' or its derivative)");
    Ptr<cv::quality::QualityBase> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityBase_getQualityMap(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityBase> * self1 = 0;
    if (!pyopencv_quality_QualityBase_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityBase' or its derivative)");
    Ptr<cv::quality::QualityBase> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:quality_QualityBase.getQualityMap", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getQualityMap(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:quality_QualityBase.getQualityMap", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getQualityMap(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getQualityMap");

    return NULL;
}



// Tables (quality_QualityBase)

static PyGetSetDef pyopencv_quality_QualityBase_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_quality_QualityBase_methods[] =
{
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBase_clear, 0), "clear() -> None\n.   @brief Implements Algorithm::clear()"},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBase_compute, 0), "compute(img) -> retval\n.   @brief Compute quality score per channel with the per-channel score in each element of the resulting cv::Scalar.  See specific algorithm for interpreting result scores\n.       @param img comparison image, or image to evalute for no-reference quality algorithms"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBase_empty, 0), "empty() -> retval\n.   @brief Implements Algorithm::empty()"},
    {"getQualityMap", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityBase_getQualityMap, 0), "getQualityMap([, dst]) -> dst\n.   @brief Returns output quality map that was generated during computation, if supported by the algorithm"},

    {NULL,          NULL}
};

// Converter (quality_QualityBase)

template<>
struct PyOpenCV_Converter< Ptr<cv::quality::QualityBase> >
{
    static PyObject* from(const Ptr<cv::quality::QualityBase>& r)
    {
        return pyopencv_quality_QualityBase_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::quality::QualityBase>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::quality::QualityBase> * dst_;
        if (pyopencv_quality_QualityBase_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::quality::QualityBase> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// quality_QualityGMSD (Generic)
//================================================================================

// GetSet (quality_QualityGMSD)



// Methods (quality_QualityGMSD)

static Napi::Value pyopencv_cv_quality_quality_QualityGMSD_clear(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityGMSD> * self1 = 0;
    if (!pyopencv_quality_QualityGMSD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityGMSD' or its derivative)");
    Ptr<cv::quality::QualityGMSD> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityGMSD_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityGMSD> * self1 = 0;
    if (!pyopencv_quality_QualityGMSD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityGMSD' or its derivative)");
    Ptr<cv::quality::QualityGMSD> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    cv::Scalar retval;

    const char* keywords[] = { "cmp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityGMSD.compute", (char**)keywords, &pyobj_cmp) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmp));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    cv::Scalar retval;

    const char* keywords[] = { "cmp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityGMSD.compute", (char**)keywords, &pyobj_cmp) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmp));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    Mat qualityMap;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:quality_QualityGMSD.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    UMat qualityMap;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:quality_QualityGMSD.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityGMSD_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Ptr<QualityGMSD> retval;

    const char* keywords[] = { "ref", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityGMSD.create", (char**)keywords, &pyobj_ref) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityGMSD::create(ref));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Ptr<QualityGMSD> retval;

    const char* keywords[] = { "ref", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityGMSD.create", (char**)keywords, &pyobj_ref) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityGMSD::create(ref));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityGMSD_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityGMSD> * self1 = 0;
    if (!pyopencv_quality_QualityGMSD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityGMSD' or its derivative)");
    Ptr<cv::quality::QualityGMSD> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (quality_QualityGMSD)

static PyGetSetDef pyopencv_quality_QualityGMSD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_quality_QualityGMSD_methods[] =
{
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityGMSD_clear, 0), "clear() -> None\n.   @brief Implements Algorithm::clear()"},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityGMSD_compute, 0), "compute(cmp) -> retval\n.   @brief Compute GMSD\n.       @param cmp comparison image\n.       @returns cv::Scalar with per-channel quality value.  Values range from 0 (worst) to 1 (best)\n\n\n\ncompute(ref, cmp[, qualityMap]) -> retval, qualityMap\n.   @brief static method for computing quality\n.       @param ref reference image\n.       @param cmp comparison image\n.       @param qualityMap output quality map, or cv::noArray()\n.       @returns cv::Scalar with per-channel quality value.  Values range from 0 (worst) to 1 (best)"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityGMSD_create_static, METH_STATIC), "create(ref) -> retval\n.   @brief Create an object which calculates image quality\n.       @param ref reference image"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityGMSD_empty, 0), "empty() -> retval\n.   @brief Implements Algorithm::empty()"},

    {NULL,          NULL}
};

// Converter (quality_QualityGMSD)

template<>
struct PyOpenCV_Converter< Ptr<cv::quality::QualityGMSD> >
{
    static PyObject* from(const Ptr<cv::quality::QualityGMSD>& r)
    {
        return pyopencv_quality_QualityGMSD_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::quality::QualityGMSD>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::quality::QualityGMSD> * dst_;
        if (pyopencv_quality_QualityGMSD_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::quality::QualityGMSD> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// quality_QualityMSE (Generic)
//================================================================================

// GetSet (quality_QualityMSE)



// Methods (quality_QualityMSE)

static Napi::Value pyopencv_cv_quality_quality_QualityMSE_clear(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityMSE> * self1 = 0;
    if (!pyopencv_quality_QualityMSE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityMSE' or its derivative)");
    Ptr<cv::quality::QualityMSE> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityMSE_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityMSE> * self1 = 0;
    if (!pyopencv_quality_QualityMSE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityMSE' or its derivative)");
    Ptr<cv::quality::QualityMSE> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_cmpImgs = NULL;
    vector_Mat cmpImgs;
    cv::Scalar retval;

    const char* keywords[] = { "cmpImgs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityMSE.compute", (char**)keywords, &pyobj_cmpImgs) &&
        jsopencv_to_safe(info, pyobj_cmpImgs, cmpImgs, ArgInfo("cmpImgs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmpImgs));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_cmpImgs = NULL;
    vector_UMat cmpImgs;
    cv::Scalar retval;

    const char* keywords[] = { "cmpImgs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityMSE.compute", (char**)keywords, &pyobj_cmpImgs) &&
        jsopencv_to_safe(info, pyobj_cmpImgs, cmpImgs, ArgInfo("cmpImgs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmpImgs));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    Mat qualityMap;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:quality_QualityMSE.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    UMat qualityMap;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:quality_QualityMSE.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityMSE_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Ptr<QualityMSE> retval;

    const char* keywords[] = { "ref", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityMSE.create", (char**)keywords, &pyobj_ref) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityMSE::create(ref));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Ptr<QualityMSE> retval;

    const char* keywords[] = { "ref", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityMSE.create", (char**)keywords, &pyobj_ref) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityMSE::create(ref));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityMSE_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityMSE> * self1 = 0;
    if (!pyopencv_quality_QualityMSE_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityMSE' or its derivative)");
    Ptr<cv::quality::QualityMSE> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (quality_QualityMSE)

static PyGetSetDef pyopencv_quality_QualityMSE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_quality_QualityMSE_methods[] =
{
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityMSE_clear, 0), "clear() -> None\n.   @brief Implements Algorithm::clear()"},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityMSE_compute, 0), "compute(cmpImgs) -> retval\n.   @brief Computes MSE for reference images supplied in class constructor and provided comparison images\n.       @param cmpImgs Comparison image(s)\n.       @returns cv::Scalar with per-channel quality values.  Values range from 0 (best) to potentially max float (worst)\n\n\n\ncompute(ref, cmp[, qualityMap]) -> retval, qualityMap\n.   @brief static method for computing quality\n.       @param ref reference image\n.       @param cmp comparison image=\n.       @param qualityMap output quality map, or cv::noArray()\n.       @returns cv::Scalar with per-channel quality values.  Values range from 0 (best) to max float (worst)"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityMSE_create_static, METH_STATIC), "create(ref) -> retval\n.   @brief Create an object which calculates quality\n.       @param ref input image to use as the reference for comparison"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityMSE_empty, 0), "empty() -> retval\n.   @brief Implements Algorithm::empty()"},

    {NULL,          NULL}
};

// Converter (quality_QualityMSE)

template<>
struct PyOpenCV_Converter< Ptr<cv::quality::QualityMSE> >
{
    static PyObject* from(const Ptr<cv::quality::QualityMSE>& r)
    {
        return pyopencv_quality_QualityMSE_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::quality::QualityMSE>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::quality::QualityMSE> * dst_;
        if (pyopencv_quality_QualityMSE_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::quality::QualityMSE> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// quality_QualityPSNR (Generic)
//================================================================================

// GetSet (quality_QualityPSNR)



// Methods (quality_QualityPSNR)

static Napi::Value pyopencv_cv_quality_quality_QualityPSNR_clear(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityPSNR> * self1 = 0;
    if (!pyopencv_quality_QualityPSNR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityPSNR' or its derivative)");
    Ptr<cv::quality::QualityPSNR> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityPSNR_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityPSNR> * self1 = 0;
    if (!pyopencv_quality_QualityPSNR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityPSNR' or its derivative)");
    Ptr<cv::quality::QualityPSNR> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    cv::Scalar retval;

    const char* keywords[] = { "cmp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityPSNR.compute", (char**)keywords, &pyobj_cmp) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmp));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    cv::Scalar retval;

    const char* keywords[] = { "cmp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityPSNR.compute", (char**)keywords, &pyobj_cmp) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmp));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    Mat qualityMap;
    Napi::Value* pyobj_maxPixelValue = NULL;
    double maxPixelValue=QualityPSNR::MAX_PIXEL_VALUE_DEFAULT;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", "maxPixelValue", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:quality_QualityPSNR.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap, &pyobj_maxPixelValue) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)) &&
        jsopencv_to_safe(info, pyobj_maxPixelValue, maxPixelValue, ArgInfo("maxPixelValue", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap, maxPixelValue));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    UMat qualityMap;
    Napi::Value* pyobj_maxPixelValue = NULL;
    double maxPixelValue=QualityPSNR::MAX_PIXEL_VALUE_DEFAULT;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", "maxPixelValue", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:quality_QualityPSNR.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap, &pyobj_maxPixelValue) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)) &&
        jsopencv_to_safe(info, pyobj_maxPixelValue, maxPixelValue, ArgInfo("maxPixelValue", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap, maxPixelValue));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityPSNR_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Napi::Value* pyobj_maxPixelValue = NULL;
    double maxPixelValue=QualityPSNR::MAX_PIXEL_VALUE_DEFAULT;
    Ptr<QualityPSNR> retval;

    const char* keywords[] = { "ref", "maxPixelValue", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:quality_QualityPSNR.create", (char**)keywords, &pyobj_ref, &pyobj_maxPixelValue) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_maxPixelValue, maxPixelValue, ArgInfo("maxPixelValue", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityPSNR::create(ref, maxPixelValue));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Napi::Value* pyobj_maxPixelValue = NULL;
    double maxPixelValue=QualityPSNR::MAX_PIXEL_VALUE_DEFAULT;
    Ptr<QualityPSNR> retval;

    const char* keywords[] = { "ref", "maxPixelValue", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:quality_QualityPSNR.create", (char**)keywords, &pyobj_ref, &pyobj_maxPixelValue) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_maxPixelValue, maxPixelValue, ArgInfo("maxPixelValue", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualityPSNR::create(ref, maxPixelValue));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityPSNR_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityPSNR> * self1 = 0;
    if (!pyopencv_quality_QualityPSNR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityPSNR' or its derivative)");
    Ptr<cv::quality::QualityPSNR> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityPSNR_getMaxPixelValue(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityPSNR> * self1 = 0;
    if (!pyopencv_quality_QualityPSNR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityPSNR' or its derivative)");
    Ptr<cv::quality::QualityPSNR> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxPixelValue());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualityPSNR_setMaxPixelValue(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualityPSNR> * self1 = 0;
    if (!pyopencv_quality_QualityPSNR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualityPSNR' or its derivative)");
    Ptr<cv::quality::QualityPSNR> _self_ = *(self1);
    Napi::Value* pyobj_val = NULL;
    double val=0;

    const char* keywords[] = { "val", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualityPSNR.setMaxPixelValue", (char**)keywords, &pyobj_val) &&
        jsopencv_to_safe(info, pyobj_val, val, ArgInfo("val", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxPixelValue(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (quality_QualityPSNR)

static PyGetSetDef pyopencv_quality_QualityPSNR_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_quality_QualityPSNR_methods[] =
{
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityPSNR_clear, 0), "clear() -> None\n.   @brief Implements Algorithm::clear()"},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityPSNR_compute, 0), "compute(cmp) -> retval\n.   @brief Compute the PSNR\n.       @param cmp Comparison image\n.       @returns Per-channel PSNR value, or std::numeric_limits<double>::infinity() if the MSE between the two images == 0\n\n\n\ncompute(ref, cmp[, qualityMap[, maxPixelValue]]) -> retval, qualityMap\n.   @brief static method for computing quality\n.       @param ref reference image\n.       @param cmp comparison image\n.       @param qualityMap output quality map, or cv::noArray()\n.       @param maxPixelValue maximum per-channel value for any individual pixel; eg 255 for uint8 image\n.       @returns PSNR value, or std::numeric_limits<double>::infinity() if the MSE between the two images == 0"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityPSNR_create_static, METH_STATIC), "create(ref[, maxPixelValue]) -> retval\n.   @brief Create an object which calculates quality\n.       @param ref input image to use as the source for comparison\n.       @param maxPixelValue maximum per-channel value for any individual pixel; eg 255 for uint8 image"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityPSNR_empty, 0), "empty() -> retval\n.   @brief Implements Algorithm::empty()"},
    {"getMaxPixelValue", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityPSNR_getMaxPixelValue, 0), "getMaxPixelValue() -> retval\n.   @brief return the maximum pixel value used for PSNR computation"},
    {"setMaxPixelValue", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualityPSNR_setMaxPixelValue, 0), "setMaxPixelValue(val) -> None\n.   @brief sets the maximum pixel value used for PSNR computation\n.       @param val Maximum pixel value"},

    {NULL,          NULL}
};

// Converter (quality_QualityPSNR)

template<>
struct PyOpenCV_Converter< Ptr<cv::quality::QualityPSNR> >
{
    static PyObject* from(const Ptr<cv::quality::QualityPSNR>& r)
    {
        return pyopencv_quality_QualityPSNR_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::quality::QualityPSNR>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::quality::QualityPSNR> * dst_;
        if (pyopencv_quality_QualityPSNR_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::quality::QualityPSNR> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// quality_QualitySSIM (Generic)
//================================================================================

// GetSet (quality_QualitySSIM)



// Methods (quality_QualitySSIM)

static Napi::Value pyopencv_cv_quality_quality_QualitySSIM_clear(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualitySSIM> * self1 = 0;
    if (!pyopencv_quality_QualitySSIM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualitySSIM' or its derivative)");
    Ptr<cv::quality::QualitySSIM> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualitySSIM_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualitySSIM> * self1 = 0;
    if (!pyopencv_quality_QualitySSIM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualitySSIM' or its derivative)");
    Ptr<cv::quality::QualitySSIM> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    cv::Scalar retval;

    const char* keywords[] = { "cmp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualitySSIM.compute", (char**)keywords, &pyobj_cmp) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmp));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    cv::Scalar retval;

    const char* keywords[] = { "cmp", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualitySSIM.compute", (char**)keywords, &pyobj_cmp) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(cmp));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Napi::Value* pyobj_cmp = NULL;
    Mat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    Mat qualityMap;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:quality_QualitySSIM.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Napi::Value* pyobj_cmp = NULL;
    UMat cmp;
    Napi::Value* pyobj_qualityMap = NULL;
    UMat qualityMap;
    cv::Scalar retval;

    const char* keywords[] = { "ref", "cmp", "qualityMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:quality_QualitySSIM.compute", (char**)keywords, &pyobj_ref, &pyobj_cmp, &pyobj_qualityMap) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)) &&
        jsopencv_to_safe(info, pyobj_cmp, cmp, ArgInfo("cmp", 0)) &&
        jsopencv_to_safe(info, pyobj_qualityMap, qualityMap, ArgInfo("qualityMap", 1)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(ref, cmp, qualityMap));
        return Py_BuildValue("(NN)", jsopencv_from(retval), jsopencv_from(qualityMap));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualitySSIM_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_ref = NULL;
    Mat ref;
    Ptr<QualitySSIM> retval;

    const char* keywords[] = { "ref", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualitySSIM.create", (char**)keywords, &pyobj_ref) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualitySSIM::create(ref));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ref = NULL;
    UMat ref;
    Ptr<QualitySSIM> retval;

    const char* keywords[] = { "ref", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:quality_QualitySSIM.create", (char**)keywords, &pyobj_ref) &&
        jsopencv_to_safe(info, pyobj_ref, ref, ArgInfo("ref", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::quality::QualitySSIM::create(ref));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_quality_quality_QualitySSIM_empty(const Napi::CallbackInfo &info)
{
    using namespace cv::quality;


    Ptr<cv::quality::QualitySSIM> * self1 = 0;
    if (!pyopencv_quality_QualitySSIM_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'quality_QualitySSIM' or its derivative)");
    Ptr<cv::quality::QualitySSIM> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->empty());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (quality_QualitySSIM)

static PyGetSetDef pyopencv_quality_QualitySSIM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_quality_QualitySSIM_methods[] =
{
    {"clear", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualitySSIM_clear, 0), "clear() -> None\n.   @brief Implements Algorithm::clear()"},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualitySSIM_compute, 0), "compute(cmp) -> retval\n.   @brief Computes SSIM\n.       @param cmp Comparison image\n.       @returns cv::Scalar with per-channel quality values.  Values range from 0 (worst) to 1 (best)\n\n\n\ncompute(ref, cmp[, qualityMap]) -> retval, qualityMap\n.   @brief static method for computing quality\n.       @param ref reference image\n.       @param cmp comparison image\n.       @param qualityMap output quality map, or cv::noArray()\n.       @returns cv::Scalar with per-channel quality values.  Values range from 0 (worst) to 1 (best)"},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualitySSIM_create_static, METH_STATIC), "create(ref) -> retval\n.   @brief Create an object which calculates quality\n.       @param ref input image to use as the reference image for comparison"},
    {"empty", CV_JS_FN_WITH_KW_(pyopencv_cv_quality_quality_QualitySSIM_empty, 0), "empty() -> retval\n.   @brief Implements Algorithm::empty()"},

    {NULL,          NULL}
};

// Converter (quality_QualitySSIM)

template<>
struct PyOpenCV_Converter< Ptr<cv::quality::QualitySSIM> >
{
    static PyObject* from(const Ptr<cv::quality::QualitySSIM>& r)
    {
        return pyopencv_quality_QualitySSIM_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::quality::QualitySSIM>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::quality::QualitySSIM> * dst_;
        if (pyopencv_quality_QualitySSIM_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::quality::QualitySSIM> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// rapid_GOSTracker (Generic)
//================================================================================

// GetSet (rapid_GOSTracker)



// Methods (rapid_GOSTracker)

static Napi::Value pyopencv_cv_rapid_rapid_GOSTracker_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::rapid;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_pts3d = NULL;
    Mat pts3d;
    Napi::Value* pyobj_tris = NULL;
    Mat tris;
    Napi::Value* pyobj_histBins = NULL;
    int histBins=4;
    Napi::Value* pyobj_sobelThesh = NULL;
    uchar sobelThesh=10;
    Ptr<OLSTracker> retval;

    const char* keywords[] = { "pts3d", "tris", "histBins", "sobelThesh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:rapid_GOSTracker.create", (char**)keywords, &pyobj_pts3d, &pyobj_tris, &pyobj_histBins, &pyobj_sobelThesh) &&
        jsopencv_to_safe(info, pyobj_pts3d, pts3d, ArgInfo("pts3d", 0)) &&
        jsopencv_to_safe(info, pyobj_tris, tris, ArgInfo("tris", 0)) &&
        jsopencv_to_safe(info, pyobj_histBins, histBins, ArgInfo("histBins", 0)) &&
        jsopencv_to_safe(info, pyobj_sobelThesh, sobelThesh, ArgInfo("sobelThesh", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::rapid::GOSTracker::create(pts3d, tris, histBins, sobelThesh));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_pts3d = NULL;
    UMat pts3d;
    Napi::Value* pyobj_tris = NULL;
    UMat tris;
    Napi::Value* pyobj_histBins = NULL;
    int histBins=4;
    Napi::Value* pyobj_sobelThesh = NULL;
    uchar sobelThesh=10;
    Ptr<OLSTracker> retval;

    const char* keywords[] = { "pts3d", "tris", "histBins", "sobelThesh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:rapid_GOSTracker.create", (char**)keywords, &pyobj_pts3d, &pyobj_tris, &pyobj_histBins, &pyobj_sobelThesh) &&
        jsopencv_to_safe(info, pyobj_pts3d, pts3d, ArgInfo("pts3d", 0)) &&
        jsopencv_to_safe(info, pyobj_tris, tris, ArgInfo("tris", 0)) &&
        jsopencv_to_safe(info, pyobj_histBins, histBins, ArgInfo("histBins", 0)) &&
        jsopencv_to_safe(info, pyobj_sobelThesh, sobelThesh, ArgInfo("sobelThesh", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::rapid::GOSTracker::create(pts3d, tris, histBins, sobelThesh));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}



// Tables (rapid_GOSTracker)

static PyGetSetDef pyopencv_rapid_GOSTracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_rapid_GOSTracker_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_rapid_rapid_GOSTracker_create_static, METH_STATIC), "create(pts3d, tris[, histBins[, sobelThesh]]) -> retval\n."},

    {NULL,          NULL}
};

// Converter (rapid_GOSTracker)

template<>
struct PyOpenCV_Converter< Ptr<cv::rapid::GOSTracker> >
{
    static PyObject* from(const Ptr<cv::rapid::GOSTracker>& r)
    {
        return pyopencv_rapid_GOSTracker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::rapid::GOSTracker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::rapid::GOSTracker> * dst_;
        if (pyopencv_rapid_GOSTracker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::rapid::GOSTracker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// rapid_OLSTracker (Generic)
//================================================================================

// GetSet (rapid_OLSTracker)



// Methods (rapid_OLSTracker)

static Napi::Value pyopencv_cv_rapid_rapid_OLSTracker_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::rapid;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_pts3d = NULL;
    Mat pts3d;
    Napi::Value* pyobj_tris = NULL;
    Mat tris;
    Napi::Value* pyobj_histBins = NULL;
    int histBins=8;
    Napi::Value* pyobj_sobelThesh = NULL;
    uchar sobelThesh=10;
    Ptr<OLSTracker> retval;

    const char* keywords[] = { "pts3d", "tris", "histBins", "sobelThesh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:rapid_OLSTracker.create", (char**)keywords, &pyobj_pts3d, &pyobj_tris, &pyobj_histBins, &pyobj_sobelThesh) &&
        jsopencv_to_safe(info, pyobj_pts3d, pts3d, ArgInfo("pts3d", 0)) &&
        jsopencv_to_safe(info, pyobj_tris, tris, ArgInfo("tris", 0)) &&
        jsopencv_to_safe(info, pyobj_histBins, histBins, ArgInfo("histBins", 0)) &&
        jsopencv_to_safe(info, pyobj_sobelThesh, sobelThesh, ArgInfo("sobelThesh", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::rapid::OLSTracker::create(pts3d, tris, histBins, sobelThesh));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_pts3d = NULL;
    UMat pts3d;
    Napi::Value* pyobj_tris = NULL;
    UMat tris;
    Napi::Value* pyobj_histBins = NULL;
    int histBins=8;
    Napi::Value* pyobj_sobelThesh = NULL;
    uchar sobelThesh=10;
    Ptr<OLSTracker> retval;

    const char* keywords[] = { "pts3d", "tris", "histBins", "sobelThesh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:rapid_OLSTracker.create", (char**)keywords, &pyobj_pts3d, &pyobj_tris, &pyobj_histBins, &pyobj_sobelThesh) &&
        jsopencv_to_safe(info, pyobj_pts3d, pts3d, ArgInfo("pts3d", 0)) &&
        jsopencv_to_safe(info, pyobj_tris, tris, ArgInfo("tris", 0)) &&
        jsopencv_to_safe(info, pyobj_histBins, histBins, ArgInfo("histBins", 0)) &&
        jsopencv_to_safe(info, pyobj_sobelThesh, sobelThesh, ArgInfo("sobelThesh", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::rapid::OLSTracker::create(pts3d, tris, histBins, sobelThesh));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}



// Tables (rapid_OLSTracker)

static PyGetSetDef pyopencv_rapid_OLSTracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_rapid_OLSTracker_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_rapid_rapid_OLSTracker_create_static, METH_STATIC), "create(pts3d, tris[, histBins[, sobelThesh]]) -> retval\n."},

    {NULL,          NULL}
};

// Converter (rapid_OLSTracker)

template<>
struct PyOpenCV_Converter< Ptr<cv::rapid::OLSTracker> >
{
    static PyObject* from(const Ptr<cv::rapid::OLSTracker>& r)
    {
        return pyopencv_rapid_OLSTracker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::rapid::OLSTracker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::rapid::OLSTracker> * dst_;
        if (pyopencv_rapid_OLSTracker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::rapid::OLSTracker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// rapid_Rapid (Generic)
//================================================================================

// GetSet (rapid_Rapid)



// Methods (rapid_Rapid)

static Napi::Value pyopencv_cv_rapid_rapid_Rapid_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::rapid;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_pts3d = NULL;
    Mat pts3d;
    Napi::Value* pyobj_tris = NULL;
    Mat tris;
    Ptr<Rapid> retval;

    const char* keywords[] = { "pts3d", "tris", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:rapid_Rapid.create", (char**)keywords, &pyobj_pts3d, &pyobj_tris) &&
        jsopencv_to_safe(info, pyobj_pts3d, pts3d, ArgInfo("pts3d", 0)) &&
        jsopencv_to_safe(info, pyobj_tris, tris, ArgInfo("tris", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::rapid::Rapid::create(pts3d, tris));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_pts3d = NULL;
    UMat pts3d;
    Napi::Value* pyobj_tris = NULL;
    UMat tris;
    Ptr<Rapid> retval;

    const char* keywords[] = { "pts3d", "tris", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:rapid_Rapid.create", (char**)keywords, &pyobj_pts3d, &pyobj_tris) &&
        jsopencv_to_safe(info, pyobj_pts3d, pts3d, ArgInfo("pts3d", 0)) &&
        jsopencv_to_safe(info, pyobj_tris, tris, ArgInfo("tris", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::rapid::Rapid::create(pts3d, tris));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}



// Tables (rapid_Rapid)

static PyGetSetDef pyopencv_rapid_Rapid_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_rapid_Rapid_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_rapid_rapid_Rapid_create_static, METH_STATIC), "create(pts3d, tris) -> retval\n."},

    {NULL,          NULL}
};

// Converter (rapid_Rapid)

template<>
struct PyOpenCV_Converter< Ptr<cv::rapid::Rapid> >
{
    static PyObject* from(const Ptr<cv::rapid::Rapid>& r)
    {
        return pyopencv_rapid_Rapid_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::rapid::Rapid>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::rapid::Rapid> * dst_;
        if (pyopencv_rapid_Rapid_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::rapid::Rapid> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// rapid_Tracker (Generic)
//================================================================================

// GetSet (rapid_Tracker)



// Methods (rapid_Tracker)

static Napi::Value pyopencv_cv_rapid_rapid_Tracker_clearState(const Napi::CallbackInfo &info)
{
    using namespace cv::rapid;


    Ptr<cv::rapid::Tracker> * self1 = 0;
    if (!pyopencv_rapid_Tracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'rapid_Tracker' or its derivative)");
    Ptr<cv::rapid::Tracker> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clearState());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_rapid_rapid_Tracker_compute(const Napi::CallbackInfo &info)
{
    using namespace cv::rapid;


    Ptr<cv::rapid::Tracker> * self1 = 0;
    if (!pyopencv_rapid_Tracker_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'rapid_Tracker' or its derivative)");
    Ptr<cv::rapid::Tracker> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_num = NULL;
    int num=0;
    Napi::Value* pyobj_len = NULL;
    int len=0;
    Napi::Value* pyobj_K = NULL;
    Mat K;
    Napi::Value* pyobj_rvec = NULL;
    Mat rvec;
    Napi::Value* pyobj_tvec = NULL;
    Mat tvec;
    Napi::Value* pyobj_termcrit = NULL;
    TermCriteria termcrit=TermCriteria(TermCriteria::MAX_ITER | TermCriteria::EPS, 5, 1.5);
    float retval;

    const char* keywords[] = { "img", "num", "len", "K", "rvec", "tvec", "termcrit", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOOO|O:rapid_Tracker.compute", (char**)keywords, &pyobj_img, &pyobj_num, &pyobj_len, &pyobj_K, &pyobj_rvec, &pyobj_tvec, &pyobj_termcrit) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_num, num, ArgInfo("num", 0)) &&
        jsopencv_to_safe(info, pyobj_len, len, ArgInfo("len", 0)) &&
        jsopencv_to_safe(info, pyobj_K, K, ArgInfo("K", 0)) &&
        jsopencv_to_safe(info, pyobj_rvec, rvec, ArgInfo("rvec", 1)) &&
        jsopencv_to_safe(info, pyobj_tvec, tvec, ArgInfo("tvec", 1)) &&
        jsopencv_to_safe(info, pyobj_termcrit, termcrit, ArgInfo("termcrit", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img, num, len, K, rvec, tvec, termcrit));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(rvec), jsopencv_from(tvec));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_num = NULL;
    int num=0;
    Napi::Value* pyobj_len = NULL;
    int len=0;
    Napi::Value* pyobj_K = NULL;
    UMat K;
    Napi::Value* pyobj_rvec = NULL;
    UMat rvec;
    Napi::Value* pyobj_tvec = NULL;
    UMat tvec;
    Napi::Value* pyobj_termcrit = NULL;
    TermCriteria termcrit=TermCriteria(TermCriteria::MAX_ITER | TermCriteria::EPS, 5, 1.5);
    float retval;

    const char* keywords[] = { "img", "num", "len", "K", "rvec", "tvec", "termcrit", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOOO|O:rapid_Tracker.compute", (char**)keywords, &pyobj_img, &pyobj_num, &pyobj_len, &pyobj_K, &pyobj_rvec, &pyobj_tvec, &pyobj_termcrit) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_num, num, ArgInfo("num", 0)) &&
        jsopencv_to_safe(info, pyobj_len, len, ArgInfo("len", 0)) &&
        jsopencv_to_safe(info, pyobj_K, K, ArgInfo("K", 0)) &&
        jsopencv_to_safe(info, pyobj_rvec, rvec, ArgInfo("rvec", 1)) &&
        jsopencv_to_safe(info, pyobj_tvec, tvec, ArgInfo("tvec", 1)) &&
        jsopencv_to_safe(info, pyobj_termcrit, termcrit, ArgInfo("termcrit", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->compute(img, num, len, K, rvec, tvec, termcrit));
        return Py_BuildValue("(NNN)", jsopencv_from(retval), jsopencv_from(rvec), jsopencv_from(tvec));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("compute");

    return NULL;
}



// Tables (rapid_Tracker)

static PyGetSetDef pyopencv_rapid_Tracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_rapid_Tracker_methods[] =
{
    {"clearState", CV_JS_FN_WITH_KW_(pyopencv_cv_rapid_rapid_Tracker_clearState, 0), "clearState() -> None\n."},
    {"compute", CV_JS_FN_WITH_KW_(pyopencv_cv_rapid_rapid_Tracker_compute, 0), "compute(img, num, len, K, rvec, tvec[, termcrit]) -> retval, rvec, tvec\n."},

    {NULL,          NULL}
};

// Converter (rapid_Tracker)

template<>
struct PyOpenCV_Converter< Ptr<cv::rapid::Tracker> >
{
    static PyObject* from(const Ptr<cv::rapid::Tracker>& r)
    {
        return pyopencv_rapid_Tracker_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::rapid::Tracker>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::rapid::Tracker> * dst_;
        if (pyopencv_rapid_Tracker_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::rapid::Tracker> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// segmentation_IntelligentScissorsMB (Generic)
//================================================================================

// GetSet (segmentation_IntelligentScissorsMB)



// Methods (segmentation_IntelligentScissorsMB)

static int pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_IntelligentScissorsMB(pyopencv_segmentation_IntelligentScissorsMB_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::segmentation;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::segmentation::IntelligentScissorsMB());
        return 0;
    }

    return -1;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_applyImage(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:segmentation_IntelligentScissorsMB.applyImage", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->applyImage(image));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:segmentation_IntelligentScissorsMB.applyImage", (char**)keywords, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->applyImage(image));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("applyImage");

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_applyImageFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_non_edge = NULL;
    Mat non_edge;
    Napi::Value* pyobj_gradient_direction = NULL;
    Mat gradient_direction;
    Napi::Value* pyobj_gradient_magnitude = NULL;
    Mat gradient_magnitude;
    Napi::Value* pyobj_image = NULL;
    Mat image;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "non_edge", "gradient_direction", "gradient_magnitude", "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:segmentation_IntelligentScissorsMB.applyImageFeatures", (char**)keywords, &pyobj_non_edge, &pyobj_gradient_direction, &pyobj_gradient_magnitude, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_non_edge, non_edge, ArgInfo("non_edge", 0)) &&
        jsopencv_to_safe(info, pyobj_gradient_direction, gradient_direction, ArgInfo("gradient_direction", 0)) &&
        jsopencv_to_safe(info, pyobj_gradient_magnitude, gradient_magnitude, ArgInfo("gradient_magnitude", 0)) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->applyImageFeatures(non_edge, gradient_direction, gradient_magnitude, image));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_non_edge = NULL;
    UMat non_edge;
    Napi::Value* pyobj_gradient_direction = NULL;
    UMat gradient_direction;
    Napi::Value* pyobj_gradient_magnitude = NULL;
    UMat gradient_magnitude;
    Napi::Value* pyobj_image = NULL;
    UMat image;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "non_edge", "gradient_direction", "gradient_magnitude", "image", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:segmentation_IntelligentScissorsMB.applyImageFeatures", (char**)keywords, &pyobj_non_edge, &pyobj_gradient_direction, &pyobj_gradient_magnitude, &pyobj_image) &&
        jsopencv_to_safe(info, pyobj_non_edge, non_edge, ArgInfo("non_edge", 0)) &&
        jsopencv_to_safe(info, pyobj_gradient_direction, gradient_direction, ArgInfo("gradient_direction", 0)) &&
        jsopencv_to_safe(info, pyobj_gradient_magnitude, gradient_magnitude, ArgInfo("gradient_magnitude", 0)) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->applyImageFeatures(non_edge, gradient_direction, gradient_magnitude, image));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("applyImageFeatures");

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_buildMap(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    Napi::Value* pyobj_sourcePt = NULL;
    Point sourcePt;

    const char* keywords[] = { "sourcePt", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:segmentation_IntelligentScissorsMB.buildMap", (char**)keywords, &pyobj_sourcePt) &&
        jsopencv_to_safe(info, pyobj_sourcePt, sourcePt, ArgInfo("sourcePt", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->buildMap(sourcePt));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_getContour(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_targetPt = NULL;
    Point targetPt;
    Napi::Value* pyobj_contour = NULL;
    Mat contour;
    Napi::Value* pyobj_backward = NULL;
    bool backward=false;

    const char* keywords[] = { "targetPt", "contour", "backward", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:segmentation_IntelligentScissorsMB.getContour", (char**)keywords, &pyobj_targetPt, &pyobj_contour, &pyobj_backward) &&
        jsopencv_to_safe(info, pyobj_targetPt, targetPt, ArgInfo("targetPt", 0)) &&
        jsopencv_to_safe(info, pyobj_contour, contour, ArgInfo("contour", 1)) &&
        jsopencv_to_safe(info, pyobj_backward, backward, ArgInfo("backward", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getContour(targetPt, contour, backward));
        return jsopencv_from(contour);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_targetPt = NULL;
    Point targetPt;
    Napi::Value* pyobj_contour = NULL;
    UMat contour;
    Napi::Value* pyobj_backward = NULL;
    bool backward=false;

    const char* keywords[] = { "targetPt", "contour", "backward", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:segmentation_IntelligentScissorsMB.getContour", (char**)keywords, &pyobj_targetPt, &pyobj_contour, &pyobj_backward) &&
        jsopencv_to_safe(info, pyobj_targetPt, targetPt, ArgInfo("targetPt", 0)) &&
        jsopencv_to_safe(info, pyobj_contour, contour, ArgInfo("contour", 1)) &&
        jsopencv_to_safe(info, pyobj_backward, backward, ArgInfo("backward", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getContour(targetPt, contour, backward));
        return jsopencv_from(contour);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getContour");

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setEdgeFeatureCannyParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    Napi::Value* pyobj_threshold1 = NULL;
    double threshold1=0;
    Napi::Value* pyobj_threshold2 = NULL;
    double threshold2=0;
    Napi::Value* pyobj_apertureSize = NULL;
    int apertureSize=3;
    Napi::Value* pyobj_L2gradient = NULL;
    bool L2gradient=false;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "threshold1", "threshold2", "apertureSize", "L2gradient", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:segmentation_IntelligentScissorsMB.setEdgeFeatureCannyParameters", (char**)keywords, &pyobj_threshold1, &pyobj_threshold2, &pyobj_apertureSize, &pyobj_L2gradient) &&
        jsopencv_to_safe(info, pyobj_threshold1, threshold1, ArgInfo("threshold1", 0)) &&
        jsopencv_to_safe(info, pyobj_threshold2, threshold2, ArgInfo("threshold2", 0)) &&
        jsopencv_to_safe(info, pyobj_apertureSize, apertureSize, ArgInfo("apertureSize", 0)) &&
        jsopencv_to_safe(info, pyobj_L2gradient, L2gradient, ArgInfo("L2gradient", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setEdgeFeatureCannyParameters(threshold1, threshold2, apertureSize, L2gradient));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setEdgeFeatureZeroCrossingParameters(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    Napi::Value* pyobj_gradient_magnitude_min_value = NULL;
    float gradient_magnitude_min_value=0.0f;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "gradient_magnitude_min_value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:segmentation_IntelligentScissorsMB.setEdgeFeatureZeroCrossingParameters", (char**)keywords, &pyobj_gradient_magnitude_min_value) &&
        jsopencv_to_safe(info, pyobj_gradient_magnitude_min_value, gradient_magnitude_min_value, ArgInfo("gradient_magnitude_min_value", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setEdgeFeatureZeroCrossingParameters(gradient_magnitude_min_value));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setGradientMagnitudeMaxLimit(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    Napi::Value* pyobj_gradient_magnitude_threshold_max = NULL;
    float gradient_magnitude_threshold_max=0.0f;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "gradient_magnitude_threshold_max", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:segmentation_IntelligentScissorsMB.setGradientMagnitudeMaxLimit", (char**)keywords, &pyobj_gradient_magnitude_threshold_max) &&
        jsopencv_to_safe(info, pyobj_gradient_magnitude_threshold_max, gradient_magnitude_threshold_max, ArgInfo("gradient_magnitude_threshold_max", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setGradientMagnitudeMaxLimit(gradient_magnitude_threshold_max));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::segmentation;


    cv::segmentation::IntelligentScissorsMB * self1 = 0;
    if (!pyopencv_segmentation_IntelligentScissorsMB_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'segmentation_IntelligentScissorsMB' or its derivative)");
    cv::segmentation::IntelligentScissorsMB* _self_ = (self1);
    Napi::Value* pyobj_weight_non_edge = NULL;
    float weight_non_edge=0.f;
    Napi::Value* pyobj_weight_gradient_direction = NULL;
    float weight_gradient_direction=0.f;
    Napi::Value* pyobj_weight_gradient_magnitude = NULL;
    float weight_gradient_magnitude=0.f;
    IntelligentScissorsMB retval;

    const char* keywords[] = { "weight_non_edge", "weight_gradient_direction", "weight_gradient_magnitude", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:segmentation_IntelligentScissorsMB.setWeights", (char**)keywords, &pyobj_weight_non_edge, &pyobj_weight_gradient_direction, &pyobj_weight_gradient_magnitude) &&
        jsopencv_to_safe(info, pyobj_weight_non_edge, weight_non_edge, ArgInfo("weight_non_edge", 0)) &&
        jsopencv_to_safe(info, pyobj_weight_gradient_direction, weight_gradient_direction, ArgInfo("weight_gradient_direction", 0)) &&
        jsopencv_to_safe(info, pyobj_weight_gradient_magnitude, weight_gradient_magnitude, ArgInfo("weight_gradient_magnitude", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->setWeights(weight_non_edge, weight_gradient_direction, weight_gradient_magnitude));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (segmentation_IntelligentScissorsMB)

static PyGetSetDef pyopencv_segmentation_IntelligentScissorsMB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_segmentation_IntelligentScissorsMB_methods[] =
{
    {"applyImage", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_applyImage, 0), "applyImage(image) -> retval\n.   @brief Specify input image and extract image features\n.        *\n.        * @param image input image. Type is #CV_8UC1 / #CV_8UC3"},
    {"applyImageFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_applyImageFeatures, 0), "applyImageFeatures(non_edge, gradient_direction, gradient_magnitude[, image]) -> retval\n.   @brief Specify custom features of input image\n.        *\n.        * Customized advanced variant of applyImage() call.\n.        *\n.        * @param non_edge Specify cost of non-edge pixels. Type is CV_8UC1. Expected values are `{0, 1}`.\n.        * @param gradient_direction Specify gradient direction feature. Type is CV_32FC2. Values are expected to be normalized: `x^2 + y^2 == 1`\n.        * @param gradient_magnitude Specify cost of gradient magnitude function: Type is CV_32FC1. Values should be in range `[0, 1]`.\n.        * @param image **Optional parameter**. Must be specified if subset of features is specified (non-specified features are calculated internally)"},
    {"buildMap", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_buildMap, 0), "buildMap(sourcePt) -> None\n.   @brief Prepares a map of optimal paths for the given source point on the image\n.        *\n.        * @note applyImage() / applyImageFeatures() must be called before this call\n.        *\n.        * @param sourcePt The source point used to find the paths"},
    {"getContour", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_getContour, 0), "getContour(targetPt[, contour[, backward]]) -> contour\n.   @brief Extracts optimal contour for the given target point on the image\n.        *\n.        * @note buildMap() must be called before this call\n.        *\n.        * @param targetPt The target point\n.        * @param[out] contour The list of pixels which contains optimal path between the source and the target points of the image. Type is CV_32SC2 (compatible with `std::vector<Point>`)\n.        * @param backward Flag to indicate reverse order of retrived pixels (use \"true\" value to fetch points from the target to the source point)"},
    {"setEdgeFeatureCannyParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setEdgeFeatureCannyParameters, 0), "setEdgeFeatureCannyParameters(threshold1, threshold2[, apertureSize[, L2gradient]]) -> retval\n.   @brief Switch edge feature extractor to use Canny edge detector\n.        *\n.        * @note \"Laplacian Zero-Crossing\" feature extractor is used by default (following to original article)\n.        *\n.        * @sa Canny"},
    {"setEdgeFeatureZeroCrossingParameters", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setEdgeFeatureZeroCrossingParameters, 0), "setEdgeFeatureZeroCrossingParameters([, gradient_magnitude_min_value]) -> retval\n.   @brief Switch to \"Laplacian Zero-Crossing\" edge feature extractor and specify its parameters\n.        *\n.        * This feature extractor is used by default according to article.\n.        *\n.        * Implementation has additional filtering for regions with low-amplitude noise.\n.        * This filtering is enabled through parameter of minimal gradient amplitude (use some small value 4, 8, 16).\n.        *\n.        * @note Current implementation of this feature extractor is based on processing of grayscale images (color image is converted to grayscale image first).\n.        *\n.        * @note Canny edge detector is a bit slower, but provides better results (especially on color images): use setEdgeFeatureCannyParameters().\n.        *\n.        * @param gradient_magnitude_min_value Minimal gradient magnitude value for edge pixels (default: 0, check is disabled)"},
    {"setGradientMagnitudeMaxLimit", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setGradientMagnitudeMaxLimit, 0), "setGradientMagnitudeMaxLimit([, gradient_magnitude_threshold_max]) -> retval\n.   @brief Specify gradient magnitude max value threshold\n.        *\n.        * Zero limit value is used to disable gradient magnitude thresholding (default behavior, as described in original article).\n.        * Otherwize pixels with `gradient magnitude >= threshold` have zero cost.\n.        *\n.        * @note Thresholding should be used for images with irregular regions (to avoid stuck on parameters from high-contract areas, like embedded logos).\n.        *\n.        * @param gradient_magnitude_threshold_max Specify gradient magnitude max value threshold (default: 0, disabled)"},
    {"setWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_segmentation_segmentation_IntelligentScissorsMB_setWeights, 0), "setWeights(weight_non_edge, weight_gradient_direction, weight_gradient_magnitude) -> retval\n.   @brief Specify weights of feature functions\n.        *\n.        * Consider keeping weights normalized (sum of weights equals to 1.0)\n.        * Discrete dynamic programming (DP) goal is minimization of costs between pixels.\n.        *\n.        * @param weight_non_edge Specify cost of non-edge pixels (default: 0.43f)\n.        * @param weight_gradient_direction Specify cost of gradient direction function (default: 0.43f)\n.        * @param weight_gradient_magnitude Specify cost of gradient magnitude function (default: 0.14f)"},

    {NULL,          NULL}
};

// Converter (segmentation_IntelligentScissorsMB)

template<>
struct PyOpenCV_Converter< cv::segmentation::IntelligentScissorsMB >
{
    static PyObject* from(const cv::segmentation::IntelligentScissorsMB& r)
    {
        return pyopencv_segmentation_IntelligentScissorsMB_Instance(r);
    }
    static bool to(PyObject* src, cv::segmentation::IntelligentScissorsMB& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::segmentation::IntelligentScissorsMB * dst_;
        if (pyopencv_segmentation_IntelligentScissorsMB_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::segmentation::IntelligentScissorsMB for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_BaseOCR (Generic)
//================================================================================

// GetSet (text_BaseOCR)



// Methods (text_BaseOCR)



// Tables (text_BaseOCR)

static PyGetSetDef pyopencv_text_BaseOCR_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_BaseOCR_methods[] =
{

    {NULL,          NULL}
};

// Converter (text_BaseOCR)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::BaseOCR> >
{
    static PyObject* from(const Ptr<cv::text::BaseOCR>& r)
    {
        return pyopencv_text_BaseOCR_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::BaseOCR>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::BaseOCR> * dst_;
        if (pyopencv_text_BaseOCR_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::BaseOCR> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_ERFilter (Generic)
//================================================================================

// GetSet (text_ERFilter)



// Methods (text_ERFilter)



// Tables (text_ERFilter)

static PyGetSetDef pyopencv_text_ERFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_ERFilter_methods[] =
{

    {NULL,          NULL}
};

// Converter (text_ERFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::ERFilter> >
{
    static PyObject* from(const Ptr<cv::text::ERFilter>& r)
    {
        return pyopencv_text_ERFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::ERFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::ERFilter> * dst_;
        if (pyopencv_text_ERFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::ERFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_ERFilter_Callback (Generic)
//================================================================================

// GetSet (text_ERFilter_Callback)



// Methods (text_ERFilter_Callback)



// Tables (text_ERFilter_Callback)

static PyGetSetDef pyopencv_text_ERFilter_Callback_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_ERFilter_Callback_methods[] =
{

    {NULL,          NULL}
};

// Converter (text_ERFilter_Callback)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::ERFilter::Callback> >
{
    static PyObject* from(const Ptr<cv::text::ERFilter::Callback>& r)
    {
        return pyopencv_text_ERFilter_Callback_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::ERFilter::Callback>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::ERFilter::Callback> * dst_;
        if (pyopencv_text_ERFilter_Callback_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::ERFilter::Callback> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_OCRBeamSearchDecoder (Generic)
//================================================================================

// GetSet (text_OCRBeamSearchDecoder)



// Methods (text_OCRBeamSearchDecoder)

static Napi::Value pyopencv_cv_text_text_OCRBeamSearchDecoder_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::text;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_classifier = NULL;
    Ptr<OCRBeamSearchDecoder::ClassifierCallback> classifier;
    Napi::Value* pyobj_vocabulary = NULL;
    std::string vocabulary;
    Napi::Value* pyobj_transition_probabilities_table = NULL;
    Mat transition_probabilities_table;
    Napi::Value* pyobj_emission_probabilities_table = NULL;
    Mat emission_probabilities_table;
    Napi::Value* pyobj_mode = NULL;
    text_decoder_mode mode=OCR_DECODER_VITERBI;
    Napi::Value* pyobj_beam_size = NULL;
    int beam_size=500;
    Ptr<OCRBeamSearchDecoder> retval;

    const char* keywords[] = { "classifier", "vocabulary", "transition_probabilities_table", "emission_probabilities_table", "mode", "beam_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OO:text_OCRBeamSearchDecoder.create", (char**)keywords, &pyobj_classifier, &pyobj_vocabulary, &pyobj_transition_probabilities_table, &pyobj_emission_probabilities_table, &pyobj_mode, &pyobj_beam_size) &&
        jsopencv_to_safe(info, pyobj_classifier, classifier, ArgInfo("classifier", 0)) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) &&
        jsopencv_to_safe(info, pyobj_transition_probabilities_table, transition_probabilities_table, ArgInfo("transition_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_emission_probabilities_table, emission_probabilities_table, ArgInfo("emission_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)) &&
        jsopencv_to_safe(info, pyobj_beam_size, beam_size, ArgInfo("beam_size", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRBeamSearchDecoder::create(classifier, vocabulary, transition_probabilities_table, emission_probabilities_table, mode, beam_size));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_classifier = NULL;
    Ptr<OCRBeamSearchDecoder::ClassifierCallback> classifier;
    Napi::Value* pyobj_vocabulary = NULL;
    std::string vocabulary;
    Napi::Value* pyobj_transition_probabilities_table = NULL;
    UMat transition_probabilities_table;
    Napi::Value* pyobj_emission_probabilities_table = NULL;
    UMat emission_probabilities_table;
    Napi::Value* pyobj_mode = NULL;
    text_decoder_mode mode=OCR_DECODER_VITERBI;
    Napi::Value* pyobj_beam_size = NULL;
    int beam_size=500;
    Ptr<OCRBeamSearchDecoder> retval;

    const char* keywords[] = { "classifier", "vocabulary", "transition_probabilities_table", "emission_probabilities_table", "mode", "beam_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OO:text_OCRBeamSearchDecoder.create", (char**)keywords, &pyobj_classifier, &pyobj_vocabulary, &pyobj_transition_probabilities_table, &pyobj_emission_probabilities_table, &pyobj_mode, &pyobj_beam_size) &&
        jsopencv_to_safe(info, pyobj_classifier, classifier, ArgInfo("classifier", 0)) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) &&
        jsopencv_to_safe(info, pyobj_transition_probabilities_table, transition_probabilities_table, ArgInfo("transition_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_emission_probabilities_table, emission_probabilities_table, ArgInfo("emission_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)) &&
        jsopencv_to_safe(info, pyobj_beam_size, beam_size, ArgInfo("beam_size", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRBeamSearchDecoder::create(classifier, vocabulary, transition_probabilities_table, emission_probabilities_table, mode, beam_size));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_text_text_OCRBeamSearchDecoder_run(const Napi::CallbackInfo &info)
{
    using namespace cv::text;


    Ptr<cv::text::OCRBeamSearchDecoder> * self1 = 0;
    if (!pyopencv_text_OCRBeamSearchDecoder_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'text_OCRBeamSearchDecoder' or its derivative)");
    Ptr<cv::text::OCRBeamSearchDecoder> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:text_OCRBeamSearchDecoder.run", (char**)keywords, &pyobj_image, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:text_OCRBeamSearchDecoder.run", (char**)keywords, &pyobj_image, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "mask", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:text_OCRBeamSearchDecoder.run", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, mask, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "mask", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:text_OCRBeamSearchDecoder.run", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, mask, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("run");

    return NULL;
}



// Tables (text_OCRBeamSearchDecoder)

static PyGetSetDef pyopencv_text_OCRBeamSearchDecoder_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_OCRBeamSearchDecoder_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRBeamSearchDecoder_create_static, METH_STATIC), "create(classifier, vocabulary, transition_probabilities_table, emission_probabilities_table[, mode[, beam_size]]) -> retval\n.   @brief Creates an instance of the OCRBeamSearchDecoder class. Initializes HMMDecoder.\n.   \n.       @param classifier The character classifier with built in feature extractor.\n.   \n.       @param vocabulary The language vocabulary (chars when ASCII English text). vocabulary.size()\n.       must be equal to the number of classes of the classifier.\n.   \n.       @param transition_probabilities_table Table with transition probabilities between character\n.       pairs. cols == rows == vocabulary.size().\n.   \n.       @param emission_probabilities_table Table with observation emission probabilities. cols ==\n.       rows == vocabulary.size().\n.   \n.       @param mode HMM Decoding algorithm. Only OCR_DECODER_VITERBI is available for the moment\n.       (<http://en.wikipedia.org/wiki/Viterbi_algorithm>).\n.   \n.       @param beam_size Size of the beam in Beam Search algorithm."},
    {"run", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRBeamSearchDecoder_run, 0), "run(image, min_confidence[, component_level]) -> retval\n.   @brief Recognize text using Beam Search.\n.   \n.       Takes image on input and returns recognized text in the output_text parameter. Optionally\n.       provides also the Rects for individual text elements found (e.g. words), and the list of those\n.       text elements with their confidence values.\n.   \n.       @param image Input binary image CV_8UC1 with a single text line (or word).\n.   \n.       @param output_text Output text. Most likely character sequence found by the HMM decoder.\n.   \n.       @param component_rects If provided the method will output a list of Rects for the individual\n.       text elements found (e.g. words).\n.   \n.       @param component_texts If provided the method will output a list of text strings for the\n.       recognition of individual text elements found (e.g. words).\n.   \n.       @param component_confidences If provided the method will output a list of confidence values\n.       for the recognition of individual text elements found (e.g. words).\n.   \n.       @param component_level Only OCR_LEVEL_WORD is supported.\n\n\n\nrun(image, mask, min_confidence[, component_level]) -> retval\n."},

    {NULL,          NULL}
};

// Converter (text_OCRBeamSearchDecoder)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::OCRBeamSearchDecoder> >
{
    static PyObject* from(const Ptr<cv::text::OCRBeamSearchDecoder>& r)
    {
        return pyopencv_text_OCRBeamSearchDecoder_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::OCRBeamSearchDecoder>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::OCRBeamSearchDecoder> * dst_;
        if (pyopencv_text_OCRBeamSearchDecoder_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::OCRBeamSearchDecoder> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_OCRBeamSearchDecoder_ClassifierCallback (Generic)
//================================================================================

// GetSet (text_OCRBeamSearchDecoder_ClassifierCallback)



// Methods (text_OCRBeamSearchDecoder_ClassifierCallback)



// Tables (text_OCRBeamSearchDecoder_ClassifierCallback)

static PyGetSetDef pyopencv_text_OCRBeamSearchDecoder_ClassifierCallback_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_OCRBeamSearchDecoder_ClassifierCallback_methods[] =
{

    {NULL,          NULL}
};

// Converter (text_OCRBeamSearchDecoder_ClassifierCallback)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::OCRBeamSearchDecoder::ClassifierCallback> >
{
    static PyObject* from(const Ptr<cv::text::OCRBeamSearchDecoder::ClassifierCallback>& r)
    {
        return pyopencv_text_OCRBeamSearchDecoder_ClassifierCallback_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::OCRBeamSearchDecoder::ClassifierCallback>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::OCRBeamSearchDecoder::ClassifierCallback> * dst_;
        if (pyopencv_text_OCRBeamSearchDecoder_ClassifierCallback_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::OCRBeamSearchDecoder::ClassifierCallback> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_OCRHMMDecoder (Generic)
//================================================================================

// GetSet (text_OCRHMMDecoder)



// Methods (text_OCRHMMDecoder)

static Napi::Value pyopencv_cv_text_text_OCRHMMDecoder_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::text;

    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_classifier = NULL;
    Ptr<OCRHMMDecoder::ClassifierCallback> classifier;
    Napi::Value* pyobj_vocabulary = NULL;
    String vocabulary;
    Napi::Value* pyobj_transition_probabilities_table = NULL;
    Mat transition_probabilities_table;
    Napi::Value* pyobj_emission_probabilities_table = NULL;
    Mat emission_probabilities_table;
    Napi::Value* pyobj_mode = NULL;
    int mode=OCR_DECODER_VITERBI;
    Ptr<OCRHMMDecoder> retval;

    const char* keywords[] = { "classifier", "vocabulary", "transition_probabilities_table", "emission_probabilities_table", "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:text_OCRHMMDecoder.create", (char**)keywords, &pyobj_classifier, &pyobj_vocabulary, &pyobj_transition_probabilities_table, &pyobj_emission_probabilities_table, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_classifier, classifier, ArgInfo("classifier", 0)) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) &&
        jsopencv_to_safe(info, pyobj_transition_probabilities_table, transition_probabilities_table, ArgInfo("transition_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_emission_probabilities_table, emission_probabilities_table, ArgInfo("emission_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRHMMDecoder::create(classifier, vocabulary, transition_probabilities_table, emission_probabilities_table, mode));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_classifier = NULL;
    Ptr<OCRHMMDecoder::ClassifierCallback> classifier;
    Napi::Value* pyobj_vocabulary = NULL;
    String vocabulary;
    Napi::Value* pyobj_transition_probabilities_table = NULL;
    UMat transition_probabilities_table;
    Napi::Value* pyobj_emission_probabilities_table = NULL;
    UMat emission_probabilities_table;
    Napi::Value* pyobj_mode = NULL;
    int mode=OCR_DECODER_VITERBI;
    Ptr<OCRHMMDecoder> retval;

    const char* keywords[] = { "classifier", "vocabulary", "transition_probabilities_table", "emission_probabilities_table", "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:text_OCRHMMDecoder.create", (char**)keywords, &pyobj_classifier, &pyobj_vocabulary, &pyobj_transition_probabilities_table, &pyobj_emission_probabilities_table, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_classifier, classifier, ArgInfo("classifier", 0)) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) &&
        jsopencv_to_safe(info, pyobj_transition_probabilities_table, transition_probabilities_table, ArgInfo("transition_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_emission_probabilities_table, emission_probabilities_table, ArgInfo("emission_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRHMMDecoder::create(classifier, vocabulary, transition_probabilities_table, emission_probabilities_table, mode));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_vocabulary = NULL;
    String vocabulary;
    Napi::Value* pyobj_transition_probabilities_table = NULL;
    Mat transition_probabilities_table;
    Napi::Value* pyobj_emission_probabilities_table = NULL;
    Mat emission_probabilities_table;
    Napi::Value* pyobj_mode = NULL;
    int mode=OCR_DECODER_VITERBI;
    Napi::Value* pyobj_classifier = NULL;
    int classifier=OCR_KNN_CLASSIFIER;
    Ptr<OCRHMMDecoder> retval;

    const char* keywords[] = { "filename", "vocabulary", "transition_probabilities_table", "emission_probabilities_table", "mode", "classifier", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OO:text_OCRHMMDecoder.create", (char**)keywords, &pyobj_filename, &pyobj_vocabulary, &pyobj_transition_probabilities_table, &pyobj_emission_probabilities_table, &pyobj_mode, &pyobj_classifier) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) &&
        jsopencv_to_safe(info, pyobj_transition_probabilities_table, transition_probabilities_table, ArgInfo("transition_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_emission_probabilities_table, emission_probabilities_table, ArgInfo("emission_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)) &&
        jsopencv_to_safe(info, pyobj_classifier, classifier, ArgInfo("classifier", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRHMMDecoder::create(filename, vocabulary, transition_probabilities_table, emission_probabilities_table, mode, classifier));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_filename = NULL;
    String filename;
    Napi::Value* pyobj_vocabulary = NULL;
    String vocabulary;
    Napi::Value* pyobj_transition_probabilities_table = NULL;
    UMat transition_probabilities_table;
    Napi::Value* pyobj_emission_probabilities_table = NULL;
    UMat emission_probabilities_table;
    Napi::Value* pyobj_mode = NULL;
    int mode=OCR_DECODER_VITERBI;
    Napi::Value* pyobj_classifier = NULL;
    int classifier=OCR_KNN_CLASSIFIER;
    Ptr<OCRHMMDecoder> retval;

    const char* keywords[] = { "filename", "vocabulary", "transition_probabilities_table", "emission_probabilities_table", "mode", "classifier", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|OO:text_OCRHMMDecoder.create", (char**)keywords, &pyobj_filename, &pyobj_vocabulary, &pyobj_transition_probabilities_table, &pyobj_emission_probabilities_table, &pyobj_mode, &pyobj_classifier) &&
        jsopencv_to_safe(info, pyobj_filename, filename, ArgInfo("filename", 0)) &&
        jsopencv_to_safe(info, pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) &&
        jsopencv_to_safe(info, pyobj_transition_probabilities_table, transition_probabilities_table, ArgInfo("transition_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_emission_probabilities_table, emission_probabilities_table, ArgInfo("emission_probabilities_table", 0)) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)) &&
        jsopencv_to_safe(info, pyobj_classifier, classifier, ArgInfo("classifier", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRHMMDecoder::create(filename, vocabulary, transition_probabilities_table, emission_probabilities_table, mode, classifier));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_text_text_OCRHMMDecoder_run(const Napi::CallbackInfo &info)
{
    using namespace cv::text;


    Ptr<cv::text::OCRHMMDecoder> * self1 = 0;
    if (!pyopencv_text_OCRHMMDecoder_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'text_OCRHMMDecoder' or its derivative)");
    Ptr<cv::text::OCRHMMDecoder> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:text_OCRHMMDecoder.run", (char**)keywords, &pyobj_image, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:text_OCRHMMDecoder.run", (char**)keywords, &pyobj_image, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "mask", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:text_OCRHMMDecoder.run", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, mask, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "mask", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:text_OCRHMMDecoder.run", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, mask, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("run");

    return NULL;
}



// Tables (text_OCRHMMDecoder)

static PyGetSetDef pyopencv_text_OCRHMMDecoder_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_OCRHMMDecoder_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRHMMDecoder_create_static, METH_STATIC), "create(classifier, vocabulary, transition_probabilities_table, emission_probabilities_table[, mode]) -> retval\n.   @brief Creates an instance of the OCRHMMDecoder class. Initializes HMMDecoder.\n.   \n.       @param classifier The character classifier with built in feature extractor.\n.   \n.       @param vocabulary The language vocabulary (chars when ascii english text). vocabulary.size()\n.       must be equal to the number of classes of the classifier.\n.   \n.       @param transition_probabilities_table Table with transition probabilities between character\n.       pairs. cols == rows == vocabulary.size().\n.   \n.       @param emission_probabilities_table Table with observation emission probabilities. cols ==\n.       rows == vocabulary.size().\n.   \n.       @param mode HMM Decoding algorithm. Only OCR_DECODER_VITERBI is available for the moment\n.       (<http://en.wikipedia.org/wiki/Viterbi_algorithm>).\n\n\n\ncreate(filename, vocabulary, transition_probabilities_table, emission_probabilities_table[, mode[, classifier]]) -> retval\n.   @brief Creates an instance of the OCRHMMDecoder class. Loads and initializes HMMDecoder from the specified path\n.   \n.        @overload"},
    {"run", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRHMMDecoder_run, 0), "run(image, min_confidence[, component_level]) -> retval\n.   @brief Recognize text using HMM.\n.   \n.       Takes an image and a mask (where each connected component corresponds to a segmented character)\n.       on input and returns recognized text in the output_text parameter. Optionally\n.       provides also the Rects for individual text elements found (e.g. words), and the list of those\n.       text elements with their confidence values.\n.   \n.       @param image Input image CV_8UC1 or CV_8UC3 with a single text line (or word).\n.       @param mask Input binary image CV_8UC1 same size as input image. Each connected component in mask corresponds to a segmented character in the input image.\n.   \n.       @param output_text Output text. Most likely character sequence found by the HMM decoder.\n.   \n.       @param component_rects If provided the method will output a list of Rects for the individual\n.       text elements found (e.g. words).\n.   \n.       @param component_texts If provided the method will output a list of text strings for the\n.       recognition of individual text elements found (e.g. words).\n.   \n.       @param component_confidences If provided the method will output a list of confidence values\n.       for the recognition of individual text elements found (e.g. words).\n.   \n.       @param component_level Only OCR_LEVEL_WORD is supported.\n\n\n\nrun(image, mask, min_confidence[, component_level]) -> retval\n."},

    {NULL,          NULL}
};

// Converter (text_OCRHMMDecoder)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::OCRHMMDecoder> >
{
    static PyObject* from(const Ptr<cv::text::OCRHMMDecoder>& r)
    {
        return pyopencv_text_OCRHMMDecoder_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::OCRHMMDecoder>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::OCRHMMDecoder> * dst_;
        if (pyopencv_text_OCRHMMDecoder_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::OCRHMMDecoder> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_OCRHMMDecoder_ClassifierCallback (Generic)
//================================================================================

// GetSet (text_OCRHMMDecoder_ClassifierCallback)



// Methods (text_OCRHMMDecoder_ClassifierCallback)



// Tables (text_OCRHMMDecoder_ClassifierCallback)

static PyGetSetDef pyopencv_text_OCRHMMDecoder_ClassifierCallback_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_OCRHMMDecoder_ClassifierCallback_methods[] =
{

    {NULL,          NULL}
};

// Converter (text_OCRHMMDecoder_ClassifierCallback)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::OCRHMMDecoder::ClassifierCallback> >
{
    static PyObject* from(const Ptr<cv::text::OCRHMMDecoder::ClassifierCallback>& r)
    {
        return pyopencv_text_OCRHMMDecoder_ClassifierCallback_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::OCRHMMDecoder::ClassifierCallback>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::OCRHMMDecoder::ClassifierCallback> * dst_;
        if (pyopencv_text_OCRHMMDecoder_ClassifierCallback_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::OCRHMMDecoder::ClassifierCallback> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_OCRTesseract (Generic)
//================================================================================

// GetSet (text_OCRTesseract)



// Methods (text_OCRTesseract)

static Napi::Value pyopencv_cv_text_text_OCRTesseract_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::text;

    char* datapath=0;
    char* language=0;
    char* char_whitelist=0;
    Napi::Value* pyobj_oem = NULL;
    int oem=OEM_DEFAULT;
    Napi::Value* pyobj_psmode = NULL;
    int psmode=PSM_AUTO;
    Ptr<OCRTesseract> retval;

    const char* keywords[] = { "datapath", "language", "char_whitelist", "oem", "psmode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|sssOO:text_OCRTesseract.create", (char**)keywords, &datapath, &language, &char_whitelist, &pyobj_oem, &pyobj_psmode) &&
        jsopencv_to_safe(info, pyobj_oem, oem, ArgInfo("oem", 0)) &&
        jsopencv_to_safe(info, pyobj_psmode, psmode, ArgInfo("psmode", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::OCRTesseract::create(datapath, language, char_whitelist, oem, psmode));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_text_text_OCRTesseract_run(const Napi::CallbackInfo &info)
{
    using namespace cv::text;


    Ptr<cv::text::OCRTesseract> * self1 = 0;
    if (!pyopencv_text_OCRTesseract_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'text_OCRTesseract' or its derivative)");
    Ptr<cv::text::OCRTesseract> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(4);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:text_OCRTesseract.run", (char**)keywords, &pyobj_image, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:text_OCRTesseract.run", (char**)keywords, &pyobj_image, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_mask = NULL;
    Mat mask;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "mask", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:text_OCRTesseract.run", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, mask, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_mask = NULL;
    UMat mask;
    Napi::Value* pyobj_min_confidence = NULL;
    int min_confidence=0;
    Napi::Value* pyobj_component_level = NULL;
    int component_level=0;
    String retval;

    const char* keywords[] = { "image", "mask", "min_confidence", "component_level", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:text_OCRTesseract.run", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_min_confidence, &pyobj_component_level) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_mask, mask, ArgInfo("mask", 0)) &&
        jsopencv_to_safe(info, pyobj_min_confidence, min_confidence, ArgInfo("min_confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_component_level, component_level, ArgInfo("component_level", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->run(image, mask, min_confidence, component_level));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("run");

    return NULL;
}

static Napi::Value pyopencv_cv_text_text_OCRTesseract_setWhiteList(const Napi::CallbackInfo &info)
{
    using namespace cv::text;


    Ptr<cv::text::OCRTesseract> * self1 = 0;
    if (!pyopencv_text_OCRTesseract_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'text_OCRTesseract' or its derivative)");
    Ptr<cv::text::OCRTesseract> _self_ = *(self1);
    Napi::Value* pyobj_char_whitelist = NULL;
    String char_whitelist;

    const char* keywords[] = { "char_whitelist", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:text_OCRTesseract.setWhiteList", (char**)keywords, &pyobj_char_whitelist) &&
        jsopencv_to_safe(info, pyobj_char_whitelist, char_whitelist, ArgInfo("char_whitelist", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWhiteList(char_whitelist));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (text_OCRTesseract)

static PyGetSetDef pyopencv_text_OCRTesseract_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_OCRTesseract_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRTesseract_create_static, METH_STATIC), "create([, datapath[, language[, char_whitelist[, oem[, psmode]]]]]) -> retval\n.   @brief Creates an instance of the OCRTesseract class. Initializes Tesseract.\n.   \n.       @param datapath the name of the parent directory of tessdata ended with \"/\", or NULL to use the\n.       system's default directory.\n.       @param language an ISO 639-3 code or NULL will default to \"eng\".\n.       @param char_whitelist specifies the list of characters used for recognition. NULL defaults to\n.       \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\".\n.       @param oem tesseract-ocr offers different OCR Engine Modes (OEM), by default\n.       tesseract::OEM_DEFAULT is used. See the tesseract-ocr API documentation for other possible\n.       values.\n.       @param psmode tesseract-ocr offers different Page Segmentation Modes (PSM) tesseract::PSM_AUTO\n.       (fully automatic layout analysis) is used. See the tesseract-ocr API documentation for other\n.       possible values."},
    {"run", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRTesseract_run, 0), "run(image, min_confidence[, component_level]) -> retval\n.   @brief Recognize text using the tesseract-ocr API.\n.   \n.       Takes image on input and returns recognized text in the output_text parameter. Optionally\n.       provides also the Rects for individual text elements found (e.g. words), and the list of those\n.       text elements with their confidence values.\n.   \n.       @param image Input image CV_8UC1 or CV_8UC3\n.       @param output_text Output text of the tesseract-ocr.\n.       @param component_rects If provided the method will output a list of Rects for the individual\n.       text elements found (e.g. words or text lines).\n.       @param component_texts If provided the method will output a list of text strings for the\n.       recognition of individual text elements found (e.g. words or text lines).\n.       @param component_confidences If provided the method will output a list of confidence values\n.       for the recognition of individual text elements found (e.g. words or text lines).\n.       @param component_level OCR_LEVEL_WORD (by default), or OCR_LEVEL_TEXTLINE.\n\n\n\nrun(image, mask, min_confidence[, component_level]) -> retval\n."},
    {"setWhiteList", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_OCRTesseract_setWhiteList, 0), "setWhiteList(char_whitelist) -> None\n."},

    {NULL,          NULL}
};

// Converter (text_OCRTesseract)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::OCRTesseract> >
{
    static PyObject* from(const Ptr<cv::text::OCRTesseract>& r)
    {
        return pyopencv_text_OCRTesseract_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::OCRTesseract>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::OCRTesseract> * dst_;
        if (pyopencv_text_OCRTesseract_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::OCRTesseract> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_TextDetector (Generic)
//================================================================================

// GetSet (text_TextDetector)



// Methods (text_TextDetector)

static Napi::Value pyopencv_cv_text_text_TextDetector_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::text;


    Ptr<cv::text::TextDetector> * self1 = 0;
    if (!pyopencv_text_TextDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'text_TextDetector' or its derivative)");
    Ptr<cv::text::TextDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_inputImage = NULL;
    Mat inputImage;
    vector_Rect Bbox;
    vector_float confidence;

    const char* keywords[] = { "inputImage", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:text_TextDetector.detect", (char**)keywords, &pyobj_inputImage) &&
        jsopencv_to_safe(info, pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(inputImage, Bbox, confidence));
        return Py_BuildValue("(NN)", jsopencv_from(Bbox), jsopencv_from(confidence));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_inputImage = NULL;
    UMat inputImage;
    vector_Rect Bbox;
    vector_float confidence;

    const char* keywords[] = { "inputImage", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:text_TextDetector.detect", (char**)keywords, &pyobj_inputImage) &&
        jsopencv_to_safe(info, pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(inputImage, Bbox, confidence));
        return Py_BuildValue("(NN)", jsopencv_from(Bbox), jsopencv_from(confidence));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}



// Tables (text_TextDetector)

static PyGetSetDef pyopencv_text_TextDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_TextDetector_methods[] =
{
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_TextDetector_detect, 0), "detect(inputImage) -> Bbox, confidence\n.   @brief Method that provides a quick and simple interface to detect text inside an image\n.   \n.       @param inputImage an image to process\n.       @param Bbox a vector of Rect that will store the detected word bounding box\n.       @param confidence a vector of float that will be updated with the confidence the classifier has for the selected bounding box"},

    {NULL,          NULL}
};

// Converter (text_TextDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::TextDetector> >
{
    static PyObject* from(const Ptr<cv::text::TextDetector>& r)
    {
        return pyopencv_text_TextDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::TextDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::TextDetector> * dst_;
        if (pyopencv_text_TextDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::TextDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// text_TextDetectorCNN (Generic)
//================================================================================

// GetSet (text_TextDetectorCNN)



// Methods (text_TextDetectorCNN)

static Napi::Value pyopencv_cv_text_text_TextDetectorCNN_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::text;

    Napi::Value* pyobj_modelArchFilename = NULL;
    String modelArchFilename;
    Napi::Value* pyobj_modelWeightsFilename = NULL;
    String modelWeightsFilename;
    Ptr<TextDetectorCNN> retval;

    const char* keywords[] = { "modelArchFilename", "modelWeightsFilename", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:text_TextDetectorCNN.create", (char**)keywords, &pyobj_modelArchFilename, &pyobj_modelWeightsFilename) &&
        jsopencv_to_safe(info, pyobj_modelArchFilename, modelArchFilename, ArgInfo("modelArchFilename", 0)) &&
        jsopencv_to_safe(info, pyobj_modelWeightsFilename, modelWeightsFilename, ArgInfo("modelWeightsFilename", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::text::TextDetectorCNN::create(modelArchFilename, modelWeightsFilename));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_text_text_TextDetectorCNN_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::text;


    Ptr<cv::text::TextDetectorCNN> * self1 = 0;
    if (!pyopencv_text_TextDetectorCNN_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'text_TextDetectorCNN' or its derivative)");
    Ptr<cv::text::TextDetectorCNN> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_inputImage = NULL;
    Mat inputImage;
    vector_Rect Bbox;
    vector_float confidence;

    const char* keywords[] = { "inputImage", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:text_TextDetectorCNN.detect", (char**)keywords, &pyobj_inputImage) &&
        jsopencv_to_safe(info, pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(inputImage, Bbox, confidence));
        return Py_BuildValue("(NN)", jsopencv_from(Bbox), jsopencv_from(confidence));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_inputImage = NULL;
    UMat inputImage;
    vector_Rect Bbox;
    vector_float confidence;

    const char* keywords[] = { "inputImage", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:text_TextDetectorCNN.detect", (char**)keywords, &pyobj_inputImage) &&
        jsopencv_to_safe(info, pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(inputImage, Bbox, confidence));
        return Py_BuildValue("(NN)", jsopencv_from(Bbox), jsopencv_from(confidence));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}



// Tables (text_TextDetectorCNN)

static PyGetSetDef pyopencv_text_TextDetectorCNN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_text_TextDetectorCNN_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_TextDetectorCNN_create_static, METH_STATIC), "create(modelArchFilename, modelWeightsFilename) -> retval\n.   @overload"},
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_text_text_TextDetectorCNN_detect, 0), "detect(inputImage) -> Bbox, confidence\n.   @overload\n.   \n.       @param inputImage an image expected to be a CV_U8C3 of any size\n.       @param Bbox a vector of Rect that will store the detected word bounding box\n.       @param confidence a vector of float that will be updated with the confidence the classifier has for the selected bounding box"},

    {NULL,          NULL}
};

// Converter (text_TextDetectorCNN)

template<>
struct PyOpenCV_Converter< Ptr<cv::text::TextDetectorCNN> >
{
    static PyObject* from(const Ptr<cv::text::TextDetectorCNN>& r)
    {
        return pyopencv_text_TextDetectorCNN_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::text::TextDetectorCNN>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::text::TextDetectorCNN> * dst_;
        if (pyopencv_text_TextDetectorCNN_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::text::TextDetectorCNN> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// utils_ClassWithKeywordProperties (Generic)
//================================================================================

// GetSet (utils_ClassWithKeywordProperties)


static PyObject* pyopencv_utils_ClassWithKeywordProperties_get_except(pyopencv_utils_ClassWithKeywordProperties_t* p, void *closure)
{
    return jsopencv_from(p->v.except);
}

static PyObject* pyopencv_utils_ClassWithKeywordProperties_get_lambda(pyopencv_utils_ClassWithKeywordProperties_t* p, void *closure)
{
    return jsopencv_from(p->v.lambda);
}

static int pyopencv_utils_ClassWithKeywordProperties_set_lambda(pyopencv_utils_ClassWithKeywordProperties_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lambda attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.lambda, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (utils_ClassWithKeywordProperties)

static int pyopencv_cv_utils_utils_ClassWithKeywordProperties_ClassWithKeywordProperties(pyopencv_utils_ClassWithKeywordProperties_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::utils;

    Napi::Value* pyobj_lambda_arg = NULL;
    int lambda_arg=24;
    Napi::Value* pyobj_except_arg = NULL;
    int except_arg=42;

    const char* keywords[] = { "lambda_arg", "except_arg", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ClassWithKeywordProperties", (char**)keywords, &pyobj_lambda_arg, &pyobj_except_arg) &&
        jsopencv_to_safe(info, pyobj_lambda_arg, lambda_arg, ArgInfo("lambda_arg", 0)) &&
        jsopencv_to_safe(info, pyobj_except_arg, except_arg, ArgInfo("except_arg", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::utils::ClassWithKeywordProperties(lambda_arg, except_arg));
        return 0;
    }

    return -1;
}



// Tables (utils_ClassWithKeywordProperties)

static PyGetSetDef pyopencv_utils_ClassWithKeywordProperties_getseters[] =
{
    {(char*)"except_", (getter)pyopencv_utils_ClassWithKeywordProperties_get_except, NULL, (char*)"except_", NULL},
    {(char*)"lambda_", (getter)pyopencv_utils_ClassWithKeywordProperties_get_lambda, (setter)pyopencv_utils_ClassWithKeywordProperties_set_lambda, (char*)"lambda_", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_utils_ClassWithKeywordProperties_methods[] =
{

    {NULL,          NULL}
};

// Converter (utils_ClassWithKeywordProperties)

template<>
struct PyOpenCV_Converter< cv::utils::ClassWithKeywordProperties >
{
    static PyObject* from(const cv::utils::ClassWithKeywordProperties& r)
    {
        return pyopencv_utils_ClassWithKeywordProperties_Instance(r);
    }
    static bool to(PyObject* src, cv::utils::ClassWithKeywordProperties& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::utils::ClassWithKeywordProperties * dst_;
        if (pyopencv_utils_ClassWithKeywordProperties_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::utils::ClassWithKeywordProperties for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// utils_nested_OriginalClassName (Generic)
//================================================================================

// GetSet (utils_nested_OriginalClassName)



// Methods (utils_nested_OriginalClassName)

static Napi::Value pyopencv_cv_utils_nested_utils_nested_OriginalClassName_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::utils::nested;

    Napi::Value* pyobj_params = NULL;
    OriginalClassName_Params params=OriginalClassName::Params();
    Ptr<OriginalClassName> retval;

    const char* keywords[] = { "params", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:utils_nested_ExportClassName.create", (char**)keywords, &pyobj_params) &&
        jsopencv_to_safe(info, pyobj_params, params, ArgInfo("params", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::utils::nested::OriginalClassName::create(params));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_utils_nested_utils_nested_OriginalClassName_getFloatParam(const Napi::CallbackInfo &info)
{
    using namespace cv::utils::nested;


    Ptr<cv::utils::nested::OriginalClassName> * self1 = 0;
    if (!pyopencv_utils_nested_OriginalClassName_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'utils_nested_OriginalClassName' or its derivative)");
    Ptr<cv::utils::nested::OriginalClassName> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFloatParam());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_utils_nested_utils_nested_OriginalClassName_getIntParam(const Napi::CallbackInfo &info)
{
    using namespace cv::utils::nested;


    Ptr<cv::utils::nested::OriginalClassName> * self1 = 0;
    if (!pyopencv_utils_nested_OriginalClassName_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'utils_nested_OriginalClassName' or its derivative)");
    Ptr<cv::utils::nested::OriginalClassName> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIntParam());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_utils_nested_utils_nested_OriginalClassName_originalName_static(const Napi::CallbackInfo &info)
{
    using namespace cv::utils::nested;

    std::string retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::utils::nested::OriginalClassName::originalName());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (utils_nested_OriginalClassName)

static PyGetSetDef pyopencv_utils_nested_OriginalClassName_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_utils_nested_OriginalClassName_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_utils_nested_utils_nested_OriginalClassName_create_static, METH_STATIC), "create([, params]) -> retval\n."},
    {"getFloatParam", CV_JS_FN_WITH_KW_(pyopencv_cv_utils_nested_utils_nested_OriginalClassName_getFloatParam, 0), "getFloatParam() -> retval\n."},
    {"getIntParam", CV_JS_FN_WITH_KW_(pyopencv_cv_utils_nested_utils_nested_OriginalClassName_getIntParam, 0), "getIntParam() -> retval\n."},
    {"originalName", CV_JS_FN_WITH_KW_(pyopencv_cv_utils_nested_utils_nested_OriginalClassName_originalName_static, METH_STATIC), "originalName() -> retval\n."},

    {NULL,          NULL}
};

// Converter (utils_nested_OriginalClassName)

template<>
struct PyOpenCV_Converter< Ptr<cv::utils::nested::OriginalClassName> >
{
    static PyObject* from(const Ptr<cv::utils::nested::OriginalClassName>& r)
    {
        return pyopencv_utils_nested_OriginalClassName_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::utils::nested::OriginalClassName>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::utils::nested::OriginalClassName> * dst_;
        if (pyopencv_utils_nested_OriginalClassName_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::utils::nested::OriginalClassName> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// utils_nested_OriginalClassName_Params (Generic)
//================================================================================

// GetSet (utils_nested_OriginalClassName_Params)


static PyObject* pyopencv_utils_nested_OriginalClassName_Params_get_float_value(pyopencv_utils_nested_OriginalClassName_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.float_value);
}

static int pyopencv_utils_nested_OriginalClassName_Params_set_float_value(pyopencv_utils_nested_OriginalClassName_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the float_value attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.float_value, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_utils_nested_OriginalClassName_Params_get_int_value(pyopencv_utils_nested_OriginalClassName_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.int_value);
}

static int pyopencv_utils_nested_OriginalClassName_Params_set_int_value(pyopencv_utils_nested_OriginalClassName_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the int_value attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.int_value, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (utils_nested_OriginalClassName_Params)

static int pyopencv_cv_utils_nested_utils_nested_OriginalClassName_Params_OriginalClassName_Params(pyopencv_utils_nested_OriginalClassName_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::utils::nested;

    Napi::Value* pyobj_int_param = NULL;
    int int_param=123;
    Napi::Value* pyobj_float_param = NULL;
    float float_param=3.5f;

    const char* keywords[] = { "int_param", "float_param", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:OriginalClassName_Params", (char**)keywords, &pyobj_int_param, &pyobj_float_param) &&
        jsopencv_to_safe(info, pyobj_int_param, int_param, ArgInfo("int_param", 0)) &&
        jsopencv_to_safe(info, pyobj_float_param, float_param, ArgInfo("float_param", 0)))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::utils::nested::OriginalClassName::Params(int_param, float_param));
        return 0;
    }

    return -1;
}



// Tables (utils_nested_OriginalClassName_Params)

static PyGetSetDef pyopencv_utils_nested_OriginalClassName_Params_getseters[] =
{
    {(char*)"float_value", (getter)pyopencv_utils_nested_OriginalClassName_Params_get_float_value, (setter)pyopencv_utils_nested_OriginalClassName_Params_set_float_value, (char*)"float_value", NULL},
    {(char*)"int_value", (getter)pyopencv_utils_nested_OriginalClassName_Params_get_int_value, (setter)pyopencv_utils_nested_OriginalClassName_Params_set_int_value, (char*)"int_value", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_utils_nested_OriginalClassName_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (utils_nested_OriginalClassName_Params)

template<>
struct PyOpenCV_Converter< cv::utils::nested::OriginalClassName::Params >
{
    static PyObject* from(const cv::utils::nested::OriginalClassName::Params& r)
    {
        return pyopencv_utils_nested_OriginalClassName_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::utils::nested::OriginalClassName::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::utils::nested::OriginalClassName::Params * dst_;
        if (pyopencv_utils_nested_OriginalClassName_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::utils::nested::OriginalClassName::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_AffineFeature2D (Generic)
//================================================================================

// GetSet (xfeatures2d_AffineFeature2D)



// Methods (xfeatures2d_AffineFeature2D)



// Tables (xfeatures2d_AffineFeature2D)

static PyGetSetDef pyopencv_xfeatures2d_AffineFeature2D_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_AffineFeature2D_methods[] =
{

    {NULL,          NULL}
};

// Converter (xfeatures2d_AffineFeature2D)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::AffineFeature2D> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::AffineFeature2D>& r)
    {
        return pyopencv_xfeatures2d_AffineFeature2D_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::AffineFeature2D>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::AffineFeature2D> * dst_;
        if (pyopencv_xfeatures2d_AffineFeature2D_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::AffineFeature2D> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_BEBLID (Generic)
//================================================================================

// GetSet (xfeatures2d_BEBLID)



// Methods (xfeatures2d_BEBLID)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;
    Napi::Value* pyobj_n_bits = NULL;
    int n_bits=BEBLID::SIZE_512_BITS;
    Ptr<BEBLID> retval;

    const char* keywords[] = { "scale_factor", "n_bits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:xfeatures2d_BEBLID.create", (char**)keywords, &pyobj_scale_factor, &pyobj_n_bits) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)) &&
        jsopencv_to_safe(info, pyobj_n_bits, n_bits, ArgInfo("n_bits", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::BEBLID::create(scale_factor, n_bits));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BEBLID> * self1 = 0;
    if (!pyopencv_xfeatures2d_BEBLID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BEBLID' or its derivative)");
    Ptr<cv::xfeatures2d::BEBLID> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_getScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BEBLID> * self1 = 0;
    if (!pyopencv_xfeatures2d_BEBLID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BEBLID' or its derivative)");
    Ptr<cv::xfeatures2d::BEBLID> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleFactor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_setScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BEBLID> * self1 = 0;
    if (!pyopencv_xfeatures2d_BEBLID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BEBLID' or its derivative)");
    Ptr<cv::xfeatures2d::BEBLID> _self_ = *(self1);
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_BEBLID.setScaleFactor", (char**)keywords, &pyobj_scale_factor) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_BEBLID)

static PyGetSetDef pyopencv_xfeatures2d_BEBLID_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_BEBLID_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_create_static, METH_STATIC), "create(scale_factor[, n_bits]) -> retval\n.   @brief Creates the BEBLID descriptor.\n.       @param scale_factor Adjust the sampling window around detected keypoints:\n.       - <b> 1.00f </b> should be the scale for ORB keypoints\n.       - <b> 6.75f </b> should be the scale for SIFT detected keypoints\n.       - <b> 6.25f </b> is default and fits for KAZE, SURF detected keypoints\n.       - <b> 5.00f </b> should be the scale for AKAZE, MSD, AGAST, FAST, BRISK keypoints\n.       @param n_bits Determine the number of bits in the descriptor. Should be either\n.        BEBLID::SIZE_512_BITS or BEBLID::SIZE_256_BITS."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"setScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BEBLID_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_BEBLID)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::BEBLID> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::BEBLID>& r)
    {
        return pyopencv_xfeatures2d_BEBLID_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::BEBLID>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::BEBLID> * dst_;
        if (pyopencv_xfeatures2d_BEBLID_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::BEBLID> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_BoostDesc (Generic)
//================================================================================

// GetSet (xfeatures2d_BoostDesc)



// Methods (xfeatures2d_BoostDesc)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_desc = NULL;
    int desc=BoostDesc::BINBOOST_256;
    Napi::Value* pyobj_use_scale_orientation = NULL;
    bool use_scale_orientation=true;
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=6.25f;
    Ptr<BoostDesc> retval;

    const char* keywords[] = { "desc", "use_scale_orientation", "scale_factor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:xfeatures2d_BoostDesc.create", (char**)keywords, &pyobj_desc, &pyobj_use_scale_orientation, &pyobj_scale_factor) &&
        jsopencv_to_safe(info, pyobj_desc, desc, ArgInfo("desc", 0)) &&
        jsopencv_to_safe(info, pyobj_use_scale_orientation, use_scale_orientation, ArgInfo("use_scale_orientation", 0)) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::BoostDesc::create(desc, use_scale_orientation, scale_factor));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BoostDesc> * self1 = 0;
    if (!pyopencv_xfeatures2d_BoostDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    Ptr<cv::xfeatures2d::BoostDesc> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BoostDesc> * self1 = 0;
    if (!pyopencv_xfeatures2d_BoostDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    Ptr<cv::xfeatures2d::BoostDesc> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleFactor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getUseScaleOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BoostDesc> * self1 = 0;
    if (!pyopencv_xfeatures2d_BoostDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    Ptr<cv::xfeatures2d::BoostDesc> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseScaleOrientation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BoostDesc> * self1 = 0;
    if (!pyopencv_xfeatures2d_BoostDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    Ptr<cv::xfeatures2d::BoostDesc> _self_ = *(self1);
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_BoostDesc.setScaleFactor", (char**)keywords, &pyobj_scale_factor) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setUseScaleOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BoostDesc> * self1 = 0;
    if (!pyopencv_xfeatures2d_BoostDesc_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    Ptr<cv::xfeatures2d::BoostDesc> _self_ = *(self1);
    Napi::Value* pyobj_use_scale_orientation = NULL;
    bool use_scale_orientation=0;

    const char* keywords[] = { "use_scale_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_BoostDesc.setUseScaleOrientation", (char**)keywords, &pyobj_use_scale_orientation) &&
        jsopencv_to_safe(info, pyobj_use_scale_orientation, use_scale_orientation, ArgInfo("use_scale_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseScaleOrientation(use_scale_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_BoostDesc)

static PyGetSetDef pyopencv_xfeatures2d_BoostDesc_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_BoostDesc_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_create_static, METH_STATIC), "create([, desc[, use_scale_orientation[, scale_factor]]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getUseScaleOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getUseScaleOrientation, 0), "getUseScaleOrientation() -> retval\n."},
    {"setScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},
    {"setUseScaleOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setUseScaleOrientation, 0), "setUseScaleOrientation(use_scale_orientation) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_BoostDesc)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::BoostDesc> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::BoostDesc>& r)
    {
        return pyopencv_xfeatures2d_BoostDesc_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::BoostDesc>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::BoostDesc> * dst_;
        if (pyopencv_xfeatures2d_BoostDesc_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::BoostDesc> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_BriefDescriptorExtractor (Generic)
//================================================================================

// GetSet (xfeatures2d_BriefDescriptorExtractor)



// Methods (xfeatures2d_BriefDescriptorExtractor)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_bytes = NULL;
    int bytes=32;
    Napi::Value* pyobj_use_orientation = NULL;
    bool use_orientation=false;
    Ptr<BriefDescriptorExtractor> retval;

    const char* keywords[] = { "bytes", "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:xfeatures2d_BriefDescriptorExtractor.create", (char**)keywords, &pyobj_bytes, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_bytes, bytes, ArgInfo("bytes", 0)) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::BriefDescriptorExtractor::create(bytes, use_orientation));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> * self1 = 0;
    if (!pyopencv_xfeatures2d_BriefDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BriefDescriptorExtractor' or its derivative)");
    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_getDescriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> * self1 = 0;
    if (!pyopencv_xfeatures2d_BriefDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BriefDescriptorExtractor' or its derivative)");
    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDescriptorSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_getUseOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> * self1 = 0;
    if (!pyopencv_xfeatures2d_BriefDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BriefDescriptorExtractor' or its derivative)");
    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseOrientation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_setDescriptorSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> * self1 = 0;
    if (!pyopencv_xfeatures2d_BriefDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BriefDescriptorExtractor' or its derivative)");
    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> _self_ = *(self1);
    Napi::Value* pyobj_bytes = NULL;
    int bytes=0;

    const char* keywords[] = { "bytes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_BriefDescriptorExtractor.setDescriptorSize", (char**)keywords, &pyobj_bytes) &&
        jsopencv_to_safe(info, pyobj_bytes, bytes, ArgInfo("bytes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDescriptorSize(bytes));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_setUseOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> * self1 = 0;
    if (!pyopencv_xfeatures2d_BriefDescriptorExtractor_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BriefDescriptorExtractor' or its derivative)");
    Ptr<cv::xfeatures2d::BriefDescriptorExtractor> _self_ = *(self1);
    Napi::Value* pyobj_use_orientation = NULL;
    bool use_orientation=0;

    const char* keywords[] = { "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_BriefDescriptorExtractor.setUseOrientation", (char**)keywords, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseOrientation(use_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_BriefDescriptorExtractor)

static PyGetSetDef pyopencv_xfeatures2d_BriefDescriptorExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_BriefDescriptorExtractor_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_create_static, METH_STATIC), "create([, bytes[, use_orientation]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDescriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_getDescriptorSize, 0), "getDescriptorSize() -> retval\n."},
    {"getUseOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_getUseOrientation, 0), "getUseOrientation() -> retval\n."},
    {"setDescriptorSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_setDescriptorSize, 0), "setDescriptorSize(bytes) -> None\n."},
    {"setUseOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_setUseOrientation, 0), "setUseOrientation(use_orientation) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_BriefDescriptorExtractor)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::BriefDescriptorExtractor> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::BriefDescriptorExtractor>& r)
    {
        return pyopencv_xfeatures2d_BriefDescriptorExtractor_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::BriefDescriptorExtractor>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::BriefDescriptorExtractor> * dst_;
        if (pyopencv_xfeatures2d_BriefDescriptorExtractor_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::BriefDescriptorExtractor> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_DAISY (Generic)
//================================================================================

// GetSet (xfeatures2d_DAISY)



// Methods (xfeatures2d_DAISY)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_radius = NULL;
    float radius=15;
    Napi::Value* pyobj_q_radius = NULL;
    int q_radius=3;
    Napi::Value* pyobj_q_theta = NULL;
    int q_theta=8;
    Napi::Value* pyobj_q_hist = NULL;
    int q_hist=8;
    Napi::Value* pyobj_norm = NULL;
    DAISY_NormalizationType norm=DAISY::NRM_NONE;
    Napi::Value* pyobj_H = NULL;
    Mat H;
    Napi::Value* pyobj_interpolation = NULL;
    bool interpolation=true;
    Napi::Value* pyobj_use_orientation = NULL;
    bool use_orientation=false;
    Ptr<DAISY> retval;

    const char* keywords[] = { "radius", "q_radius", "q_theta", "q_hist", "norm", "H", "interpolation", "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOO:xfeatures2d_DAISY.create", (char**)keywords, &pyobj_radius, &pyobj_q_radius, &pyobj_q_theta, &pyobj_q_hist, &pyobj_norm, &pyobj_H, &pyobj_interpolation, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)) &&
        jsopencv_to_safe(info, pyobj_q_radius, q_radius, ArgInfo("q_radius", 0)) &&
        jsopencv_to_safe(info, pyobj_q_theta, q_theta, ArgInfo("q_theta", 0)) &&
        jsopencv_to_safe(info, pyobj_q_hist, q_hist, ArgInfo("q_hist", 0)) &&
        jsopencv_to_safe(info, pyobj_norm, norm, ArgInfo("norm", 0)) &&
        jsopencv_to_safe(info, pyobj_H, H, ArgInfo("H", 0)) &&
        jsopencv_to_safe(info, pyobj_interpolation, interpolation, ArgInfo("interpolation", 0)) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::DAISY::create(radius, q_radius, q_theta, q_hist, norm, H, interpolation, use_orientation));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_radius = NULL;
    float radius=15;
    Napi::Value* pyobj_q_radius = NULL;
    int q_radius=3;
    Napi::Value* pyobj_q_theta = NULL;
    int q_theta=8;
    Napi::Value* pyobj_q_hist = NULL;
    int q_hist=8;
    Napi::Value* pyobj_norm = NULL;
    DAISY_NormalizationType norm=DAISY::NRM_NONE;
    Napi::Value* pyobj_H = NULL;
    UMat H;
    Napi::Value* pyobj_interpolation = NULL;
    bool interpolation=true;
    Napi::Value* pyobj_use_orientation = NULL;
    bool use_orientation=false;
    Ptr<DAISY> retval;

    const char* keywords[] = { "radius", "q_radius", "q_theta", "q_hist", "norm", "H", "interpolation", "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOO:xfeatures2d_DAISY.create", (char**)keywords, &pyobj_radius, &pyobj_q_radius, &pyobj_q_theta, &pyobj_q_hist, &pyobj_norm, &pyobj_H, &pyobj_interpolation, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)) &&
        jsopencv_to_safe(info, pyobj_q_radius, q_radius, ArgInfo("q_radius", 0)) &&
        jsopencv_to_safe(info, pyobj_q_theta, q_theta, ArgInfo("q_theta", 0)) &&
        jsopencv_to_safe(info, pyobj_q_hist, q_hist, ArgInfo("q_hist", 0)) &&
        jsopencv_to_safe(info, pyobj_norm, norm, ArgInfo("norm", 0)) &&
        jsopencv_to_safe(info, pyobj_H, H, ArgInfo("H", 0)) &&
        jsopencv_to_safe(info, pyobj_interpolation, interpolation, ArgInfo("interpolation", 0)) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::DAISY::create(radius, q_radius, q_theta, q_hist, norm, H, interpolation, use_orientation));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getH(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    cv::Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getH());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getInterpolation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getInterpolation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getNorm(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNorm());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getQHist(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getQHist());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getQRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getQRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getQTheta(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getQTheta());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getUseOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseOrientation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setH(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_H = NULL;
    Mat H;

    const char* keywords[] = { "H", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setH", (char**)keywords, &pyobj_H) &&
        jsopencv_to_safe(info, pyobj_H, H, ArgInfo("H", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setH(H));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_H = NULL;
    UMat H;

    const char* keywords[] = { "H", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setH", (char**)keywords, &pyobj_H) &&
        jsopencv_to_safe(info, pyobj_H, H, ArgInfo("H", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setH(H));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setH");

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setInterpolation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_interpolation = NULL;
    bool interpolation=0;

    const char* keywords[] = { "interpolation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setInterpolation", (char**)keywords, &pyobj_interpolation) &&
        jsopencv_to_safe(info, pyobj_interpolation, interpolation, ArgInfo("interpolation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInterpolation(interpolation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setNorm(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_norm = NULL;
    int norm=0;

    const char* keywords[] = { "norm", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setNorm", (char**)keywords, &pyobj_norm) &&
        jsopencv_to_safe(info, pyobj_norm, norm, ArgInfo("norm", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNorm(norm));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setQHist(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_q_hist = NULL;
    int q_hist=0;

    const char* keywords[] = { "q_hist", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setQHist", (char**)keywords, &pyobj_q_hist) &&
        jsopencv_to_safe(info, pyobj_q_hist, q_hist, ArgInfo("q_hist", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setQHist(q_hist));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setQRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_q_radius = NULL;
    int q_radius=0;

    const char* keywords[] = { "q_radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setQRadius", (char**)keywords, &pyobj_q_radius) &&
        jsopencv_to_safe(info, pyobj_q_radius, q_radius, ArgInfo("q_radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setQRadius(q_radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setQTheta(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_q_theta = NULL;
    int q_theta=0;

    const char* keywords[] = { "q_theta", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setQTheta", (char**)keywords, &pyobj_q_theta) &&
        jsopencv_to_safe(info, pyobj_q_theta, q_theta, ArgInfo("q_theta", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setQTheta(q_theta));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_radius = NULL;
    float radius=0.f;

    const char* keywords[] = { "radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setRadius", (char**)keywords, &pyobj_radius) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRadius(radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setUseOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::DAISY> * self1 = 0;
    if (!pyopencv_xfeatures2d_DAISY_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_DAISY' or its derivative)");
    Ptr<cv::xfeatures2d::DAISY> _self_ = *(self1);
    Napi::Value* pyobj_use_orientation = NULL;
    bool use_orientation=0;

    const char* keywords[] = { "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_DAISY.setUseOrientation", (char**)keywords, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseOrientation(use_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_DAISY)

static PyGetSetDef pyopencv_xfeatures2d_DAISY_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_DAISY_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_create_static, METH_STATIC), "create([, radius[, q_radius[, q_theta[, q_hist[, norm[, H[, interpolation[, use_orientation]]]]]]]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getH", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getH, 0), "getH() -> retval\n."},
    {"getInterpolation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getInterpolation, 0), "getInterpolation() -> retval\n."},
    {"getNorm", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getNorm, 0), "getNorm() -> retval\n."},
    {"getQHist", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getQHist, 0), "getQHist() -> retval\n."},
    {"getQRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getQRadius, 0), "getQRadius() -> retval\n."},
    {"getQTheta", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getQTheta, 0), "getQTheta() -> retval\n."},
    {"getRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getRadius, 0), "getRadius() -> retval\n."},
    {"getUseOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_getUseOrientation, 0), "getUseOrientation() -> retval\n."},
    {"setH", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setH, 0), "setH(H) -> None\n."},
    {"setInterpolation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setInterpolation, 0), "setInterpolation(interpolation) -> None\n."},
    {"setNorm", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setNorm, 0), "setNorm(norm) -> None\n."},
    {"setQHist", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setQHist, 0), "setQHist(q_hist) -> None\n."},
    {"setQRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setQRadius, 0), "setQRadius(q_radius) -> None\n."},
    {"setQTheta", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setQTheta, 0), "setQTheta(q_theta) -> None\n."},
    {"setRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setRadius, 0), "setRadius(radius) -> None\n."},
    {"setUseOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_setUseOrientation, 0), "setUseOrientation(use_orientation) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_DAISY)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::DAISY> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::DAISY>& r)
    {
        return pyopencv_xfeatures2d_DAISY_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::DAISY>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::DAISY> * dst_;
        if (pyopencv_xfeatures2d_DAISY_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::DAISY> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_FREAK (Generic)
//================================================================================

// GetSet (xfeatures2d_FREAK)



// Methods (xfeatures2d_FREAK)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_orientationNormalized = NULL;
    bool orientationNormalized=true;
    Napi::Value* pyobj_scaleNormalized = NULL;
    bool scaleNormalized=true;
    Napi::Value* pyobj_patternScale = NULL;
    float patternScale=22.0f;
    Napi::Value* pyobj_nOctaves = NULL;
    int nOctaves=4;
    Napi::Value* pyobj_selectedPairs = NULL;
    vector_int selectedPairs=std::vector<int>();
    Ptr<FREAK> retval;

    const char* keywords[] = { "orientationNormalized", "scaleNormalized", "patternScale", "nOctaves", "selectedPairs", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:xfeatures2d_FREAK.create", (char**)keywords, &pyobj_orientationNormalized, &pyobj_scaleNormalized, &pyobj_patternScale, &pyobj_nOctaves, &pyobj_selectedPairs) &&
        jsopencv_to_safe(info, pyobj_orientationNormalized, orientationNormalized, ArgInfo("orientationNormalized", 0)) &&
        jsopencv_to_safe(info, pyobj_scaleNormalized, scaleNormalized, ArgInfo("scaleNormalized", 0)) &&
        jsopencv_to_safe(info, pyobj_patternScale, patternScale, ArgInfo("patternScale", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaves, nOctaves, ArgInfo("nOctaves", 0)) &&
        jsopencv_to_safe(info, pyobj_selectedPairs, selectedPairs, ArgInfo("selectedPairs", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::FREAK::create(orientationNormalized, scaleNormalized, patternScale, nOctaves, selectedPairs));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaves());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getOrientationNormalized(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getOrientationNormalized());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getPatternScale(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPatternScale());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getScaleNormalized(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleNormalized());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    Napi::Value* pyobj_nOctaves = NULL;
    int nOctaves=0;

    const char* keywords[] = { "nOctaves", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_FREAK.setNOctaves", (char**)keywords, &pyobj_nOctaves) &&
        jsopencv_to_safe(info, pyobj_nOctaves, nOctaves, ArgInfo("nOctaves", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaves(nOctaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setOrientationNormalized(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    Napi::Value* pyobj_orientationNormalized = NULL;
    bool orientationNormalized=0;

    const char* keywords[] = { "orientationNormalized", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_FREAK.setOrientationNormalized", (char**)keywords, &pyobj_orientationNormalized) &&
        jsopencv_to_safe(info, pyobj_orientationNormalized, orientationNormalized, ArgInfo("orientationNormalized", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setOrientationNormalized(orientationNormalized));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setPatternScale(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    Napi::Value* pyobj_patternScale = NULL;
    double patternScale=0;

    const char* keywords[] = { "patternScale", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_FREAK.setPatternScale", (char**)keywords, &pyobj_patternScale) &&
        jsopencv_to_safe(info, pyobj_patternScale, patternScale, ArgInfo("patternScale", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPatternScale(patternScale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setScaleNormalized(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::FREAK> * self1 = 0;
    if (!pyopencv_xfeatures2d_FREAK_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_FREAK' or its derivative)");
    Ptr<cv::xfeatures2d::FREAK> _self_ = *(self1);
    Napi::Value* pyobj_scaleNormalized = NULL;
    bool scaleNormalized=0;

    const char* keywords[] = { "scaleNormalized", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_FREAK.setScaleNormalized", (char**)keywords, &pyobj_scaleNormalized) &&
        jsopencv_to_safe(info, pyobj_scaleNormalized, scaleNormalized, ArgInfo("scaleNormalized", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleNormalized(scaleNormalized));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_FREAK)

static PyGetSetDef pyopencv_xfeatures2d_FREAK_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_FREAK_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_create_static, METH_STATIC), "create([, orientationNormalized[, scaleNormalized[, patternScale[, nOctaves[, selectedPairs]]]]]) -> retval\n.   @param orientationNormalized Enable orientation normalization.\n.       @param scaleNormalized Enable scale normalization.\n.       @param patternScale Scaling of the description pattern.\n.       @param nOctaves Number of octaves covered by the detected keypoints.\n.       @param selectedPairs (Optional) user defined selected pairs indexes,"},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getOrientationNormalized", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getOrientationNormalized, 0), "getOrientationNormalized() -> retval\n."},
    {"getPatternScale", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getPatternScale, 0), "getPatternScale() -> retval\n."},
    {"getScaleNormalized", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_getScaleNormalized, 0), "getScaleNormalized() -> retval\n."},
    {"setNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setNOctaves, 0), "setNOctaves(nOctaves) -> None\n."},
    {"setOrientationNormalized", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setOrientationNormalized, 0), "setOrientationNormalized(orientationNormalized) -> None\n."},
    {"setPatternScale", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setPatternScale, 0), "setPatternScale(patternScale) -> None\n."},
    {"setScaleNormalized", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_setScaleNormalized, 0), "setScaleNormalized(scaleNormalized) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_FREAK)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::FREAK> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::FREAK>& r)
    {
        return pyopencv_xfeatures2d_FREAK_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::FREAK>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::FREAK> * dst_;
        if (pyopencv_xfeatures2d_FREAK_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::FREAK> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_HarrisLaplaceFeatureDetector (Generic)
//================================================================================

// GetSet (xfeatures2d_HarrisLaplaceFeatureDetector)



// Methods (xfeatures2d_HarrisLaplaceFeatureDetector)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_numOctaves = NULL;
    int numOctaves=6;
    Napi::Value* pyobj_corn_thresh = NULL;
    float corn_thresh=0.01f;
    Napi::Value* pyobj_DOG_thresh = NULL;
    float DOG_thresh=0.01f;
    Napi::Value* pyobj_maxCorners = NULL;
    int maxCorners=5000;
    Napi::Value* pyobj_num_layers = NULL;
    int num_layers=4;
    Ptr<HarrisLaplaceFeatureDetector> retval;

    const char* keywords[] = { "numOctaves", "corn_thresh", "DOG_thresh", "maxCorners", "num_layers", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:xfeatures2d_HarrisLaplaceFeatureDetector.create", (char**)keywords, &pyobj_numOctaves, &pyobj_corn_thresh, &pyobj_DOG_thresh, &pyobj_maxCorners, &pyobj_num_layers) &&
        jsopencv_to_safe(info, pyobj_numOctaves, numOctaves, ArgInfo("numOctaves", 0)) &&
        jsopencv_to_safe(info, pyobj_corn_thresh, corn_thresh, ArgInfo("corn_thresh", 0)) &&
        jsopencv_to_safe(info, pyobj_DOG_thresh, DOG_thresh, ArgInfo("DOG_thresh", 0)) &&
        jsopencv_to_safe(info, pyobj_maxCorners, maxCorners, ArgInfo("maxCorners", 0)) &&
        jsopencv_to_safe(info, pyobj_num_layers, num_layers, ArgInfo("num_layers", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::HarrisLaplaceFeatureDetector::create(numOctaves, corn_thresh, DOG_thresh, maxCorners, num_layers));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getCornThresh(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCornThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getDOGThresh(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDOGThresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getMaxCorners(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxCorners());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getNumLayers(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumLayers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getNumOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumOctaves());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setCornThresh(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_corn_thresh_ = NULL;
    float corn_thresh_=0.f;

    const char* keywords[] = { "corn_thresh_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_HarrisLaplaceFeatureDetector.setCornThresh", (char**)keywords, &pyobj_corn_thresh_) &&
        jsopencv_to_safe(info, pyobj_corn_thresh_, corn_thresh_, ArgInfo("corn_thresh_", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCornThresh(corn_thresh_));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setDOGThresh(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_DOG_thresh_ = NULL;
    float DOG_thresh_=0.f;

    const char* keywords[] = { "DOG_thresh_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_HarrisLaplaceFeatureDetector.setDOGThresh", (char**)keywords, &pyobj_DOG_thresh_) &&
        jsopencv_to_safe(info, pyobj_DOG_thresh_, DOG_thresh_, ArgInfo("DOG_thresh_", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDOGThresh(DOG_thresh_));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setMaxCorners(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_maxCorners_ = NULL;
    int maxCorners_=0;

    const char* keywords[] = { "maxCorners_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_HarrisLaplaceFeatureDetector.setMaxCorners", (char**)keywords, &pyobj_maxCorners_) &&
        jsopencv_to_safe(info, pyobj_maxCorners_, maxCorners_, ArgInfo("maxCorners_", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxCorners(maxCorners_));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setNumLayers(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_num_layers_ = NULL;
    int num_layers_=0;

    const char* keywords[] = { "num_layers_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_HarrisLaplaceFeatureDetector.setNumLayers", (char**)keywords, &pyobj_num_layers_) &&
        jsopencv_to_safe(info, pyobj_num_layers_, num_layers_, ArgInfo("num_layers_", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumLayers(num_layers_));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setNumOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_HarrisLaplaceFeatureDetector' or its derivative)");
    Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> _self_ = *(self1);
    Napi::Value* pyobj_numOctaves_ = NULL;
    int numOctaves_=0;

    const char* keywords[] = { "numOctaves_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_HarrisLaplaceFeatureDetector.setNumOctaves", (char**)keywords, &pyobj_numOctaves_) &&
        jsopencv_to_safe(info, pyobj_numOctaves_, numOctaves_, ArgInfo("numOctaves_", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNumOctaves(numOctaves_));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_HarrisLaplaceFeatureDetector)

static PyGetSetDef pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_create_static, METH_STATIC), "create([, numOctaves[, corn_thresh[, DOG_thresh[, maxCorners[, num_layers]]]]]) -> retval\n.   * @brief Creates a new implementation instance.\n.        *\n.        * @param numOctaves the number of octaves in the scale-space pyramid\n.        * @param corn_thresh the threshold for the Harris cornerness measure\n.        * @param DOG_thresh the threshold for the Difference-of-Gaussians scale selection\n.        * @param maxCorners the maximum number of corners to consider\n.        * @param num_layers the number of intermediate scales per octave"},
    {"getCornThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getCornThresh, 0), "getCornThresh() -> retval\n."},
    {"getDOGThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getDOGThresh, 0), "getDOGThresh() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getMaxCorners", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getMaxCorners, 0), "getMaxCorners() -> retval\n."},
    {"getNumLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getNumLayers, 0), "getNumLayers() -> retval\n."},
    {"getNumOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_getNumOctaves, 0), "getNumOctaves() -> retval\n."},
    {"setCornThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setCornThresh, 0), "setCornThresh(corn_thresh_) -> None\n."},
    {"setDOGThresh", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setDOGThresh, 0), "setDOGThresh(DOG_thresh_) -> None\n."},
    {"setMaxCorners", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setMaxCorners, 0), "setMaxCorners(maxCorners_) -> None\n."},
    {"setNumLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setNumLayers, 0), "setNumLayers(num_layers_) -> None\n."},
    {"setNumOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_setNumOctaves, 0), "setNumOctaves(numOctaves_) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_HarrisLaplaceFeatureDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector>& r)
    {
        return pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> * dst_;
        if (pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_LATCH (Generic)
//================================================================================

// GetSet (xfeatures2d_LATCH)



// Methods (xfeatures2d_LATCH)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_bytes = NULL;
    int bytes=32;
    Napi::Value* pyobj_rotationInvariance = NULL;
    bool rotationInvariance=true;
    Napi::Value* pyobj_half_ssd_size = NULL;
    int half_ssd_size=3;
    Napi::Value* pyobj_sigma = NULL;
    double sigma=2.0;
    Ptr<LATCH> retval;

    const char* keywords[] = { "bytes", "rotationInvariance", "half_ssd_size", "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOO:xfeatures2d_LATCH.create", (char**)keywords, &pyobj_bytes, &pyobj_rotationInvariance, &pyobj_half_ssd_size, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_bytes, bytes, ArgInfo("bytes", 0)) &&
        jsopencv_to_safe(info, pyobj_rotationInvariance, rotationInvariance, ArgInfo("rotationInvariance", 0)) &&
        jsopencv_to_safe(info, pyobj_half_ssd_size, half_ssd_size, ArgInfo("half_ssd_size", 0)) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::LATCH::create(bytes, rotationInvariance, half_ssd_size, sigma));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getBytes(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBytes());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getHalfSSDsize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getHalfSSDsize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getRotationInvariance(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRotationInvariance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setBytes(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    Napi::Value* pyobj_bytes = NULL;
    int bytes=0;

    const char* keywords[] = { "bytes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_LATCH.setBytes", (char**)keywords, &pyobj_bytes) &&
        jsopencv_to_safe(info, pyobj_bytes, bytes, ArgInfo("bytes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBytes(bytes));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setHalfSSDsize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    Napi::Value* pyobj_half_ssd_size = NULL;
    int half_ssd_size=0;

    const char* keywords[] = { "half_ssd_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_LATCH.setHalfSSDsize", (char**)keywords, &pyobj_half_ssd_size) &&
        jsopencv_to_safe(info, pyobj_half_ssd_size, half_ssd_size, ArgInfo("half_ssd_size", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setHalfSSDsize(half_ssd_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setRotationInvariance(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    Napi::Value* pyobj_rotationInvariance = NULL;
    bool rotationInvariance=0;

    const char* keywords[] = { "rotationInvariance", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_LATCH.setRotationInvariance", (char**)keywords, &pyobj_rotationInvariance) &&
        jsopencv_to_safe(info, pyobj_rotationInvariance, rotationInvariance, ArgInfo("rotationInvariance", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRotationInvariance(rotationInvariance));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LATCH> * self1 = 0;
    if (!pyopencv_xfeatures2d_LATCH_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LATCH' or its derivative)");
    Ptr<cv::xfeatures2d::LATCH> _self_ = *(self1);
    Napi::Value* pyobj_sigma = NULL;
    double sigma=0;

    const char* keywords[] = { "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_LATCH.setSigma", (char**)keywords, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigma(sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_LATCH)

static PyGetSetDef pyopencv_xfeatures2d_LATCH_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_LATCH_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_create_static, METH_STATIC), "create([, bytes[, rotationInvariance[, half_ssd_size[, sigma]]]]) -> retval\n."},
    {"getBytes", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getBytes, 0), "getBytes() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getHalfSSDsize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getHalfSSDsize, 0), "getHalfSSDsize() -> retval\n."},
    {"getRotationInvariance", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getRotationInvariance, 0), "getRotationInvariance() -> retval\n."},
    {"getSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_getSigma, 0), "getSigma() -> retval\n."},
    {"setBytes", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setBytes, 0), "setBytes(bytes) -> None\n."},
    {"setHalfSSDsize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setHalfSSDsize, 0), "setHalfSSDsize(half_ssd_size) -> None\n."},
    {"setRotationInvariance", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setRotationInvariance, 0), "setRotationInvariance(rotationInvariance) -> None\n."},
    {"setSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_setSigma, 0), "setSigma(sigma) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_LATCH)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::LATCH> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::LATCH>& r)
    {
        return pyopencv_xfeatures2d_LATCH_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::LATCH>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::LATCH> * dst_;
        if (pyopencv_xfeatures2d_LATCH_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::LATCH> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_LUCID (Generic)
//================================================================================

// GetSet (xfeatures2d_LUCID)



// Methods (xfeatures2d_LUCID)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_lucid_kernel = NULL;
    int lucid_kernel=1;
    Napi::Value* pyobj_blur_kernel = NULL;
    int blur_kernel=2;
    Ptr<LUCID> retval;

    const char* keywords[] = { "lucid_kernel", "blur_kernel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:xfeatures2d_LUCID.create", (char**)keywords, &pyobj_lucid_kernel, &pyobj_blur_kernel) &&
        jsopencv_to_safe(info, pyobj_lucid_kernel, lucid_kernel, ArgInfo("lucid_kernel", 0)) &&
        jsopencv_to_safe(info, pyobj_blur_kernel, blur_kernel, ArgInfo("blur_kernel", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::LUCID::create(lucid_kernel, blur_kernel));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_getBlurKernel(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LUCID> * self1 = 0;
    if (!pyopencv_xfeatures2d_LUCID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LUCID' or its derivative)");
    Ptr<cv::xfeatures2d::LUCID> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBlurKernel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LUCID> * self1 = 0;
    if (!pyopencv_xfeatures2d_LUCID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LUCID' or its derivative)");
    Ptr<cv::xfeatures2d::LUCID> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_getLucidKernel(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LUCID> * self1 = 0;
    if (!pyopencv_xfeatures2d_LUCID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LUCID' or its derivative)");
    Ptr<cv::xfeatures2d::LUCID> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLucidKernel());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_setBlurKernel(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LUCID> * self1 = 0;
    if (!pyopencv_xfeatures2d_LUCID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LUCID' or its derivative)");
    Ptr<cv::xfeatures2d::LUCID> _self_ = *(self1);
    Napi::Value* pyobj_blur_kernel = NULL;
    int blur_kernel=0;

    const char* keywords[] = { "blur_kernel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_LUCID.setBlurKernel", (char**)keywords, &pyobj_blur_kernel) &&
        jsopencv_to_safe(info, pyobj_blur_kernel, blur_kernel, ArgInfo("blur_kernel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBlurKernel(blur_kernel));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_setLucidKernel(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::LUCID> * self1 = 0;
    if (!pyopencv_xfeatures2d_LUCID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_LUCID' or its derivative)");
    Ptr<cv::xfeatures2d::LUCID> _self_ = *(self1);
    Napi::Value* pyobj_lucid_kernel = NULL;
    int lucid_kernel=0;

    const char* keywords[] = { "lucid_kernel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_LUCID.setLucidKernel", (char**)keywords, &pyobj_lucid_kernel) &&
        jsopencv_to_safe(info, pyobj_lucid_kernel, lucid_kernel, ArgInfo("lucid_kernel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLucidKernel(lucid_kernel));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_LUCID)

static PyGetSetDef pyopencv_xfeatures2d_LUCID_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_LUCID_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_create_static, METH_STATIC), "create([, lucid_kernel[, blur_kernel]]) -> retval\n.   * @param lucid_kernel kernel for descriptor construction, where 1=3x3, 2=5x5, 3=7x7 and so forth\n.        * @param blur_kernel kernel for blurring image prior to descriptor construction, where 1=3x3, 2=5x5, 3=7x7 and so forth"},
    {"getBlurKernel", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_getBlurKernel, 0), "getBlurKernel() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getLucidKernel", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_getLucidKernel, 0), "getLucidKernel() -> retval\n."},
    {"setBlurKernel", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_setBlurKernel, 0), "setBlurKernel(blur_kernel) -> None\n."},
    {"setLucidKernel", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_setLucidKernel, 0), "setLucidKernel(lucid_kernel) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_LUCID)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::LUCID> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::LUCID>& r)
    {
        return pyopencv_xfeatures2d_LUCID_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::LUCID>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::LUCID> * dst_;
        if (pyopencv_xfeatures2d_LUCID_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::LUCID> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_MSDDetector (Generic)
//================================================================================

// GetSet (xfeatures2d_MSDDetector)



// Methods (xfeatures2d_MSDDetector)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_m_patch_radius = NULL;
    int m_patch_radius=3;
    Napi::Value* pyobj_m_search_area_radius = NULL;
    int m_search_area_radius=5;
    Napi::Value* pyobj_m_nms_radius = NULL;
    int m_nms_radius=5;
    Napi::Value* pyobj_m_nms_scale_radius = NULL;
    int m_nms_scale_radius=0;
    Napi::Value* pyobj_m_th_saliency = NULL;
    float m_th_saliency=250.0f;
    Napi::Value* pyobj_m_kNN = NULL;
    int m_kNN=4;
    Napi::Value* pyobj_m_scale_factor = NULL;
    float m_scale_factor=1.25f;
    Napi::Value* pyobj_m_n_scales = NULL;
    int m_n_scales=-1;
    Napi::Value* pyobj_m_compute_orientation = NULL;
    bool m_compute_orientation=false;
    Ptr<MSDDetector> retval;

    const char* keywords[] = { "m_patch_radius", "m_search_area_radius", "m_nms_radius", "m_nms_scale_radius", "m_th_saliency", "m_kNN", "m_scale_factor", "m_n_scales", "m_compute_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOOO:xfeatures2d_MSDDetector.create", (char**)keywords, &pyobj_m_patch_radius, &pyobj_m_search_area_radius, &pyobj_m_nms_radius, &pyobj_m_nms_scale_radius, &pyobj_m_th_saliency, &pyobj_m_kNN, &pyobj_m_scale_factor, &pyobj_m_n_scales, &pyobj_m_compute_orientation) &&
        jsopencv_to_safe(info, pyobj_m_patch_radius, m_patch_radius, ArgInfo("m_patch_radius", 0)) &&
        jsopencv_to_safe(info, pyobj_m_search_area_radius, m_search_area_radius, ArgInfo("m_search_area_radius", 0)) &&
        jsopencv_to_safe(info, pyobj_m_nms_radius, m_nms_radius, ArgInfo("m_nms_radius", 0)) &&
        jsopencv_to_safe(info, pyobj_m_nms_scale_radius, m_nms_scale_radius, ArgInfo("m_nms_scale_radius", 0)) &&
        jsopencv_to_safe(info, pyobj_m_th_saliency, m_th_saliency, ArgInfo("m_th_saliency", 0)) &&
        jsopencv_to_safe(info, pyobj_m_kNN, m_kNN, ArgInfo("m_kNN", 0)) &&
        jsopencv_to_safe(info, pyobj_m_scale_factor, m_scale_factor, ArgInfo("m_scale_factor", 0)) &&
        jsopencv_to_safe(info, pyobj_m_n_scales, m_n_scales, ArgInfo("m_n_scales", 0)) &&
        jsopencv_to_safe(info, pyobj_m_compute_orientation, m_compute_orientation, ArgInfo("m_compute_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::MSDDetector::create(m_patch_radius, m_search_area_radius, m_nms_radius, m_nms_scale_radius, m_th_saliency, m_kNN, m_scale_factor, m_n_scales, m_compute_orientation));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getComputeOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getComputeOrientation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getKNN(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getKNN());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getNScales(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNScales());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getNmsRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNmsRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getNmsScaleRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNmsScaleRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getPatchRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getPatchRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleFactor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getSearchAreaRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSearchAreaRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getThSaliency(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getThSaliency());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setComputeOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_compute_orientation = NULL;
    bool compute_orientation=0;

    const char* keywords[] = { "compute_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setComputeOrientation", (char**)keywords, &pyobj_compute_orientation) &&
        jsopencv_to_safe(info, pyobj_compute_orientation, compute_orientation, ArgInfo("compute_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setComputeOrientation(compute_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setKNN(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_kNN = NULL;
    int kNN=0;

    const char* keywords[] = { "kNN", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setKNN", (char**)keywords, &pyobj_kNN) &&
        jsopencv_to_safe(info, pyobj_kNN, kNN, ArgInfo("kNN", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setKNN(kNN));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setNScales(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_use_orientation = NULL;
    int use_orientation=0;

    const char* keywords[] = { "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setNScales", (char**)keywords, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNScales(use_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setNmsRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_nms_radius = NULL;
    int nms_radius=0;

    const char* keywords[] = { "nms_radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setNmsRadius", (char**)keywords, &pyobj_nms_radius) &&
        jsopencv_to_safe(info, pyobj_nms_radius, nms_radius, ArgInfo("nms_radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNmsRadius(nms_radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setNmsScaleRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_nms_scale_radius = NULL;
    int nms_scale_radius=0;

    const char* keywords[] = { "nms_scale_radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setNmsScaleRadius", (char**)keywords, &pyobj_nms_scale_radius) &&
        jsopencv_to_safe(info, pyobj_nms_scale_radius, nms_scale_radius, ArgInfo("nms_scale_radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNmsScaleRadius(nms_scale_radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setPatchRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_patch_radius = NULL;
    int patch_radius=0;

    const char* keywords[] = { "patch_radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setPatchRadius", (char**)keywords, &pyobj_patch_radius) &&
        jsopencv_to_safe(info, pyobj_patch_radius, patch_radius, ArgInfo("patch_radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setPatchRadius(patch_radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setScaleFactor", (char**)keywords, &pyobj_scale_factor) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setSearchAreaRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_use_orientation = NULL;
    int use_orientation=0;

    const char* keywords[] = { "use_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setSearchAreaRadius", (char**)keywords, &pyobj_use_orientation) &&
        jsopencv_to_safe(info, pyobj_use_orientation, use_orientation, ArgInfo("use_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSearchAreaRadius(use_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setThSaliency(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::MSDDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_MSDDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_MSDDetector' or its derivative)");
    Ptr<cv::xfeatures2d::MSDDetector> _self_ = *(self1);
    Napi::Value* pyobj_th_saliency = NULL;
    float th_saliency=0.f;

    const char* keywords[] = { "th_saliency", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_MSDDetector.setThSaliency", (char**)keywords, &pyobj_th_saliency) &&
        jsopencv_to_safe(info, pyobj_th_saliency, th_saliency, ArgInfo("th_saliency", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setThSaliency(th_saliency));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_MSDDetector)

static PyGetSetDef pyopencv_xfeatures2d_MSDDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_MSDDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_create_static, METH_STATIC), "create([, m_patch_radius[, m_search_area_radius[, m_nms_radius[, m_nms_scale_radius[, m_th_saliency[, m_kNN[, m_scale_factor[, m_n_scales[, m_compute_orientation]]]]]]]]]) -> retval\n."},
    {"getComputeOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getComputeOrientation, 0), "getComputeOrientation() -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getKNN", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getKNN, 0), "getKNN() -> retval\n."},
    {"getNScales", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getNScales, 0), "getNScales() -> retval\n."},
    {"getNmsRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getNmsRadius, 0), "getNmsRadius() -> retval\n."},
    {"getNmsScaleRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getNmsScaleRadius, 0), "getNmsScaleRadius() -> retval\n."},
    {"getPatchRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getPatchRadius, 0), "getPatchRadius() -> retval\n."},
    {"getScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getSearchAreaRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getSearchAreaRadius, 0), "getSearchAreaRadius() -> retval\n."},
    {"getThSaliency", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_getThSaliency, 0), "getThSaliency() -> retval\n."},
    {"setComputeOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setComputeOrientation, 0), "setComputeOrientation(compute_orientation) -> None\n."},
    {"setKNN", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setKNN, 0), "setKNN(kNN) -> None\n."},
    {"setNScales", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setNScales, 0), "setNScales(use_orientation) -> None\n."},
    {"setNmsRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setNmsRadius, 0), "setNmsRadius(nms_radius) -> None\n."},
    {"setNmsScaleRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setNmsScaleRadius, 0), "setNmsScaleRadius(nms_scale_radius) -> None\n."},
    {"setPatchRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setPatchRadius, 0), "setPatchRadius(patch_radius) -> None\n."},
    {"setScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},
    {"setSearchAreaRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setSearchAreaRadius, 0), "setSearchAreaRadius(use_orientation) -> None\n."},
    {"setThSaliency", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_MSDDetector_setThSaliency, 0), "setThSaliency(th_saliency) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_MSDDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::MSDDetector> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::MSDDetector>& r)
    {
        return pyopencv_xfeatures2d_MSDDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::MSDDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::MSDDetector> * dst_;
        if (pyopencv_xfeatures2d_MSDDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::MSDDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_PCTSignatures (Generic)
//================================================================================

// GetSet (xfeatures2d_PCTSignatures)



// Methods (xfeatures2d_PCTSignatures)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignature(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_signature = NULL;
    Mat signature;

    const char* keywords[] = { "image", "signature", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:xfeatures2d_PCTSignatures.computeSignature", (char**)keywords, &pyobj_image, &pyobj_signature) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_signature, signature, ArgInfo("signature", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->computeSignature(image, signature));
        return jsopencv_from(signature);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_signature = NULL;
    UMat signature;

    const char* keywords[] = { "image", "signature", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:xfeatures2d_PCTSignatures.computeSignature", (char**)keywords, &pyobj_image, &pyobj_signature) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_signature, signature, ArgInfo("signature", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->computeSignature(image, signature));
        return jsopencv_from(signature);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("computeSignature");

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignatures(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_images = NULL;
    vector_Mat images;
    Napi::Value* pyobj_signatures = NULL;
    vector_Mat signatures;

    const char* keywords[] = { "images", "signatures", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignatures.computeSignatures", (char**)keywords, &pyobj_images, &pyobj_signatures) &&
        jsopencv_to_safe(info, pyobj_images, images, ArgInfo("images", 0)) &&
        jsopencv_to_safe(info, pyobj_signatures, signatures, ArgInfo("signatures", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->computeSignatures(images, signatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    pyPrepareArgumentConversionErrorsStorage(3);

    {
    Napi::Value* pyobj_initSampleCount = NULL;
    int initSampleCount=2000;
    Napi::Value* pyobj_initSeedCount = NULL;
    int initSeedCount=400;
    Napi::Value* pyobj_pointDistribution = NULL;
    int pointDistribution=0;
    Ptr<PCTSignatures> retval;

    const char* keywords[] = { "initSampleCount", "initSeedCount", "pointDistribution", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:xfeatures2d_PCTSignatures.create", (char**)keywords, &pyobj_initSampleCount, &pyobj_initSeedCount, &pyobj_pointDistribution) &&
        jsopencv_to_safe(info, pyobj_initSampleCount, initSampleCount, ArgInfo("initSampleCount", 0)) &&
        jsopencv_to_safe(info, pyobj_initSeedCount, initSeedCount, ArgInfo("initSeedCount", 0)) &&
        jsopencv_to_safe(info, pyobj_pointDistribution, pointDistribution, ArgInfo("pointDistribution", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::PCTSignatures::create(initSampleCount, initSeedCount, pointDistribution));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_initSamplingPoints = NULL;
    vector_Point2f initSamplingPoints;
    Napi::Value* pyobj_initSeedCount = NULL;
    int initSeedCount=0;
    Ptr<PCTSignatures> retval;

    const char* keywords[] = { "initSamplingPoints", "initSeedCount", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignatures.create", (char**)keywords, &pyobj_initSamplingPoints, &pyobj_initSeedCount) &&
        jsopencv_to_safe(info, pyobj_initSamplingPoints, initSamplingPoints, ArgInfo("initSamplingPoints", 0)) &&
        jsopencv_to_safe(info, pyobj_initSeedCount, initSeedCount, ArgInfo("initSeedCount", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::PCTSignatures::create(initSamplingPoints, initSeedCount));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_initSamplingPoints = NULL;
    vector_Point2f initSamplingPoints;
    Napi::Value* pyobj_initClusterSeedIndexes = NULL;
    vector_int initClusterSeedIndexes;
    Ptr<PCTSignatures> retval;

    const char* keywords[] = { "initSamplingPoints", "initClusterSeedIndexes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignatures.create", (char**)keywords, &pyobj_initSamplingPoints, &pyobj_initClusterSeedIndexes) &&
        jsopencv_to_safe(info, pyobj_initSamplingPoints, initSamplingPoints, ArgInfo("initSamplingPoints", 0)) &&
        jsopencv_to_safe(info, pyobj_initClusterSeedIndexes, initClusterSeedIndexes, ArgInfo("initClusterSeedIndexes", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::PCTSignatures::create(initSamplingPoints, initClusterSeedIndexes));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("create");

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_drawSignature_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_source = NULL;
    Mat source;
    Napi::Value* pyobj_signature = NULL;
    Mat signature;
    Napi::Value* pyobj_result = NULL;
    Mat result;
    Napi::Value* pyobj_radiusToShorterSideRatio = NULL;
    float radiusToShorterSideRatio=1.0 / 8;
    Napi::Value* pyobj_borderThickness = NULL;
    int borderThickness=1;

    const char* keywords[] = { "source", "signature", "result", "radiusToShorterSideRatio", "borderThickness", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:xfeatures2d_PCTSignatures.drawSignature", (char**)keywords, &pyobj_source, &pyobj_signature, &pyobj_result, &pyobj_radiusToShorterSideRatio, &pyobj_borderThickness) &&
        jsopencv_to_safe(info, pyobj_source, source, ArgInfo("source", 0)) &&
        jsopencv_to_safe(info, pyobj_signature, signature, ArgInfo("signature", 0)) &&
        jsopencv_to_safe(info, pyobj_result, result, ArgInfo("result", 1)) &&
        jsopencv_to_safe(info, pyobj_radiusToShorterSideRatio, radiusToShorterSideRatio, ArgInfo("radiusToShorterSideRatio", 0)) &&
        jsopencv_to_safe(info, pyobj_borderThickness, borderThickness, ArgInfo("borderThickness", 0)))
    {
        ERRWRAP2_NAPI(info, cv::xfeatures2d::PCTSignatures::drawSignature(source, signature, result, radiusToShorterSideRatio, borderThickness));
        return jsopencv_from(result);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_source = NULL;
    UMat source;
    Napi::Value* pyobj_signature = NULL;
    UMat signature;
    Napi::Value* pyobj_result = NULL;
    UMat result;
    Napi::Value* pyobj_radiusToShorterSideRatio = NULL;
    float radiusToShorterSideRatio=1.0 / 8;
    Napi::Value* pyobj_borderThickness = NULL;
    int borderThickness=1;

    const char* keywords[] = { "source", "signature", "result", "radiusToShorterSideRatio", "borderThickness", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:xfeatures2d_PCTSignatures.drawSignature", (char**)keywords, &pyobj_source, &pyobj_signature, &pyobj_result, &pyobj_radiusToShorterSideRatio, &pyobj_borderThickness) &&
        jsopencv_to_safe(info, pyobj_source, source, ArgInfo("source", 0)) &&
        jsopencv_to_safe(info, pyobj_signature, signature, ArgInfo("signature", 0)) &&
        jsopencv_to_safe(info, pyobj_result, result, ArgInfo("result", 1)) &&
        jsopencv_to_safe(info, pyobj_radiusToShorterSideRatio, radiusToShorterSideRatio, ArgInfo("radiusToShorterSideRatio", 0)) &&
        jsopencv_to_safe(info, pyobj_borderThickness, borderThickness, ArgInfo("borderThickness", 0)))
    {
        ERRWRAP2_NAPI(info, cv::xfeatures2d::PCTSignatures::drawSignature(source, signature, result, radiusToShorterSideRatio, borderThickness));
        return jsopencv_from(result);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("drawSignature");

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_generateInitPoints_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_initPoints = NULL;
    vector_Point2f initPoints;
    Napi::Value* pyobj_count = NULL;
    int count=0;
    Napi::Value* pyobj_pointDistribution = NULL;
    int pointDistribution=0;

    const char* keywords[] = { "initPoints", "count", "pointDistribution", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:xfeatures2d_PCTSignatures.generateInitPoints", (char**)keywords, &pyobj_initPoints, &pyobj_count, &pyobj_pointDistribution) &&
        jsopencv_to_safe(info, pyobj_initPoints, initPoints, ArgInfo("initPoints", 0)) &&
        jsopencv_to_safe(info, pyobj_count, count, ArgInfo("count", 0)) &&
        jsopencv_to_safe(info, pyobj_pointDistribution, pointDistribution, ArgInfo("pointDistribution", 0)))
    {
        ERRWRAP2_NAPI(info, cv::xfeatures2d::PCTSignatures::generateInitPoints(initPoints, count, pointDistribution));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getClusterMinSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getClusterMinSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDistanceFunction(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDistanceFunction());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDropThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDropThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getGrayscaleBits(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGrayscaleBits());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedCount(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getInitSeedCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedIndexes(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    std::vector<int> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getInitSeedIndexes());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getIterationCount(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getIterationCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getJoiningDistance(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getJoiningDistance());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getMaxClustersCount(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxClustersCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSampleCount(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSampleCount());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSamplingPoints(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    std::vector<Point2f> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSamplingPoints());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightA(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightA());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightB(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightB());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightContrast(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightContrast());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightEntropy(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightEntropy());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightL(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightL());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightX(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightX());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightY(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWeightY());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWindowRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getWindowRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setClusterMinSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_clusterMinSize = NULL;
    int clusterMinSize=0;

    const char* keywords[] = { "clusterMinSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setClusterMinSize", (char**)keywords, &pyobj_clusterMinSize) &&
        jsopencv_to_safe(info, pyobj_clusterMinSize, clusterMinSize, ArgInfo("clusterMinSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setClusterMinSize(clusterMinSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDistanceFunction(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_distanceFunction = NULL;
    int distanceFunction=0;

    const char* keywords[] = { "distanceFunction", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setDistanceFunction", (char**)keywords, &pyobj_distanceFunction) &&
        jsopencv_to_safe(info, pyobj_distanceFunction, distanceFunction, ArgInfo("distanceFunction", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDistanceFunction(distanceFunction));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDropThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_dropThreshold = NULL;
    float dropThreshold=0.f;

    const char* keywords[] = { "dropThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setDropThreshold", (char**)keywords, &pyobj_dropThreshold) &&
        jsopencv_to_safe(info, pyobj_dropThreshold, dropThreshold, ArgInfo("dropThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDropThreshold(dropThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setGrayscaleBits(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_grayscaleBits = NULL;
    int grayscaleBits=0;

    const char* keywords[] = { "grayscaleBits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setGrayscaleBits", (char**)keywords, &pyobj_grayscaleBits) &&
        jsopencv_to_safe(info, pyobj_grayscaleBits, grayscaleBits, ArgInfo("grayscaleBits", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGrayscaleBits(grayscaleBits));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setInitSeedIndexes(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_initSeedIndexes = NULL;
    vector_int initSeedIndexes;

    const char* keywords[] = { "initSeedIndexes", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setInitSeedIndexes", (char**)keywords, &pyobj_initSeedIndexes) &&
        jsopencv_to_safe(info, pyobj_initSeedIndexes, initSeedIndexes, ArgInfo("initSeedIndexes", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setInitSeedIndexes(initSeedIndexes));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setIterationCount(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_iterationCount = NULL;
    int iterationCount=0;

    const char* keywords[] = { "iterationCount", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setIterationCount", (char**)keywords, &pyobj_iterationCount) &&
        jsopencv_to_safe(info, pyobj_iterationCount, iterationCount, ArgInfo("iterationCount", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setIterationCount(iterationCount));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setJoiningDistance(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_joiningDistance = NULL;
    float joiningDistance=0.f;

    const char* keywords[] = { "joiningDistance", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setJoiningDistance", (char**)keywords, &pyobj_joiningDistance) &&
        jsopencv_to_safe(info, pyobj_joiningDistance, joiningDistance, ArgInfo("joiningDistance", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setJoiningDistance(joiningDistance));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setMaxClustersCount(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_maxClustersCount = NULL;
    int maxClustersCount=0;

    const char* keywords[] = { "maxClustersCount", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setMaxClustersCount", (char**)keywords, &pyobj_maxClustersCount) &&
        jsopencv_to_safe(info, pyobj_maxClustersCount, maxClustersCount, ArgInfo("maxClustersCount", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxClustersCount(maxClustersCount));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setSamplingPoints(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_samplingPoints = NULL;
    vector_Point2f samplingPoints;

    const char* keywords[] = { "samplingPoints", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setSamplingPoints", (char**)keywords, &pyobj_samplingPoints) &&
        jsopencv_to_safe(info, pyobj_samplingPoints, samplingPoints, ArgInfo("samplingPoints", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSamplingPoints(samplingPoints));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_idx = NULL;
    int idx=0;
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "idx", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignatures.setTranslation", (char**)keywords, &pyobj_idx, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTranslation(idx, value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslations(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_translations = NULL;
    vector_float translations;

    const char* keywords[] = { "translations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setTranslations", (char**)keywords, &pyobj_translations) &&
        jsopencv_to_safe(info, pyobj_translations, translations, ArgInfo("translations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setTranslations(translations));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeight(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_idx = NULL;
    int idx=0;
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "idx", "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignatures.setWeight", (char**)keywords, &pyobj_idx, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_idx, idx, ArgInfo("idx", 0)) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeight(idx, value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightA(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightA", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightA(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightB(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightB", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightB(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightContrast(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightContrast", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightContrast(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightEntropy(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightEntropy", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightEntropy(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightL(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightL", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightL(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightX(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightX", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightX(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightY(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeightY", (char**)keywords, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeightY(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeights(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_weights = NULL;
    vector_float weights;

    const char* keywords[] = { "weights", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWeights", (char**)keywords, &pyobj_weights) &&
        jsopencv_to_safe(info, pyobj_weights, weights, ArgInfo("weights", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWeights(weights));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWindowRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignatures> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignatures_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignatures> _self_ = *(self1);
    Napi::Value* pyobj_radius = NULL;
    int radius=0;

    const char* keywords[] = { "radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_PCTSignatures.setWindowRadius", (char**)keywords, &pyobj_radius) &&
        jsopencv_to_safe(info, pyobj_radius, radius, ArgInfo("radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setWindowRadius(radius));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_PCTSignatures)

static PyGetSetDef pyopencv_xfeatures2d_PCTSignatures_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_PCTSignatures_methods[] =
{
    {"computeSignature", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignature, 0), "computeSignature(image[, signature]) -> signature\n.   * @brief Computes signature of given image.\n.       * @param image Input image of CV_8U type.\n.       * @param signature Output computed signature."},
    {"computeSignatures", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignatures, 0), "computeSignatures(images, signatures) -> None\n.   * @brief Computes signatures for multiple images in parallel.\n.       * @param images Vector of input images of CV_8U type.\n.       * @param signatures Vector of computed signatures."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_create_static, METH_STATIC), "create([, initSampleCount[, initSeedCount[, pointDistribution]]]) -> retval\n.   * @brief Creates PCTSignatures algorithm using sample and seed count.\n.       *       It generates its own sets of sampling points and clusterization seed indexes.\n.       * @param initSampleCount Number of points used for image sampling.\n.       * @param initSeedCount Number of initial clusterization seeds.\n.       *       Must be lower or equal to initSampleCount\n.       * @param pointDistribution Distribution of generated points. Default: UNIFORM.\n.       *       Available: UNIFORM, REGULAR, NORMAL.\n.       * @return Created algorithm.\n\n\n\ncreate(initSamplingPoints, initSeedCount) -> retval\n.   * @brief Creates PCTSignatures algorithm using pre-generated sampling points\n.       *       and number of clusterization seeds. It uses the provided\n.       *       sampling points and generates its own clusterization seed indexes.\n.       * @param initSamplingPoints Sampling points used in image sampling.\n.       * @param initSeedCount Number of initial clusterization seeds.\n.       *       Must be lower or equal to initSamplingPoints.size().\n.       * @return Created algorithm.\n\n\n\ncreate(initSamplingPoints, initClusterSeedIndexes) -> retval\n.   * @brief Creates PCTSignatures algorithm using pre-generated sampling points\n.       *       and clusterization seeds indexes.\n.       * @param initSamplingPoints Sampling points used in image sampling.\n.       * @param initClusterSeedIndexes Indexes of initial clusterization seeds.\n.       *       Its size must be lower or equal to initSamplingPoints.size().\n.       * @return Created algorithm."},
    {"drawSignature", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_drawSignature_static, METH_STATIC), "drawSignature(source, signature[, result[, radiusToShorterSideRatio[, borderThickness]]]) -> result\n.   * @brief Draws signature in the source image and outputs the result.\n.       *       Signatures are visualized as a circle\n.       *       with radius based on signature weight\n.       *       and color based on signature color.\n.       *       Contrast and entropy are not visualized.\n.       * @param source Source image.\n.       * @param signature Image signature.\n.       * @param result Output result.\n.       * @param radiusToShorterSideRatio Determines maximal radius of signature in the output image.\n.       * @param borderThickness Border thickness of the visualized signature."},
    {"generateInitPoints", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_generateInitPoints_static, METH_STATIC), "generateInitPoints(initPoints, count, pointDistribution) -> None\n.   * @brief Generates initial sampling points according to selected point distribution.\n.       * @param initPoints Output vector where the generated points will be saved.\n.       * @param count Number of points to generate.\n.       * @param pointDistribution Point distribution selector.\n.       *       Available: UNIFORM, REGULAR, NORMAL.\n.       * @note Generated coordinates are in range [0..1)"},
    {"getClusterMinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getClusterMinSize, 0), "getClusterMinSize() -> retval\n.   * @brief This parameter multiplied by the index of iteration gives lower limit for cluster size.\n.       *       Clusters containing fewer points than specified by the limit have their centroid dismissed\n.       *       and points are reassigned."},
    {"getDistanceFunction", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDistanceFunction, 0), "getDistanceFunction() -> retval\n.   * @brief Distance function selector used for measuring distance between two points in k-means."},
    {"getDropThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDropThreshold, 0), "getDropThreshold() -> retval\n.   * @brief Remove centroids in k-means whose weight is lesser or equal to given threshold."},
    {"getGrayscaleBits", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getGrayscaleBits, 0), "getGrayscaleBits() -> retval\n.   * @brief Color resolution of the greyscale bitmap represented in allocated bits\n.       *       (i.e., value 4 means that 16 shades of grey are used).\n.       *       The greyscale bitmap is used for computing contrast and entropy values."},
    {"getInitSeedCount", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedCount, 0), "getInitSeedCount() -> retval\n.   * @brief Number of initial seeds (initial number of clusters) for the k-means algorithm."},
    {"getInitSeedIndexes", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedIndexes, 0), "getInitSeedIndexes() -> retval\n.   * @brief Initial seeds (initial number of clusters) for the k-means algorithm."},
    {"getIterationCount", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getIterationCount, 0), "getIterationCount() -> retval\n.   * @brief Number of iterations of the k-means clustering.\n.       *       We use fixed number of iterations, since the modified clustering is pruning clusters\n.       *       (not iteratively refining k clusters)."},
    {"getJoiningDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getJoiningDistance, 0), "getJoiningDistance() -> retval\n.   * @brief Threshold euclidean distance between two centroids.\n.       *       If two cluster centers are closer than this distance,\n.       *       one of the centroid is dismissed and points are reassigned."},
    {"getMaxClustersCount", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getMaxClustersCount, 0), "getMaxClustersCount() -> retval\n.   * @brief Maximal number of generated clusters. If the number is exceeded,\n.       *       the clusters are sorted by their weights and the smallest clusters are cropped."},
    {"getSampleCount", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSampleCount, 0), "getSampleCount() -> retval\n.   * @brief Number of initial samples taken from the image."},
    {"getSamplingPoints", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSamplingPoints, 0), "getSamplingPoints() -> retval\n.   * @brief Initial samples taken from the image.\n.       *       These sampled features become the input for clustering."},
    {"getWeightA", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightA, 0), "getWeightA() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightB", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightB, 0), "getWeightB() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightContrast", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightContrast, 0), "getWeightContrast() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightEntropy", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightEntropy, 0), "getWeightEntropy() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightL", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightL, 0), "getWeightL() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightX", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightX, 0), "getWeightX() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightY", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightY, 0), "getWeightY() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWindowRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWindowRadius, 0), "getWindowRadius() -> retval\n.   * @brief Size of the texture sampling window used to compute contrast and entropy\n.       *       (center of the window is always in the pixel selected by x,y coordinates\n.       *       of the corresponding feature sample)."},
    {"setClusterMinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setClusterMinSize, 0), "setClusterMinSize(clusterMinSize) -> None\n.   * @brief This parameter multiplied by the index of iteration gives lower limit for cluster size.\n.       *       Clusters containing fewer points than specified by the limit have their centroid dismissed\n.       *       and points are reassigned."},
    {"setDistanceFunction", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDistanceFunction, 0), "setDistanceFunction(distanceFunction) -> None\n.   * @brief Distance function selector used for measuring distance between two points in k-means.\n.       *       Available: L0_25, L0_5, L1, L2, L2SQUARED, L5, L_INFINITY."},
    {"setDropThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDropThreshold, 0), "setDropThreshold(dropThreshold) -> None\n.   * @brief Remove centroids in k-means whose weight is lesser or equal to given threshold."},
    {"setGrayscaleBits", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setGrayscaleBits, 0), "setGrayscaleBits(grayscaleBits) -> None\n.   * @brief Color resolution of the greyscale bitmap represented in allocated bits\n.       *       (i.e., value 4 means that 16 shades of grey are used).\n.       *       The greyscale bitmap is used for computing contrast and entropy values."},
    {"setInitSeedIndexes", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setInitSeedIndexes, 0), "setInitSeedIndexes(initSeedIndexes) -> None\n.   * @brief Initial seed indexes for the k-means algorithm."},
    {"setIterationCount", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setIterationCount, 0), "setIterationCount(iterationCount) -> None\n.   * @brief Number of iterations of the k-means clustering.\n.       *       We use fixed number of iterations, since the modified clustering is pruning clusters\n.       *       (not iteratively refining k clusters)."},
    {"setJoiningDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setJoiningDistance, 0), "setJoiningDistance(joiningDistance) -> None\n.   * @brief Threshold euclidean distance between two centroids.\n.       *       If two cluster centers are closer than this distance,\n.       *       one of the centroid is dismissed and points are reassigned."},
    {"setMaxClustersCount", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setMaxClustersCount, 0), "setMaxClustersCount(maxClustersCount) -> None\n.   * @brief Maximal number of generated clusters. If the number is exceeded,\n.       *       the clusters are sorted by their weights and the smallest clusters are cropped."},
    {"setSamplingPoints", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setSamplingPoints, 0), "setSamplingPoints(samplingPoints) -> None\n.   * @brief Sets sampling points used to sample the input image.\n.       * @param samplingPoints Vector of sampling points in range [0..1)\n.       * @note Number of sampling points must be greater or equal to clusterization seed count."},
    {"setTranslation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslation, 0), "setTranslation(idx, value) -> None\n.   * @brief Translations of the individual axes of the feature space.\n.       * @param idx ID of the translation\n.       * @param value Value of the translation\n.       * @note\n.       *       WEIGHT_IDX = 0;\n.       *       X_IDX = 1;\n.       *       Y_IDX = 2;\n.       *       L_IDX = 3;\n.       *       A_IDX = 4;\n.       *       B_IDX = 5;\n.       *       CONTRAST_IDX = 6;\n.       *       ENTROPY_IDX = 7;"},
    {"setTranslations", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslations, 0), "setTranslations(translations) -> None\n.   * @brief Translations of the individual axes of the feature space.\n.       * @param translations Values of all translations.\n.       * @note\n.       *       WEIGHT_IDX = 0;\n.       *       X_IDX = 1;\n.       *       Y_IDX = 2;\n.       *       L_IDX = 3;\n.       *       A_IDX = 4;\n.       *       B_IDX = 5;\n.       *       CONTRAST_IDX = 6;\n.       *       ENTROPY_IDX = 7;"},
    {"setWeight", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeight, 0), "setWeight(idx, value) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space.\n.       * @param idx ID of the weight\n.       * @param value Value of the weight\n.       * @note\n.       *       WEIGHT_IDX = 0;\n.       *       X_IDX = 1;\n.       *       Y_IDX = 2;\n.       *       L_IDX = 3;\n.       *       A_IDX = 4;\n.       *       B_IDX = 5;\n.       *       CONTRAST_IDX = 6;\n.       *       ENTROPY_IDX = 7;"},
    {"setWeightA", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightA, 0), "setWeightA(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightB", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightB, 0), "setWeightB(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightContrast", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightContrast, 0), "setWeightContrast(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightEntropy", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightEntropy, 0), "setWeightEntropy(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightL", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightL, 0), "setWeightL(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightX", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightX, 0), "setWeightX(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightY", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightY, 0), "setWeightY(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.       *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeights", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeights, 0), "setWeights(weights) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space.\n.       * @param weights Values of all weights.\n.       * @note\n.       *       WEIGHT_IDX = 0;\n.       *       X_IDX = 1;\n.       *       Y_IDX = 2;\n.       *       L_IDX = 3;\n.       *       A_IDX = 4;\n.       *       B_IDX = 5;\n.       *       CONTRAST_IDX = 6;\n.       *       ENTROPY_IDX = 7;"},
    {"setWindowRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWindowRadius, 0), "setWindowRadius(radius) -> None\n.   * @brief Size of the texture sampling window used to compute contrast and entropy\n.       *       (center of the window is always in the pixel selected by x,y coordinates\n.       *       of the corresponding feature sample)."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_PCTSignatures)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::PCTSignatures> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::PCTSignatures>& r)
    {
        return pyopencv_xfeatures2d_PCTSignatures_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::PCTSignatures>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::PCTSignatures> * dst_;
        if (pyopencv_xfeatures2d_PCTSignatures_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::PCTSignatures> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_PCTSignaturesSQFD (Generic)
//================================================================================

// GetSet (xfeatures2d_PCTSignaturesSQFD)



// Methods (xfeatures2d_PCTSignaturesSQFD)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistance(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignaturesSQFD> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignaturesSQFD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignaturesSQFD' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignaturesSQFD> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj__signature0 = NULL;
    Mat _signature0;
    Napi::Value* pyobj__signature1 = NULL;
    Mat _signature1;
    float retval;

    const char* keywords[] = { "_signature0", "_signature1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistance", (char**)keywords, &pyobj__signature0, &pyobj__signature1) &&
        jsopencv_to_safe(info, pyobj__signature0, _signature0, ArgInfo("_signature0", 0)) &&
        jsopencv_to_safe(info, pyobj__signature1, _signature1, ArgInfo("_signature1", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->computeQuadraticFormDistance(_signature0, _signature1));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__signature0 = NULL;
    UMat _signature0;
    Napi::Value* pyobj__signature1 = NULL;
    UMat _signature1;
    float retval;

    const char* keywords[] = { "_signature0", "_signature1", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistance", (char**)keywords, &pyobj__signature0, &pyobj__signature1) &&
        jsopencv_to_safe(info, pyobj__signature0, _signature0, ArgInfo("_signature0", 0)) &&
        jsopencv_to_safe(info, pyobj__signature1, _signature1, ArgInfo("_signature1", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->computeQuadraticFormDistance(_signature0, _signature1));
        return jsopencv_from(retval);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("computeQuadraticFormDistance");

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistances(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::PCTSignaturesSQFD> * self1 = 0;
    if (!pyopencv_xfeatures2d_PCTSignaturesSQFD_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignaturesSQFD' or its derivative)");
    Ptr<cv::xfeatures2d::PCTSignaturesSQFD> _self_ = *(self1);
    Napi::Value* pyobj_sourceSignature = NULL;
    Mat sourceSignature;
    Napi::Value* pyobj_imageSignatures = NULL;
    vector_Mat imageSignatures;
    Napi::Value* pyobj_distances = NULL;
    vector_float distances;

    const char* keywords[] = { "sourceSignature", "imageSignatures", "distances", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistances", (char**)keywords, &pyobj_sourceSignature, &pyobj_imageSignatures, &pyobj_distances) &&
        jsopencv_to_safe(info, pyobj_sourceSignature, sourceSignature, ArgInfo("sourceSignature", 0)) &&
        jsopencv_to_safe(info, pyobj_imageSignatures, imageSignatures, ArgInfo("imageSignatures", 0)) &&
        jsopencv_to_safe(info, pyobj_distances, distances, ArgInfo("distances", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->computeQuadraticFormDistances(sourceSignature, imageSignatures, distances));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_distanceFunction = NULL;
    int distanceFunction=3;
    Napi::Value* pyobj_similarityFunction = NULL;
    int similarityFunction=2;
    Napi::Value* pyobj_similarityParameter = NULL;
    float similarityParameter=1.0f;
    Ptr<PCTSignaturesSQFD> retval;

    const char* keywords[] = { "distanceFunction", "similarityFunction", "similarityParameter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:xfeatures2d_PCTSignaturesSQFD.create", (char**)keywords, &pyobj_distanceFunction, &pyobj_similarityFunction, &pyobj_similarityParameter) &&
        jsopencv_to_safe(info, pyobj_distanceFunction, distanceFunction, ArgInfo("distanceFunction", 0)) &&
        jsopencv_to_safe(info, pyobj_similarityFunction, similarityFunction, ArgInfo("similarityFunction", 0)) &&
        jsopencv_to_safe(info, pyobj_similarityParameter, similarityParameter, ArgInfo("similarityParameter", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::PCTSignaturesSQFD::create(distanceFunction, similarityFunction, similarityParameter));
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (xfeatures2d_PCTSignaturesSQFD)

static PyGetSetDef pyopencv_xfeatures2d_PCTSignaturesSQFD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_PCTSignaturesSQFD_methods[] =
{
    {"computeQuadraticFormDistance", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistance, 0), "computeQuadraticFormDistance(_signature0, _signature1) -> retval\n.   * @brief Computes Signature Quadratic Form Distance of two signatures.\n.       * @param _signature0 The first signature.\n.       * @param _signature1 The second signature."},
    {"computeQuadraticFormDistances", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistances, 0), "computeQuadraticFormDistances(sourceSignature, imageSignatures, distances) -> None\n.   * @brief Computes Signature Quadratic Form Distance between the reference signature\n.       *       and each of the other image signatures.\n.       * @param sourceSignature The signature to measure distance of other signatures from.\n.       * @param imageSignatures Vector of signatures to measure distance from the source signature.\n.       * @param distances Output vector of measured distances."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_create_static, METH_STATIC), "create([, distanceFunction[, similarityFunction[, similarityParameter]]]) -> retval\n.   * @brief Creates the algorithm instance using selected distance function,\n.       *       similarity function and similarity function parameter.\n.       * @param distanceFunction Distance function selector. Default: L2\n.       *       Available: L0_25, L0_5, L1, L2, L2SQUARED, L5, L_INFINITY\n.       * @param similarityFunction Similarity function selector. Default: HEURISTIC\n.       *       Available: MINUS, GAUSSIAN, HEURISTIC\n.       * @param similarityParameter Parameter of the similarity function."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_PCTSignaturesSQFD)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::PCTSignaturesSQFD> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::PCTSignaturesSQFD>& r)
    {
        return pyopencv_xfeatures2d_PCTSignaturesSQFD_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::PCTSignaturesSQFD>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::PCTSignaturesSQFD> * dst_;
        if (pyopencv_xfeatures2d_PCTSignaturesSQFD_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::PCTSignaturesSQFD> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_SURF (Generic)
//================================================================================

// GetSet (xfeatures2d_SURF)



// Methods (xfeatures2d_SURF)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_hessianThreshold = NULL;
    double hessianThreshold=100;
    Napi::Value* pyobj_nOctaves = NULL;
    int nOctaves=4;
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=3;
    Napi::Value* pyobj_extended = NULL;
    bool extended=false;
    Napi::Value* pyobj_upright = NULL;
    bool upright=false;
    Ptr<SURF> retval;

    const char* keywords[] = { "hessianThreshold", "nOctaves", "nOctaveLayers", "extended", "upright", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:xfeatures2d_SURF.create", (char**)keywords, &pyobj_hessianThreshold, &pyobj_nOctaves, &pyobj_nOctaveLayers, &pyobj_extended, &pyobj_upright) &&
        jsopencv_to_safe(info, pyobj_hessianThreshold, hessianThreshold, ArgInfo("hessianThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaves, nOctaves, ArgInfo("nOctaves", 0)) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)) &&
        jsopencv_to_safe(info, pyobj_extended, extended, ArgInfo("extended", 0)) &&
        jsopencv_to_safe(info, pyobj_upright, upright, ArgInfo("upright", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::SURF::create(hessianThreshold, nOctaves, nOctaveLayers, extended, upright));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getExtended(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getExtended());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getHessianThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getHessianThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaveLayers());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNOctaves());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getUpright(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUpright());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setExtended(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    Napi::Value* pyobj_extended = NULL;
    bool extended=0;

    const char* keywords[] = { "extended", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_SURF.setExtended", (char**)keywords, &pyobj_extended) &&
        jsopencv_to_safe(info, pyobj_extended, extended, ArgInfo("extended", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setExtended(extended));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setHessianThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    Napi::Value* pyobj_hessianThreshold = NULL;
    double hessianThreshold=0;

    const char* keywords[] = { "hessianThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_SURF.setHessianThreshold", (char**)keywords, &pyobj_hessianThreshold) &&
        jsopencv_to_safe(info, pyobj_hessianThreshold, hessianThreshold, ArgInfo("hessianThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setHessianThreshold(hessianThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaveLayers(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    Napi::Value* pyobj_nOctaveLayers = NULL;
    int nOctaveLayers=0;

    const char* keywords[] = { "nOctaveLayers", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_SURF.setNOctaveLayers", (char**)keywords, &pyobj_nOctaveLayers) &&
        jsopencv_to_safe(info, pyobj_nOctaveLayers, nOctaveLayers, ArgInfo("nOctaveLayers", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaveLayers(nOctaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaves(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    Napi::Value* pyobj_nOctaves = NULL;
    int nOctaves=0;

    const char* keywords[] = { "nOctaves", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_SURF.setNOctaves", (char**)keywords, &pyobj_nOctaves) &&
        jsopencv_to_safe(info, pyobj_nOctaves, nOctaves, ArgInfo("nOctaves", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNOctaves(nOctaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setUpright(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::SURF> * self1 = 0;
    if (!pyopencv_xfeatures2d_SURF_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    Ptr<cv::xfeatures2d::SURF> _self_ = *(self1);
    Napi::Value* pyobj_upright = NULL;
    bool upright=0;

    const char* keywords[] = { "upright", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_SURF.setUpright", (char**)keywords, &pyobj_upright) &&
        jsopencv_to_safe(info, pyobj_upright, upright, ArgInfo("upright", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUpright(upright));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_SURF)

static PyGetSetDef pyopencv_xfeatures2d_SURF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_SURF_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_create_static, METH_STATIC), "create([, hessianThreshold[, nOctaves[, nOctaveLayers[, extended[, upright]]]]]) -> retval\n.   @param hessianThreshold Threshold for hessian keypoint detector used in SURF.\n.       @param nOctaves Number of pyramid octaves the keypoint detector will use.\n.       @param nOctaveLayers Number of octave layers within each octave.\n.       @param extended Extended descriptor flag (true - use extended 128-element descriptors; false - use\n.       64-element descriptors).\n.       @param upright Up-right or rotated features flag (true - do not compute orientation of features;\n.       false - compute orientation)."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getExtended", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getExtended, 0), "getExtended() -> retval\n."},
    {"getHessianThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getHessianThreshold, 0), "getHessianThreshold() -> retval\n."},
    {"getNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getUpright", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getUpright, 0), "getUpright() -> retval\n."},
    {"setExtended", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setExtended, 0), "setExtended(extended) -> None\n."},
    {"setHessianThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setHessianThreshold, 0), "setHessianThreshold(hessianThreshold) -> None\n."},
    {"setNOctaveLayers", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaveLayers, 0), "setNOctaveLayers(nOctaveLayers) -> None\n."},
    {"setNOctaves", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaves, 0), "setNOctaves(nOctaves) -> None\n."},
    {"setUpright", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setUpright, 0), "setUpright(upright) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_SURF)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::SURF> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::SURF>& r)
    {
        return pyopencv_xfeatures2d_SURF_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::SURF>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::SURF> * dst_;
        if (pyopencv_xfeatures2d_SURF_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::SURF> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_StarDetector (Generic)
//================================================================================

// GetSet (xfeatures2d_StarDetector)



// Methods (xfeatures2d_StarDetector)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_maxSize = NULL;
    int maxSize=45;
    Napi::Value* pyobj_responseThreshold = NULL;
    int responseThreshold=30;
    Napi::Value* pyobj_lineThresholdProjected = NULL;
    int lineThresholdProjected=10;
    Napi::Value* pyobj_lineThresholdBinarized = NULL;
    int lineThresholdBinarized=8;
    Napi::Value* pyobj_suppressNonmaxSize = NULL;
    int suppressNonmaxSize=5;
    Ptr<StarDetector> retval;

    const char* keywords[] = { "maxSize", "responseThreshold", "lineThresholdProjected", "lineThresholdBinarized", "suppressNonmaxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOO:xfeatures2d_StarDetector.create", (char**)keywords, &pyobj_maxSize, &pyobj_responseThreshold, &pyobj_lineThresholdProjected, &pyobj_lineThresholdBinarized, &pyobj_suppressNonmaxSize) &&
        jsopencv_to_safe(info, pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) &&
        jsopencv_to_safe(info, pyobj_responseThreshold, responseThreshold, ArgInfo("responseThreshold", 0)) &&
        jsopencv_to_safe(info, pyobj_lineThresholdProjected, lineThresholdProjected, ArgInfo("lineThresholdProjected", 0)) &&
        jsopencv_to_safe(info, pyobj_lineThresholdBinarized, lineThresholdBinarized, ArgInfo("lineThresholdBinarized", 0)) &&
        jsopencv_to_safe(info, pyobj_suppressNonmaxSize, suppressNonmaxSize, ArgInfo("suppressNonmaxSize", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::StarDetector::create(maxSize, responseThreshold, lineThresholdProjected, lineThresholdBinarized, suppressNonmaxSize));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getLineThresholdBinarized(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLineThresholdBinarized());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getLineThresholdProjected(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLineThresholdProjected());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getMaxSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getResponseThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getResponseThreshold());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getSuppressNonmaxSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSuppressNonmaxSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setLineThresholdBinarized(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    Napi::Value* pyobj__lineThresholdBinarized = NULL;
    int _lineThresholdBinarized=0;

    const char* keywords[] = { "_lineThresholdBinarized", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_StarDetector.setLineThresholdBinarized", (char**)keywords, &pyobj__lineThresholdBinarized) &&
        jsopencv_to_safe(info, pyobj__lineThresholdBinarized, _lineThresholdBinarized, ArgInfo("_lineThresholdBinarized", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLineThresholdBinarized(_lineThresholdBinarized));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setLineThresholdProjected(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    Napi::Value* pyobj__lineThresholdProjected = NULL;
    int _lineThresholdProjected=0;

    const char* keywords[] = { "_lineThresholdProjected", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_StarDetector.setLineThresholdProjected", (char**)keywords, &pyobj__lineThresholdProjected) &&
        jsopencv_to_safe(info, pyobj__lineThresholdProjected, _lineThresholdProjected, ArgInfo("_lineThresholdProjected", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLineThresholdProjected(_lineThresholdProjected));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setMaxSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    Napi::Value* pyobj__maxSize = NULL;
    int _maxSize=0;

    const char* keywords[] = { "_maxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_StarDetector.setMaxSize", (char**)keywords, &pyobj__maxSize) &&
        jsopencv_to_safe(info, pyobj__maxSize, _maxSize, ArgInfo("_maxSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxSize(_maxSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setResponseThreshold(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    Napi::Value* pyobj__responseThreshold = NULL;
    int _responseThreshold=0;

    const char* keywords[] = { "_responseThreshold", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_StarDetector.setResponseThreshold", (char**)keywords, &pyobj__responseThreshold) &&
        jsopencv_to_safe(info, pyobj__responseThreshold, _responseThreshold, ArgInfo("_responseThreshold", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setResponseThreshold(_responseThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setSuppressNonmaxSize(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::StarDetector> * self1 = 0;
    if (!pyopencv_xfeatures2d_StarDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_StarDetector' or its derivative)");
    Ptr<cv::xfeatures2d::StarDetector> _self_ = *(self1);
    Napi::Value* pyobj__suppressNonmaxSize = NULL;
    int _suppressNonmaxSize=0;

    const char* keywords[] = { "_suppressNonmaxSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_StarDetector.setSuppressNonmaxSize", (char**)keywords, &pyobj__suppressNonmaxSize) &&
        jsopencv_to_safe(info, pyobj__suppressNonmaxSize, _suppressNonmaxSize, ArgInfo("_suppressNonmaxSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSuppressNonmaxSize(_suppressNonmaxSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_StarDetector)

static PyGetSetDef pyopencv_xfeatures2d_StarDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_StarDetector_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_create_static, METH_STATIC), "create([, maxSize[, responseThreshold[, lineThresholdProjected[, lineThresholdBinarized[, suppressNonmaxSize]]]]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getLineThresholdBinarized", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getLineThresholdBinarized, 0), "getLineThresholdBinarized() -> retval\n."},
    {"getLineThresholdProjected", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getLineThresholdProjected, 0), "getLineThresholdProjected() -> retval\n."},
    {"getMaxSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getMaxSize, 0), "getMaxSize() -> retval\n."},
    {"getResponseThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getResponseThreshold, 0), "getResponseThreshold() -> retval\n."},
    {"getSuppressNonmaxSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_getSuppressNonmaxSize, 0), "getSuppressNonmaxSize() -> retval\n."},
    {"setLineThresholdBinarized", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setLineThresholdBinarized, 0), "setLineThresholdBinarized(_lineThresholdBinarized) -> None\n."},
    {"setLineThresholdProjected", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setLineThresholdProjected, 0), "setLineThresholdProjected(_lineThresholdProjected) -> None\n."},
    {"setMaxSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setMaxSize, 0), "setMaxSize(_maxSize) -> None\n."},
    {"setResponseThreshold", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setResponseThreshold, 0), "setResponseThreshold(_responseThreshold) -> None\n."},
    {"setSuppressNonmaxSize", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_setSuppressNonmaxSize, 0), "setSuppressNonmaxSize(_suppressNonmaxSize) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_StarDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::StarDetector> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::StarDetector>& r)
    {
        return pyopencv_xfeatures2d_StarDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::StarDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::StarDetector> * dst_;
        if (pyopencv_xfeatures2d_StarDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::StarDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_TBMR (Generic)
//================================================================================

// GetSet (xfeatures2d_TBMR)



// Methods (xfeatures2d_TBMR)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_min_area = NULL;
    int min_area=60;
    Napi::Value* pyobj_max_area_relative = NULL;
    float max_area_relative=0.01f;
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=1.25f;
    Napi::Value* pyobj_n_scales = NULL;
    int n_scales=-1;
    Ptr<TBMR> retval;

    const char* keywords[] = { "min_area", "max_area_relative", "scale_factor", "n_scales", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOO:xfeatures2d_TBMR.create", (char**)keywords, &pyobj_min_area, &pyobj_max_area_relative, &pyobj_scale_factor, &pyobj_n_scales) &&
        jsopencv_to_safe(info, pyobj_min_area, min_area, ArgInfo("min_area", 0)) &&
        jsopencv_to_safe(info, pyobj_max_area_relative, max_area_relative, ArgInfo("max_area_relative", 0)) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)) &&
        jsopencv_to_safe(info, pyobj_n_scales, n_scales, ArgInfo("n_scales", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::TBMR::create(min_area, max_area_relative, scale_factor, n_scales));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getMaxAreaRelative(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxAreaRelative());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getMinArea(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinArea());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getNScales(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNScales());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleFactor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setMaxAreaRelative(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    Napi::Value* pyobj_maxArea = NULL;
    float maxArea=0.f;

    const char* keywords[] = { "maxArea", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_TBMR.setMaxAreaRelative", (char**)keywords, &pyobj_maxArea) &&
        jsopencv_to_safe(info, pyobj_maxArea, maxArea, ArgInfo("maxArea", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxAreaRelative(maxArea));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setMinArea(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    Napi::Value* pyobj_minArea = NULL;
    int minArea=0;

    const char* keywords[] = { "minArea", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_TBMR.setMinArea", (char**)keywords, &pyobj_minArea) &&
        jsopencv_to_safe(info, pyobj_minArea, minArea, ArgInfo("minArea", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinArea(minArea));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setNScales(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    Napi::Value* pyobj_n_scales = NULL;
    int n_scales=0;

    const char* keywords[] = { "n_scales", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_TBMR.setNScales", (char**)keywords, &pyobj_n_scales) &&
        jsopencv_to_safe(info, pyobj_n_scales, n_scales, ArgInfo("n_scales", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setNScales(n_scales));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TBMR> * self1 = 0;
    if (!pyopencv_xfeatures2d_TBMR_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TBMR' or its derivative)");
    Ptr<cv::xfeatures2d::TBMR> _self_ = *(self1);
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_TBMR.setScaleFactor", (char**)keywords, &pyobj_scale_factor) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_TBMR)

static PyGetSetDef pyopencv_xfeatures2d_TBMR_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_TBMR_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_create_static, METH_STATIC), "create([, min_area[, max_area_relative[, scale_factor[, n_scales]]]]) -> retval\n."},
    {"getMaxAreaRelative", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getMaxAreaRelative, 0), "getMaxAreaRelative() -> retval\n."},
    {"getMinArea", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getMinArea, 0), "getMinArea() -> retval\n."},
    {"getNScales", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getNScales, 0), "getNScales() -> retval\n."},
    {"getScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"setMaxAreaRelative", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setMaxAreaRelative, 0), "setMaxAreaRelative(maxArea) -> None\n."},
    {"setMinArea", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setMinArea, 0), "setMinArea(minArea) -> None\n."},
    {"setNScales", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setNScales, 0), "setNScales(n_scales) -> None\n."},
    {"setScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TBMR_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_TBMR)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::TBMR> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::TBMR>& r)
    {
        return pyopencv_xfeatures2d_TBMR_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::TBMR>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::TBMR> * dst_;
        if (pyopencv_xfeatures2d_TBMR_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::TBMR> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_TEBLID (Generic)
//================================================================================

// GetSet (xfeatures2d_TEBLID)



// Methods (xfeatures2d_TEBLID)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TEBLID_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;
    Napi::Value* pyobj_n_bits = NULL;
    int n_bits=TEBLID::SIZE_256_BITS;
    Ptr<TEBLID> retval;

    const char* keywords[] = { "scale_factor", "n_bits", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:xfeatures2d_TEBLID.create", (char**)keywords, &pyobj_scale_factor, &pyobj_n_bits) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)) &&
        jsopencv_to_safe(info, pyobj_n_bits, n_bits, ArgInfo("n_bits", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::TEBLID::create(scale_factor, n_bits));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_TEBLID_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::TEBLID> * self1 = 0;
    if (!pyopencv_xfeatures2d_TEBLID_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_TEBLID' or its derivative)");
    Ptr<cv::xfeatures2d::TEBLID> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}



// Tables (xfeatures2d_TEBLID)

static PyGetSetDef pyopencv_xfeatures2d_TEBLID_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_TEBLID_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TEBLID_create_static, METH_STATIC), "create(scale_factor[, n_bits]) -> retval\n.   @brief Creates the TEBLID descriptor.\n.       @param scale_factor Adjust the sampling window around detected keypoints:\n.       - <b> 1.00f </b> should be the scale for ORB keypoints\n.       - <b> 6.75f </b> should be the scale for SIFT detected keypoints\n.       - <b> 6.25f </b> is default and fits for KAZE, SURF detected keypoints\n.       - <b> 5.00f </b> should be the scale for AKAZE, MSD, AGAST, FAST, BRISK keypoints\n.       @param n_bits Determine the number of bits in the descriptor. Should be either\n.        TEBLID::SIZE_256_BITS or TEBLID::SIZE_512_BITS."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_TEBLID_getDefaultName, 0), "getDefaultName() -> retval\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_TEBLID)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::TEBLID> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::TEBLID>& r)
    {
        return pyopencv_xfeatures2d_TEBLID_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::TEBLID>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::TEBLID> * dst_;
        if (pyopencv_xfeatures2d_TEBLID_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::TEBLID> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// xfeatures2d_VGG (Generic)
//================================================================================

// GetSet (xfeatures2d_VGG)



// Methods (xfeatures2d_VGG)

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;

    Napi::Value* pyobj_desc = NULL;
    int desc=VGG::VGG_120;
    Napi::Value* pyobj_isigma = NULL;
    float isigma=1.4f;
    Napi::Value* pyobj_img_normalize = NULL;
    bool img_normalize=true;
    Napi::Value* pyobj_use_scale_orientation = NULL;
    bool use_scale_orientation=true;
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=6.25f;
    Napi::Value* pyobj_dsc_normalize = NULL;
    bool dsc_normalize=false;
    Ptr<VGG> retval;

    const char* keywords[] = { "desc", "isigma", "img_normalize", "use_scale_orientation", "scale_factor", "dsc_normalize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOO:xfeatures2d_VGG.create", (char**)keywords, &pyobj_desc, &pyobj_isigma, &pyobj_img_normalize, &pyobj_use_scale_orientation, &pyobj_scale_factor, &pyobj_dsc_normalize) &&
        jsopencv_to_safe(info, pyobj_desc, desc, ArgInfo("desc", 0)) &&
        jsopencv_to_safe(info, pyobj_isigma, isigma, ArgInfo("isigma", 0)) &&
        jsopencv_to_safe(info, pyobj_img_normalize, img_normalize, ArgInfo("img_normalize", 0)) &&
        jsopencv_to_safe(info, pyobj_use_scale_orientation, use_scale_orientation, ArgInfo("use_scale_orientation", 0)) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)) &&
        jsopencv_to_safe(info, pyobj_dsc_normalize, dsc_normalize, ArgInfo("dsc_normalize", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::xfeatures2d::VGG::create(desc, isigma, img_normalize, use_scale_orientation, scale_factor, dsc_normalize));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getDefaultName(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    String retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDefaultName());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getScaleFactor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeDescriptor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseNormalizeDescriptor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeImage(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseNormalizeImage());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseScaleOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseScaleOrientation());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setScaleFactor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    Napi::Value* pyobj_scale_factor = NULL;
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_VGG.setScaleFactor", (char**)keywords, &pyobj_scale_factor) &&
        jsopencv_to_safe(info, pyobj_scale_factor, scale_factor, ArgInfo("scale_factor", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    Napi::Value* pyobj_isigma = NULL;
    float isigma=0.f;

    const char* keywords[] = { "isigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_VGG.setSigma", (char**)keywords, &pyobj_isigma) &&
        jsopencv_to_safe(info, pyobj_isigma, isigma, ArgInfo("isigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigma(isigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeDescriptor(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    Napi::Value* pyobj_dsc_normalize = NULL;
    bool dsc_normalize=0;

    const char* keywords[] = { "dsc_normalize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_VGG.setUseNormalizeDescriptor", (char**)keywords, &pyobj_dsc_normalize) &&
        jsopencv_to_safe(info, pyobj_dsc_normalize, dsc_normalize, ArgInfo("dsc_normalize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseNormalizeDescriptor(dsc_normalize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeImage(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    Napi::Value* pyobj_img_normalize = NULL;
    bool img_normalize=0;

    const char* keywords[] = { "img_normalize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_VGG.setUseNormalizeImage", (char**)keywords, &pyobj_img_normalize) &&
        jsopencv_to_safe(info, pyobj_img_normalize, img_normalize, ArgInfo("img_normalize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseNormalizeImage(img_normalize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseScaleOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::xfeatures2d;


    Ptr<cv::xfeatures2d::VGG> * self1 = 0;
    if (!pyopencv_xfeatures2d_VGG_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    Ptr<cv::xfeatures2d::VGG> _self_ = *(self1);
    Napi::Value* pyobj_use_scale_orientation = NULL;
    bool use_scale_orientation=0;

    const char* keywords[] = { "use_scale_orientation", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:xfeatures2d_VGG.setUseScaleOrientation", (char**)keywords, &pyobj_use_scale_orientation) &&
        jsopencv_to_safe(info, pyobj_use_scale_orientation, use_scale_orientation, ArgInfo("use_scale_orientation", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseScaleOrientation(use_scale_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (xfeatures2d_VGG)

static PyGetSetDef pyopencv_xfeatures2d_VGG_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_xfeatures2d_VGG_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_create_static, METH_STATIC), "create([, desc[, isigma[, img_normalize[, use_scale_orientation[, scale_factor[, dsc_normalize]]]]]]) -> retval\n."},
    {"getDefaultName", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getSigma, 0), "getSigma() -> retval\n."},
    {"getUseNormalizeDescriptor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeDescriptor, 0), "getUseNormalizeDescriptor() -> retval\n."},
    {"getUseNormalizeImage", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeImage, 0), "getUseNormalizeImage() -> retval\n."},
    {"getUseScaleOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseScaleOrientation, 0), "getUseScaleOrientation() -> retval\n."},
    {"setScaleFactor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},
    {"setSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setSigma, 0), "setSigma(isigma) -> None\n."},
    {"setUseNormalizeDescriptor", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeDescriptor, 0), "setUseNormalizeDescriptor(dsc_normalize) -> None\n."},
    {"setUseNormalizeImage", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeImage, 0), "setUseNormalizeImage(img_normalize) -> None\n."},
    {"setUseScaleOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseScaleOrientation, 0), "setUseScaleOrientation(use_scale_orientation) -> None\n."},

    {NULL,          NULL}
};

// Converter (xfeatures2d_VGG)

template<>
struct PyOpenCV_Converter< Ptr<cv::xfeatures2d::VGG> >
{
    static PyObject* from(const Ptr<cv::xfeatures2d::VGG>& r)
    {
        return pyopencv_xfeatures2d_VGG_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::xfeatures2d::VGG>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::xfeatures2d::VGG> * dst_;
        if (pyopencv_xfeatures2d_VGG_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::xfeatures2d::VGG> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_AdaptiveManifoldFilter (Generic)
//================================================================================

// GetSet (ximgproc_AdaptiveManifoldFilter)



// Methods (ximgproc_AdaptiveManifoldFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_collectGarbage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::AdaptiveManifoldFilter> * self1 = 0;
    if (!pyopencv_ximgproc_AdaptiveManifoldFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_AdaptiveManifoldFilter' or its derivative)");
    Ptr<cv::ximgproc::AdaptiveManifoldFilter> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->collectGarbage());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;

    Ptr<AdaptiveManifoldFilter> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = cv::ximgproc::AdaptiveManifoldFilter::create());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_filter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::AdaptiveManifoldFilter> * self1 = 0;
    if (!pyopencv_ximgproc_AdaptiveManifoldFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_AdaptiveManifoldFilter' or its derivative)");
    Ptr<cv::ximgproc::AdaptiveManifoldFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_joint = NULL;
    Mat joint;

    const char* keywords[] = { "src", "dst", "joint", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ximgproc_AdaptiveManifoldFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_joint) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_joint, joint, ArgInfo("joint", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst, joint));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_joint = NULL;
    UMat joint;

    const char* keywords[] = { "src", "dst", "joint", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ximgproc_AdaptiveManifoldFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_joint) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_joint, joint, ArgInfo("joint", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst, joint));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("filter");

    return NULL;
}



// Tables (ximgproc_AdaptiveManifoldFilter)

static PyGetSetDef pyopencv_ximgproc_AdaptiveManifoldFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_AdaptiveManifoldFilter_methods[] =
{
    {"collectGarbage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_collectGarbage, 0), "collectGarbage() -> None\n."},
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_create_static, METH_STATIC), "create() -> retval\n."},
    {"filter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_filter, 0), "filter(src[, dst[, joint]]) -> dst\n.   @brief Apply high-dimensional filtering using adaptive manifolds.\n.   \n.       @param src filtering image with any numbers of channels.\n.   \n.       @param dst output image.\n.   \n.       @param joint optional joint (also called as guided) image with any numbers of channels."},

    {NULL,          NULL}
};

// Converter (ximgproc_AdaptiveManifoldFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::AdaptiveManifoldFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::AdaptiveManifoldFilter>& r)
    {
        return pyopencv_ximgproc_AdaptiveManifoldFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::AdaptiveManifoldFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::AdaptiveManifoldFilter> * dst_;
        if (pyopencv_ximgproc_AdaptiveManifoldFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::AdaptiveManifoldFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_ContourFitting (Generic)
//================================================================================

// GetSet (ximgproc_ContourFitting)



// Methods (ximgproc_ContourFitting)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ContourFitting_estimateTransformation(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ContourFitting> * self1 = 0;
    if (!pyopencv_ximgproc_ContourFitting_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    Ptr<cv::ximgproc::ContourFitting> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_alphaPhiST = NULL;
    Mat alphaPhiST;
    double dist;
    Napi::Value* pyobj_fdContour = NULL;
    bool fdContour=false;

    const char* keywords[] = { "src", "dst", "alphaPhiST", "fdContour", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:ximgproc_ContourFitting.estimateTransformation", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_alphaPhiST, &pyobj_fdContour) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)) &&
        jsopencv_to_safe(info, pyobj_alphaPhiST, alphaPhiST, ArgInfo("alphaPhiST", 1)) &&
        jsopencv_to_safe(info, pyobj_fdContour, fdContour, ArgInfo("fdContour", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->estimateTransformation(src, dst, alphaPhiST, dist, fdContour));
        return Py_BuildValue("(NN)", jsopencv_from(alphaPhiST), jsopencv_from(dist));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_alphaPhiST = NULL;
    UMat alphaPhiST;
    double dist;
    Napi::Value* pyobj_fdContour = NULL;
    bool fdContour=false;

    const char* keywords[] = { "src", "dst", "alphaPhiST", "fdContour", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OO:ximgproc_ContourFitting.estimateTransformation", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_alphaPhiST, &pyobj_fdContour) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 0)) &&
        jsopencv_to_safe(info, pyobj_alphaPhiST, alphaPhiST, ArgInfo("alphaPhiST", 1)) &&
        jsopencv_to_safe(info, pyobj_fdContour, fdContour, ArgInfo("fdContour", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->estimateTransformation(src, dst, alphaPhiST, dist, fdContour));
        return Py_BuildValue("(NN)", jsopencv_from(alphaPhiST), jsopencv_from(dist));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("estimateTransformation");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ContourFitting_getCtrSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ContourFitting> * self1 = 0;
    if (!pyopencv_ximgproc_ContourFitting_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    Ptr<cv::ximgproc::ContourFitting> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getCtrSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ContourFitting_getFDSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ContourFitting> * self1 = 0;
    if (!pyopencv_ximgproc_ContourFitting_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    Ptr<cv::ximgproc::ContourFitting> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFDSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ContourFitting_setCtrSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ContourFitting> * self1 = 0;
    if (!pyopencv_ximgproc_ContourFitting_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    Ptr<cv::ximgproc::ContourFitting> _self_ = *(self1);
    Napi::Value* pyobj_n = NULL;
    int n=0;

    const char* keywords[] = { "n", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_ContourFitting.setCtrSize", (char**)keywords, &pyobj_n) &&
        jsopencv_to_safe(info, pyobj_n, n, ArgInfo("n", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCtrSize(n));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ContourFitting_setFDSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ContourFitting> * self1 = 0;
    if (!pyopencv_ximgproc_ContourFitting_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    Ptr<cv::ximgproc::ContourFitting> _self_ = *(self1);
    Napi::Value* pyobj_n = NULL;
    int n=0;

    const char* keywords[] = { "n", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_ContourFitting.setFDSize", (char**)keywords, &pyobj_n) &&
        jsopencv_to_safe(info, pyobj_n, n, ArgInfo("n", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFDSize(n));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_ContourFitting)

static PyGetSetDef pyopencv_ximgproc_ContourFitting_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_ContourFitting_methods[] =
{
    {"estimateTransformation", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_estimateTransformation, 0), "estimateTransformation(src, dst[, alphaPhiST[, fdContour]]) -> alphaPhiST, dist\n.   @brief Fit two closed curves using fourier descriptors. More details in @cite PersoonFu1977 and @cite BergerRaghunathan1998\n.   \n.           @param src Contour defining first shape.\n.           @param dst Contour defining second shape (Target).\n.           @param alphaPhiST : \\f$ \\alpha \\f$=alphaPhiST(0,0), \\f$ \\phi \\f$=alphaPhiST(0,1) (in radian), s=alphaPhiST(0,2), Tx=alphaPhiST(0,3), Ty=alphaPhiST(0,4) rotation center\n.           @param dist distance between src and dst after matching.\n.           @param fdContour false then src and dst are contours and true src and dst are fourier descriptors."},
    {"getCtrSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_getCtrSize, 0), "getCtrSize() -> retval\n.   @returns number of fourier descriptors"},
    {"getFDSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_getFDSize, 0), "getFDSize() -> retval\n.   @returns number of fourier descriptors used for optimal curve matching"},
    {"setCtrSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_setCtrSize, 0), "setCtrSize(n) -> None\n.   @brief set number of Fourier descriptors used in estimateTransformation\n.   \n.           @param n number of Fourier descriptors equal to number of contour points after resampling."},
    {"setFDSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_setFDSize, 0), "setFDSize(n) -> None\n.   @brief set number of Fourier descriptors when estimateTransformation used vector<Point>\n.   \n.           @param n number of fourier descriptors used for optimal curve matching."},

    {NULL,          NULL}
};

// Converter (ximgproc_ContourFitting)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::ContourFitting> >
{
    static PyObject* from(const Ptr<cv::ximgproc::ContourFitting>& r)
    {
        return pyopencv_ximgproc_ContourFitting_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::ContourFitting>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::ContourFitting> * dst_;
        if (pyopencv_ximgproc_ContourFitting_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::ContourFitting> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_DTFilter (Generic)
//================================================================================

// GetSet (ximgproc_DTFilter)



// Methods (ximgproc_DTFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DTFilter_filter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DTFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DTFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DTFilter' or its derivative)");
    Ptr<cv::ximgproc::DTFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_dDepth = NULL;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ximgproc_DTFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_dDepth) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_dDepth, dDepth, ArgInfo("dDepth", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst, dDepth));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_dDepth = NULL;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ximgproc_DTFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_dDepth) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_dDepth, dDepth, ArgInfo("dDepth", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst, dDepth));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("filter");

    return NULL;
}



// Tables (ximgproc_DTFilter)

static PyGetSetDef pyopencv_ximgproc_DTFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_DTFilter_methods[] =
{
    {"filter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DTFilter_filter, 0), "filter(src[, dst[, dDepth]]) -> dst\n.   @brief Produce domain transform filtering operation on source image.\n.   \n.       @param src filtering image with unsigned 8-bit or floating-point 32-bit depth and up to 4 channels.\n.   \n.       @param dst destination image.\n.   \n.       @param dDepth optional depth of the output image. dDepth can be set to -1, which will be equivalent\n.       to src.depth()."},

    {NULL,          NULL}
};

// Converter (ximgproc_DTFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::DTFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::DTFilter>& r)
    {
        return pyopencv_ximgproc_DTFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::DTFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::DTFilter> * dst_;
        if (pyopencv_ximgproc_DTFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::DTFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_DisparityFilter (Generic)
//================================================================================

// GetSet (ximgproc_DisparityFilter)



// Methods (ximgproc_DisparityFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityFilter_filter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_disparity_map_left = NULL;
    Mat disparity_map_left;
    Napi::Value* pyobj_left_view = NULL;
    Mat left_view;
    Napi::Value* pyobj_filtered_disparity_map = NULL;
    Mat filtered_disparity_map;
    Napi::Value* pyobj_disparity_map_right = NULL;
    Mat disparity_map_right;
    Napi::Value* pyobj_ROI = NULL;
    Rect ROI;
    Napi::Value* pyobj_right_view = NULL;
    Mat right_view;

    const char* keywords[] = { "disparity_map_left", "left_view", "filtered_disparity_map", "disparity_map_right", "ROI", "right_view", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOOO:ximgproc_DisparityFilter.filter", (char**)keywords, &pyobj_disparity_map_left, &pyobj_left_view, &pyobj_filtered_disparity_map, &pyobj_disparity_map_right, &pyobj_ROI, &pyobj_right_view) &&
        jsopencv_to_safe(info, pyobj_disparity_map_left, disparity_map_left, ArgInfo("disparity_map_left", 0)) &&
        jsopencv_to_safe(info, pyobj_left_view, left_view, ArgInfo("left_view", 0)) &&
        jsopencv_to_safe(info, pyobj_filtered_disparity_map, filtered_disparity_map, ArgInfo("filtered_disparity_map", 1)) &&
        jsopencv_to_safe(info, pyobj_disparity_map_right, disparity_map_right, ArgInfo("disparity_map_right", 0)) &&
        jsopencv_to_safe(info, pyobj_ROI, ROI, ArgInfo("ROI", 0)) &&
        jsopencv_to_safe(info, pyobj_right_view, right_view, ArgInfo("right_view", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(disparity_map_left, left_view, filtered_disparity_map, disparity_map_right, ROI, right_view));
        return jsopencv_from(filtered_disparity_map);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_disparity_map_left = NULL;
    UMat disparity_map_left;
    Napi::Value* pyobj_left_view = NULL;
    UMat left_view;
    Napi::Value* pyobj_filtered_disparity_map = NULL;
    UMat filtered_disparity_map;
    Napi::Value* pyobj_disparity_map_right = NULL;
    UMat disparity_map_right;
    Napi::Value* pyobj_ROI = NULL;
    Rect ROI;
    Napi::Value* pyobj_right_view = NULL;
    UMat right_view;

    const char* keywords[] = { "disparity_map_left", "left_view", "filtered_disparity_map", "disparity_map_right", "ROI", "right_view", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOOO:ximgproc_DisparityFilter.filter", (char**)keywords, &pyobj_disparity_map_left, &pyobj_left_view, &pyobj_filtered_disparity_map, &pyobj_disparity_map_right, &pyobj_ROI, &pyobj_right_view) &&
        jsopencv_to_safe(info, pyobj_disparity_map_left, disparity_map_left, ArgInfo("disparity_map_left", 0)) &&
        jsopencv_to_safe(info, pyobj_left_view, left_view, ArgInfo("left_view", 0)) &&
        jsopencv_to_safe(info, pyobj_filtered_disparity_map, filtered_disparity_map, ArgInfo("filtered_disparity_map", 1)) &&
        jsopencv_to_safe(info, pyobj_disparity_map_right, disparity_map_right, ArgInfo("disparity_map_right", 0)) &&
        jsopencv_to_safe(info, pyobj_ROI, ROI, ArgInfo("ROI", 0)) &&
        jsopencv_to_safe(info, pyobj_right_view, right_view, ArgInfo("right_view", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(disparity_map_left, left_view, filtered_disparity_map, disparity_map_right, ROI, right_view));
        return jsopencv_from(filtered_disparity_map);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("filter");

    return NULL;
}



// Tables (ximgproc_DisparityFilter)

static PyGetSetDef pyopencv_ximgproc_DisparityFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_DisparityFilter_methods[] =
{
    {"filter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityFilter_filter, 0), "filter(disparity_map_left, left_view[, filtered_disparity_map[, disparity_map_right[, ROI[, right_view]]]]) -> filtered_disparity_map\n.   @brief Apply filtering to the disparity map.\n.   \n.       @param disparity_map_left disparity map of the left view, 1 channel, CV_16S type. Implicitly assumes that disparity\n.       values are scaled by 16 (one-pixel disparity corresponds to the value of 16 in the disparity map). Disparity map\n.       can have any resolution, it will be automatically resized to fit left_view resolution.\n.   \n.       @param left_view left view of the original stereo-pair to guide the filtering process, 8-bit single-channel\n.       or three-channel image.\n.   \n.       @param filtered_disparity_map output disparity map.\n.   \n.       @param disparity_map_right optional argument, some implementations might also use the disparity map\n.       of the right view to compute confidence maps, for instance.\n.   \n.       @param ROI region of the disparity map to filter. Optional, usually it should be set automatically.\n.   \n.       @param right_view optional argument, some implementations might also use the right view of the original\n.       stereo-pair."},

    {NULL,          NULL}
};

// Converter (ximgproc_DisparityFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::DisparityFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::DisparityFilter>& r)
    {
        return pyopencv_ximgproc_DisparityFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::DisparityFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::DisparityFilter> * dst_;
        if (pyopencv_ximgproc_DisparityFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::DisparityFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_DisparityWLSFilter (Generic)
//================================================================================

// GetSet (ximgproc_DisparityWLSFilter)



// Methods (ximgproc_DisparityWLSFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getConfidenceMap(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    Mat retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getConfidenceMap());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getDepthDiscontinuityRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getDepthDiscontinuityRadius());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLRCthresh(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLRCthresh());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLambda());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getROI(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    Rect retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getROI());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getSigmaColor(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigmaColor());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setDepthDiscontinuityRadius(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    Napi::Value* pyobj__disc_radius = NULL;
    int _disc_radius=0;

    const char* keywords[] = { "_disc_radius", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_DisparityWLSFilter.setDepthDiscontinuityRadius", (char**)keywords, &pyobj__disc_radius) &&
        jsopencv_to_safe(info, pyobj__disc_radius, _disc_radius, ArgInfo("_disc_radius", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setDepthDiscontinuityRadius(_disc_radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLRCthresh(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    Napi::Value* pyobj__LRC_thresh = NULL;
    int _LRC_thresh=0;

    const char* keywords[] = { "_LRC_thresh", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_DisparityWLSFilter.setLRCthresh", (char**)keywords, &pyobj__LRC_thresh) &&
        jsopencv_to_safe(info, pyobj__LRC_thresh, _LRC_thresh, ArgInfo("_LRC_thresh", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLRCthresh(_LRC_thresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    Napi::Value* pyobj__lambda = NULL;
    double _lambda=0;

    const char* keywords[] = { "_lambda", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_DisparityWLSFilter.setLambda", (char**)keywords, &pyobj__lambda) &&
        jsopencv_to_safe(info, pyobj__lambda, _lambda, ArgInfo("_lambda", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLambda(_lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setSigmaColor(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::DisparityWLSFilter> * self1 = 0;
    if (!pyopencv_ximgproc_DisparityWLSFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Ptr<cv::ximgproc::DisparityWLSFilter> _self_ = *(self1);
    Napi::Value* pyobj__sigma_color = NULL;
    double _sigma_color=0;

    const char* keywords[] = { "_sigma_color", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_DisparityWLSFilter.setSigmaColor", (char**)keywords, &pyobj__sigma_color) &&
        jsopencv_to_safe(info, pyobj__sigma_color, _sigma_color, ArgInfo("_sigma_color", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigmaColor(_sigma_color));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_DisparityWLSFilter)

static PyGetSetDef pyopencv_ximgproc_DisparityWLSFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_DisparityWLSFilter_methods[] =
{
    {"getConfidenceMap", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getConfidenceMap, 0), "getConfidenceMap() -> retval\n.   @brief Get the confidence map that was used in the last filter call. It is a CV_32F one-channel image\n.       with values ranging from 0.0 (totally untrusted regions of the raw disparity map) to 255.0 (regions containing\n.       correct disparity values with a high degree of confidence)."},
    {"getDepthDiscontinuityRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getDepthDiscontinuityRadius, 0), "getDepthDiscontinuityRadius() -> retval\n.   @brief DepthDiscontinuityRadius is a parameter used in confidence computation. It defines the size of\n.       low-confidence regions around depth discontinuities."},
    {"getLRCthresh", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLRCthresh, 0), "getLRCthresh() -> retval\n.   @brief LRCthresh is a threshold of disparity difference used in left-right-consistency check during\n.       confidence map computation. The default value of 24 (1.5 pixels) is virtually always good enough."},
    {"getLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLambda, 0), "getLambda() -> retval\n.   @brief Lambda is a parameter defining the amount of regularization during filtering. Larger values force\n.       filtered disparity map edges to adhere more to source image edges. Typical value is 8000."},
    {"getROI", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getROI, 0), "getROI() -> retval\n.   @brief Get the ROI used in the last filter call"},
    {"getSigmaColor", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getSigmaColor, 0), "getSigmaColor() -> retval\n.   @brief SigmaColor is a parameter defining how sensitive the filtering process is to source image edges.\n.       Large values can lead to disparity leakage through low-contrast edges. Small values can make the filter too\n.       sensitive to noise and textures in the source image. Typical values range from 0.8 to 2.0."},
    {"setDepthDiscontinuityRadius", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setDepthDiscontinuityRadius, 0), "setDepthDiscontinuityRadius(_disc_radius) -> None\n.   @see getDepthDiscontinuityRadius"},
    {"setLRCthresh", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLRCthresh, 0), "setLRCthresh(_LRC_thresh) -> None\n.   @see getLRCthresh"},
    {"setLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLambda, 0), "setLambda(_lambda) -> None\n.   @see getLambda"},
    {"setSigmaColor", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setSigmaColor, 0), "setSigmaColor(_sigma_color) -> None\n.   @see getSigmaColor"},

    {NULL,          NULL}
};

// Converter (ximgproc_DisparityWLSFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::DisparityWLSFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::DisparityWLSFilter>& r)
    {
        return pyopencv_ximgproc_DisparityWLSFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::DisparityWLSFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::DisparityWLSFilter> * dst_;
        if (pyopencv_ximgproc_DisparityWLSFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::DisparityWLSFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_EdgeAwareInterpolator (Generic)
//================================================================================

// GetSet (ximgproc_EdgeAwareInterpolator)



// Methods (ximgproc_EdgeAwareInterpolator)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFGSLambda());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFGSSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getK(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getK());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getLambda());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getUsePostProcessing(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUsePostProcessing());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setCostMap(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__costMap = NULL;
    Mat _costMap;

    const char* keywords[] = { "_costMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setCostMap", (char**)keywords, &pyobj__costMap) &&
        jsopencv_to_safe(info, pyobj__costMap, _costMap, ArgInfo("_costMap", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCostMap(_costMap));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__lambda = NULL;
    float _lambda=0.f;

    const char* keywords[] = { "_lambda", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setFGSLambda", (char**)keywords, &pyobj__lambda) &&
        jsopencv_to_safe(info, pyobj__lambda, _lambda, ArgInfo("_lambda", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFGSLambda(_lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__sigma = NULL;
    float _sigma=0.f;

    const char* keywords[] = { "_sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setFGSSigma", (char**)keywords, &pyobj__sigma) &&
        jsopencv_to_safe(info, pyobj__sigma, _sigma, ArgInfo("_sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFGSSigma(_sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setK(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__k = NULL;
    int _k=0;

    const char* keywords[] = { "_k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setK", (char**)keywords, &pyobj__k) &&
        jsopencv_to_safe(info, pyobj__k, _k, ArgInfo("_k", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setK(_k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__lambda = NULL;
    float _lambda=0.f;

    const char* keywords[] = { "_lambda", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setLambda", (char**)keywords, &pyobj__lambda) &&
        jsopencv_to_safe(info, pyobj__lambda, _lambda, ArgInfo("_lambda", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setLambda(_lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__sigma = NULL;
    float _sigma=0.f;

    const char* keywords[] = { "_sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setSigma", (char**)keywords, &pyobj__sigma) &&
        jsopencv_to_safe(info, pyobj__sigma, _sigma, ArgInfo("_sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigma(_sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setUsePostProcessing(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeAwareInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeAwareInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    Ptr<cv::ximgproc::EdgeAwareInterpolator> _self_ = *(self1);
    Napi::Value* pyobj__use_post_proc = NULL;
    bool _use_post_proc=0;

    const char* keywords[] = { "_use_post_proc", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeAwareInterpolator.setUsePostProcessing", (char**)keywords, &pyobj__use_post_proc) &&
        jsopencv_to_safe(info, pyobj__use_post_proc, _use_post_proc, ArgInfo("_use_post_proc", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUsePostProcessing(_use_post_proc));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_EdgeAwareInterpolator)

static PyGetSetDef pyopencv_ximgproc_EdgeAwareInterpolator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_EdgeAwareInterpolator_methods[] =
{
    {"getFGSLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSLambda, 0), "getFGSLambda() -> retval\n.   @see setFGSLambda"},
    {"getFGSSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSSigma, 0), "getFGSSigma() -> retval\n.   @see setFGSLambda"},
    {"getK", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getK, 0), "getK() -> retval\n.   @see setK"},
    {"getLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getLambda, 0), "getLambda() -> retval\n.   @see setLambda"},
    {"getSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getSigma, 0), "getSigma() -> retval\n.   @see setSigma"},
    {"getUsePostProcessing", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getUsePostProcessing, 0), "getUsePostProcessing() -> retval\n.   @see setUsePostProcessing"},
    {"setCostMap", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setCostMap, 0), "setCostMap(_costMap) -> None\n.   @brief Interface to provide a more elaborated cost map, i.e. edge map, for the edge-aware term.\n.        *  This implementation is based on a rather simple gradient-based edge map estimation.\n.        *  To used more complex edge map estimator (e.g. StructuredEdgeDetection that has been\n.        *  used in the original publication) that may lead to improved accuracies, the internal\n.        *  edge map estimation can be bypassed here.\n.        *  @param _costMap a type CV_32FC1 Mat is required.\n.        *  @see cv::ximgproc::createSuperpixelSLIC"},
    {"setFGSLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSLambda, 0), "setFGSLambda(_lambda) -> None\n.   @brief Sets the respective fastGlobalSmootherFilter() parameter."},
    {"setFGSSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSSigma, 0), "setFGSSigma(_sigma) -> None\n.   @see setFGSLambda"},
    {"setK", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setK, 0), "setK(_k) -> None\n.   @brief K is a number of nearest-neighbor matches considered, when fitting a locally affine\n.       model. Usually it should be around 128. However, lower values would make the interpolation\n.       noticeably faster."},
    {"setLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setLambda, 0), "setLambda(_lambda) -> None\n.   @brief Lambda is a parameter defining the weight of the edge-aware term in geodesic distance,\n.       should be in the range of 0 to 1000."},
    {"setSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setSigma, 0), "setSigma(_sigma) -> None\n.   @brief Sigma is a parameter defining how fast the weights decrease in the locally-weighted affine\n.       fitting. Higher values can help preserve fine details, lower values can help to get rid of noise in the\n.       output flow."},
    {"setUsePostProcessing", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setUsePostProcessing, 0), "setUsePostProcessing(_use_post_proc) -> None\n.   @brief Sets whether the fastGlobalSmootherFilter() post-processing is employed. It is turned on by\n.       default."},

    {NULL,          NULL}
};

// Converter (ximgproc_EdgeAwareInterpolator)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::EdgeAwareInterpolator> >
{
    static PyObject* from(const Ptr<cv::ximgproc::EdgeAwareInterpolator>& r)
    {
        return pyopencv_ximgproc_EdgeAwareInterpolator_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::EdgeAwareInterpolator>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::EdgeAwareInterpolator> * dst_;
        if (pyopencv_ximgproc_EdgeAwareInterpolator_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::EdgeAwareInterpolator> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_EdgeBoxes (Generic)
//================================================================================

// GetSet (ximgproc_EdgeBoxes)



// Methods (ximgproc_EdgeBoxes)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlpha());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBeta(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getBeta());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBoundingBoxes(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_edge_map = NULL;
    Mat edge_map;
    Napi::Value* pyobj_orientation_map = NULL;
    Mat orientation_map;
    vector_Rect boxes;
    Napi::Value* pyobj_scores = NULL;
    Mat scores;

    const char* keywords[] = { "edge_map", "orientation_map", "scores", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ximgproc_EdgeBoxes.getBoundingBoxes", (char**)keywords, &pyobj_edge_map, &pyobj_orientation_map, &pyobj_scores) &&
        jsopencv_to_safe(info, pyobj_edge_map, edge_map, ArgInfo("edge_map", 0)) &&
        jsopencv_to_safe(info, pyobj_orientation_map, orientation_map, ArgInfo("orientation_map", 0)) &&
        jsopencv_to_safe(info, pyobj_scores, scores, ArgInfo("scores", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getBoundingBoxes(edge_map, orientation_map, boxes, scores));
        return Py_BuildValue("(NN)", jsopencv_from(boxes), jsopencv_from(scores));
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_edge_map = NULL;
    UMat edge_map;
    Napi::Value* pyobj_orientation_map = NULL;
    UMat orientation_map;
    vector_Rect boxes;
    Napi::Value* pyobj_scores = NULL;
    UMat scores;

    const char* keywords[] = { "edge_map", "orientation_map", "scores", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ximgproc_EdgeBoxes.getBoundingBoxes", (char**)keywords, &pyobj_edge_map, &pyobj_orientation_map, &pyobj_scores) &&
        jsopencv_to_safe(info, pyobj_edge_map, edge_map, ArgInfo("edge_map", 0)) &&
        jsopencv_to_safe(info, pyobj_orientation_map, orientation_map, ArgInfo("orientation_map", 0)) &&
        jsopencv_to_safe(info, pyobj_scores, scores, ArgInfo("scores", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getBoundingBoxes(edge_map, orientation_map, boxes, scores));
        return Py_BuildValue("(NN)", jsopencv_from(boxes), jsopencv_from(scores));
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getBoundingBoxes");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getClusterMinMag(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getClusterMinMag());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMergeThr(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEdgeMergeThr());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMinMag(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEdgeMinMag());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEta(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getEta());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getGamma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getGamma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getKappa(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getKappa());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxAspectRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxAspectRatio());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxBoxes(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxBoxes());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinBoxArea(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinBoxArea());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinScore(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinScore());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setAlpha", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAlpha(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setBeta(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setBeta", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBeta(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setClusterMinMag(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setClusterMinMag", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setClusterMinMag(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMergeThr(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setEdgeMergeThr", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEdgeMergeThr(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMinMag(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setEdgeMinMag", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEdgeMinMag(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEta(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setEta", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setEta(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setGamma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setGamma", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setGamma(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setKappa(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setKappa", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setKappa(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxAspectRatio(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setMaxAspectRatio", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxAspectRatio(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxBoxes(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    int value=0;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setMaxBoxes", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxBoxes(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinBoxArea(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setMinBoxArea", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinBoxArea(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinScore(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeBoxes> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeBoxes_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    Ptr<cv::ximgproc::EdgeBoxes> _self_ = *(self1);
    Napi::Value* pyobj_value = NULL;
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeBoxes.setMinScore", (char**)keywords, &pyobj_value) &&
        jsopencv_to_safe(info, pyobj_value, value, ArgInfo("value", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinScore(value));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_EdgeBoxes)

static PyGetSetDef pyopencv_ximgproc_EdgeBoxes_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_EdgeBoxes_methods[] =
{
    {"getAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getAlpha, 0), "getAlpha() -> retval\n.   @brief Returns the step size of sliding window search."},
    {"getBeta", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBeta, 0), "getBeta() -> retval\n.   @brief Returns the nms threshold for object proposals."},
    {"getBoundingBoxes", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBoundingBoxes, 0), "getBoundingBoxes(edge_map, orientation_map[, scores]) -> boxes, scores\n.   @brief Returns array containing proposal boxes.\n.   \n.       @param edge_map edge image.\n.       @param orientation_map orientation map.\n.       @param boxes proposal boxes.\n.       @param scores of the proposal boxes, provided a vector of float types."},
    {"getClusterMinMag", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getClusterMinMag, 0), "getClusterMinMag() -> retval\n.   @brief Returns the cluster min magnitude."},
    {"getEdgeMergeThr", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMergeThr, 0), "getEdgeMergeThr() -> retval\n.   @brief Returns the edge merge threshold."},
    {"getEdgeMinMag", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMinMag, 0), "getEdgeMinMag() -> retval\n.   @brief Returns the edge min magnitude."},
    {"getEta", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEta, 0), "getEta() -> retval\n.   @brief Returns adaptation rate for nms threshold."},
    {"getGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getGamma, 0), "getGamma() -> retval\n.   @brief Returns the affinity sensitivity."},
    {"getKappa", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getKappa, 0), "getKappa() -> retval\n.   @brief Returns the scale sensitivity."},
    {"getMaxAspectRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxAspectRatio, 0), "getMaxAspectRatio() -> retval\n.   @brief Returns the max aspect ratio of boxes."},
    {"getMaxBoxes", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxBoxes, 0), "getMaxBoxes() -> retval\n.   @brief Returns the max number of boxes to detect."},
    {"getMinBoxArea", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinBoxArea, 0), "getMinBoxArea() -> retval\n.   @brief Returns the minimum area of boxes."},
    {"getMinScore", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinScore, 0), "getMinScore() -> retval\n.   @brief Returns the min score of boxes to detect."},
    {"setAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setAlpha, 0), "setAlpha(value) -> None\n.   @brief Sets the step size of sliding window search."},
    {"setBeta", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setBeta, 0), "setBeta(value) -> None\n.   @brief Sets the nms threshold for object proposals."},
    {"setClusterMinMag", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setClusterMinMag, 0), "setClusterMinMag(value) -> None\n.   @brief Sets the cluster min magnitude."},
    {"setEdgeMergeThr", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMergeThr, 0), "setEdgeMergeThr(value) -> None\n.   @brief Sets the edge merge threshold."},
    {"setEdgeMinMag", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMinMag, 0), "setEdgeMinMag(value) -> None\n.   @brief Sets the edge min magnitude."},
    {"setEta", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEta, 0), "setEta(value) -> None\n.   @brief Sets the adaptation rate for nms threshold."},
    {"setGamma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setGamma, 0), "setGamma(value) -> None\n.   @brief Sets the affinity sensitivity"},
    {"setKappa", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setKappa, 0), "setKappa(value) -> None\n.   @brief Sets the scale sensitivity."},
    {"setMaxAspectRatio", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxAspectRatio, 0), "setMaxAspectRatio(value) -> None\n.   @brief Sets the max aspect ratio of boxes."},
    {"setMaxBoxes", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxBoxes, 0), "setMaxBoxes(value) -> None\n.   @brief Sets max number of boxes to detect."},
    {"setMinBoxArea", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinBoxArea, 0), "setMinBoxArea(value) -> None\n.   @brief Sets the minimum area of boxes."},
    {"setMinScore", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinScore, 0), "setMinScore(value) -> None\n.   @brief Sets the min score of boxes to detect."},

    {NULL,          NULL}
};

// Converter (ximgproc_EdgeBoxes)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::EdgeBoxes> >
{
    static PyObject* from(const Ptr<cv::ximgproc::EdgeBoxes>& r)
    {
        return pyopencv_ximgproc_EdgeBoxes_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::EdgeBoxes>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::EdgeBoxes> * dst_;
        if (pyopencv_ximgproc_EdgeBoxes_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::EdgeBoxes> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_EdgeDrawing (Generic)
//================================================================================

// GetSet (ximgproc_EdgeDrawing)



// Methods (ximgproc_EdgeDrawing)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_detectEdges(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;

    const char* keywords[] = { "src", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeDrawing.detectEdges", (char**)keywords, &pyobj_src) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectEdges(src));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;

    const char* keywords[] = { "src", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeDrawing.detectEdges", (char**)keywords, &pyobj_src) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->detectEdges(src));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectEdges");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_detectEllipses(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_ellipses = NULL;
    Mat ellipses;

    const char* keywords[] = { "ellipses", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.detectEllipses", (char**)keywords, &pyobj_ellipses) &&
        jsopencv_to_safe(info, pyobj_ellipses, ellipses, ArgInfo("ellipses", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectEllipses(ellipses));
        return jsopencv_from(ellipses);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_ellipses = NULL;
    UMat ellipses;

    const char* keywords[] = { "ellipses", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.detectEllipses", (char**)keywords, &pyobj_ellipses) &&
        jsopencv_to_safe(info, pyobj_ellipses, ellipses, ArgInfo("ellipses", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectEllipses(ellipses));
        return jsopencv_from(ellipses);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectEllipses");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_detectLines(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_lines = NULL;
    Mat lines;

    const char* keywords[] = { "lines", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.detectLines", (char**)keywords, &pyobj_lines) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectLines(lines));
        return jsopencv_from(lines);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_lines = NULL;
    UMat lines;

    const char* keywords[] = { "lines", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.detectLines", (char**)keywords, &pyobj_lines) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectLines(lines));
        return jsopencv_from(lines);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectLines");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getEdgeImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.getEdgeImage", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getEdgeImage(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.getEdgeImage", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getEdgeImage(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getEdgeImage");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getGradientImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.getGradientImage", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getGradientImage(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_EdgeDrawing.getGradientImage", (char**)keywords, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getGradientImage(dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getGradientImage");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getSegmentIndicesOfLines(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    std::vector<int> retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSegmentIndicesOfLines());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getSegments(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    std::vector<std::vector<Point> > retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSegments());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_setParams(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::EdgeDrawing> * self1 = 0;
    if (!pyopencv_ximgproc_EdgeDrawing_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeDrawing' or its derivative)");
    Ptr<cv::ximgproc::EdgeDrawing> _self_ = *(self1);
    Napi::Value* pyobj_parameters = NULL;
    EdgeDrawing_Params parameters;

    const char* keywords[] = { "parameters", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_EdgeDrawing.setParams", (char**)keywords, &pyobj_parameters) &&
        jsopencv_to_safe(info, pyobj_parameters, parameters, ArgInfo("parameters", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setParams(parameters));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_EdgeDrawing)

static PyGetSetDef pyopencv_ximgproc_EdgeDrawing_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_EdgeDrawing_methods[] =
{
    {"detectEdges", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_detectEdges, 0), "detectEdges(src) -> None\n.   @brief Detects edges in a grayscale image and prepares them to detect lines and ellipses.\n.   \n.       @param src 8-bit, single-channel, grayscale input image."},
    {"detectEllipses", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_detectEllipses, 0), "detectEllipses([, ellipses]) -> ellipses\n.   @brief Detects circles and ellipses.\n.   \n.       @param ellipses  output Vec<6d> contains center point and perimeter for circles, center point, axes and angle for ellipses.\n.       @note you should call detectEdges() before calling this function."},
    {"detectLines", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_detectLines, 0), "detectLines([, lines]) -> lines\n.   @brief Detects lines.\n.   \n.       @param lines  output Vec<4f> contains the start point and the end point of detected lines.\n.       @note you should call detectEdges() before calling this function."},
    {"getEdgeImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getEdgeImage, 0), "getEdgeImage([, dst]) -> dst\n.   @brief returns Edge Image prepared by detectEdges() function.\n.   \n.       @param dst returns 8-bit, single-channel output image."},
    {"getGradientImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getGradientImage, 0), "getGradientImage([, dst]) -> dst\n.   @brief returns Gradient Image prepared by detectEdges() function.\n.   \n.       @param dst returns 16-bit, single-channel output image."},
    {"getSegmentIndicesOfLines", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getSegmentIndicesOfLines, 0), "getSegmentIndicesOfLines() -> retval\n.   @brief Returns for each line found in detectLines() its edge segment index in getSegments()"},
    {"getSegments", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_getSegments, 0), "getSegments() -> retval\n.   @brief Returns std::vector<std::vector<Point>> of detected edge segments, see detectEdges()"},
    {"setParams", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_setParams, 0), "setParams(parameters) -> None\n.   @brief sets parameters.\n.   \n.       this function is meant to be used for parameter setting in other languages than c++ like python.\n.       @param parameters"},

    {NULL,          NULL}
};

// Converter (ximgproc_EdgeDrawing)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::EdgeDrawing> >
{
    static PyObject* from(const Ptr<cv::ximgproc::EdgeDrawing>& r)
    {
        return pyopencv_ximgproc_EdgeDrawing_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::EdgeDrawing>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::EdgeDrawing> * dst_;
        if (pyopencv_ximgproc_EdgeDrawing_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::EdgeDrawing> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_EdgeDrawing_Params (Generic)
//================================================================================

// GetSet (ximgproc_EdgeDrawing_Params)


static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_AnchorThresholdValue(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.AnchorThresholdValue);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_AnchorThresholdValue(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the AnchorThresholdValue attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.AnchorThresholdValue, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_EdgeDetectionOperator(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.EdgeDetectionOperator);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_EdgeDetectionOperator(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the EdgeDetectionOperator attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.EdgeDetectionOperator, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_GradientThresholdValue(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.GradientThresholdValue);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_GradientThresholdValue(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the GradientThresholdValue attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.GradientThresholdValue, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_LineFitErrorThreshold(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.LineFitErrorThreshold);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_LineFitErrorThreshold(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the LineFitErrorThreshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.LineFitErrorThreshold, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_MaxDistanceBetweenTwoLines(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.MaxDistanceBetweenTwoLines);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_MaxDistanceBetweenTwoLines(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the MaxDistanceBetweenTwoLines attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.MaxDistanceBetweenTwoLines, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_MaxErrorThreshold(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.MaxErrorThreshold);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_MaxErrorThreshold(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the MaxErrorThreshold attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.MaxErrorThreshold, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_MinLineLength(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.MinLineLength);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_MinLineLength(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the MinLineLength attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.MinLineLength, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_MinPathLength(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.MinPathLength);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_MinPathLength(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the MinPathLength attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.MinPathLength, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_NFAValidation(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.NFAValidation);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_NFAValidation(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the NFAValidation attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.NFAValidation, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_PFmode(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.PFmode);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_PFmode(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the PFmode attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.PFmode, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_ScanInterval(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.ScanInterval);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_ScanInterval(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the ScanInterval attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.ScanInterval, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_Sigma(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.Sigma);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_Sigma(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the Sigma attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.Sigma, ArgInfo("value", false)) ? 0 : -1;
}

static PyObject* pyopencv_ximgproc_EdgeDrawing_Params_get_SumFlag(pyopencv_ximgproc_EdgeDrawing_Params_t* p, void *closure)
{
    return jsopencv_from(p->v.SumFlag);
}

static int pyopencv_ximgproc_EdgeDrawing_Params_set_SumFlag(pyopencv_ximgproc_EdgeDrawing_Params_t* p, PyObject *value, void *closure)
{
    if (!value)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the SumFlag attribute");
        return -1;
    }
    return pyopencv_to_safe(value, p->v.SumFlag, ArgInfo("value", false)) ? 0 : -1;
}


// Methods (ximgproc_EdgeDrawing_Params)

static int pyopencv_cv_ximgproc_ximgproc_EdgeDrawing_Params_EdgeDrawing_Params(pyopencv_ximgproc_EdgeDrawing_Params_t* self, PyObject* py_args, PyObject* kw)
{
    using namespace cv::ximgproc;


    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2_NAPI(info, new (&(self->v)) cv::ximgproc::EdgeDrawing::Params());
        return 0;
    }

    return -1;
}



// Tables (ximgproc_EdgeDrawing_Params)

static PyGetSetDef pyopencv_ximgproc_EdgeDrawing_Params_getseters[] =
{
    {(char*)"AnchorThresholdValue", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_AnchorThresholdValue, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_AnchorThresholdValue, (char*)"AnchorThresholdValue", NULL},
    {(char*)"EdgeDetectionOperator", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_EdgeDetectionOperator, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_EdgeDetectionOperator, (char*)"EdgeDetectionOperator", NULL},
    {(char*)"GradientThresholdValue", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_GradientThresholdValue, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_GradientThresholdValue, (char*)"GradientThresholdValue", NULL},
    {(char*)"LineFitErrorThreshold", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_LineFitErrorThreshold, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_LineFitErrorThreshold, (char*)"LineFitErrorThreshold", NULL},
    {(char*)"MaxDistanceBetweenTwoLines", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_MaxDistanceBetweenTwoLines, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_MaxDistanceBetweenTwoLines, (char*)"MaxDistanceBetweenTwoLines", NULL},
    {(char*)"MaxErrorThreshold", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_MaxErrorThreshold, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_MaxErrorThreshold, (char*)"MaxErrorThreshold", NULL},
    {(char*)"MinLineLength", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_MinLineLength, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_MinLineLength, (char*)"MinLineLength", NULL},
    {(char*)"MinPathLength", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_MinPathLength, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_MinPathLength, (char*)"MinPathLength", NULL},
    {(char*)"NFAValidation", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_NFAValidation, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_NFAValidation, (char*)"NFAValidation", NULL},
    {(char*)"PFmode", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_PFmode, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_PFmode, (char*)"PFmode", NULL},
    {(char*)"ScanInterval", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_ScanInterval, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_ScanInterval, (char*)"ScanInterval", NULL},
    {(char*)"Sigma", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_Sigma, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_Sigma, (char*)"Sigma", NULL},
    {(char*)"SumFlag", (getter)pyopencv_ximgproc_EdgeDrawing_Params_get_SumFlag, (setter)pyopencv_ximgproc_EdgeDrawing_Params_set_SumFlag, (char*)"SumFlag", NULL},
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_EdgeDrawing_Params_methods[] =
{

    {NULL,          NULL}
};

// Converter (ximgproc_EdgeDrawing_Params)

template<>
struct PyOpenCV_Converter< cv::ximgproc::EdgeDrawing::Params >
{
    static PyObject* from(const cv::ximgproc::EdgeDrawing::Params& r)
    {
        return pyopencv_ximgproc_EdgeDrawing_Params_Instance(r);
    }
    static bool to(PyObject* src, cv::ximgproc::EdgeDrawing::Params& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        cv::ximgproc::EdgeDrawing::Params * dst_;
        if (pyopencv_ximgproc_EdgeDrawing_Params_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected cv::ximgproc::EdgeDrawing::Params for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_FastBilateralSolverFilter (Generic)
//================================================================================

// GetSet (ximgproc_FastBilateralSolverFilter)



// Methods (ximgproc_FastBilateralSolverFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_FastBilateralSolverFilter_filter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::FastBilateralSolverFilter> * self1 = 0;
    if (!pyopencv_ximgproc_FastBilateralSolverFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastBilateralSolverFilter' or its derivative)");
    Ptr<cv::ximgproc::FastBilateralSolverFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_confidence = NULL;
    Mat confidence;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "confidence", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ximgproc_FastBilateralSolverFilter.filter", (char**)keywords, &pyobj_src, &pyobj_confidence, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_confidence, confidence, ArgInfo("confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, confidence, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_confidence = NULL;
    UMat confidence;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "confidence", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|O:ximgproc_FastBilateralSolverFilter.filter", (char**)keywords, &pyobj_src, &pyobj_confidence, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_confidence, confidence, ArgInfo("confidence", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, confidence, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("filter");

    return NULL;
}



// Tables (ximgproc_FastBilateralSolverFilter)

static PyGetSetDef pyopencv_ximgproc_FastBilateralSolverFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_FastBilateralSolverFilter_methods[] =
{
    {"filter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastBilateralSolverFilter_filter, 0), "filter(src, confidence[, dst]) -> dst\n.   @brief Apply smoothing operation to the source image.\n.   \n.       @param src source image for filtering with unsigned 8-bit or signed 16-bit or floating-point 32-bit depth and up to 3 channels.\n.   \n.       @param confidence confidence image with unsigned 8-bit or floating-point 32-bit confidence and 1 channel.\n.   \n.       @param dst destination image.\n.   \n.       @note Confidence images with CV_8U depth are expected to in [0, 255] and CV_32F in [0, 1] range."},

    {NULL,          NULL}
};

// Converter (ximgproc_FastBilateralSolverFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::FastBilateralSolverFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::FastBilateralSolverFilter>& r)
    {
        return pyopencv_ximgproc_FastBilateralSolverFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::FastBilateralSolverFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::FastBilateralSolverFilter> * dst_;
        if (pyopencv_ximgproc_FastBilateralSolverFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::FastBilateralSolverFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_FastGlobalSmootherFilter (Generic)
//================================================================================

// GetSet (ximgproc_FastGlobalSmootherFilter)



// Methods (ximgproc_FastGlobalSmootherFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_FastGlobalSmootherFilter_filter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::FastGlobalSmootherFilter> * self1 = 0;
    if (!pyopencv_ximgproc_FastGlobalSmootherFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastGlobalSmootherFilter' or its derivative)");
    Ptr<cv::ximgproc::FastGlobalSmootherFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_FastGlobalSmootherFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_FastGlobalSmootherFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("filter");

    return NULL;
}



// Tables (ximgproc_FastGlobalSmootherFilter)

static PyGetSetDef pyopencv_ximgproc_FastGlobalSmootherFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_FastGlobalSmootherFilter_methods[] =
{
    {"filter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastGlobalSmootherFilter_filter, 0), "filter(src[, dst]) -> dst\n.   @brief Apply smoothing operation to the source image.\n.   \n.       @param src source image for filtering with unsigned 8-bit or signed 16-bit or floating-point 32-bit depth and up to 4 channels.\n.   \n.       @param dst destination image."},

    {NULL,          NULL}
};

// Converter (ximgproc_FastGlobalSmootherFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::FastGlobalSmootherFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::FastGlobalSmootherFilter>& r)
    {
        return pyopencv_ximgproc_FastGlobalSmootherFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::FastGlobalSmootherFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::FastGlobalSmootherFilter> * dst_;
        if (pyopencv_ximgproc_FastGlobalSmootherFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::FastGlobalSmootherFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_FastLineDetector (Generic)
//================================================================================

// GetSet (ximgproc_FastLineDetector)



// Methods (ximgproc_FastLineDetector)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_FastLineDetector_detect(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::FastLineDetector> * self1 = 0;
    if (!pyopencv_ximgproc_FastLineDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastLineDetector' or its derivative)");
    Ptr<cv::ximgproc::FastLineDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_lines = NULL;
    Mat lines;

    const char* keywords[] = { "image", "lines", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_FastLineDetector.detect", (char**)keywords, &pyobj_image, &pyobj_lines) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, lines));
        return jsopencv_from(lines);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_lines = NULL;
    UMat lines;

    const char* keywords[] = { "image", "lines", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_FastLineDetector.detect", (char**)keywords, &pyobj_image, &pyobj_lines) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 0)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detect(image, lines));
        return jsopencv_from(lines);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detect");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_FastLineDetector_drawSegments(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::FastLineDetector> * self1 = 0;
    if (!pyopencv_ximgproc_FastLineDetector_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastLineDetector' or its derivative)");
    Ptr<cv::ximgproc::FastLineDetector> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_lines = NULL;
    Mat lines;
    Napi::Value* pyobj_draw_arrow = NULL;
    bool draw_arrow=false;
    Napi::Value* pyobj_linecolor = NULL;
    Scalar linecolor=Scalar(0, 0, 255);
    Napi::Value* pyobj_linethickness = NULL;
    int linethickness=1;

    const char* keywords[] = { "image", "lines", "draw_arrow", "linecolor", "linethickness", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:ximgproc_FastLineDetector.drawSegments", (char**)keywords, &pyobj_image, &pyobj_lines, &pyobj_draw_arrow, &pyobj_linecolor, &pyobj_linethickness) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 0)) &&
        jsopencv_to_safe(info, pyobj_draw_arrow, draw_arrow, ArgInfo("draw_arrow", 0)) &&
        jsopencv_to_safe(info, pyobj_linecolor, linecolor, ArgInfo("linecolor", 0)) &&
        jsopencv_to_safe(info, pyobj_linethickness, linethickness, ArgInfo("linethickness", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->drawSegments(image, lines, draw_arrow, linecolor, linethickness));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_lines = NULL;
    UMat lines;
    Napi::Value* pyobj_draw_arrow = NULL;
    bool draw_arrow=false;
    Napi::Value* pyobj_linecolor = NULL;
    Scalar linecolor=Scalar(0, 0, 255);
    Napi::Value* pyobj_linethickness = NULL;
    int linethickness=1;

    const char* keywords[] = { "image", "lines", "draw_arrow", "linecolor", "linethickness", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOO:ximgproc_FastLineDetector.drawSegments", (char**)keywords, &pyobj_image, &pyobj_lines, &pyobj_draw_arrow, &pyobj_linecolor, &pyobj_linethickness) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_lines, lines, ArgInfo("lines", 0)) &&
        jsopencv_to_safe(info, pyobj_draw_arrow, draw_arrow, ArgInfo("draw_arrow", 0)) &&
        jsopencv_to_safe(info, pyobj_linecolor, linecolor, ArgInfo("linecolor", 0)) &&
        jsopencv_to_safe(info, pyobj_linethickness, linethickness, ArgInfo("linethickness", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->drawSegments(image, lines, draw_arrow, linecolor, linethickness));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("drawSegments");

    return NULL;
}



// Tables (ximgproc_FastLineDetector)

static PyGetSetDef pyopencv_ximgproc_FastLineDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_FastLineDetector_methods[] =
{
    {"detect", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastLineDetector_detect, 0), "detect(image[, lines]) -> lines\n.   @brief Finds lines in the input image.\n.         This is the output of the default parameters of the algorithm on the above\n.         shown image.\n.   \n.         ![image](pics/corridor_fld.jpg)\n.   \n.         @param image A grayscale (CV_8UC1) input image. If only a roi needs to be\n.         selected, use: `fld_ptr-\\>detect(image(roi), lines, ...);\n.         lines += Scalar(roi.x, roi.y, roi.x, roi.y);`\n.         @param lines A vector of Vec4f elements specifying the beginning\n.         and ending point of a line.  Where Vec4f is (x1, y1, x2, y2), point\n.         1 is the start, point 2 - end. Returned lines are directed so that the\n.         brighter side is on their left."},
    {"drawSegments", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastLineDetector_drawSegments, 0), "drawSegments(image, lines[, draw_arrow[, linecolor[, linethickness]]]) -> image\n.   @brief Draws the line segments on a given image.\n.         @param image The image, where the lines will be drawn. Should be bigger\n.         or equal to the image, where the lines were found.\n.         @param lines A vector of the lines that needed to be drawn.\n.         @param draw_arrow If true, arrow heads will be drawn.\n.         @param linecolor Line color.\n.         @param linethickness Line thickness."},

    {NULL,          NULL}
};

// Converter (ximgproc_FastLineDetector)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::FastLineDetector> >
{
    static PyObject* from(const Ptr<cv::ximgproc::FastLineDetector>& r)
    {
        return pyopencv_ximgproc_FastLineDetector_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::FastLineDetector>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::FastLineDetector> * dst_;
        if (pyopencv_ximgproc_FastLineDetector_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::FastLineDetector> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_GuidedFilter (Generic)
//================================================================================

// GetSet (ximgproc_GuidedFilter)



// Methods (ximgproc_GuidedFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_GuidedFilter_filter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::GuidedFilter> * self1 = 0;
    if (!pyopencv_ximgproc_GuidedFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_GuidedFilter' or its derivative)");
    Ptr<cv::ximgproc::GuidedFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_dDepth = NULL;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ximgproc_GuidedFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_dDepth) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_dDepth, dDepth, ArgInfo("dDepth", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst, dDepth));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_dDepth = NULL;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|OO:ximgproc_GuidedFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_dDepth) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_dDepth, dDepth, ArgInfo("dDepth", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->filter(src, dst, dDepth));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("filter");

    return NULL;
}



// Tables (ximgproc_GuidedFilter)

static PyGetSetDef pyopencv_ximgproc_GuidedFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_GuidedFilter_methods[] =
{
    {"filter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_GuidedFilter_filter, 0), "filter(src[, dst[, dDepth]]) -> dst\n.   @brief Apply Guided Filter to the filtering image.\n.   \n.       @param src filtering image with any numbers of channels.\n.   \n.       @param dst output image.\n.   \n.       @param dDepth optional depth of the output image. dDepth can be set to -1, which will be equivalent\n.       to src.depth()."},

    {NULL,          NULL}
};

// Converter (ximgproc_GuidedFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::GuidedFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::GuidedFilter>& r)
    {
        return pyopencv_ximgproc_GuidedFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::GuidedFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::GuidedFilter> * dst_;
        if (pyopencv_ximgproc_GuidedFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::GuidedFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_RFFeatureGetter (Generic)
//================================================================================

// GetSet (ximgproc_RFFeatureGetter)



// Methods (ximgproc_RFFeatureGetter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RFFeatureGetter_getFeatures(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RFFeatureGetter> * self1 = 0;
    if (!pyopencv_ximgproc_RFFeatureGetter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RFFeatureGetter' or its derivative)");
    Ptr<cv::ximgproc::RFFeatureGetter> _self_ = *(self1);
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_features = NULL;
    Mat features;
    Napi::Value* pyobj_gnrmRad = NULL;
    int gnrmRad=0;
    Napi::Value* pyobj_gsmthRad = NULL;
    int gsmthRad=0;
    Napi::Value* pyobj_shrink = NULL;
    int shrink=0;
    Napi::Value* pyobj_outNum = NULL;
    int outNum=0;
    Napi::Value* pyobj_gradNum = NULL;
    int gradNum=0;

    const char* keywords[] = { "src", "features", "gnrmRad", "gsmthRad", "shrink", "outNum", "gradNum", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOOOOO:ximgproc_RFFeatureGetter.getFeatures", (char**)keywords, &pyobj_src, &pyobj_features, &pyobj_gnrmRad, &pyobj_gsmthRad, &pyobj_shrink, &pyobj_outNum, &pyobj_gradNum) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_features, features, ArgInfo("features", 0)) &&
        jsopencv_to_safe(info, pyobj_gnrmRad, gnrmRad, ArgInfo("gnrmRad", 0)) &&
        jsopencv_to_safe(info, pyobj_gsmthRad, gsmthRad, ArgInfo("gsmthRad", 0)) &&
        jsopencv_to_safe(info, pyobj_shrink, shrink, ArgInfo("shrink", 0)) &&
        jsopencv_to_safe(info, pyobj_outNum, outNum, ArgInfo("outNum", 0)) &&
        jsopencv_to_safe(info, pyobj_gradNum, gradNum, ArgInfo("gradNum", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getFeatures(src, features, gnrmRad, gsmthRad, shrink, outNum, gradNum));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_RFFeatureGetter)

static PyGetSetDef pyopencv_ximgproc_RFFeatureGetter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_RFFeatureGetter_methods[] =
{
    {"getFeatures", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RFFeatureGetter_getFeatures, 0), "getFeatures(src, features, gnrmRad, gsmthRad, shrink, outNum, gradNum) -> None\n."},

    {NULL,          NULL}
};

// Converter (ximgproc_RFFeatureGetter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::RFFeatureGetter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::RFFeatureGetter>& r)
    {
        return pyopencv_ximgproc_RFFeatureGetter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::RFFeatureGetter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::RFFeatureGetter> * dst_;
        if (pyopencv_ximgproc_RFFeatureGetter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::RFFeatureGetter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_RICInterpolator (Generic)
//================================================================================

// GetSet (ximgproc_RICInterpolator)



// Methods (ximgproc_RICInterpolator)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getAlpha());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getFGSLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFGSLambda());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getFGSSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getFGSSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getK(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getK());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getMaxFlow(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMaxFlow());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getModelIter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getModelIter());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getRefineModels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getRefineModels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelMode(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSuperpixelMode());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelNNCnt(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSuperpixelNNCnt());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelRuler(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSuperpixelRuler());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSuperpixelSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getUseGlobalSmootherFilter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseGlobalSmootherFilter());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getUseVariationalRefinement(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    bool retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getUseVariationalRefinement());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setAlpha(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_alpha = NULL;
    float alpha=0.7f;

    const char* keywords[] = { "alpha", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setAlpha", (char**)keywords, &pyobj_alpha) &&
        jsopencv_to_safe(info, pyobj_alpha, alpha, ArgInfo("alpha", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setAlpha(alpha));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setCostMap(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_costMap = NULL;
    Mat costMap;

    const char* keywords[] = { "costMap", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_RICInterpolator.setCostMap", (char**)keywords, &pyobj_costMap) &&
        jsopencv_to_safe(info, pyobj_costMap, costMap, ArgInfo("costMap", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setCostMap(costMap));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setFGSLambda(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_lambda = NULL;
    float lambda=500.f;

    const char* keywords[] = { "lambda_", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setFGSLambda", (char**)keywords, &pyobj_lambda) &&
        jsopencv_to_safe(info, pyobj_lambda, lambda, ArgInfo("lambda", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFGSLambda(lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setFGSSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_sigma = NULL;
    float sigma=1.5f;

    const char* keywords[] = { "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setFGSSigma", (char**)keywords, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setFGSSigma(sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setK(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_k = NULL;
    int k=32;

    const char* keywords[] = { "k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setK", (char**)keywords, &pyobj_k) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setK(k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setMaxFlow(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_maxFlow = NULL;
    float maxFlow=250.f;

    const char* keywords[] = { "maxFlow", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setMaxFlow", (char**)keywords, &pyobj_maxFlow) &&
        jsopencv_to_safe(info, pyobj_maxFlow, maxFlow, ArgInfo("maxFlow", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMaxFlow(maxFlow));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setModelIter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_modelIter = NULL;
    int modelIter=4;

    const char* keywords[] = { "modelIter", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setModelIter", (char**)keywords, &pyobj_modelIter) &&
        jsopencv_to_safe(info, pyobj_modelIter, modelIter, ArgInfo("modelIter", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setModelIter(modelIter));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setRefineModels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_refineModles = NULL;
    bool refineModles=true;

    const char* keywords[] = { "refineModles", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setRefineModels", (char**)keywords, &pyobj_refineModles) &&
        jsopencv_to_safe(info, pyobj_refineModles, refineModles, ArgInfo("refineModles", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setRefineModels(refineModles));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelMode(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_mode = NULL;
    int mode=100;

    const char* keywords[] = { "mode", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setSuperpixelMode", (char**)keywords, &pyobj_mode) &&
        jsopencv_to_safe(info, pyobj_mode, mode, ArgInfo("mode", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSuperpixelMode(mode));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelNNCnt(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_spNN = NULL;
    int spNN=150;

    const char* keywords[] = { "spNN", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setSuperpixelNNCnt", (char**)keywords, &pyobj_spNN) &&
        jsopencv_to_safe(info, pyobj_spNN, spNN, ArgInfo("spNN", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSuperpixelNNCnt(spNN));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelRuler(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_ruler = NULL;
    float ruler=15.f;

    const char* keywords[] = { "ruler", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setSuperpixelRuler", (char**)keywords, &pyobj_ruler) &&
        jsopencv_to_safe(info, pyobj_ruler, ruler, ArgInfo("ruler", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSuperpixelRuler(ruler));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_spSize = NULL;
    int spSize=15;

    const char* keywords[] = { "spSize", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setSuperpixelSize", (char**)keywords, &pyobj_spSize) &&
        jsopencv_to_safe(info, pyobj_spSize, spSize, ArgInfo("spSize", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSuperpixelSize(spSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setUseGlobalSmootherFilter(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_use_FGS = NULL;
    bool use_FGS=true;

    const char* keywords[] = { "use_FGS", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setUseGlobalSmootherFilter", (char**)keywords, &pyobj_use_FGS) &&
        jsopencv_to_safe(info, pyobj_use_FGS, use_FGS, ArgInfo("use_FGS", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseGlobalSmootherFilter(use_FGS));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setUseVariationalRefinement(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RICInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_RICInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RICInterpolator' or its derivative)");
    Ptr<cv::ximgproc::RICInterpolator> _self_ = *(self1);
    Napi::Value* pyobj_use_variational_refinement = NULL;
    bool use_variational_refinement=false;

    const char* keywords[] = { "use_variational_refinement", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_RICInterpolator.setUseVariationalRefinement", (char**)keywords, &pyobj_use_variational_refinement) &&
        jsopencv_to_safe(info, pyobj_use_variational_refinement, use_variational_refinement, ArgInfo("use_variational_refinement", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setUseVariationalRefinement(use_variational_refinement));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_RICInterpolator)

static PyGetSetDef pyopencv_ximgproc_RICInterpolator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_RICInterpolator_methods[] =
{
    {"getAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getAlpha, 0), "getAlpha() -> retval\n.   @copybrief setAlpha\n.        *  @see setAlpha"},
    {"getFGSLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getFGSLambda, 0), "getFGSLambda() -> retval\n.   @copybrief setFGSLambda\n.        *  @see setFGSLambda"},
    {"getFGSSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getFGSSigma, 0), "getFGSSigma() -> retval\n.   @copybrief setFGSSigma\n.        *  @see setFGSSigma"},
    {"getK", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getK, 0), "getK() -> retval\n.   @copybrief setK\n.        *  @see setK"},
    {"getMaxFlow", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getMaxFlow, 0), "getMaxFlow() -> retval\n.   @copybrief setMaxFlow\n.        *  @see setMaxFlow"},
    {"getModelIter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getModelIter, 0), "getModelIter() -> retval\n.   @copybrief setModelIter\n.        *  @see setModelIter"},
    {"getRefineModels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getRefineModels, 0), "getRefineModels() -> retval\n.   @copybrief setRefineModels\n.        *  @see setRefineModels"},
    {"getSuperpixelMode", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelMode, 0), "getSuperpixelMode() -> retval\n.   @copybrief setSuperpixelMode\n.        *  @see setSuperpixelMode"},
    {"getSuperpixelNNCnt", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelNNCnt, 0), "getSuperpixelNNCnt() -> retval\n.   @copybrief setSuperpixelNNCnt\n.        *  @see setSuperpixelNNCnt"},
    {"getSuperpixelRuler", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelRuler, 0), "getSuperpixelRuler() -> retval\n.   @copybrief setSuperpixelRuler\n.        *  @see setSuperpixelRuler"},
    {"getSuperpixelSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getSuperpixelSize, 0), "getSuperpixelSize() -> retval\n.   @copybrief setSuperpixelSize\n.        *  @see setSuperpixelSize"},
    {"getUseGlobalSmootherFilter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getUseGlobalSmootherFilter, 0), "getUseGlobalSmootherFilter() -> retval\n.   @copybrief setUseGlobalSmootherFilter\n.        *  @see setUseGlobalSmootherFilter"},
    {"getUseVariationalRefinement", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_getUseVariationalRefinement, 0), "getUseVariationalRefinement() -> retval\n.   @copybrief setUseVariationalRefinement\n.        *  @see setUseVariationalRefinement"},
    {"setAlpha", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setAlpha, 0), "setAlpha([, alpha]) -> None\n.   @brief Alpha is a parameter defining a global weight for transforming geodesic distance into weight."},
    {"setCostMap", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setCostMap, 0), "setCostMap(costMap) -> None\n.   @brief Interface to provide a more elaborated cost map, i.e. edge map, for the edge-aware term.\n.        *  This implementation is based on a rather simple gradient-based edge map estimation.\n.        *  To used more complex edge map estimator (e.g. StructuredEdgeDetection that has been\n.        *  used in the original publication) that may lead to improved accuracies, the internal\n.        *  edge map estimation can be bypassed here.\n.        *  @param costMap a type CV_32FC1 Mat is required.\n.        *  @see cv::ximgproc::createSuperpixelSLIC"},
    {"setFGSLambda", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setFGSLambda, 0), "setFGSLambda([, lambda_]) -> None\n.   @brief Sets the respective fastGlobalSmootherFilter() parameter."},
    {"setFGSSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setFGSSigma, 0), "setFGSSigma([, sigma]) -> None\n.   @brief Sets the respective fastGlobalSmootherFilter() parameter."},
    {"setK", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setK, 0), "setK([, k]) -> None\n.   @brief K is a number of nearest-neighbor matches considered, when fitting a locally affine\n.        *model for a superpixel segment. However, lower values would make the interpolation\n.        *noticeably faster. The original implementation of @cite Hu2017 uses 32."},
    {"setMaxFlow", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setMaxFlow, 0), "setMaxFlow([, maxFlow]) -> None\n.   @brief MaxFlow is a threshold to validate the predictions using a certain piece-wise affine model.\n.        * If the prediction exceeds the treshold the translational model will be applied instead."},
    {"setModelIter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setModelIter, 0), "setModelIter([, modelIter]) -> None\n.   @brief Parameter defining the number of iterations for piece-wise affine model estimation."},
    {"setRefineModels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setRefineModels, 0), "setRefineModels([, refineModles]) -> None\n.   @brief Parameter to choose wether additional refinement of the piece-wise affine models is employed."},
    {"setSuperpixelMode", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelMode, 0), "setSuperpixelMode([, mode]) -> None\n.   @brief Parameter to choose superpixel algorithm variant to use:\n.        * - cv::ximgproc::SLICType SLIC segments image using a desired region_size (value: 100)\n.        * - cv::ximgproc::SLICType SLICO will optimize using adaptive compactness factor (value: 101)\n.        * - cv::ximgproc::SLICType MSLIC will optimize using manifold methods resulting in more content-sensitive superpixels (value: 102).\n.        *  @see cv::ximgproc::createSuperpixelSLIC"},
    {"setSuperpixelNNCnt", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelNNCnt, 0), "setSuperpixelNNCnt([, spNN]) -> None\n.   @brief Parameter defines the number of nearest-neighbor matches for each superpixel considered, when fitting a locally affine\n.        *model."},
    {"setSuperpixelRuler", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelRuler, 0), "setSuperpixelRuler([, ruler]) -> None\n.   @brief Parameter to tune enforcement of superpixel smoothness factor used for oversegmentation.\n.        *  @see cv::ximgproc::createSuperpixelSLIC"},
    {"setSuperpixelSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setSuperpixelSize, 0), "setSuperpixelSize([, spSize]) -> None\n.   @brief Get the internal cost, i.e. edge map, used for estimating the edge-aware term.\n.        *  @see setCostMap"},
    {"setUseGlobalSmootherFilter", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setUseGlobalSmootherFilter, 0), "setUseGlobalSmootherFilter([, use_FGS]) -> None\n.   @brief Sets whether the fastGlobalSmootherFilter() post-processing is employed."},
    {"setUseVariationalRefinement", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RICInterpolator_setUseVariationalRefinement, 0), "setUseVariationalRefinement([, use_variational_refinement]) -> None\n.   @brief Parameter to choose wether the VariationalRefinement post-processing  is employed."},

    {NULL,          NULL}
};

// Converter (ximgproc_RICInterpolator)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::RICInterpolator> >
{
    static PyObject* from(const Ptr<cv::ximgproc::RICInterpolator>& r)
    {
        return pyopencv_ximgproc_RICInterpolator_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::RICInterpolator>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::RICInterpolator> * dst_;
        if (pyopencv_ximgproc_RICInterpolator_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::RICInterpolator> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_RidgeDetectionFilter (Generic)
//================================================================================

// GetSet (ximgproc_RidgeDetectionFilter)



// Methods (ximgproc_RidgeDetectionFilter)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_create_static(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;

    Napi::Value* pyobj_ddepth = NULL;
    int ddepth=CV_32FC1;
    Napi::Value* pyobj_dx = NULL;
    int dx=1;
    Napi::Value* pyobj_dy = NULL;
    int dy=1;
    Napi::Value* pyobj_ksize = NULL;
    int ksize=3;
    Napi::Value* pyobj_out_dtype = NULL;
    int out_dtype=CV_8UC1;
    Napi::Value* pyobj_scale = NULL;
    double scale=1;
    Napi::Value* pyobj_delta = NULL;
    double delta=0;
    Napi::Value* pyobj_borderType = NULL;
    int borderType=BORDER_DEFAULT;
    Ptr<RidgeDetectionFilter> retval;

    const char* keywords[] = { "ddepth", "dx", "dy", "ksize", "out_dtype", "scale", "delta", "borderType", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOOOOOOO:ximgproc_RidgeDetectionFilter.create", (char**)keywords, &pyobj_ddepth, &pyobj_dx, &pyobj_dy, &pyobj_ksize, &pyobj_out_dtype, &pyobj_scale, &pyobj_delta, &pyobj_borderType) &&
        jsopencv_to_safe(info, pyobj_ddepth, ddepth, ArgInfo("ddepth", 0)) &&
        jsopencv_to_safe(info, pyobj_dx, dx, ArgInfo("dx", 0)) &&
        jsopencv_to_safe(info, pyobj_dy, dy, ArgInfo("dy", 0)) &&
        jsopencv_to_safe(info, pyobj_ksize, ksize, ArgInfo("ksize", 0)) &&
        jsopencv_to_safe(info, pyobj_out_dtype, out_dtype, ArgInfo("out_dtype", 0)) &&
        jsopencv_to_safe(info, pyobj_scale, scale, ArgInfo("scale", 0)) &&
        jsopencv_to_safe(info, pyobj_delta, delta, ArgInfo("delta", 0)) &&
        jsopencv_to_safe(info, pyobj_borderType, borderType, ArgInfo("borderType", 0)))
    {
        ERRWRAP2_NAPI(info, retval = cv::ximgproc::RidgeDetectionFilter::create(ddepth, dx, dy, ksize, out_dtype, scale, delta, borderType));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_getRidgeFilteredImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::RidgeDetectionFilter> * self1 = 0;
    if (!pyopencv_ximgproc_RidgeDetectionFilter_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_RidgeDetectionFilter' or its derivative)");
    Ptr<cv::ximgproc::RidgeDetectionFilter> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj__img = NULL;
    Mat _img;
    Napi::Value* pyobj_out = NULL;
    Mat out;

    const char* keywords[] = { "_img", "out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_RidgeDetectionFilter.getRidgeFilteredImage", (char**)keywords, &pyobj__img, &pyobj_out) &&
        jsopencv_to_safe(info, pyobj__img, _img, ArgInfo("_img", 0)) &&
        jsopencv_to_safe(info, pyobj_out, out, ArgInfo("out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getRidgeFilteredImage(_img, out));
        return jsopencv_from(out);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj__img = NULL;
    UMat _img;
    Napi::Value* pyobj_out = NULL;
    UMat out;

    const char* keywords[] = { "_img", "out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_RidgeDetectionFilter.getRidgeFilteredImage", (char**)keywords, &pyobj__img, &pyobj_out) &&
        jsopencv_to_safe(info, pyobj__img, _img, ArgInfo("_img", 0)) &&
        jsopencv_to_safe(info, pyobj_out, out, ArgInfo("out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getRidgeFilteredImage(_img, out));
        return jsopencv_from(out);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getRidgeFilteredImage");

    return NULL;
}



// Tables (ximgproc_RidgeDetectionFilter)

static PyGetSetDef pyopencv_ximgproc_RidgeDetectionFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_RidgeDetectionFilter_methods[] =
{
    {"create", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_create_static, METH_STATIC), "create([, ddepth[, dx[, dy[, ksize[, out_dtype[, scale[, delta[, borderType]]]]]]]]) -> retval\n.   @brief Create pointer to the Ridge detection filter.\n.       @param ddepth  Specifies output image depth. Defualt is CV_32FC1\n.       @param dx Order of derivative x, default is 1\n.       @param dy  Order of derivative y, default is 1\n.       @param ksize Sobel kernel size , default is 3\n.       @param out_dtype Converted format for output, default is CV_8UC1\n.       @param scale Optional scale value for derivative values, default is 1\n.       @param delta  Optional bias added to output, default is 0\n.       @param borderType Pixel extrapolation method, default is BORDER_DEFAULT\n.       @see Sobel, threshold, getStructuringElement, morphologyEx.( for additional refinement)"},
    {"getRidgeFilteredImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_getRidgeFilteredImage, 0), "getRidgeFilteredImage(_img[, out]) -> out\n.   @brief Apply Ridge detection filter on input image.\n.       @param _img InputArray as supported by Sobel. img can be 1-Channel or 3-Channels.\n.       @param out OutputAray of structure as RidgeDetectionFilter::ddepth. Output image with ridges."},

    {NULL,          NULL}
};

// Converter (ximgproc_RidgeDetectionFilter)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::RidgeDetectionFilter> >
{
    static PyObject* from(const Ptr<cv::ximgproc::RidgeDetectionFilter>& r)
    {
        return pyopencv_ximgproc_RidgeDetectionFilter_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::RidgeDetectionFilter>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::RidgeDetectionFilter> * dst_;
        if (pyopencv_ximgproc_RidgeDetectionFilter_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::RidgeDetectionFilter> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_ScanSegment (Generic)
//================================================================================

// GetSet (ximgproc_ScanSegment)



// Methods (ximgproc_ScanSegment)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ScanSegment_getLabelContourMask(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ScanSegment> * self1 = 0;
    if (!pyopencv_ximgproc_ScanSegment_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ScanSegment' or its derivative)");
    Ptr<cv::ximgproc::ScanSegment> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=false;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_ScanSegment.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=false;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_ScanSegment.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabelContourMask");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ScanSegment_getLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ScanSegment> * self1 = 0;
    if (!pyopencv_ximgproc_ScanSegment_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ScanSegment' or its derivative)");
    Ptr<cv::ximgproc::ScanSegment> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_ScanSegment.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_ScanSegment.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabels");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ScanSegment_getNumberOfSuperpixels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ScanSegment> * self1 = 0;
    if (!pyopencv_ximgproc_ScanSegment_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ScanSegment' or its derivative)");
    Ptr<cv::ximgproc::ScanSegment> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumberOfSuperpixels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_ScanSegment_iterate(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::ScanSegment> * self1 = 0;
    if (!pyopencv_ximgproc_ScanSegment_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_ScanSegment' or its derivative)");
    Ptr<cv::ximgproc::ScanSegment> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_ScanSegment.iterate", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->iterate(img));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_ScanSegment.iterate", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->iterate(img));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("iterate");

    return NULL;
}



// Tables (ximgproc_ScanSegment)

static PyGetSetDef pyopencv_ximgproc_ScanSegment_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_ScanSegment_methods[] =
{
    {"getLabelContourMask", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ScanSegment_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in the ScanSegment object.\n.   \n.       The function return the boundaries of the superpixel segmentation.\n.   \n.       @param image Return: CV_8UC1 image mask where -1 indicates that the pixel is a superpixel border, and 0 otherwise.\n.       @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border are masked."},
    {"getLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ScanSegment_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.       Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.       @param labels_out Return: A CV_32UC1 integer array containing the labels of the superpixel\n.       segmentation. The labels are in the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ScanSegment_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Returns the actual superpixel segmentation from the last image processed using iterate.\n.   \n.       Returns zero if no image has been processed."},
    {"iterate", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ScanSegment_iterate, 0), "iterate(img) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.       parameters in the ScanSegment object.\n.   \n.       This function can be called again for other images without the need of initializing the algorithm with createScanSegment().\n.       This save the computational cost of allocating memory for all the structures of the algorithm.\n.   \n.       @param img Input image. Supported format: CV_8UC3. Image size must match with the initialized\n.       image size with the function createScanSegment(). It MUST be in Lab color space."},

    {NULL,          NULL}
};

// Converter (ximgproc_ScanSegment)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::ScanSegment> >
{
    static PyObject* from(const Ptr<cv::ximgproc::ScanSegment>& r)
    {
        return pyopencv_ximgproc_ScanSegment_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::ScanSegment>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::ScanSegment> * dst_;
        if (pyopencv_ximgproc_ScanSegment_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::ScanSegment> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_SparseMatchInterpolator (Generic)
//================================================================================

// GetSet (ximgproc_SparseMatchInterpolator)



// Methods (ximgproc_SparseMatchInterpolator)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SparseMatchInterpolator_interpolate(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SparseMatchInterpolator> * self1 = 0;
    if (!pyopencv_ximgproc_SparseMatchInterpolator_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SparseMatchInterpolator' or its derivative)");
    Ptr<cv::ximgproc::SparseMatchInterpolator> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_from_image = NULL;
    Mat from_image;
    Napi::Value* pyobj_from_points = NULL;
    Mat from_points;
    Napi::Value* pyobj_to_image = NULL;
    Mat to_image;
    Napi::Value* pyobj_to_points = NULL;
    Mat to_points;
    Napi::Value* pyobj_dense_flow = NULL;
    Mat dense_flow;

    const char* keywords[] = { "from_image", "from_points", "to_image", "to_points", "dense_flow", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:ximgproc_SparseMatchInterpolator.interpolate", (char**)keywords, &pyobj_from_image, &pyobj_from_points, &pyobj_to_image, &pyobj_to_points, &pyobj_dense_flow) &&
        jsopencv_to_safe(info, pyobj_from_image, from_image, ArgInfo("from_image", 0)) &&
        jsopencv_to_safe(info, pyobj_from_points, from_points, ArgInfo("from_points", 0)) &&
        jsopencv_to_safe(info, pyobj_to_image, to_image, ArgInfo("to_image", 0)) &&
        jsopencv_to_safe(info, pyobj_to_points, to_points, ArgInfo("to_points", 0)) &&
        jsopencv_to_safe(info, pyobj_dense_flow, dense_flow, ArgInfo("dense_flow", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->interpolate(from_image, from_points, to_image, to_points, dense_flow));
        return jsopencv_from(dense_flow);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_from_image = NULL;
    UMat from_image;
    Napi::Value* pyobj_from_points = NULL;
    UMat from_points;
    Napi::Value* pyobj_to_image = NULL;
    UMat to_image;
    Napi::Value* pyobj_to_points = NULL;
    UMat to_points;
    Napi::Value* pyobj_dense_flow = NULL;
    UMat dense_flow;

    const char* keywords[] = { "from_image", "from_points", "to_image", "to_points", "dense_flow", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOOO|O:ximgproc_SparseMatchInterpolator.interpolate", (char**)keywords, &pyobj_from_image, &pyobj_from_points, &pyobj_to_image, &pyobj_to_points, &pyobj_dense_flow) &&
        jsopencv_to_safe(info, pyobj_from_image, from_image, ArgInfo("from_image", 0)) &&
        jsopencv_to_safe(info, pyobj_from_points, from_points, ArgInfo("from_points", 0)) &&
        jsopencv_to_safe(info, pyobj_to_image, to_image, ArgInfo("to_image", 0)) &&
        jsopencv_to_safe(info, pyobj_to_points, to_points, ArgInfo("to_points", 0)) &&
        jsopencv_to_safe(info, pyobj_dense_flow, dense_flow, ArgInfo("dense_flow", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->interpolate(from_image, from_points, to_image, to_points, dense_flow));
        return jsopencv_from(dense_flow);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("interpolate");

    return NULL;
}



// Tables (ximgproc_SparseMatchInterpolator)

static PyGetSetDef pyopencv_ximgproc_SparseMatchInterpolator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_SparseMatchInterpolator_methods[] =
{
    {"interpolate", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SparseMatchInterpolator_interpolate, 0), "interpolate(from_image, from_points, to_image, to_points[, dense_flow]) -> dense_flow\n.   @brief Interpolate input sparse matches.\n.   \n.       @param from_image first of the two matched images, 8-bit single-channel or three-channel.\n.   \n.       @param from_points points of the from_image for which there are correspondences in the\n.       to_image (Point2f vector or Mat of depth CV_32F)\n.   \n.       @param to_image second of the two matched images, 8-bit single-channel or three-channel.\n.   \n.       @param to_points points in the to_image corresponding to from_points\n.       (Point2f vector or Mat of depth CV_32F)\n.   \n.       @param dense_flow output dense matching (two-channel CV_32F image)"},

    {NULL,          NULL}
};

// Converter (ximgproc_SparseMatchInterpolator)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::SparseMatchInterpolator> >
{
    static PyObject* from(const Ptr<cv::ximgproc::SparseMatchInterpolator>& r)
    {
        return pyopencv_ximgproc_SparseMatchInterpolator_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::SparseMatchInterpolator>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::SparseMatchInterpolator> * dst_;
        if (pyopencv_ximgproc_SparseMatchInterpolator_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::SparseMatchInterpolator> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_StructuredEdgeDetection (Generic)
//================================================================================

// GetSet (ximgproc_StructuredEdgeDetection)



// Methods (ximgproc_StructuredEdgeDetection)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_computeOrientation(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::StructuredEdgeDetection> * self1 = 0;
    if (!pyopencv_ximgproc_StructuredEdgeDetection_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_StructuredEdgeDetection' or its derivative)");
    Ptr<cv::ximgproc::StructuredEdgeDetection> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_StructuredEdgeDetection.computeOrientation", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->computeOrientation(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_StructuredEdgeDetection.computeOrientation", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->computeOrientation(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("computeOrientation");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_detectEdges(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::StructuredEdgeDetection> * self1 = 0;
    if (!pyopencv_ximgproc_StructuredEdgeDetection_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_StructuredEdgeDetection' or its derivative)");
    Ptr<cv::ximgproc::StructuredEdgeDetection> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_StructuredEdgeDetection.detectEdges", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectEdges(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_StructuredEdgeDetection.detectEdges", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->detectEdges(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("detectEdges");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_edgesNms(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::StructuredEdgeDetection> * self1 = 0;
    if (!pyopencv_ximgproc_StructuredEdgeDetection_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_StructuredEdgeDetection' or its derivative)");
    Ptr<cv::ximgproc::StructuredEdgeDetection> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_edge_image = NULL;
    Mat edge_image;
    Napi::Value* pyobj_orientation_image = NULL;
    Mat orientation_image;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;
    Napi::Value* pyobj_r = NULL;
    int r=2;
    Napi::Value* pyobj_s = NULL;
    int s=0;
    Napi::Value* pyobj_m = NULL;
    float m=1;
    Napi::Value* pyobj_isParallel = NULL;
    bool isParallel=true;

    const char* keywords[] = { "edge_image", "orientation_image", "dst", "r", "s", "m", "isParallel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOOOO:ximgproc_StructuredEdgeDetection.edgesNms", (char**)keywords, &pyobj_edge_image, &pyobj_orientation_image, &pyobj_dst, &pyobj_r, &pyobj_s, &pyobj_m, &pyobj_isParallel) &&
        jsopencv_to_safe(info, pyobj_edge_image, edge_image, ArgInfo("edge_image", 0)) &&
        jsopencv_to_safe(info, pyobj_orientation_image, orientation_image, ArgInfo("orientation_image", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_r, r, ArgInfo("r", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_isParallel, isParallel, ArgInfo("isParallel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->edgesNms(edge_image, orientation_image, dst, r, s, m, isParallel));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_edge_image = NULL;
    UMat edge_image;
    Napi::Value* pyobj_orientation_image = NULL;
    UMat orientation_image;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;
    Napi::Value* pyobj_r = NULL;
    int r=2;
    Napi::Value* pyobj_s = NULL;
    int s=0;
    Napi::Value* pyobj_m = NULL;
    float m=1;
    Napi::Value* pyobj_isParallel = NULL;
    bool isParallel=true;

    const char* keywords[] = { "edge_image", "orientation_image", "dst", "r", "s", "m", "isParallel", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO|OOOOO:ximgproc_StructuredEdgeDetection.edgesNms", (char**)keywords, &pyobj_edge_image, &pyobj_orientation_image, &pyobj_dst, &pyobj_r, &pyobj_s, &pyobj_m, &pyobj_isParallel) &&
        jsopencv_to_safe(info, pyobj_edge_image, edge_image, ArgInfo("edge_image", 0)) &&
        jsopencv_to_safe(info, pyobj_orientation_image, orientation_image, ArgInfo("orientation_image", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)) &&
        jsopencv_to_safe(info, pyobj_r, r, ArgInfo("r", 0)) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)) &&
        jsopencv_to_safe(info, pyobj_m, m, ArgInfo("m", 0)) &&
        jsopencv_to_safe(info, pyobj_isParallel, isParallel, ArgInfo("isParallel", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->edgesNms(edge_image, orientation_image, dst, r, s, m, isParallel));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("edgesNms");

    return NULL;
}



// Tables (ximgproc_StructuredEdgeDetection)

static PyGetSetDef pyopencv_ximgproc_StructuredEdgeDetection_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_StructuredEdgeDetection_methods[] =
{
    {"computeOrientation", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_computeOrientation, 0), "computeOrientation(src[, dst]) -> dst\n.   @brief The function computes orientation from edge image.\n.   \n.       @param src edge image.\n.       @param dst orientation image."},
    {"detectEdges", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_detectEdges, 0), "detectEdges(src[, dst]) -> dst\n.   @brief The function detects edges in src and draw them to dst.\n.   \n.       The algorithm underlies this function is much more robust to texture presence, than common\n.       approaches, e.g. Sobel\n.       @param src source image (RGB, float, in [0;1]) to detect edges\n.       @param dst destination image (grayscale, float, in [0;1]) where edges are drawn\n.       @sa Sobel, Canny"},
    {"edgesNms", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_edgesNms, 0), "edgesNms(edge_image, orientation_image[, dst[, r[, s[, m[, isParallel]]]]]) -> dst\n.   @brief The function edgenms in edge image and suppress edges where edge is stronger in orthogonal direction.\n.   \n.       @param edge_image edge image from detectEdges function.\n.       @param orientation_image orientation image from computeOrientation function.\n.       @param dst suppressed image (grayscale, float, in [0;1])\n.       @param r radius for NMS suppression.\n.       @param s radius for boundary suppression.\n.       @param m multiplier for conservative suppression.\n.       @param isParallel enables/disables parallel computing."},

    {NULL,          NULL}
};

// Converter (ximgproc_StructuredEdgeDetection)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::StructuredEdgeDetection> >
{
    static PyObject* from(const Ptr<cv::ximgproc::StructuredEdgeDetection>& r)
    {
        return pyopencv_ximgproc_StructuredEdgeDetection_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::StructuredEdgeDetection>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::StructuredEdgeDetection> * dst_;
        if (pyopencv_ximgproc_StructuredEdgeDetection_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::StructuredEdgeDetection> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_SuperpixelLSC (Generic)
//================================================================================

// GetSet (ximgproc_SuperpixelLSC)



// Methods (ximgproc_SuperpixelLSC)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_enforceLabelConnectivity(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelLSC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelLSC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelLSC> _self_ = *(self1);
    Napi::Value* pyobj_min_element_size = NULL;
    int min_element_size=25;

    const char* keywords[] = { "min_element_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelLSC.enforceLabelConnectivity", (char**)keywords, &pyobj_min_element_size) &&
        jsopencv_to_safe(info, pyobj_min_element_size, min_element_size, ArgInfo("min_element_size", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->enforceLabelConnectivity(min_element_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabelContourMask(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelLSC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelLSC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelLSC> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_SuperpixelLSC.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_SuperpixelLSC.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabelContourMask");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelLSC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelLSC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelLSC> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelLSC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelLSC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabels");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getNumberOfSuperpixels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelLSC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelLSC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelLSC> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumberOfSuperpixels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_iterate(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelLSC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelLSC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelLSC> _self_ = *(self1);
    Napi::Value* pyobj_num_iterations = NULL;
    int num_iterations=10;

    const char* keywords[] = { "num_iterations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelLSC.iterate", (char**)keywords, &pyobj_num_iterations) &&
        jsopencv_to_safe(info, pyobj_num_iterations, num_iterations, ArgInfo("num_iterations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->iterate(num_iterations));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_SuperpixelLSC)

static PyGetSetDef pyopencv_ximgproc_SuperpixelLSC_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_SuperpixelLSC_methods[] =
{
    {"enforceLabelConnectivity", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_enforceLabelConnectivity, 0), "enforceLabelConnectivity([, min_element_size]) -> None\n.   @brief Enforce label connectivity.\n.   \n.       @param min_element_size The minimum element size in percents that should be absorbed into a bigger\n.       superpixel. Given resulted average superpixel size valid value should be in 0-100 range, 25 means\n.       that less then a quarter sized superpixel should be absorbed, this is default.\n.   \n.       The function merge component that is too small, assigning the previously found adjacent label\n.       to this component. Calling this function may change the final number of superpixels."},
    {"getLabelContourMask", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in SuperpixelLSC object.\n.   \n.       @param image Return: CV_8U1 image mask where -1 indicates that the pixel is a superpixel border,\n.       and 0 otherwise.\n.   \n.       @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border\n.       are masked.\n.   \n.       The function return the boundaries of the superpixel segmentation."},
    {"getLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.       Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.       @param labels_out Return: A CV_32SC1 integer array containing the labels of the superpixel\n.       segmentation. The labels are in the range [0, getNumberOfSuperpixels()].\n.   \n.       The function returns an image with the labels of the superpixel segmentation. The labels are in\n.       the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Calculates the actual amount of superpixels on a given segmentation computed\n.       and stored in SuperpixelLSC object."},
    {"iterate", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_iterate, 0), "iterate([, num_iterations]) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.       parameters in the SuperpixelLSC object.\n.   \n.       This function can be called again without the need of initializing the algorithm with\n.       createSuperpixelLSC(). This save the computational cost of allocating memory for all the\n.       structures of the algorithm.\n.   \n.       @param num_iterations Number of iterations. Higher number improves the result.\n.   \n.       The function computes the superpixels segmentation of an image with the parameters initialized\n.       with the function createSuperpixelLSC(). The algorithms starts from a grid of superpixels and\n.       then refines the boundaries by proposing updates of edges boundaries."},

    {NULL,          NULL}
};

// Converter (ximgproc_SuperpixelLSC)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::SuperpixelLSC> >
{
    static PyObject* from(const Ptr<cv::ximgproc::SuperpixelLSC>& r)
    {
        return pyopencv_ximgproc_SuperpixelLSC_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::SuperpixelLSC>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::SuperpixelLSC> * dst_;
        if (pyopencv_ximgproc_SuperpixelLSC_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::SuperpixelLSC> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_SuperpixelSEEDS (Generic)
//================================================================================

// GetSet (ximgproc_SuperpixelSEEDS)



// Methods (ximgproc_SuperpixelSEEDS)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabelContourMask(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSEEDS> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSEEDS_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSEEDS> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=false;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_SuperpixelSEEDS.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=false;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_SuperpixelSEEDS.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabelContourMask");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSEEDS> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSEEDS_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSEEDS> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelSEEDS.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelSEEDS.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabels");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getNumberOfSuperpixels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSEEDS> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSEEDS_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSEEDS> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumberOfSuperpixels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_iterate(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSEEDS> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSEEDS_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSEEDS> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_num_iterations = NULL;
    int num_iterations=4;

    const char* keywords[] = { "img", "num_iterations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_SuperpixelSEEDS.iterate", (char**)keywords, &pyobj_img, &pyobj_num_iterations) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_num_iterations, num_iterations, ArgInfo("num_iterations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->iterate(img, num_iterations));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_num_iterations = NULL;
    int num_iterations=4;

    const char* keywords[] = { "img", "num_iterations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_SuperpixelSEEDS.iterate", (char**)keywords, &pyobj_img, &pyobj_num_iterations) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_num_iterations, num_iterations, ArgInfo("num_iterations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->iterate(img, num_iterations));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("iterate");

    return NULL;
}



// Tables (ximgproc_SuperpixelSEEDS)

static PyGetSetDef pyopencv_ximgproc_SuperpixelSEEDS_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_SuperpixelSEEDS_methods[] =
{
    {"getLabelContourMask", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in SuperpixelSEEDS object.\n.   \n.       @param image Return: CV_8UC1 image mask where -1 indicates that the pixel is a superpixel border,\n.       and 0 otherwise.\n.   \n.       @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border\n.       are masked.\n.   \n.       The function return the boundaries of the superpixel segmentation.\n.   \n.       @note\n.          -   (Python) A demo on how to generate superpixels in images from the webcam can be found at\n.               opencv_source_code/samples/python2/seeds.py\n.           -   (cpp) A demo on how to generate superpixels in images from the webcam can be found at\n.               opencv_source_code/modules/ximgproc/samples/seeds.cpp. By adding a file image as a command\n.               line argument, the static image will be used instead of the webcam.\n.           -   It will show a window with the video from the webcam with the superpixel boundaries marked\n.               in red (see below). Use Space to switch between different output modes. At the top of the\n.               window there are 4 sliders, from which the user can change on-the-fly the number of\n.               superpixels, the number of block levels, the strength of the boundary prior term to modify\n.               the shape, and the number of iterations at pixel level. This is useful to play with the\n.               parameters and set them to the user convenience. In the console the frame-rate of the\n.               algorithm is indicated.\n.   \n.       ![image](pics/superpixels_demo.png)"},
    {"getLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.       Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.       @param labels_out Return: A CV_32UC1 integer array containing the labels of the superpixel\n.       segmentation. The labels are in the range [0, getNumberOfSuperpixels()].\n.   \n.       The function returns an image with ssthe labels of the superpixel segmentation. The labels are in\n.       the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Calculates the superpixel segmentation on a given image stored in SuperpixelSEEDS object.\n.   \n.       The function computes the superpixels segmentation of an image with the parameters initialized\n.       with the function createSuperpixelSEEDS()."},
    {"iterate", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_iterate, 0), "iterate(img[, num_iterations]) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.       parameters in the SuperpixelSEEDS object.\n.   \n.       This function can be called again for other images without the need of initializing the\n.       algorithm with createSuperpixelSEEDS(). This save the computational cost of allocating memory\n.       for all the structures of the algorithm.\n.   \n.       @param img Input image. Supported formats: CV_8U, CV_16U, CV_32F. Image size & number of\n.       channels must match with the initialized image size & channels with the function\n.       createSuperpixelSEEDS(). It should be in HSV or Lab color space. Lab is a bit better, but also\n.       slower.\n.   \n.       @param num_iterations Number of pixel level iterations. Higher number improves the result.\n.   \n.       The function computes the superpixels segmentation of an image with the parameters initialized\n.       with the function createSuperpixelSEEDS(). The algorithms starts from a grid of superpixels and\n.       then refines the boundaries by proposing updates of blocks of pixels that lie at the boundaries\n.       from large to smaller size, finalizing with proposing pixel updates. An illustrative example\n.       can be seen below.\n.   \n.       ![image](pics/superpixels_blocks2.png)"},

    {NULL,          NULL}
};

// Converter (ximgproc_SuperpixelSEEDS)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::SuperpixelSEEDS> >
{
    static PyObject* from(const Ptr<cv::ximgproc::SuperpixelSEEDS>& r)
    {
        return pyopencv_ximgproc_SuperpixelSEEDS_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::SuperpixelSEEDS>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::SuperpixelSEEDS> * dst_;
        if (pyopencv_ximgproc_SuperpixelSEEDS_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::SuperpixelSEEDS> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_SuperpixelSLIC (Generic)
//================================================================================

// GetSet (ximgproc_SuperpixelSLIC)



// Methods (ximgproc_SuperpixelSLIC)

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_enforceLabelConnectivity(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSLIC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSLIC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSLIC> _self_ = *(self1);
    Napi::Value* pyobj_min_element_size = NULL;
    int min_element_size=25;

    const char* keywords[] = { "min_element_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelSLIC.enforceLabelConnectivity", (char**)keywords, &pyobj_min_element_size) &&
        jsopencv_to_safe(info, pyobj_min_element_size, min_element_size, ArgInfo("min_element_size", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->enforceLabelConnectivity(min_element_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabelContourMask(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSLIC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSLIC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSLIC> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_image = NULL;
    Mat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_SuperpixelSLIC.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_image = NULL;
    UMat image;
    Napi::Value* pyobj_thick_line = NULL;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_SuperpixelSLIC.getLabelContourMask", (char**)keywords, &pyobj_image, &pyobj_thick_line) &&
        jsopencv_to_safe(info, pyobj_image, image, ArgInfo("image", 1)) &&
        jsopencv_to_safe(info, pyobj_thick_line, thick_line, ArgInfo("thick_line", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabelContourMask(image, thick_line));
        return jsopencv_from(image);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabelContourMask");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSLIC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSLIC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSLIC> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelSLIC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelSLIC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        jsopencv_to_safe(info, pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->getLabels(labels_out));
        return jsopencv_from(labels_out);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("getLabels");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getNumberOfSuperpixels(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSLIC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSLIC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSLIC> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getNumberOfSuperpixels());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_iterate(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc;


    Ptr<cv::ximgproc::SuperpixelSLIC> * self1 = 0;
    if (!pyopencv_ximgproc_SuperpixelSLIC_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    Ptr<cv::ximgproc::SuperpixelSLIC> _self_ = *(self1);
    Napi::Value* pyobj_num_iterations = NULL;
    int num_iterations=10;

    const char* keywords[] = { "num_iterations", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|O:ximgproc_SuperpixelSLIC.iterate", (char**)keywords, &pyobj_num_iterations) &&
        jsopencv_to_safe(info, pyobj_num_iterations, num_iterations, ArgInfo("num_iterations", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->iterate(num_iterations));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_SuperpixelSLIC)

static PyGetSetDef pyopencv_ximgproc_SuperpixelSLIC_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_SuperpixelSLIC_methods[] =
{
    {"enforceLabelConnectivity", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_enforceLabelConnectivity, 0), "enforceLabelConnectivity([, min_element_size]) -> None\n.   @brief Enforce label connectivity.\n.   \n.       @param min_element_size The minimum element size in percents that should be absorbed into a bigger\n.       superpixel. Given resulted average superpixel size valid value should be in 0-100 range, 25 means\n.       that less then a quarter sized superpixel should be absorbed, this is default.\n.   \n.       The function merge component that is too small, assigning the previously found adjacent label\n.       to this component. Calling this function may change the final number of superpixels."},
    {"getLabelContourMask", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in SuperpixelSLIC object.\n.   \n.       @param image Return: CV_8U1 image mask where -1 indicates that the pixel is a superpixel border,\n.       and 0 otherwise.\n.   \n.       @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border\n.       are masked.\n.   \n.       The function return the boundaries of the superpixel segmentation."},
    {"getLabels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.       Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.       @param labels_out Return: A CV_32SC1 integer array containing the labels of the superpixel\n.       segmentation. The labels are in the range [0, getNumberOfSuperpixels()].\n.   \n.       The function returns an image with the labels of the superpixel segmentation. The labels are in\n.       the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Calculates the actual amount of superpixels on a given segmentation computed\n.       and stored in SuperpixelSLIC object."},
    {"iterate", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_iterate, 0), "iterate([, num_iterations]) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.       parameters in the SuperpixelSLIC object.\n.   \n.       This function can be called again without the need of initializing the algorithm with\n.       createSuperpixelSLIC(). This save the computational cost of allocating memory for all the\n.       structures of the algorithm.\n.   \n.       @param num_iterations Number of iterations. Higher number improves the result.\n.   \n.       The function computes the superpixels segmentation of an image with the parameters initialized\n.       with the function createSuperpixelSLIC(). The algorithms starts from a grid of superpixels and\n.       then refines the boundaries by proposing updates of edges boundaries."},

    {NULL,          NULL}
};

// Converter (ximgproc_SuperpixelSLIC)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::SuperpixelSLIC> >
{
    static PyObject* from(const Ptr<cv::ximgproc::SuperpixelSLIC>& r)
    {
        return pyopencv_ximgproc_SuperpixelSLIC_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::SuperpixelSLIC>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::SuperpixelSLIC> * dst_;
        if (pyopencv_ximgproc_SuperpixelSLIC_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::SuperpixelSLIC> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_GraphSegmentation (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_GraphSegmentation)



// Methods (ximgproc_segmentation_GraphSegmentation)

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getK(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    float retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getK());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getMinSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    int retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getMinSize());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    double retval;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, retval = _self_->getSigma());
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_processImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_src = NULL;
    Mat src;
    Napi::Value* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_segmentation_GraphSegmentation.processImage", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->processImage(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_src = NULL;
    UMat src;
    Napi::Value* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O|O:ximgproc_segmentation_GraphSegmentation.processImage", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        jsopencv_to_safe(info, pyobj_src, src, ArgInfo("src", 0)) &&
        jsopencv_to_safe(info, pyobj_dst, dst, ArgInfo("dst", 1)))
    {
        ERRWRAP2_NAPI(info, _self_->processImage(src, dst));
        return jsopencv_from(dst);
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("processImage");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setK(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_k = NULL;
    float k=0.f;

    const char* keywords[] = { "k", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_GraphSegmentation.setK", (char**)keywords, &pyobj_k) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setK(k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setMinSize(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_min_size = NULL;
    int min_size=0;

    const char* keywords[] = { "min_size", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_GraphSegmentation.setMinSize", (char**)keywords, &pyobj_min_size) &&
        jsopencv_to_safe(info, pyobj_min_size, min_size, ArgInfo("min_size", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setMinSize(min_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setSigma(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::GraphSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_GraphSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::GraphSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_sigma = NULL;
    double sigma=0;

    const char* keywords[] = { "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_GraphSegmentation.setSigma", (char**)keywords, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setSigma(sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_segmentation_GraphSegmentation)

static PyGetSetDef pyopencv_ximgproc_segmentation_GraphSegmentation_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_GraphSegmentation_methods[] =
{
    {"getK", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getK, 0), "getK() -> retval\n."},
    {"getMinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getMinSize, 0), "getMinSize() -> retval\n."},
    {"getSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getSigma, 0), "getSigma() -> retval\n."},
    {"processImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_processImage, 0), "processImage(src[, dst]) -> dst\n.   @brief Segment an image and store output in dst\n.                                   @param src The input image. Any number of channel (1 (Eg: Gray), 3 (Eg: RGB), 4 (Eg: RGB-D)) can be provided\n.                                   @param dst The output segmentation. It's a CV_32SC1 Mat with the same number of cols and rows as input image, with an unique, sequential, id for each pixel."},
    {"setK", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setK, 0), "setK(k) -> None\n."},
    {"setMinSize", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setMinSize, 0), "setMinSize(min_size) -> None\n."},
    {"setSigma", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setSigma, 0), "setSigma(sigma) -> None\n."},

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_GraphSegmentation)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::GraphSegmentation> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::GraphSegmentation>& r)
    {
        return pyopencv_ximgproc_segmentation_GraphSegmentation_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::GraphSegmentation>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::GraphSegmentation> * dst_;
        if (pyopencv_ximgproc_segmentation_GraphSegmentation_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::GraphSegmentation> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentation (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentation)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentation)

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addGraphSegmentation(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_g = NULL;
    Ptr<GraphSegmentation> g;

    const char* keywords[] = { "g", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addGraphSegmentation", (char**)keywords, &pyobj_g) &&
        jsopencv_to_safe(info, pyobj_g, g, ArgInfo("g", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->addGraphSegmentation(g));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addImage", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->addImage(img));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addImage", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->addImage(img));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("addImage");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addStrategy(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_s = NULL;
    Ptr<SelectiveSearchSegmentationStrategy> s;

    const char* keywords[] = { "s", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addStrategy", (char**)keywords, &pyobj_s) &&
        jsopencv_to_safe(info, pyobj_s, s, ArgInfo("s", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->addStrategy(s));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearGraphSegmentations(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clearGraphSegmentations());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearImages(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clearImages());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearStrategies(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clearStrategies());
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_process(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    vector_Rect rects;

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->process(rects));
        return jsopencv_from(rects);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_setBaseImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_SelectiveSearchSegmentation.setBaseImage", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBaseImage(img));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;

    const char* keywords[] = { "img", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "O:ximgproc_segmentation_SelectiveSearchSegmentation.setBaseImage", (char**)keywords, &pyobj_img) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setBaseImage(img));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setBaseImage");

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchFast(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_base_k = NULL;
    int base_k=150;
    Napi::Value* pyobj_inc_k = NULL;
    int inc_k=150;
    Napi::Value* pyobj_sigma = NULL;
    float sigma=0.8f;

    const char* keywords[] = { "base_k", "inc_k", "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:ximgproc_segmentation_SelectiveSearchSegmentation.switchToSelectiveSearchFast", (char**)keywords, &pyobj_base_k, &pyobj_inc_k, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_base_k, base_k, ArgInfo("base_k", 0)) &&
        jsopencv_to_safe(info, pyobj_inc_k, inc_k, ArgInfo("inc_k", 0)) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->switchToSelectiveSearchFast(base_k, inc_k, sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchQuality(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_base_k = NULL;
    int base_k=150;
    Napi::Value* pyobj_inc_k = NULL;
    int inc_k=150;
    Napi::Value* pyobj_sigma = NULL;
    float sigma=0.8f;

    const char* keywords[] = { "base_k", "inc_k", "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OOO:ximgproc_segmentation_SelectiveSearchSegmentation.switchToSelectiveSearchQuality", (char**)keywords, &pyobj_base_k, &pyobj_inc_k, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_base_k, base_k, ArgInfo("base_k", 0)) &&
        jsopencv_to_safe(info, pyobj_inc_k, inc_k, ArgInfo("inc_k", 0)) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->switchToSelectiveSearchQuality(base_k, inc_k, sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSingleStrategy(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> _self_ = *(self1);
    Napi::Value* pyobj_k = NULL;
    int k=200;
    Napi::Value* pyobj_sigma = NULL;
    float sigma=0.8f;

    const char* keywords[] = { "k", "sigma", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "|OO:ximgproc_segmentation_SelectiveSearchSegmentation.switchToSingleStrategy", (char**)keywords, &pyobj_k, &pyobj_sigma) &&
        jsopencv_to_safe(info, pyobj_k, k, ArgInfo("k", 0)) &&
        jsopencv_to_safe(info, pyobj_sigma, sigma, ArgInfo("sigma", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->switchToSingleStrategy(k, sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_segmentation_SelectiveSearchSegmentation)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_methods[] =
{
    {"addGraphSegmentation", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addGraphSegmentation, 0), "addGraphSegmentation(g) -> None\n.   @brief Add a new graph segmentation in the list of graph segementations to process.\n.                                   @param g The graph segmentation"},
    {"addImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addImage, 0), "addImage(img) -> None\n.   @brief Add a new image in the list of images to process.\n.                                   @param img The image"},
    {"addStrategy", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addStrategy, 0), "addStrategy(s) -> None\n.   @brief Add a new strategy in the list of strategy to process.\n.                                   @param s The strategy"},
    {"clearGraphSegmentations", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearGraphSegmentations, 0), "clearGraphSegmentations() -> None\n.   @brief Clear the list of graph segmentations to process;"},
    {"clearImages", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearImages, 0), "clearImages() -> None\n.   @brief Clear the list of images to process"},
    {"clearStrategies", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearStrategies, 0), "clearStrategies() -> None\n.   @brief Clear the list of strategy to process;"},
    {"process", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_process, 0), "process() -> rects\n.   @brief Based on all images, graph segmentations and stragies, computes all possible rects and return them\n.                                   @param rects The list of rects. The first ones are more relevents than the lasts ones."},
    {"setBaseImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_setBaseImage, 0), "setBaseImage(img) -> None\n.   @brief Set a image used by switch* functions to initialize the class\n.                                   @param img The image"},
    {"switchToSelectiveSearchFast", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchFast, 0), "switchToSelectiveSearchFast([, base_k[, inc_k[, sigma]]]) -> None\n.   @brief Initialize the class with the 'Selective search fast' parameters describled in @cite uijlings2013selective.\n.                                   @param base_k The k parameter for the first graph segmentation\n.                                   @param inc_k The increment of the k parameter for all graph segmentations\n.                                   @param sigma The sigma parameter for the graph segmentation"},
    {"switchToSelectiveSearchQuality", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchQuality, 0), "switchToSelectiveSearchQuality([, base_k[, inc_k[, sigma]]]) -> None\n.   @brief Initialize the class with the 'Selective search fast' parameters describled in @cite uijlings2013selective.\n.                                   @param base_k The k parameter for the first graph segmentation\n.                                   @param inc_k The increment of the k parameter for all graph segmentations\n.                                   @param sigma The sigma parameter for the graph segmentation"},
    {"switchToSingleStrategy", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSingleStrategy, 0), "switchToSingleStrategy([, k[, sigma]]) -> None\n.   @brief Initialize the class with the 'Single stragegy' parameters describled in @cite uijlings2013selective.\n.                                   @param k The k parameter for the graph segmentation\n.                                   @param sigma The sigma parameter for the graph segmentation"},

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentation)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentationStrategy (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentationStrategy)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentationStrategy)

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_get(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategy' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> _self_ = *(self1);
    Napi::Value* pyobj_r1 = NULL;
    int r1=0;
    Napi::Value* pyobj_r2 = NULL;
    int r2=0;
    float retval;

    const char* keywords[] = { "r1", "r2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.get", (char**)keywords, &pyobj_r1, &pyobj_r2) &&
        jsopencv_to_safe(info, pyobj_r1, r1, ArgInfo("r1", 0)) &&
        jsopencv_to_safe(info, pyobj_r2, r2, ArgInfo("r2", 0)))
    {
        ERRWRAP2_NAPI(info, retval = _self_->get(r1, r2));
        return jsopencv_from(retval);
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_merge(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategy' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> _self_ = *(self1);
    Napi::Value* pyobj_r1 = NULL;
    int r1=0;
    Napi::Value* pyobj_r2 = NULL;
    int r2=0;

    const char* keywords[] = { "r1", "r2", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.merge", (char**)keywords, &pyobj_r1, &pyobj_r2) &&
        jsopencv_to_safe(info, pyobj_r1, r1, ArgInfo("r1", 0)) &&
        jsopencv_to_safe(info, pyobj_r2, r2, ArgInfo("r2", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->merge(r1, r2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_setImage(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategy' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> _self_ = *(self1);
    pyPrepareArgumentConversionErrorsStorage(2);

    {
    Napi::Value* pyobj_img = NULL;
    Mat img;
    Napi::Value* pyobj_regions = NULL;
    Mat regions;
    Napi::Value* pyobj_sizes = NULL;
    Mat sizes;
    Napi::Value* pyobj_image_id = NULL;
    int image_id=-1;

    const char* keywords[] = { "img", "regions", "sizes", "image_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.setImage", (char**)keywords, &pyobj_img, &pyobj_regions, &pyobj_sizes, &pyobj_image_id) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_regions, regions, ArgInfo("regions", 0)) &&
        jsopencv_to_safe(info, pyobj_sizes, sizes, ArgInfo("sizes", 0)) &&
        jsopencv_to_safe(info, pyobj_image_id, image_id, ArgInfo("image_id", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setImage(img, regions, sizes, image_id));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    

    {
    Napi::Value* pyobj_img = NULL;
    UMat img;
    Napi::Value* pyobj_regions = NULL;
    UMat regions;
    Napi::Value* pyobj_sizes = NULL;
    UMat sizes;
    Napi::Value* pyobj_image_id = NULL;
    int image_id=-1;

    const char* keywords[] = { "img", "regions", "sizes", "image_id", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OOO|O:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.setImage", (char**)keywords, &pyobj_img, &pyobj_regions, &pyobj_sizes, &pyobj_image_id) &&
        jsopencv_to_safe(info, pyobj_img, img, ArgInfo("img", 0)) &&
        jsopencv_to_safe(info, pyobj_regions, regions, ArgInfo("regions", 0)) &&
        jsopencv_to_safe(info, pyobj_sizes, sizes, ArgInfo("sizes", 0)) &&
        jsopencv_to_safe(info, pyobj_image_id, image_id, ArgInfo("image_id", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->setImage(img, regions, sizes, image_id));
        Py_RETURN_NONE;
    }


        pyPopulateArgumentConversionErrors();
    }
    pyRaiseCVOverloadException("setImage");

    return NULL;
}



// Tables (ximgproc_segmentation_SelectiveSearchSegmentationStrategy)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_methods[] =
{
    {"get", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_get, 0), "get(r1, r2) -> retval\n.   @brief Return the score between two regions (between 0 and 1)\n.                                   @param r1 The first region\n.                                   @param r2 The second region"},
    {"merge", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_merge, 0), "merge(r1, r2) -> None\n.   @brief Inform the strategy that two regions will be merged\n.                                   @param r1 The first region\n.                                   @param r2 The second region"},
    {"setImage", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_setImage, 0), "setImage(img, regions, sizes[, image_id]) -> None\n.   @brief Set a initial image, with a segmentation.\n.                                   @param img The input image. Any number of channel can be provided\n.                                   @param regions A segmentation of the image. The parameter must be the same size of img.\n.                                   @param sizes The sizes of different regions\n.                                   @param image_id If not set to -1, try to cache pre-computations. If the same set og (img, regions, size) is used, the image_id need to be the same."},

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentationStrategy)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor)



// Tables (ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_methods[] =
{

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill)



// Tables (ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_methods[] =
{

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple)

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_addStrategy(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> _self_ = *(self1);
    Napi::Value* pyobj_g = NULL;
    Ptr<SelectiveSearchSegmentationStrategy> g;
    Napi::Value* pyobj_weight = NULL;
    float weight=0.f;

    const char* keywords[] = { "g", "weight", NULL };
    if (JsArg_ParseTupleAndKeywords(info, "OO:ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple.addStrategy", (char**)keywords, &pyobj_g, &pyobj_weight) &&
        jsopencv_to_safe(info, pyobj_g, g, ArgInfo("g", 0)) &&
        jsopencv_to_safe(info, pyobj_weight, weight, ArgInfo("weight", 0)))
    {
        ERRWRAP2_NAPI(info, _self_->addStrategy(g, weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static Napi::Value pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_clearStrategies(const Napi::CallbackInfo &info)
{
    using namespace cv::ximgproc::segmentation;


    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> * self1 = 0;
    if (!pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_getp(self, self1))
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple' or its derivative)");
    Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> _self_ = *(self1);

    if (PyObject_Size(py_args) == 0 && (!kw || PyObject_Size(kw) == 0))
    {
        ERRWRAP2_NAPI(info, _self_->clearStrategies());
        Py_RETURN_NONE;
    }

    return NULL;
}



// Tables (ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_methods[] =
{
    {"addStrategy", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_addStrategy, 0), "addStrategy(g, weight) -> None\n.   @brief Add a new sub-strategy\n.                                   @param g The strategy\n.                                   @param weight The weight of the strategy"},
    {"clearStrategies", CV_JS_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_clearStrategies, 0), "clearStrategies() -> None\n.   @brief Remove all sub-strategies"},

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentationStrategySize (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentationStrategySize)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentationStrategySize)



// Tables (ximgproc_segmentation_SelectiveSearchSegmentationStrategySize)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_methods[] =
{

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentationStrategySize)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize> for argument '%s'", info.name);
        return false;
    }
};

//================================================================================
// ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture (Generic)
//================================================================================

// GetSet (ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture)



// Methods (ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture)



// Tables (ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture)

static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_methods[] =
{

    {NULL,          NULL}
};

// Converter (ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture)

template<>
struct PyOpenCV_Converter< Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture> >
{
    static PyObject* from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture>& r)
    {
        return pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Instance(r);
    }
    static bool to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture>& dst, const ArgInfo& info)
    {
        if(!src || src == Py_None)
            return true;
        Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture> * dst_;
        if (pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_getp(src, dst_))
        {
            dst = *dst_;
            return true;
        }
        
        failmsg("Expected Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture> for argument '%s'", info.name);
        return false;
    }
};

